<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d9" for="edge" attr.name="file_path" attr.type="string"/>
<key id="d8" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d7" for="edge" attr.name="keywords" attr.type="string"/>
<key id="d6" for="edge" attr.name="description" attr.type="string"/>
<key id="d5" for="edge" attr.name="weight" attr.type="double"/>
<key id="d4" for="node" attr.name="file_path" attr.type="string"/>
<key id="d3" for="node" attr.name="source_id" attr.type="string"/>
<key id="d2" for="node" attr.name="description" attr.type="string"/>
<key id="d1" for="node" attr.name="entity_type" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_id" attr.type="string"/>
<graph edgedefault="undirected"><node id="Codex">
  <data key="d0">Codex</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An AI code generation model similar to GPT, specialized for programming tasks, used for generating exercises, explanations, and solutions.&lt;SEP&gt;An AI model similar to GPT, specialized for code generation, used in educational contexts to generate programming exercises and answers.&lt;SEP&gt;Codex is a GPT language model fine-tuned on publicly available code from GitHub, designed to generate Python code and assist in program synthesis.&lt;SEP&gt;Codex is a language model derived from GPT-3, specifically fine-tuned on a large corpus of code repositories to generate code and solve programming problems.&lt;SEP&gt;Codex is a large language model trained for code generation and understanding, used for fine-tuning and evaluating programming tasks.&lt;SEP&gt;Codex is a large language model trained on code, used as a decision-making tool and search engine that suggests programming packages and methods, influencing user behavior and the software market.&lt;SEP&gt;Codex is a specialized GPT model fine-tuned on code, designed to excel at coding tasks and powering tools like GitHub Copilot.&lt;SEP&gt;Codex is a specialized language model derived from GPT-3, fine-tuned on code repositories to generate and solve programming problems.&lt;SEP&gt;Codex is a specialized version of GPT, fine-tuned on programming code to perform code synthesis and related tasks, powering tools like GitHub Copilot.&lt;SEP&gt;Codex is an AI code-generation model with applications in software development and potential misuse scenarios, including malicious activities and security vulnerabilities.&lt;SEP&gt;Codex is an AI language model specialized in code generation, with potential applications and misuse scenarios, including malware development and vulnerability discovery.&lt;SEP&gt;Codex is an AI model designed for code generation, trained on large datasets of internet code, and used for various programming tasks. It consumes significant compute resources and has implications related to environmental impact, legal considerations, and responsible AI deployment.&lt;SEP&gt;Codex is an AI-powered code generation model trained on programming languages like Python, capable of producing code, documentation, and tests.&lt;SEP&gt;Codex is an AI-powered code generation tool trained on programming languages like Python, capable of producing code and related outputs.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-735818e62b066d03c2144cdb5db162c1&lt;SEP&gt;chunk-bf1121c44634b616af61c49cd3adf29a&lt;SEP&gt;chunk-d8479e6cf592a5f52ce4d6ea93fb04e9&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e&lt;SEP&gt;chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Python code-writing capabilities">
  <data key="d0">Python code-writing capabilities</data>
  <data key="d1">Results</data>
  <data key="d2">The model's ability to generate correct Python programs from docstrings, with a performance of 28.8% on HumanEval, surpassing GPT-3 and GPT-J, and achieving 70.2% success with repeated sampling.</data>
  <data key="d3">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="HumanEval">
  <data key="d0">HumanEval</data>
  <data key="d1">Dataset/Benchmark</data>
  <data key="d2">A benchmark dataset consisting of coding problems and associated unit tests, used to evaluate language models' coding abilities.&lt;SEP&gt;A benchmark dataset used for evaluating code generation models with multiple problems.&lt;SEP&gt;A new evaluation set created to measure the functional correctness of code generated from docstrings, used to assess the performance of Codex and baseline models.&lt;SEP&gt;A standard benchmark dataset for assessing code generation models across multiple problems.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b&lt;SEP&gt;chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GPT-3">
  <data key="d0">GPT-3</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A large language model that, in this context, serves as a baseline for code generation performance, solving 0% of problems on HumanEval.&lt;SEP&gt;A large-scale language model capable of few-shot and zero-shot learning across diverse NLP tasks, significantly advancing AI capabilities.&lt;SEP&gt;GPT-3 is a large language model developed by OpenAI, designed for natural language understanding and generation, with limited explicit training for code generation.&lt;SEP&gt;GPT-3 is a large language model developed by OpenAI, used as a benchmark or comparison in AI performance and capabilities.&lt;SEP&gt;GPT-3 is a large language model developed by OpenAI, used as a benchmark or comparison point in AI research and capabilities.&lt;SEP&gt;GPT-3 is a large language model trained for natural language understanding and generation, but not explicitly for code generation.&lt;SEP&gt;GPT-3 is a large-scale language model known for its strong NLP and code generation capabilities, serving as the foundation for Codex.&lt;SEP&gt;GPT-3 is a state-of-the-art language model known for its performance in NLP and code generation tasks, serving as the foundation for Codex.&lt;SEP&gt;GPT-3 is an advanced language model developed by OpenAI, with capabilities and limitations that influence AI applications and societal implications.&lt;SEP&gt;GPT-3 is an advanced language model developed by OpenAI, with capabilities influencing AI applications, societal issues, and technological development.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252&lt;SEP&gt;chunk-54d0e5495bc4b24b68d3cbc1c13d835e&lt;SEP&gt;chunk-bf1121c44634b616af61c49cd3adf29a&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GPT-J">
  <data key="d0">GPT-J</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A large language model similar in size to Codex-12B, trained on the Pile dataset, assessed for code generation capabilities.&lt;SEP&gt;An open-source language model that solves 11.4% of problems on HumanEval, used as a comparison for Codex.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Repeated Sampling">
  <data key="d0">Repeated Sampling</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique where multiple samples are generated from the model to increase the likelihood of producing correct code solutions, leading to a 70.2% success rate.</data>
  <data key="d3">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Limitations">
  <data key="d0">Limitations</data>
  <data key="d1">Limitations</data>
  <data key="d2">Careful investigation of Codex reveals difficulties with long chains of operations in docstrings and with binding operations to variables, indicating areas for improvement.&lt;SEP&gt;Constraints or weaknesses that restrict the scope or validity of research or activity.&lt;SEP&gt;Limitations include incomplete problem solving, biases, and the necessity of heuristic methods for selecting best code samples for deployment.&lt;SEP&gt;Limitations include the models' inability to solve all problems, potential biases, and the need for heuristic selection of best code samples for deployment.&lt;SEP&gt;Limitations refer to the identified constraints and weaknesses of Codex, including inefficiency in sampling, overfitting tendencies, and performance degradation with increased complexity.&lt;SEP&gt;Constraints and challenges in the current evaluation framework, such as limited problem diversity, scope of models tested, or evaluation metrics.&lt;SEP&gt;Constraints or weaknesses in the study design, data, or analysis that affect the interpretation or generalizability of results.&lt;SEP&gt;Recognition that performance plateaus suggest an upper limit to the models' ability to generate correct parallel code regardless of attempts.&lt;SEP&gt;The evaluation highlights limitations such as low efficiency of generated parallel code and the models' varying performance across different execution models.&lt;SEP&gt;Current limitations include less effective code generation for less mature models like HIP, and the need for prompt optimization, indicating areas for further improvement.&lt;SEP&gt;Current limitations include less effective code generation for less mature models such as HIP, and the need for prompt optimization, indicating areas for future improvement.&lt;SEP&gt;Limitations refer to the constraints, challenges, and potential shortcomings of the new technology, including issues related to accuracy, dataset dependency, and applicability.&lt;SEP&gt;Current limitations include the inability to assign tasklets to specific cores explicitly in some frameworks, which affects fine-grained load balancing and optimization.&lt;SEP&gt;Constraints and challenges identified in the study, such as data quality issues and model generalization limits.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7&lt;SEP&gt;chunk-b943bff32a36cd0ce9ab43893da92b5b&lt;SEP&gt;chunk-d25502af8737c59d857916af40cc4020&lt;SEP&gt;chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Program Synthesis">
  <data key="d0">Program Synthesis</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Approach where models generate complete programs from specifications or data, complementing program induction.&lt;SEP&gt;The process of automatically generating code or programs from specifications, often involving natural language or formal grammars, to create functioning code.&lt;SEP&gt;The process of automatically generating programs from specifications, often involving natural language inputs, with classical approaches using probabilistic context-free grammars (PCFGs) and neural models.&lt;SEP&gt;The task of automatically generating programs from natural language descriptions or docstrings, a primary focus of Codex's capabilities.</data>
  <data key="d3">chunk-8fe9cc07243d88be8beafa4c1840509b&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Generation">
  <data key="d0">Code Generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A downstream task where the model generates syntactically correct and meaningful HPC code based on prompts.&lt;SEP&gt;A task where the model generates syntactically correct and meaningful HPC code based on prompts, assessing its code synthesis ability.&lt;SEP&gt;Code generation involves creating programming code automatically, often evaluated through correctness and functionality metrics.&lt;SEP&gt;Code generation refers to the automated creation of programming code, often evaluated through correctness metrics such as unit tests and pass@k scores.&lt;SEP&gt;Codex rarely produces code identical to training data (&lt;0.1%), primarily generating novel code based on learned patterns rather than copying.&lt;SEP&gt;The process of automatically producing computer code, which is the main activity enabled by Codex and evaluated in the study.&lt;SEP&gt;Code generation involves using language models trained on source code data to automatically produce or complete code snippets.&lt;SEP&gt;Code generation involves using language models trained on source code to automatically produce or complete code snippets, aiding automation.&lt;SEP&gt;The task of generating computer code using language models, evaluated through pass@k metrics and syntactic correctness.&lt;SEP&gt;Code generation is the process of automatically producing executable code from higher-level representations, often involving optimization and transformations.&lt;SEP&gt;The final stage produces optimized C++ code from the AMT, incorporating inlining and loop unrolling to improve performance.&lt;SEP&gt;The final stage produces optimized C++ code from the AMT, incorporating techniques like function-inlining and loop unrolling to improve performance.&lt;SEP&gt;Code generation involves automatically producing computer code from natural language descriptions or other inputs, enhancing developer productivity and enabling automation.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-5f6ee49c82aaaf22c167cac791d43a0f&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluation Metrics">
  <data key="d0">Evaluation Metrics</data>
  <data key="d1">Variables</data>
  <data key="d2">Criteria used to assess the effectiveness and accuracy of domain-specific LLMs in various tasks.&lt;SEP&gt;Evaluation metrics such as pass@k and temperature settings are used to assess the performance and sampling strategies of Codex models in code generation tasks.&lt;SEP&gt;Metrics like pass@k and manual grading are used to assess the quality and correctness of generated code and docstrings.&lt;SEP&gt;Metrics such as pass@k and BLEU scores used to quantify the performance and quality of generated code solutions.&lt;SEP&gt;Metrics such as success rate on HumanEval and effectiveness of repeated sampling used to measure Codex's performance.&lt;SEP&gt;Metrics like pass@k and compilation success rate used to quantify model performance.&lt;SEP&gt;Metrics like pass@k and correctness ratios are used to evaluate the quality of generated code samples in benchmarks.&lt;SEP&gt;Metrics used to assess the accuracy and effectiveness of the model on downstream tasks, such as correctness of generated code and prediction accuracy.&lt;SEP&gt;Metrics used to evaluate model performance on downstream tasks, such as correctness of generated code, accuracy of pragma labeling, and prediction accuracy of performance.&lt;SEP&gt;Quantitative measures used to assess the performance of LLMs on tasks, such as accuracy, precision, recall, and reasoning quality.&lt;SEP&gt;Standards and benchmarks used to assess the performance of domain-adapted LLMs in specific tasks.&lt;SEP&gt;Evaluation metrics such as pass@k are used to quantify the correctness of code generated by language models, estimating the probability of generating correct solutions over multiple attempts.&lt;SEP&gt;Evaluation metrics such as pass@k measure the correctness of code generated by language models, estimating the probability of producing correct solutions within k attempts.&lt;SEP&gt;Metrics such as correctness, execution success, and similarity scores are used to evaluate translation and code generation quality.&lt;SEP&gt;Metrics used to categorize and assess the correctness and quality of AI-generated code suggestions.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Safety, Security, Economics">
  <data key="d0">Safety, Security, Economics</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Broader impacts discussed regarding deploying powerful code generation models like Codex, including safety, security, and economic considerations.</data>
  <data key="d3">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Large Language Models">
  <data key="d0">Large Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Advanced NLP models that serve as foundational, task-agnostic systems capable of supporting a wide range of language understanding and generation tasks across multiple applications.&lt;SEP&gt;Advanced neural network models trained on extensive text corpora, capable of performing a variety of NLP tasks, including question answering and decision-making support.&lt;SEP&gt;Advanced predictive models trained on extensive source code data to perform tasks like code generation, completion, and performance modeling, especially in the context of scientific computing.&lt;SEP&gt;Large language models are AI architectures trained on extensive datasets to generate human-like text, demonstrating success across multiple modalities.&lt;SEP&gt;Large language models are AI systems trained on vast data to understand and generate human-like text, with success demonstrated across various modalities.&lt;SEP&gt;Artificial intelligence models capable of understanding and generating human-like language across diverse tasks and domains. Despite their versatility, they face limitations such as unstable result formats, inability to access real-time information, tendency to hallucinate facts, and lack of precision in certain tasks.&lt;SEP&gt;Extensive neural network models trained on large corpora, capable of performing diverse NLP tasks including question answering, decision support, and knowledge retrieval.&lt;SEP&gt;Large Language Models (LLMs) are advanced AI models trained on vast datasets to understand and generate human-like text, enabling a wide range of NLP tasks.&lt;SEP&gt;Large Language Models (LLMs) are advanced natural language processing models that serve as foundational tools across various applications, characterized by their ability to handle diverse NLP tasks in a task-agnostic manner.&lt;SEP&gt;Large Language Models (LLMs) are artificial general intelligence models capable of performing a wide range of tasks and domains. Despite their versatility, they face limitations such as unstable result formats, inability to access up-to-date information, tendency to hallucinate facts, and lack of precision in specific tasks like arithmetic.&lt;SEP&gt;Advanced neural network-based systems capable of understanding, modeling, and generating human-like text and source code, increasingly used in software development tasks such as code completion, summarization, translation, and lookup.&lt;SEP&gt;Large language models (LLMs) are advanced neural network-based systems capable of understanding and generating human-like text, including source code, and are increasingly used in software development, including code completion, summarization, translation, and lookup.&lt;SEP&gt;Large Language Models (LLMs) are AI models trained on vast datasets to understand and generate human-like text, widely used for code generation and natural language processing tasks.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00&lt;SEP&gt;chunk-d8772ec3475b4e5b1b113032265f9d74&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="HumanEval Dataset">
  <data key="d0">HumanEval Dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A benchmark dataset used to evaluate code generation models by measuring pass@k performance on real programming problems.&lt;SEP&gt;The HumanEval dataset consists of 164 programming problems with associated unit tests, used to benchmark and evaluate the correctness of code generated by models.&lt;SEP&gt;The HumanEval dataset consists of programming problems with unit tests used to evaluate the correctness of generated code samples.&lt;SEP&gt;A dataset of programming problems used to evaluate the correctness and validity of AI-generated code.&lt;SEP&gt;A dataset of programming problems used to evaluate the correctness and validity of code generated by AI tools like Copilot.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Unit Tests">
  <data key="d0">Unit Tests</data>
  <data key="d1">Tools</data>
  <data key="d2">Automated tests used to verify the correctness of generated code solutions in datasets like APPS and HumanEval.&lt;SEP&gt;Automated tests used to verify the correctness of solutions generated by models, with filtering based on passing these tests to determine success.&lt;SEP&gt;Unit tests are automated testing procedures designed to verify individual functions or code components by checking their outputs against expected results, often derived from code examples or problem statements.&lt;SEP&gt;Unit tests are automated testing scripts that verify specific functionalities of generated code, serving as a standard for assessing functional correctness.&lt;SEP&gt;Unit tests are automated tests that check specific functionalities of code to ensure correctness and are used to evaluate models' code outputs.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pass@k Metric">
  <data key="d0">Pass@k Metric</data>
  <data key="d1">Results</data>
  <data key="d2">A metric estimating the probability that at least one of k samples generated by the model is correct, based on multiple trials and samples.&lt;SEP&gt;The pass@k metric measures the likelihood that at least one of the top k generated code samples passes all unit tests, used to evaluate code generation performance.&lt;SEP&gt;The pass@k metric measures the probability that at least one of the top k generated code samples passes all unit tests, used to evaluate model performance.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Functional Correctness">
  <data key="d0">Functional Correctness</data>
  <data key="d1">Results</data>
  <data key="d2">Functional correctness assesses whether generated code passes predefined unit tests, serving as a key performance measure for code models.&lt;SEP&gt;Functional correctness is a key evaluation criterion, indicating whether generated code passes all relevant unit tests, reflecting real-world code utility.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Synthesis from Docstrings">
  <data key="d0">Code Synthesis from Docstrings</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The research investigates how well models like Codex can generate correct standalone functions from natural language descriptions (docstrings).&lt;SEP&gt;The study investigates how effectively models like Codex can generate correct, standalone functions from natural language descriptions (docstrings).</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Fine-tuning">
  <data key="d0">Model Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adjusting pre-trained LLMs on domain-specific data to improve performance in targeted applications.&lt;SEP&gt;Fine-tuning involves training Codex on a dataset of correctly implemented functions to improve accuracy and reliability in code synthesis tasks.&lt;SEP&gt;Fine-tuning involves training models like Codex on correctly implemented functions to improve their performance in code synthesis tasks.&lt;SEP&gt;Model fine-tuning involves updating the internal parameters of an LLM with domain-specific data to enhance performance in targeted areas.&lt;SEP&gt;Model fine-tuning involves updating the internal parameters of an LLM with domain-specific data, requiring full access to the model, to enhance its performance in specific areas.&lt;SEP&gt;Process of adjusting pre-trained models on domain-specific data to improve performance in targeted applications.&lt;SEP&gt;Fine-tuning involves additional training of pre-trained models on specific datasets to adapt their outputs to particular tasks like HPC code generation.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-3c24b46980eea930fd6590d61f0b0f25&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Performance">
  <data key="d0">Model Performance</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment outcomes based on the ability of models to generate correct code for synthetic tasks.&lt;SEP&gt;Codex-S, fine-tuned on correct functions, solves approximately 37.7% of problems with one sample, and up to 77.5% within 100 samples, indicating high potential for practical code generation.&lt;SEP&gt;Fine-tuned Codex-S achieves a 37.7% success rate with a single sample, and up to 77.5% within 100 samples, demonstrating high potential for practical code generation.&lt;SEP&gt;The measured success rates of different models on code generation tasks, including pass@k, filtered pass@k, and time-out solutions.&lt;SEP&gt;Quantitative assessment of models' ability to generate correct and syntactically valid code.&lt;SEP&gt;The effectiveness of a model in achieving its intended tasks, often measured by accuracy, efficiency, or other metrics.&lt;SEP&gt;Model performance is assessed through metrics like pass@1, speedup, and efficiency, indicating the models' accuracy and effectiveness in code tasks.&lt;SEP&gt;Performance varies across different models and parallel execution models, with serial and OpenMP generally yielding higher accuracy.&lt;SEP&gt;The effectiveness of RAG models, showing that retrieving more documents can improve or peak performance depending on the specific model variant.&lt;SEP&gt;Measured by Pass@1 scores on the ParEval benchmark, indicating the accuracy of code generation models in producing correct MPI code.&lt;SEP&gt;Model performance measures how accurately and efficiently a model generates code, assessed via benchmarks and validation datasets.&lt;SEP&gt;Model performance refers to how well a trained model generates accurate, efficient, and relevant code outputs, often measured against benchmarks.&lt;SEP&gt;The effectiveness of a fine-tuned model, measured in metrics such as code accuracy, influenced by factors like data amount, data quality, and model size.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-7d9c04128411fd2962a424d26641c6ad&lt;SEP&gt;chunk-31212d7716c4c20f6aabfd8fb9a1905c&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation Framework">
  <data key="d0">Evaluation Framework</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework for assessing code synthesis and generation capabilities, referencing prior research in ML and synthesis communities.&lt;SEP&gt;A systematic approach for assessing the capabilities of code synthesis and generation models, emphasizing benchmarks based on attributes like expressivity and complexity.&lt;SEP&gt;The evaluation framework includes metrics like pass@k, a dataset of programming problems, and a sandbox environment for safe code execution.&lt;SEP&gt;The set of procedures including pass@k metrics, filtering, and testing to assess model performance.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sandbox Environment">
  <data key="d0">Sandbox Environment</data>
  <data key="d1">Tools</data>
  <data key="d2">A secure environment used to execute and test model-generated code safely, ensuring correctness without risking system security.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Fine-tuning Data">
  <data key="d0">Model Fine-tuning Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The data used for fine-tuning Codex consists of correctly implemented standalone functions aimed at improving code synthesis performance.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Size">
  <data key="d0">Model Size</data>
  <data key="d1">Variables</data>
  <data key="d2">Model size refers to the number of parameters within a neural network-based language model, such as GPT-3.5, GPT-4, StarCoder-Base, CodeLlama-34B, Codex, GPT-J, HPC-Coder-1.3B, and StarCoder2-3B. The size of these models, typically measured in billions (e.g., 1.3B, 6.7B, 12B, 16B, 34B), significantly influences their capacity, performance, and generalization abilities across various tasks. Larger models generally have greater capacity to learn complex patterns, perform better in code generation, and handle parallelization tasks more effectively. They also require more memory and computational resources. Conversely, smaller models (e.g., 300M, 1.3B) tend to have reduced capacity but are more efficient in terms of speed and resource consumption. The model size impacts not only the accuracy and correctness in code generation but also the effectiveness of fine-tuning and the model's ability to generalize from training data. Overall, the choice of model size balances performance needs with computational constraints, with medium-sized models often showing notable improvements in capabilities such as parallel code generation and generalization.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Comparison">
  <data key="d0">Model Comparison</data>
  <data key="d1">Results</data>
  <data key="d2">Codex-S outperforms Codex by an average margin of 6.5 percentage points on pass@1 and 15.1 percentage points on pass@100 across model sizes, indicating superior effectiveness in code generation.&lt;SEP&gt;Comparisons between models like GPT-12B, Codex, Codex-S, and GPT-J show differences in success rates for code synthesis tasks.&lt;SEP&gt;Comparison of different models like CodeLlama and StarCoderBase based on their translation accuracy, code quality, and performance metrics.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Algorithmic Tasks">
  <data key="d0">Algorithmic Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specific computational problems like addition, memorization, or program synthesis that neural models aim to learn or execute.&lt;SEP&gt;The programming problems in HumanEval assess language comprehension, algorithms, and basic mathematics, serving as benchmarks for code generation.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Sampling">
  <data key="d0">Model Sampling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Generating multiple code samples per problem and selecting the best based on passing unit tests or heuristic scores to improve success rates.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Heuristic Selection">
  <data key="d0">Heuristic Selection</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Using heuristics such as highest mean log-probability to select the most promising code sample from multiple outputs.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Broader Impacts">
  <data key="d0">Broader Impacts</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The development and deployment of powerful code-generating models have implications for software development, automation, and potential biases or misuse.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Limitations and Challenges">
  <data key="d0">Limitations and Challenges</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current models do not solve all programming problems, may contain biases, and require heuristic methods for optimal sample selection.</data>
  <data key="d3">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="test-driven development">
  <data key="d0">test-driven development</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A software development framework that emphasizes converting requirements into test cases before implementation, with success defined by passing tests.&lt;SEP&gt;A software development methodology that emphasizes writing tests before implementation, ensuring code correctness through passing predefined test cases.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="unit tests">
  <data key="d0">unit tests</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Automated tests designed to verify individual units or components of code function correctly, integral to test-driven development.&lt;SEP&gt;Tests designed to verify individual units of code function correctly, integral to test-driven development and code evaluation.&lt;SEP&gt;Unit tests are automated checks used to evaluate whether generated code solutions meet specified correctness criteria, used to assess model performance.&lt;SEP&gt;Unit tests are automated tests that verify the correctness of individual functions or components by checking their outputs against expected results.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="pass@k metric">
  <data key="d0">pass@k metric</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An evaluation metric that estimates the probability that at least one of k generated code samples passes unit tests, used to assess model performance on code generation tasks.&lt;SEP&gt;An evaluation metric used to estimate the probability that at least one of k generated code samples passes unit tests, accounting for variance in code correctness assessments.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="functional correctness">
  <data key="d0">functional correctness</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The actual operational behavior of code, as opposed to syntactic similarity, used as an evaluation criterion.&lt;SEP&gt;The actual operational behavior of code, focusing on whether the code performs its intended function, used as an evaluation criterion.&lt;SEP&gt;The concept that a program performs its intended functions correctly, assessed through test cases and metrics like pass@k.&lt;SEP&gt;The concept that a program performs its intended functions correctly, often assessed through test cases and metrics like pass@k.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Large Language Models trained on code">
  <data key="d0">Large Language Models trained on code</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Advanced AI models like Codex trained on large datasets of source code to assist in code generation and automation, impacting workflows and labor dynamics.&lt;SEP&gt;Investigate the ability of large language models, such as Codex, to generate functionally correct code solutions based on training data and evaluation metrics.&lt;SEP&gt;Investigate the capability of models like Codex to generate functionally correct code solutions based on training data and evaluation metrics.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="HumanEval dataset">
  <data key="d0">HumanEval dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A dataset comprising 164 hand-written programming problems used to evaluate models' functional correctness, including problem descriptions and unit tests.&lt;SEP&gt;The HumanEval dataset is a benchmark consisting of programming problems used to evaluate the performance of code-generating models like Codex.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="code evaluation">
  <data key="d0">code evaluation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The process of assessing generated code for correctness and performance, impacting model development, safety, and reliability.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="sandbox environment">
  <data key="d0">sandbox environment</data>
  <data key="d1">Tools</data>
  <data key="d2">A secure execution environment designed to run untrusted code safely, preventing malicious activities and resource modification, utilizing gVisor and eBPF firewall rules.&lt;SEP&gt;A secure execution environment designed to run untrusted code safely, preventing malicious activities and resource modifications, utilizing gVisor and eBPF firewalls.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="gVisor container runtime">
  <data key="d0">gVisor container runtime</data>
  <data key="d1">Tools</data>
  <data key="d2">A container runtime that emulates host resources to isolate and protect the host system from potentially malicious containers during code execution.&lt;SEP&gt;A container runtime that provides a security boundary by emulating host resources, isolating untrusted code during execution.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kubernetes">
  <data key="d0">Kubernetes</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A container orchestration platform used as part of the infrastructure for training and deploying code generation models in cloud environments.&lt;SEP&gt;A container orchestration platform used to manage training and deployment environments for code-generating models in cloud infrastructure.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security">
  <data key="d0">security</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A field concerned with protecting systems and data from malicious activities, especially relevant in sandboxing untrusted code.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model evaluation">
  <data key="d0">model evaluation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Model evaluation is conducted by generating multiple samples per prompt, assessing correctness, and calculating metrics like pass@k to compare efficacy across models.&lt;SEP&gt;Model evaluation is conducted by generating multiple samples per prompt, testing correctness, and calculating metrics like pass@k to compare models' efficacy.&lt;SEP&gt;The process and outcomes of assessing the correctness and performance of code-generating models like Codex using datasets and metrics.&lt;SEP&gt;The process of assessing the model's performance on tasks like code generation, labeling, and performance prediction, demonstrating its effectiveness.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="code generation">
  <data key="d0">code generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Code generation involves creating programming code automatically, which can have economic benefits and security risks.&lt;SEP&gt;The process of automatically producing code snippets or programs from prompts or problem descriptions, central to the research.&lt;SEP&gt;The activity of automatically producing source code snippets, which the HPC-Coder model performs at a higher rate than other models in HPC-specific tasks.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-c27a9a886dd19125bb1adb49ea7038f7&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="programming problems">
  <data key="d0">programming problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specific tasks or challenges designed to test code generation models, such as those in the HumanEval dataset.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model performance">
  <data key="d0">model performance</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of how well LLMs generate correct, efficient, and scalable parallel code, compared across models and scenarios.&lt;SEP&gt;Model performance is assessed through metrics like pass@k, speedup, and efficiency, indicating the accuracy and runtime effectiveness of code generated.&lt;SEP&gt;Quantitative measures of how well models generate correct and functional code, often based on pass@k and other metrics.&lt;SEP&gt;The performance of different LLMs in generating parallel code, measured by pass@1 scores and correctness, indicating their effectiveness.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04&lt;SEP&gt;chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="training data">
  <data key="d0">training data</data>
  <data key="d1">Variables</data>
  <data key="d2">Examples of correct code implementations used to train or guide LLMs for better code generation.&lt;SEP&gt;Large datasets of code (e.g., GitHub repositories) used to train models like Codex, influencing their capabilities.&lt;SEP&gt;Large datasets used to pre-train and fine-tune models like Codex, which can contain insecure or malicious code patterns influencing model outputs.&lt;SEP&gt;Training data comprises large datasets used to pre-train and fine-tune models like Codex, which can contain insecure code patterns.&lt;SEP&gt;Training data encompasses the datasets used to train Codex, which may contain biases or outdated information affecting its suggestions.&lt;SEP&gt;The dataset used to train or fine-tune prompts and models, which impacts performance and transferability.&lt;SEP&gt;Training data refers to the datasets used to train large language models, impacting their code generation capabilities.&lt;SEP&gt;Training data encompasses datasets like HPC-I NSTRUCT and synthetic datasets, which influence model learning and performance.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-735818e62b066d03c2144cdb5db162c1&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd&lt;SEP&gt;chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="model fine-tuning">
  <data key="d0">model fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of adapting pre-trained language models on specific datasets, such as code, to improve performance in targeted tasks.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security risk">
  <data key="d0">security risk</data>
  <data key="d1">Limitations</data>
  <data key="d2">Potential vulnerabilities and malicious activities associated with executing untrusted code, mitigated by sandboxing techniques.</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fine-Tuning">
  <data key="d0">Fine-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process of training a pre-trained model on specific data to improve performance on a particular task or domain.&lt;SEP&gt;Adjusting pre-trained language models on specific datasets (HPC source code) to improve task-specific performance.&lt;SEP&gt;Fine-tuning is the process of further training a pre-trained language model, such as GPT, on specific datasets to adapt it for specialized tasks like code generation, resulting in models like Codex.&lt;SEP&gt;Fine-tuning refers to the process of adapting a pre-trained language model, such as GPT, to specific tasks or datasets by further training it on targeted data, in this case, code datasets to produce models like Codex.&lt;SEP&gt;The process of adapting a pre-trained AI model like Codex to specific tasks or datasets, involving additional training with targeted data.&lt;SEP&gt;Training process where models are further trained on curated, high-quality datasets or human feedback to improve performance and alignment.&lt;SEP&gt;The process of adapting a pre-trained language model to specific tasks or domains by additional training on task-specific data.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="GPT models">
  <data key="d0">GPT models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GPT models are large-scale language models trained on natural language data, capable of understanding and generating human-like text.&lt;SEP&gt;GPT models are large-scale language models trained on natural language data, capable of understanding, generating, and performing tasks across various domains, including code.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python files">
  <data key="d0">Python files</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Python files collected from GitHub repositories serve as the training data for fine-tuning Codex.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Data Collection">
  <data key="d0">Data Collection</data>
  <data key="d1">Methodology</data>
  <data key="d2">Data collection involved gathering publicly available code repositories from GitHub, filtering for quality and relevance to create a dataset for training models.&lt;SEP&gt;Data collection involves tracing function calls and capturing inputs and outputs during program execution to generate datasets for problem creation and model training.&lt;SEP&gt;Process of gathering HPC source code from GitHub, filtering by language and HPC relevance, and removing duplicates and small/large files.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model training">
  <data key="d0">Model training</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Model training involved optimizing Codex using the Adam optimizer, specific learning rates, and tokenization strategies to improve code generation performance.&lt;SEP&gt;The process of fine-tuning models on specific datasets to adapt them for tasks like pragma generation or performance classification.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="pass@k">
  <data key="d0">pass@k</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A metric indicating the probability that at least one of k generated samples passes unit tests, used to assess code generation quality.&lt;SEP&gt;A metric representing the probability that among k generated samples, at least one passes the test, used to measure code generation success.&lt;SEP&gt;Pass@k is a metric used to evaluate the probability that at least one of k generated samples passes the unit tests, reflecting the model's effectiveness in solving programming tasks.&lt;SEP&gt;pass@k is a performance metric indicating the percentage of test cases passed by a model within the top k generated solutions, used for evaluating code generation models.&lt;SEP&gt;pass@k is a performance metric measuring the probability that at least one of the top k generated samples passes all test cases in code evaluation.&lt;SEP&gt;A metric measuring the probability that at least one correct answer is produced within k attempts, used to evaluate model performance over multiple tries.&lt;SEP&gt;pass@k is a correctness metric estimating the probability that a language model will generate a correct solution within k attempts, commonly used to evaluate code generation performance.&lt;SEP&gt;pass@k is a metric estimating the probability that a language model will generate a correct solution within k attempts, commonly used to evaluate code correctness.&lt;SEP&gt;pass@k measures the probability that at least one correct answer is generated within k attempts, providing insight into model performance over multiple tries.&lt;SEP&gt;pass@k is a probabilistic metric estimating the likelihood that a model produces at least one correct solution within k attempts, used for evaluating code generation accuracy.&lt;SEP&gt;pass@k is a probabilistic metric estimating the likelihood that an LLM produces at least one correct solution within k attempts, used for evaluating code correctness.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-3ac45c735d3bc9849e74dd1a2c194b20&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="power law scaling">
  <data key="d0">power law scaling</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The performance of language models, including test loss and pass@k, follows a power law with respect to model size, indicating predictable improvements as models grow larger.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="nucleus sampling">
  <data key="d0">nucleus sampling</data>
  <data key="d1">Tools</data>
  <data key="d2">A sampling method used during code generation to produce diverse outputs by selecting tokens based on a cumulative probability threshold (top p=0.95&lt;SEP&gt;A sampling technique used during code generation to produce diverse outputs by selecting tokens based on a cumulative probability threshold (top p=0.95).</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="learning rate">
  <data key="d0">learning rate</data>
  <data key="d1">Variables</data>
  <data key="d2">The learning rate controls the step size during model optimization, affecting convergence speed and training stability.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model size">
  <data key="d0">model size</data>
  <data key="d1">Variables</data>
  <data key="d2">Model size pertains to the number of parameters and the overall capacity of the model, which correlates with its performance, risks of misalignment, and potential for harm.&lt;SEP&gt;The number of parameters in the language model, which influences its capacity, performance, and the power law relationship with test loss.&lt;SEP&gt;The number of parameters in a language model, influencing the effectiveness and robustness of prompt tuning.&lt;SEP&gt;Model size pertains to the number of parameters in the language models (e.g., 1.3B, 6.7B, 16B, 70B), influencing their performance, resource requirements, and practicality.&lt;SEP&gt;Model size refers to the number of parameters in each language model (e.g., 1.3B, 6.7B, 16B, 70B), impacting their computational resource needs and performance capabilities.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-eec372249525c58f53cd0e9c24744ffb&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="tokenizer">
  <data key="d0">tokenizer</data>
  <data key="d1">Tools</data>
  <data key="d2">The GPT-3 text tokenizer is used to encode code and natural language for training and evaluation, with modifications to better handle whitespace in code.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Data collection">
  <data key="d0">Data collection</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of gathering publicly available code repositories from GitHub, filtering for quality and relevance to create a training dataset for Codex.</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="training data size">
  <data key="d0">training data size</data>
  <data key="d1">Variables</data>
  <data key="d2">The total amount of data (e.g., 159 GB after filtering</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Language Modeling">
  <data key="d0">Language Modeling</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Language modeling involves predicting the next token in a sequence to understand and generate human language, often used as a foundational technique in NLP tasks.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sampling Strategies">
  <data key="d0">Sampling Strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Sampling strategies refer to methods for selecting samples from language models, such as choosing based on highest mean token log probability or sum log probability, to improve performance.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Token Log Probability">
  <data key="d0">Token Log Probability</data>
  <data key="d1">Variables</data>
  <data key="d2">Token log probability measures the likelihood of individual tokens in generated samples, used to evaluate and select high-quality outputs.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Performance Metrics">
  <data key="d0">Model Performance Metrics</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics like pass@k and BLEU scores are used to evaluate the effectiveness of language models in generating correct solutions and their similarity to reference outputs.&lt;SEP&gt;Metrics such as perplexity and token prediction accuracy are recorded during training and validation to evaluate model effectiveness.&lt;SEP&gt;Metrics such as factuality percentage, diversity scores, and accuracy within specific thresholds used to compare RAG and BART models.&lt;SEP&gt;Metrics such as Pass@1 used to evaluate code generation accuracy and effectiveness across different models and training configurations.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-93a5a4d02febb50f7431a4688bb7fe2f&lt;SEP&gt;chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Codex-12B">
  <data key="d0">Codex-12B</data>
  <data key="d1">Model</data>
  <data key="d2">A large language model trained on code, used to generate and evaluate code solutions in the study.&lt;SEP&gt;Codex-12B is a large language model trained on a vast dataset of publicly available Python code, used for code generation tasks.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GPT-Neo">
  <data key="d0">GPT-Neo</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An open-source language model trained on the Pile dataset, evaluated for coding performance.&lt;SEP&gt;An open-source large language model based on autoregressive architecture, designed for NLP tasks.&lt;SEP&gt;GPT-Neo is an open-source language model evaluated for code generation performance after fine-tuning on HPC datasets.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="APPS Dataset">
  <data key="d0">APPS Dataset</data>
  <data key="d1">Study Population/Dataset</data>
  <data key="d2">A dataset containing coding problems and unit tests used to evaluate the coding competence of language models.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="BLEU Score">
  <data key="d0">BLEU Score</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A metric for evaluating the similarity between generated solutions and reference solutions, used as an indicator of correctness.&lt;SEP&gt;A metric for quantifying the similarity between generated solutions and reference solutions, used to assess output quality.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pass@k">
  <data key="d0">Pass@k</data>
  <data key="d1">Results</data>
  <data key="d2">A metric indicating the percentage of problems where at least one of the top-k generated solutions passes all unit tests, used to evaluate model effectiveness.&lt;SEP&gt;A performance metric indicating the percentage of problems where the model's top-k solutions pass all unit tests.&lt;SEP&gt;Pass@k is a performance metric indicating the proportion of test cases passed within the top k generated solutions, used to evaluate model accuracy.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sample Selection Strategies">
  <data key="d0">Sample Selection Strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies such as mean log probability ranking and back-translation scoring are used to select high-quality samples for deployment.&lt;SEP&gt;Techniques such as choosing samples based on highest mean or sum log probability to improve model performance in code generation tasks.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Temperature">
  <data key="d0">Temperature</data>
  <data key="d1">Tools</data>
  <data key="d2">A parameter controlling the randomness of model outputs, with higher temperatures increasing diversity, affecting sample variety and performance.&lt;SEP&gt;Temperature is a parameter controlling the randomness or diversity of the model's output during generation, with different optimal values for different evaluation metrics.&lt;SEP&gt;Temperature controls the randomness of token sampling by adjusting the model's confidence distribution, influencing diversity in generated text.&lt;SEP&gt;Temperature controls the randomness of token selection by scaling logits before softmax, affecting the diversity of generated text.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Scaling Behavior">
  <data key="d0">Scaling Behavior</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The relationship showing how model performance improves as model size increases, often following a sigmoid trend in log-parameters.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Oracle">
  <data key="d0">Oracle</data>
  <data key="d1">Organization</data>
  <data key="d2">A hypothetical ideal evaluator with prior knowledge of the correct solutions, used as a benchmark for maximum achievable performance.&lt;SEP&gt;A supporter contributing resources and backing to the project.&lt;SEP&gt;A supporter of the research, contributing resources and support.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Model Comparisons">
  <data key="d0">Model Comparisons</data>
  <data key="d1">Results</data>
  <data key="d2">Comparative assessments between models like Codex, GPT-Neo, GPT-J, and Tabnine to evaluate relative coding performance.&lt;SEP&gt;RAG models are compared to other models such as BART, REALM, T5+SSM, and DPR, demonstrating superior performance.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Code Synthesis Tasks">
  <data key="d0">Code Synthesis Tasks</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Tasks involving generating full programs or functions from problem descriptions, used as benchmarks for model capabilities.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training Datasets">
  <data key="d0">Training Datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Sources of data such as The Pile dataset, which include text and code used to train models like GPT-Neo and GPT-J.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Performance Scaling">
  <data key="d0">Model Performance Scaling</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The phenomenon where model performance improves smoothly as the number of parameters increases, often following a sigmoid curve in log scale.</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Variance Reduction Measure">
  <data key="d0">Variance Reduction Measure</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A statistical approach used to reduce variability in measurement results by focusing on a specific metric, in this case, 'strict accuracy', to improve reliability of evaluations.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Strict Accuracy">
  <data key="d0">Strict Accuracy</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A performance metric that emphasizes precise correctness, used as the primary focus in evaluating model performance in this context.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evaluating Large Language Models Trained on Code">
  <data key="d0">Evaluating Large Language Models Trained on Code</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A 2021 study assessing the performance and capabilities of large language models specifically trained on programming code.&lt;SEP&gt;A research study focusing on benchmarking methods, evaluation metrics, and challenges in assessing large language models' performance on code tasks.&lt;SEP&gt;A study investigating the performance of large language models on code-related tasks, specifically measuring pass@k metrics across different models.&lt;SEP&gt;Research framework for assessing the capabilities, limitations, and risks of models like Codex.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628&lt;SEP&gt;chunk-9605dfcbeda0fc2efdc5d6c829baafeb&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="GPT-J pass@1">
  <data key="d0">GPT-J pass@1</data>
  <data key="d1">Results</data>
  <data key="d2">Performance result indicating that GPT-J achieves a pass@1 rate between Codex-85M and Codex-300M, showing its effectiveness in code evaluation.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="PASS @k">
  <data key="d0">PASS @k</data>
  <data key="d1">Results</data>
  <data key="d2">A set of metrics (k=1, 10, 100) used to quantify the success rate of models in passing code tests at different thresholds.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GPT-N EO125M, EO1.3B, EO2.7B, GPT-J 6B, TABNINE, CODEX models">
  <data key="d0">GPT-N EO125M, EO1.3B, EO2.7B, GPT-J 6B, TABNINE, CODEX models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Various language models evaluated for their code generation and correctness performance, with different sizes and architectures.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Evaluation Metrics">
  <data key="d0">Code Evaluation Metrics</data>
  <data key="d1">Tools</data>
  <data key="d2">Metrics such as pass@k used to assess the correctness and efficiency of generated code solutions by models.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Filtered pass@k">
  <data key="d0">Filtered pass@k</data>
  <data key="d1">Results</data>
  <data key="d2">A performance measure calculated after filtering solutions that pass specific unit tests, providing a more stringent evaluation of model output.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Time-out Solutions">
  <data key="d0">Time-out Solutions</data>
  <data key="d1">Results</data>
  <data key="d2">Solutions that do not fail unit tests but exceed time limits during execution, highlighting issues with efficiency despite correctness.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Supervised Fine-Tuning">
  <data key="d0">Supervised Fine-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach where models are further trained on curated datasets of correct solutions to improve performance.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Problems from Competitive Programming">
  <data key="d0">Problems from Competitive Programming</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Programming problems from online contests used to generate training data for fine-tuning models, with well-defined test cases and problem statements.&lt;SEP&gt;These problems are used as data sources for supervised fine-tuning, providing well-structured examples and test cases, often from online contest websites.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Problems from Continuous Integration">
  <data key="d0">Problems from Continuous Integration</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Programming problems derived from real-world open source projects by collecting function inputs and outputs during automated testing, used to adapt models to practical codebases.&lt;SEP&gt;Real-world programming problems derived from open source projects by collecting function inputs and outputs during automated testing, used to adapt models to practical coding environments.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training Data">
  <data key="d0">Training Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Data sources including public repositories across languages that influence Copilot's suggestion quality.&lt;SEP&gt;Data sources used to train LLMs, which are explicitly excluded from the prompts to prevent models from copying solutions.&lt;SEP&gt;Datasets used for training Codex and GPT-3, which encode social biases and stereotypes.&lt;SEP&gt;The curated datasets of problems and solutions from competitive programming websites and open source repositories used for supervised fine-tuning models.&lt;SEP&gt;Training data includes internet sources such as GitHub repositories used to develop Codex, raising questions about copyright and fair use.&lt;SEP&gt;The datasets used to fine-tune models, including HPC source code, affecting the models' capabilities.&lt;SEP&gt;Training data consists of datasets used to train models; in this context, it is the data on which the language model is fine-tuned and evaluated.&lt;SEP&gt;Large text corpora used to pre-train and fine-tune LLMs for specific domains.&lt;SEP&gt;The large corpus of text data used to pre-train LLMs, enabling their language understanding and generation capabilities.&lt;SEP&gt;Data used to train GitHub Copilot, including public repositories across multiple languages, influencing suggestion quality.&lt;SEP&gt;The data used to train the program, containing examples or demonstrations.&lt;SEP&gt;Training data consists of datasets like HPC-I NSTRUCT, Magicoder-OSS-Instruct-75K, and Evol-Instruct-Code-80k-v1, used to fine-tune models.&lt;SEP&gt;Training data refers to datasets used to fine-tune language models, influencing their ability to learn specific tasks such as MPI code generation. The quality and quantity of training data impact model performance.&lt;SEP&gt;Training data refers to the datasets used to fine-tune language models, impacting their performance and generalization capabilities.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-10b08670a9cf75866c6b05fa5b5cfc12&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Benchmarking">
  <data key="d0">Benchmarking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Benchmarking involves creating standardized tests and metrics, such as ParEval, to evaluate the performance of language models in generating parallel code, including correctness and efficiency.&lt;SEP&gt;Benchmarking involves systematically testing models on predefined code translation and generation tasks to assess their performance.&lt;SEP&gt;Comparative evaluation of multiple models using standardized metrics to determine relative performance.&lt;SEP&gt;The process of designing and applying standardized tests and metrics, such as ParEval, to systematically evaluate the performance of language models in generating parallel code, including correctness and efficiency.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fine-tuning Approaches">
  <data key="d0">Fine-tuning Approaches</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Supervised fine-tuning on curated datasets from programming contests and open source projects to improve model accuracy and robustness.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Test Cases">
  <data key="d0">Test Cases</data>
  <data key="d1">Tools</data>
  <data key="d2">Input/output examples used to validate solutions in datasets and during evaluation.&lt;SEP&gt;Test cases are specific input-output pairs used to evaluate whether a program functions correctly, derived from problem statements or incorrect solutions.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automated Testing">
  <data key="d0">Automated Testing</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of running code solutions against predefined test cases to verify correctness and performance.</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Problems">
  <data key="d0">Problems</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Problems are structured tasks designed to evaluate core functionality of algorithms or models, with variations to test understanding and robustness.&lt;SEP&gt;Problems are structured tasks designed to test core functionalities and understanding of a problem type, often with variations to evaluate model performance.&lt;SEP&gt;Problems refer to programming challenges curated from open source projects, used to evaluate or train models, involving input-output data and test cases.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Continuous Integration">
  <data key="d0">Continuous Integration</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Continuous Integration (CI) involves automated testing and deployment processes, using tools like Travis and Tox to automate build, test, and integration workflows in software development.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="sys.setprofile">
  <data key="d0">sys.setprofile</data>
  <data key="d1">Tools</data>
  <data key="d2">sys.setprofile is a Python function used to trace and collect function call data during program execution, aiding in creating unit tests and analyzing code behavior.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open Source Projects">
  <data key="d0">Open Source Projects</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Open source projects are publicly available software repositories from which programming problems and code can be sourced for analysis and problem creation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GitHub Repos">
  <data key="d0">GitHub Repos</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">GitHub repositories are hosting platforms for open source projects, often using CI tools like Travis and Tox to manage continuous integration workflows.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="PyPI">
  <data key="d0">PyPI</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Python Package Index (PyPI) is a repository of Python packages, used as a source of code and modules for problem creation and model training.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Finetuned GPT-Neo">
  <data key="d0">Finetuned GPT-Neo</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">GPT-Neo is a language model that has been fine-tuned on specific programming problems to improve its performance on code-related tasks.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="temperature">
  <data key="d0">temperature</data>
  <data key="d1">Variables</data>
  <data key="d2">Temperature is a parameter controlling randomness in language model sampling; higher temperatures lead to more diverse outputs, affecting model performance on code tasks.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="sandboxed environment">
  <data key="d0">sandboxed environment</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A sandboxed environment is a controlled, isolated computing environment used to safely run untrusted code, such as in testing or code evaluation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="tracing methodology">
  <data key="d0">tracing methodology</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Tracing methodology involves monitoring function calls, inputs, and outputs during program execution to gather data for problem creation and analysis.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="filtering problems">
  <data key="d0">filtering problems</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Filtering problems involves selecting or discarding curated programming problems based on criteria such as pass rates, ambiguity, or non-determinism to ensure quality.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="training problems">
  <data key="d0">training problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Training problems are curated programming tasks used to fine-tune language models, guiding their ability to generate correct code solutions.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="reference solution">
  <data key="d0">reference solution</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A reference solution is an example of a correct program or code snippet used as a target during model training or evaluation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="prompt">
  <data key="d0">prompt</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A prompt is an input instruction or question provided to the language model to generate code.&lt;SEP&gt;A prompt is an input or instruction provided to a language model to generate a response or code, often formatted with padding for batch processing.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="validation loss">
  <data key="d0">validation loss</data>
  <data key="d1">Results</data>
  <data key="d2">Validation loss measures the discrepancy between the model's output and the reference solutions on validation data, used to determine training convergence.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Incorrect Solutions">
  <data key="d0">Incorrect Solutions</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Incorrect solutions are flawed code submissions used as sources for extracting test cases and understanding common errors.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Problem Statements">
  <data key="d0">Problem Statements</data>
  <data key="d1">Research Questions</data>
  <data key="d2">Problem statements define the requirements and constraints of programming problems, serving as foundational inputs for creating test cases and problems.&lt;SEP&gt;The questions posed define the problems to be solved, guiding the calculation and reasoning process.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Input-Output Data">
  <data key="d0">Input-Output Data</data>
  <data key="d1">Variables</data>
  <data key="d2">Input-output data refers to the collected data during function execution, used to generate problems and evaluate model performance.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Function Calls">
  <data key="d0">Function Calls</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Function calls during program execution are monitored to trace inputs and outputs, aiding in creating training problems and understanding code behavior.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Objects">
  <data key="d0">Code Objects</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code objects include functions, modules, or components collected from source code for problem creation and analysis.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sandboxed Environment">
  <data key="d0">Sandboxed Environment</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A sandboxed environment is an isolated computing setup used to safely execute untrusted code, essential for testing and problem validation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Filtering Criteria">
  <data key="d0">Filtering Criteria</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Filtering criteria are rules applied to curated problems to remove ambiguous, non-deterministic, or low-quality tasks, ensuring dataset quality.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Evaluation">
  <data key="d0">Model Evaluation</data>
  <data key="d1">Results</data>
  <data key="d2">Evaluation involves assessing the performance of fine-tuned code LLMs against benchmarks like ParEval to determine their effectiveness in generating parallel code.&lt;SEP&gt;Model evaluation involves measuring performance metrics such as pass@k and validation loss to assess the accuracy and robustness of code generation models.&lt;SEP&gt;Model evaluation involves testing the trained model on validation or test datasets to assess its capabilities and generalization.&lt;SEP&gt;Model performance is assessed through benchmarks like ParEval to determine their ability to generate parallel code.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Dataset">
  <data key="d0">Training Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The dataset used for fine-tuning consists of code solutions and natural language data, separated into training and validation subsets.&lt;SEP&gt;The training involves datasets of code and docstrings, with sizes such as less than 10 billion tokens for fine-tuning.&lt;SEP&gt;Training datasets consist of curated problems, reference solutions, and collected data used to fine-tune models like Codex and GPT-Neo.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-93a5a4d02febb50f7431a4688bb7fe2f&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fine-tuning">
  <data key="d0">Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Fine-tuning is a process that involves further training pre-trained language models on domain-specific datasets or tasks to enhance their performance, robustness, and efficiency. This process adjusts the internal parameters of the models through regularized optimization, enabling them to produce more profound behavioral changes tailored to specific applications. Fine-tuning is commonly used to adapt models to particular domains such as high-performance computing (HPC), Earth science, finance, law, human-computer interaction (HCI), and software engineering. For example, it involves training models on specialized datasets like HPC code or curated programming problems to improve their ability to generate accurate and efficient code, including parallel code and OpenMP pragmas. Additionally, fine-tuning can optimize models for specific tasks such as instruction following, code generation, or reducing model size and increasing efficiency on hardware like A100 GPUs. Overall, fine-tuning serves as a transfer learning technique that refines pre-trained models to better suit targeted domains and tasks, resulting in more specialized and effective performance.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-f3c94db4d83b03209953445c36e8cb0d&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-4bb8bb2fc231e52a13ed0e91512e55eb&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Temperature Parameter">
  <data key="d0">Temperature Parameter</data>
  <data key="d1">Variables</data>
  <data key="d2">The temperature parameter controls the randomness of model output sampling, affecting diversity and the likelihood of passing test cases during code generation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Validation Loss">
  <data key="d0">Validation Loss</data>
  <data key="d1">Results</data>
  <data key="d2">Validation loss measures the model's performance on unseen data during training, indicating when to stop training to avoid overfitting.&lt;SEP&gt;Validation loss quantifies the difference between model outputs and reference solutions on validation data, guiding training until convergence.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Quality Control">
  <data key="d0">Quality Control</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Quality control involves using automated filtering, multiple sampling, and testing to ensure only high-quality, deterministic problems are used for training and evaluation.</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="pass@1">
  <data key="d0">pass@1</data>
  <data key="d1">Results</data>
  <data key="d2">A performance metric indicating the proportion of correct answers generated by language models on the first attempt in code generation tasks.&lt;SEP&gt;pass@1 is a metric indicating the probability that the model's top prediction is correct, used to evaluate code generation and translation accuracy.&lt;SEP&gt;pass@1 is a performance metric indicating the likelihood that the model's top prediction is correct, used to evaluate code generation and translation accuracy.&lt;SEP&gt;pass@1 is a performance metric used to evaluate the accuracy of language models in generating correct code answers on the first attempt.&lt;SEP&gt;pass@1 is the success rate when only the top sample is considered, indicating the model's ability to generate correct code on the first attempt.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="pass@100">
  <data key="d0">pass@100</data>
  <data key="d1">Results</data>
  <data key="d2">pass@100 measures the success rate considering the top 100 samples, reflecting the diversity and quality of multiple generated outputs.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Codex-S">
  <data key="d0">Codex-S</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Codex-S is a variant of Codex that captures a narrower distribution, requiring higher sampling temperatures and demonstrating improved performance over the original Codex in certain metrics.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sample Selection Heuristics">
  <data key="d0">Sample Selection Heuristics</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Heuristics like ranking samples by mean log probability are used to select the best generated code samples, with benefits observed over random selection.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Docstring Generation">
  <data key="d0">Docstring Generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Generating descriptive comments (docstrings) from code to specify the intent behind functions, useful for safety and understanding.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Codex-D">
  <data key="d0">Codex-D</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Codex-D is a model trained specifically to generate docstrings from code, aiming to produce accurate descriptions of code functionality.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evaluation of Docstrings">
  <data key="d0">Evaluation of Docstrings</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Manual grading of 10 samples per problem from Codex-D-12B at temperature 0.8 to assess correctness based on accuracy and completeness of generated descriptions.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Challenges in Docstring Evaluation">
  <data key="d0">Challenges in Docstring Evaluation</data>
  <data key="d1">Limitations</data>
  <data key="d2">Automatic evaluation is not feasible; manual grading is time-consuming and subjective, and models may produce low-quality or generic descriptions.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Performance Comparison">
  <data key="d0">Performance Comparison</data>
  <data key="d1">Results</data>
  <data key="d2">Pass rates for Codex-D are lower but comparable to Codex-S, with Codex-D achieving 20.3% pass@1 and 46.5% pass@10, indicating moderate success in generating accurate docstrings.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Back-Translation Objective">
  <data key="d0">Back-Translation Objective</data>
  <data key="d1">Variables</data>
  <data key="d2">Back-translation objective evaluates how well generated docstrings can be used to retrieve the original code, serving as an alternative ranking method for sample selection.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ranking Strategies">
  <data key="d0">Ranking Strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Ranking generated samples via mean log probability generally outperforms back-translation-based ranking, though the latter provides an alternative approach.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fine-tuning Codex">
  <data key="d0">Fine-tuning Codex</data>
  <data key="d1">Methodology</data>
  <data key="d2">The process of adapting the Codex model to specific tasks or datasets by training it further on relevant data to improve performance.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Tokens">
  <data key="d0">Tokens</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tokens are the basic units of text input processed by the language model, with less than 10 billion tokens used for training.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Optimal Temperature">
  <data key="d0">Optimal Temperature</data>
  <data key="d1">Variables</data>
  <data key="d2">The temperature parameter controls the randomness in model sampling, with optimal values differing based on evaluation metrics like pass@k.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sample Ranking">
  <data key="d0">Sample Ranking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Sample ranking involves ordering generated samples based on criteria such as log probability or back-translation scores to select the best output.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Back-Translation">
  <data key="d0">Back-Translation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Back-translation involves translating generated code or docstrings back into the original form to evaluate the quality of the generated output.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Negative Log-Likelihood">
  <data key="d0">Negative Log-Likelihood</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Negative log-likelihood is a loss function minimized during training to improve the model's probability estimates of reference outputs.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code-Conditional Docstring Generation">
  <data key="d0">Code-Conditional Docstring Generation</data>
  <data key="d1">Methodology</data>
  <data key="d2">Training models to generate descriptive docstrings from code snippets to improve code understanding and safety.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sample Diversity">
  <data key="d0">Sample Diversity</data>
  <data key="d1">Variables</data>
  <data key="d2">Sample diversity refers to the variety of generated outputs, influenced by parameters like temperature and sampling strategies.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Performance Margin">
  <data key="d0">Performance Margin</data>
  <data key="d1">Results</data>
  <data key="d2">The performance margin of 6.5 percentage points on pass@1 and 15.1 on pass@100 indicates the degree of improvement of Codex-S over Codex.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Efficiency">
  <data key="d0">Model Efficiency</data>
  <data key="d1">Results</data>
  <data key="d2">Codex-S is one or two orders of magnitude more parameter efficient than Codex, indicating better performance relative to size.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generated Code Quality">
  <data key="d0">Generated Code Quality</data>
  <data key="d1">Results</data>
  <data key="d2">The quality of generated code is assessed via pass@k metrics, manual grading, and sample ranking methods.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Limitations in Automatic Evaluation">
  <data key="d0">Limitations in Automatic Evaluation</data>
  <data key="d1">Limitations</data>
  <data key="d2">Automatic evaluation of generated docstrings is challenging; manual grading is required, which is time-consuming and subjective.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Safety Reasons">
  <data key="d0">Safety Reasons</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Generating accurate docstrings can improve code safety, documentation, and maintainability by clarifying function purposes.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Overfitting">
  <data key="d0">Model Overfitting</data>
  <data key="d1">Limitations</data>
  <data key="d2">Overfitting during training can be mitigated by monitoring validation loss and early stopping.</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Figure 7">
  <data key="d0">Figure 7</data>
  <data key="d1">Results</data>
  <data key="d2">Figure 7 illustrates the comparison of ranking methods for code generation samples, showing that back-translation underperforms mean log-probability ranking but outperforms random ranking.&lt;SEP&gt;Figure 7 presents a comparison of the models’ performance in terms of sample correctness and build rates, illustrating their relative effectiveness and correlation between build success and correctness.&lt;SEP&gt;Figure 7 presents a comparison of the performance of different code models, specifically Poly-Coder and PolyCoder+HPC, in terms of sample compilation correctness and build rates, indicating their efficiency and correlation between build success and correctness rates.&lt;SEP&gt;Figure 7 presents data on efficiency@1 for MPI, OpenMP, and Kokkos prompts across different models and thread counts, illustrating comparative performance and resource utilization.&lt;SEP&gt;Figure 7 presents data on efficiency@1 for MPI, OpenMP, and Kokkos prompts across different models and thread counts, illustrating comparative performance.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-d25502af8737c59d857916af40cc4020&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python code on GitHub">
  <data key="d0">Python code on GitHub</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large dataset comprising hundreds of millions of lines of publicly available Python code used for training Codex.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="performance">
  <data key="d0">performance</data>
  <data key="d1">Results</data>
  <data key="d2">Codex's performance is evaluated through pass rates on various programming problems, showing decline as task complexity increases, especially with longer docstrings and chained operations.&lt;SEP&gt;The ability of models to produce correct code solutions, with performance improved by training on similar distributions and generating multiple samples.&lt;SEP&gt;The effectiveness of models in generating correct code solutions, which improves when trained on similar data distributions and by producing multiple samples.&lt;SEP&gt;Performance in computing refers to how efficiently and effectively code runs, often related to optimization, resource utilization, and execution speed.&lt;SEP&gt;Performance refers to the efficiency and effectiveness of code execution, which can be studied using various models and data to improve computational outcomes.&lt;SEP&gt;Measurement of how efficiently the generated code runs, including metrics like speedup and efficiency, highlighting the current performance gaps of LLMs in parallel code generation.&lt;SEP&gt;Measurement of how efficiently the generated code runs, including speedup and scalability metrics, highlighting the current performance gaps of LLMs in parallel code generation.&lt;SEP&gt;Performance refers to the ability of developers to write efficient parallel code, encompassing aspects like correctness, scalability, and speed.&lt;SEP&gt;Performance refers to the ability of developers to write efficient, correct, and scalable parallel code, encompassing various metrics and evaluation methods.&lt;SEP&gt;The runtime efficiency and scalability of generated parallel code, assessed through metrics like efficiency and speedup.&lt;SEP&gt;The runtime efficiency and scalability of generated parallel code, which are critical for assessing model effectiveness.&lt;SEP&gt;The evaluation shows that increasing kernel complexity reduces the accuracy and quality of AI-generated code, with simpler kernels yielding better results.&lt;SEP&gt;The evaluation shows that the complexity of kernels impacts the quality of AI-generated code, with simpler kernels yielding better results.&lt;SEP&gt;Model performance is assessed through metrics like pass@k, throughput, and memory requirements, to determine efficacy in parallel code generation tasks.&lt;SEP&gt;Model performance is evaluated based on metrics like pass@k, throughput, and memory requirements, to assess efficacy in parallel code generation.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-d25502af8737c59d857916af40cc4020&lt;SEP&gt;chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="synthetic problems dataset">
  <data key="d0">synthetic problems dataset</data>
  <data key="d1">Study Design</data>
  <data key="d2">A dataset created from basic building blocks to measure how performance decreases as the number of chained components in docstrings increases.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="behavior degradation">
  <data key="d0">behavior degradation</data>
  <data key="d1">Results</data>
  <data key="d2">Performance of Codex-12B decreases exponentially with increasing length and complexity of docstrings, especially with chained components.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="system-level synthe-sis capabilities">
  <data key="d0">system-level synthe-sis capabilities</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Refers to the ability of Codex to understand and generate code involving system-level operations, which is limited, leading to mistakes in variable binding and operation execution.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="hazards of code generation">
  <data key="d0">hazards of code generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Potential safety and societal risks associated with deploying Codex, including misaligned outputs and misuse.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="hazard analysis">
  <data key="d0">hazard analysis</data>
  <data key="d1">Study Design</data>
  <data key="d2">A systematic assessment of risks associated with Codex, focusing on potential causes of harm and societal impacts.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Broader impacts">
  <data key="d0">Broader impacts</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The potential societal benefits and hazards of Codex, including aiding onboarding, education, but also safety challenges and misuse risks.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="performance degradation">
  <data key="d0">performance degradation</data>
  <data key="d1">Results</data>
  <data key="d2">Codex's performance declines exponentially as the complexity of chained components in docstrings increases, indicating a limitation in handling complex, multi-step instructions.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="overfitting">
  <data key="d0">overfitting</data>
  <data key="d1">Results</data>
  <data key="d2">Back-translation ranking heuristic appears to overfit quickly, reducing generalization performance.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="training dataset">
  <data key="d0">training dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The dataset consists of publicly available Python code from GitHub, used to train Codex.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="metrics for code evaluation">
  <data key="d0">metrics for code evaluation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Qualitative metrics developed to measure Codex's ability to generate syntactically correct and contextually appropriate code, controlling for complexity and abstraction.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="performance metrics">
  <data key="d0">performance metrics</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics such as pass rates show that performance deteriorates with increased docstring length and complexity.&lt;SEP&gt;Quantitative measures used to evaluate the effectiveness of prompt tuning, such as accuracy or convergence speed.&lt;SEP&gt;Metrics such as pass@1 and pass@k quantify the correctness and efficiency of generated code.&lt;SEP&gt;Metrics such as runtime and pass@1 scores used to evaluate the efficiency and accuracy of generated code.&lt;SEP&gt;Performance metrics include measures such as runtime, efficiency, and pass@1 scores that evaluate the quality and effectiveness of the generated code.&lt;SEP&gt;Performance metrics include pass@k, speedup n@k, and efficiency n@k, used to quantify correctness and efficiency of code generated by language models.&lt;SEP&gt;Quantitative measures such as accuracy percentages that evaluate the effectiveness of DSPy-enhanced systems.&lt;SEP&gt;Quantitative measures such as accuracy, computational efficiency, and developer efficiency used to evaluate system performance.&lt;SEP&gt;Performance metrics include pass@k, throughput, and memory requirements, used to compare models' effectiveness and efficiency.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c&lt;SEP&gt;chunk-d25502af8737c59d857916af40cc4020&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="system-level operation">
  <data key="d0">system-level operation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Refers to the ability of code models to understand and generate code involving system-level functions, which is limited in Codex.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="safety challenges">
  <data key="d0">safety challenges</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Risks related to safety, including generation of incorrect code, misalignment with user intent, and potential misuse.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="societal impacts">
  <data key="d0">societal impacts</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Broader societal consequences of deploying Codex, including both positive applications and potential hazards.</data>
  <data key="d3">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="safety features">
  <data key="d0">safety features</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Safety features refer to the attributes and mechanisms designed to ensure the safe use of code generation models, including risk mitigation strategies and safety protocols.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="models">
  <data key="d0">models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Models in this context are specific machine learning architectures, such as code generation systems like Codex, which are analyzed for their properties, capabilities, and limitations.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="impact analysis">
  <data key="d0">impact analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Impact analysis involves systematically evaluating the potential effects, risks, and safety implications of deploying code generation models in various contexts.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="risk mitigation">
  <data key="d0">risk mitigation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Risk mitigation encompasses strategies and practices aimed at reducing the potential harms and vulnerabilities associated with the use of code generation models.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="over-reliance">
  <data key="d0">over-reliance</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Over-reliance examines the extent to which users depend on generated outputs from models like Codex, and the safety implications of such dependence, especially for novice programmers.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="misalignment">
  <data key="d0">misalignment</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Misalignment describes a situation where the model's outputs do not align with user intentions, often producing unhelpful or harmful code despite having the capability to assist effectively.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="bias and representation">
  <data key="d0">bias and representation</data>
  <data key="d1">Results</data>
  <data key="d2">Bias and representation issues refer to the tendency of models like Codex to generate outputs that reflect stereotypes or harmful biases related to gender, race, or social characteristics, raising safety and ethical concerns.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="training distribution">
  <data key="d0">training distribution</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The training distribution is the dataset on which the model is trained, influencing its capabilities, biases, and the likelihood of generating problematic outputs.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="human oversight">
  <data key="d0">human oversight</data>
  <data key="d1">Tools</data>
  <data key="d2">Human oversight involves active monitoring and intervention by users or developers to ensure safe and appropriate use of code generation models.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="UI designs">
  <data key="d0">UI designs</data>
  <data key="d1">Tools</data>
  <data key="d2">UI designs refer to the user interface aspects that influence how users interact with models like Codex, impacting vigilance, reliance, and safety practices.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="empirical investigation">
  <data key="d0">empirical investigation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Empirical investigation involves collecting data through experiments or observations to understand model behavior, safety issues, and effective mitigation strategies.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model capabilities">
  <data key="d0">model capabilities</data>
  <data key="d1">Variables</data>
  <data key="d2">Model capabilities denote the abilities of the models to generate code, perform tasks, and adapt to user prompts, which directly relate to risks and safety considerations.&lt;SEP&gt;Model capabilities encompass the abilities and functionalities of models like Codex, which influence their potential for misuse or insecure outputs.&lt;SEP&gt;The abilities and functionalities of models like Codex, which influence their potential for misuse and insecure outputs.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="future work">
  <data key="d0">future work</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future work explores areas requiring further research, such as improving safety, reducing bias, and addressing misalignment as models evolve.&lt;SEP&gt;Future work involves addressing current limitations such as improving MPI utilization, reducing memory overhead, and supporting dynamic applications.&lt;SEP&gt;Indicates areas for further investigation, such as tasks under controlled conditions and broader evaluations.&lt;SEP&gt;Indicates areas for further research, such as tasks under controlled conditions, broader evaluations, and improvements in systematic reasoning.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-eec372249525c58f53cd0e9c24744ffb&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Impact of code generation systems">
  <data key="d0">Impact of code generation systems</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The impact of code generation systems refers to the potential positive and negative effects these models have on users, safety, and society, including risks such as over-reliance, misalignment, and bias.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Alignment issues">
  <data key="d0">Alignment issues</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Alignment issues describe the misalignment between model outputs and user intentions, leading to unhelpful or harmful code suggestions despite the model's capabilities.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Safety risks">
  <data key="d0">Safety risks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Safety risks encompass the potential harms associated with code generation models, such as suggesting insecure code, over-reliance, and biased outputs, which require mitigation and oversight.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Risk mitigation strategies">
  <data key="d0">Risk mitigation strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies designed to reduce safety risks, including human oversight, documentation, UI design improvements, and empirical testing.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Over-reliance on models">
  <data key="d0">Over-reliance on models</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Over-reliance examines how dependence on generated code affects safety, especially for novice users, and how to improve vigilance.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias and harmful stereotypes">
  <data key="d0">Bias and harmful stereotypes</data>
  <data key="d1">Results</data>
  <data key="d2">Models like Codex can generate biased outputs reflecting stereotypes related to gender, race, and social characteristics, raising safety and ethical issues.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training data biases">
  <data key="d0">Training data biases</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The training data used for models influences their biases and the likelihood of generating harmful or stereotypical outputs.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model size">
  <data key="d0">Model size</data>
  <data key="d1">Variables</data>
  <data key="d2">The size of the model, measured by parameters, affects its capabilities, risks of misalignment, bias, and safety concerns.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Human oversight">
  <data key="d0">Human oversight</data>
  <data key="d1">Tools</data>
  <data key="d2">Human oversight involves active monitoring and intervention by users or developers to ensure safety and correctness of generated code.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="User interface (UI) designs">
  <data key="d0">User interface (UI) designs</data>
  <data key="d1">Tools</data>
  <data key="d2">UI designs impact how users interact with models, influencing vigilance, reliance, and safety practices.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Empirical investigation">
  <data key="d0">Empirical investigation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Empirical investigation involves collecting data through experiments or observations to evaluate model safety, biases, and effectiveness of mitigation strategies.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model capabilities">
  <data key="d0">Model capabilities</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model capabilities refer to the abilities of models like Codex to generate code, perform tasks, and assist users, which relate directly to safety and risks.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Future research directions">
  <data key="d0">Future research directions</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future research explores improving safety, reducing biases, addressing misalignment, and understanding model impacts as capabilities grow.</data>
  <data key="d3">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias and Representation Issues">
  <data key="d0">Bias and Representation Issues</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Bias and representation issues in code generation models, including stereotypes related to gender, race, emotion, class, and name structures, which pose safety and ethical concerns.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Natural Language Processing">
  <data key="d0">Natural Language Processing</data>
  <data key="d1">Discipline</data>
  <data key="d2">NLP is a field of AI focused on enabling computers to understand and generate human language.&lt;SEP&gt;Natural Language Processing is a field focused on enabling computers to understand and generate human language.&lt;SEP&gt;Natural Language Processing is the discipline focused on enabling computers to understand, interpret, and generate human language.&lt;SEP&gt;The scientific study of language models and their societal impacts, including biases reflected in generated code.&lt;SEP&gt;The study of language models and their societal impacts, including biases reflected in generated code.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d&lt;SEP&gt;chunk-96b489f22f60397ff887486ccf77f457&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Code Generation Models">
  <data key="d0">Code Generation Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Artificial intelligence models designed to automatically generate computer code based on natural language prompts, with potential biases and safety implications.&lt;SEP&gt;Artificial intelligence systems designed to automatically generate computer code from natural language prompts, with potential biases and safety implications.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Filtration and Moderation">
  <data key="d0">Filtration and Moderation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques such as filtering generated outputs and documenting interventions aimed at mitigating risks associated with biased or unsafe code.&lt;SEP&gt;Techniques such as filtering generated outputs, documenting interventions, and moderation strategies to reduce risks of biased or unsafe code.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Economic and Labor Market Impacts">
  <data key="d0">Economic and Labor Market Impacts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Effects of code generation capabilities on productivity, employment, and economic dynamics, including potential shifts in software-related labor markets.&lt;SEP&gt;The effects of code generation capabilities on productivity, employment patterns, and economic factors, including shifts in software development labor markets.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Programmer Productivity">
  <data key="d0">Programmer Productivity</data>
  <data key="d1">Variables</data>
  <data key="d2">A measure of efficiency and output of programmers, which may be influenced by AI tools such as Codex.&lt;SEP&gt;A variable measuring the efficiency and output of programmers, which may be affected by AI tools like Codex.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Software Development Tasks">
  <data key="d0">Software Development Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Activities like writing design specifications, collaborating with colleagues, and upgrading software stacks, which are affected by AI assistance.&lt;SEP&gt;Activities such as writing design specifications, collaborating, and upgrading software, which influence the actual impact of code generation tools.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Labor Market">
  <data key="d0">Labor Market</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The employment landscape for programmers and software developers, potentially affected by automation and AI assistance.&lt;SEP&gt;The employment landscape for software developers and programmers, potentially impacted by automation and AI tools.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Security Implications">
  <data key="d0">Security Implications</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Risks related to the security of AI-generated code, including vulnerabilities, misuse for cybercrime, and challenges in malware detection.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Malware Development">
  <data key="d0">Malware Development</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Malicious software creation potentially facilitated or complicated by AI systems like Codex, with implications for cybersecurity.&lt;SEP&gt;Malicious software creation, which could be facilitated or complicated by AI-generated code, impacting cybersecurity.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Vulnerabilities">
  <data key="d0">Code Vulnerabilities</data>
  <data key="d1">Results</data>
  <data key="d2">Instances of insecure or misaligned code generated by Codex that could be exploited or pose safety risks.&lt;SEP&gt;Instances of insecure or misaligned code produced by Codex that may be exploited or cause safety issues.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Training Data">
  <data key="d0">Model Training Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Source code repositories used to train Codex, which may contain sensitive data and influence model behavior.&lt;SEP&gt;The datasets comprising publicly available source code repositories used for training Codex, affecting bias, security, and behavior.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Environmental Impacts">
  <data key="d0">Environmental Impacts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Energy consumption and carbon footprint associated with training and inference of large language models like Codex.&lt;SEP&gt;The energy footprint, carbon emissions, and environmental costs associated with training and deploying large language models like Codex.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Compute Resources">
  <data key="d0">Compute Resources</data>
  <data key="d1">Variables</data>
  <data key="d2">The computational power and energy used during training and inference, contributing to environmental impacts.&lt;SEP&gt;The computational power used during model training and inference, contributing to environmental impact.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carbon Footprint">
  <data key="d0">Carbon Footprint</data>
  <data key="d1">Results</data>
  <data key="d2">Measured environmental impact of training large models, with efforts to reduce it through renewable energy sourcing.&lt;SEP&gt;The measurable environmental impact, including carbon emissions, resulting from training large models like Codex.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Safety Risks">
  <data key="d0">Safety Risks</data>
  <data key="d1">Results</data>
  <data key="d2">Potential safety hazards arising from biased or insecure code generated by models like Codex, including safety implications for users.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training Data Sources">
  <data key="d0">Training Data Sources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Public repositories of source code used to train Codex, which may contain sensitive or biased data influencing model outputs.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Energy Consumption">
  <data key="d0">Energy Consumption</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount of energy used during training and inference processes of large models, influencing environmental impact.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Renewable Energy Sources">
  <data key="d0">Renewable Energy Sources</data>
  <data key="d1">Tools</data>
  <data key="d2">Sources of energy such as renewable power used during training to reduce carbon footprint.</data>
  <data key="d3">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Compute Consumption">
  <data key="d0">Compute Consumption</data>
  <data key="d1">Variables</data>
  <data key="d2">Compute consumption refers to the amount of computational resources used during training and inference of AI models like Codex, impacting environmental footprint and operational costs.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Azure Platform">
  <data key="d0">Azure Platform</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Azure is a cloud platform used for training Codex, sourcing renewable energy and purchasing carbon credits to reduce its environmental footprint.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Legal Considerations">
  <data key="d0">Legal Considerations</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Legal issues surrounding AI-generated code include questions about fair use, copyright, and intellectual property, especially regarding training data and code output.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="User Control">
  <data key="d0">User Control</data>
  <data key="d1">Tools</data>
  <data key="d2">Users retain control over editing and accepting generated code, making the process similar to auto-completion features in programming environments.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Environmental Impact">
  <data key="d0">Environmental Impact</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The environmental footprint of compute use in training and inference is significant, prompting efforts to source renewable energy and improve efficiency.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Risk Mitigation">
  <data key="d0">Risk Mitigation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies such as documentation, content filtering, user review, use restrictions, and monitoring are employed to reduce harms like offensive outputs or insecure code.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Responsible AI">
  <data key="d0">Responsible AI</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Responsible AI practices involve ongoing engagement with policymakers, attention to intellectual property issues, and development of safety measures to maximize social benefits and minimize harms.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Program Learning">
  <data key="d0">Neural Program Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Neural program learning encompasses approaches like program induction and synthesis, enabling models to generate or execute programs directly from data or latent representations.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Program Induction">
  <data key="d0">Program Induction</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Program induction involves models generating program outputs from latent representations, demonstrated in tasks like addition, memorization, and more complex algorithmic tasks using neural architectures like Neural Turing Machines.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Turing Machine">
  <data key="d0">Neural Turing Machine</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural network architecture designed to mimic the behavior of a Turing machine, enabling learning of algorithms and memory-augmented computations.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Petaﬂop/S-Days of Compute">
  <data key="d0">Petaﬂop/S-Days of Compute</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The measure of computational resources used during AI training and inference, indicating the scale and environmental impact of models like Codex.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carbon Credits">
  <data key="d0">Carbon Credits</data>
  <data key="d1">Tools</data>
  <data key="d2">Market-based instruments purchased by platforms like Azure to offset carbon emissions associated with compute operations, supporting environmental sustainability.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Renewable Energy">
  <data key="d0">Renewable Energy</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sources of sustainable power used by data centers to reduce carbon footprint, such as wind or solar energy.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Environmental Footprint">
  <data key="d0">Environmental Footprint</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The total environmental impact resulting from compute resource usage, including carbon emissions, energy consumption, and associated ecological costs.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Legal Implications">
  <data key="d0">Legal Implications</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Legal considerations related to AI-generated code, including copyright, fair use, and intellectual property issues arising from training data and generated outputs.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fair Use">
  <data key="d0">Fair Use</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Legal doctrine that allows limited use of copyrighted material without permission, relevant to training AI on publicly available internet data.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Snippets">
  <data key="d0">Code Snippets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Segments of programming code that may appear in training data and generated outputs, with implications for copyright and originality.&lt;SEP&gt;Various code implementations used to evaluate model performance.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="In-Distribution Code">
  <data key="d0">In-Distribution Code</data>
  <data key="d1">Results</data>
  <data key="d2">Code generated by models that closely resembles training data, raising concerns about copying versus pattern learning.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Predictive Weightings">
  <data key="d0">Predictive Weightings</data>
  <data key="d1">Variables</data>
  <data key="d2">Parameters within the model that influence output, determining whether code appears similar to training data or is novel.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Auto-suggest/Auto-completion">
  <data key="d0">Auto-suggest/Auto-completion</data>
  <data key="d1">Tools</data>
  <data key="d2">Features in coding environments where generated code assists programmers, highlighting user control and safety considerations.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Responsible and Safe AI">
  <data key="d0">Responsible and Safe AI</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Approach emphasizing ethical development, deployment, and oversight of AI systems like Codex to maximize benefits and minimize harms.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Policy Engagement">
  <data key="d0">Policy Engagement</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Ongoing interaction with policymakers and experts to shape regulations and guidelines for AI and intellectual property.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Intellectual Property">
  <data key="d0">Intellectual Property</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Legal rights and protections related to creative works, including code, that are impacted by AI training and generation processes.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Development">
  <data key="d0">Model Development</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of designing, training, and fine-tuning AI models like Codex, involving large-scale data and compute resources.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Generation Risks">
  <data key="d0">Code Generation Risks</data>
  <data key="d1">Results</data>
  <data key="d2">Potential harms such as offensive content, insecure code, or malicious use, requiring mitigation strategies.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Content Filtering">
  <data key="d0">Content Filtering</data>
  <data key="d1">Tools</data>
  <data key="d2">Mechanisms to prevent harmful or insecure outputs from AI models, enhancing safety.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="User Review and Use Restrictions">
  <data key="d0">User Review and Use Restrictions</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Procedures to monitor and control the use of AI-generated code, especially in high-stakes or sensitive domains.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Monitoring and Rate Limiting">
  <data key="d0">Monitoring and Rate Limiting</data>
  <data key="d1">Tools</data>
  <data key="d2">Operational policies to prevent misuse, malicious activity, or overreliance on AI systems.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Program Interpreter">
  <data key="d0">Neural Program Interpreter</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A model designed to interpret and execute programs, utilizing neural network techniques, with studies by Reed &amp; de Freitas (2016), Shin et al. (2018), and Pierrot et al. (2021).&lt;SEP&gt;A neural model designed to interpret and execute code or programs, with studies by Reed &amp; de Freitas (2016), Shin et al. (2018), and Pierrot et al. (2021) exploring its capabilities.&lt;SEP&gt;A neural network architecture that interprets and executes programs, related to neural program learning approaches.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Memory Networks">
  <data key="d0">Memory Networks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Neural architectures that incorporate external memory components to enhance reasoning and contextual understanding for tasks like question answering.&lt;SEP&gt;Neural architectures that incorporate external memory components to improve reasoning over data sequences.&lt;SEP&gt;Neural architectures that incorporate memory components to enhance learning and reasoning, relevant to program induction.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Differentiable Neural Computer">
  <data key="d0">Differentiable Neural Computer</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A neural network architecture introduced by Graves et al. (2016), combining neural networks with external memory to perform complex data manipulations.&lt;SEP&gt;A neural network architecture introduced by Graves et al. (2016), designed to perform complex data manipulation tasks, combining neural networks with external memory components.&lt;SEP&gt;A neural network with external memory that allows learning algorithms and complex data manipulation, related to neural program learning.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Recurrent Neural Networks">
  <data key="d0">Recurrent Neural Networks</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Neural networks with recurrent connections, useful in sequence modeling and program induction.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deep Learning Resurgence">
  <data key="d0">Deep Learning Resurgence</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The renewed interest and advances in neural network-based AI, leading to progress in program learning and code generation.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training on Internet Data">
  <data key="d0">Training on Internet Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The datasets, including repositories like GitHub, used to train models like Codex, raising legal and ethical issues.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Inference">
  <data key="d0">Inference</data>
  <data key="d1">Variables</data>
  <data key="d2">The process of generating outputs from a trained model, which can involve significant compute and environmental costs, especially at scale.</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Utskever">
  <data key="d0">Utskever</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Utskever is referenced as a contributor to neural network research, likely related to neural computation or deep learning methodologies.&lt;SEP&gt;Utskever is referenced as a researcher contributing to neural network advancements, particularly in deep learning and neural computation.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Universal Transformer">
  <data key="d0">Universal Transformer</data>
  <data key="d1">Models/Techniques</data>
  <data key="d2">A transformer architecture with recurrence, improving sequence modeling, as discussed by Dehghani et al. (2019).&lt;SEP&gt;A transformer-based model that incorporates recurrence to improve performance on sequence modeling tasks, as discussed by Dehghani et al. (2019).</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Probabilistic Context-Free Grammar (PCFG)">
  <data key="d0">Probabilistic Context-Free Grammar (PCFG)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A classical formalism used to generate program syntax trees (ASTs) in program synthesis, enabling probabilistic modeling of program structures.&lt;SEP&gt;A formal grammar used in classical program synthesis to generate syntactic structures (ASTs) probabilistically.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AST (Abstract Syntax Tree)">
  <data key="d0">AST (Abstract Syntax Tree)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A tree representation of the syntactic structure of source code, used in program synthesis and code generation tasks.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Maddison &amp; Tarlow (2014)">
  <data key="d0">Maddison &amp; Tarlow (2014)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">Proposed improvements to program synthesis by learning a state vector to condition child node expansion in syntax trees.&lt;SEP&gt;Proposed learning a state vector to condition child node expansion in syntax trees for improved program generation.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Allamanis et al. (2015)">
  <data key="d0">Allamanis et al. (2015)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">Applied program synthesis ideas to text-to-code retrieval tasks, improving code search capabilities.&lt;SEP&gt;Applied syntax-based models to text-to-code retrieval, enhancing code search capabilities.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Yin &amp; Neubig (2017)">
  <data key="d0">Yin &amp; Neubig (2017)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">Utilized neural models for text-conditional code generation, enhancing the ability to generate code from natural language.&lt;SEP&gt;Utilized neural models for text-conditional code generation, improving natural language to code translation.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code2seq (Alon et al., 2018)">
  <data key="d0">Code2seq (Alon et al., 2018)</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural approach leveraging ASTs for code-to-text generation, translating code snippets into natural language descriptions.&lt;SEP&gt;Leveraged ASTs for code-to-text generation, translating code snippets into natural language descriptions.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="n-gram Language Models of Code">
  <data key="d0">n-gram Language Models of Code</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Hindle et al. (2012) investigated code predictability using n-gram models, showing code is more predictable than natural language.&lt;SEP&gt;Hindle et al. (2012) investigated predictability of code using n-gram models, finding code more predictable than natural language.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Latent Predictor Networks (Ling et al., 2016)">
  <data key="d0">Latent Predictor Networks (Ling et al., 2016)</data>
  <data key="d1">Tools</data>
  <data key="d2">Neural models generating code, such as for Magic: The Gathering cards, using character-level language modeling with latent modes.&lt;SEP&gt;Neural network models generating code for Magic: The Gathering cards, using character-level language modeling with latent modes.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="DeepCoder (Balog et al., 2017)">
  <data key="d0">DeepCoder (Balog et al., 2017)</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural system trained to predict functions in source code, guiding program search based on function predictions.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Large Scale Transformers for Program Synthesis">
  <data key="d0">Large Scale Transformers for Program Synthesis</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Application of transformer models trained on large datasets (e.g., Devlin et al., 2018; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) to generate code, leveraging natural language understanding.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeBERT (Feng et al., 2020)">
  <data key="d0">CodeBERT (Feng et al., 2020)</data>
  <data key="d1">Tools</data>
  <data key="d2">A transformer model trained on paired docstrings and code, achieving strong results in code search and understanding tasks.&lt;SEP&gt;Transformer-based model trained on paired docstrings and code, achieving strong results in code search and understanding.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="PyMT5 (Clement et al., 2020)">
  <data key="d0">PyMT5 (Clement et al., 2020)</data>
  <data key="d1">Tools</data>
  <data key="d2">A multilingual transformer model trained with the T5 objective to translate between code and natural language, enabling code understanding and generation.&lt;SEP&gt;Multilingual transformer trained with T5 objective to translate between code and natural language, enabling multilingual code understanding and translation.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="SPoC (Kulal et al., 2019)">
  <data key="d0">SPoC (Kulal et al., 2019)</data>
  <data key="d1">Methods/Models</data>
  <data key="d2">A system focusing on producing functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics.&lt;SEP&gt;System designed to produce functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="TransCoder (Lachaux et al., 2020)">
  <data key="d0">TransCoder (Lachaux et al., 2020)</data>
  <data key="d1">Tools</data>
  <data key="d2">A model trained to translate code between programming languages in an unsupervised manner, emphasizing functional correctness over BLEU scores.&lt;SEP&gt;Unsupervised code translation model that translates between programming languages, emphasizing functional correctness over BLEU scores.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ContraCode (Jain et al., 2020)">
  <data key="d0">ContraCode (Jain et al., 2020)</data>
  <data key="d1">Tools</data>
  <data key="d2">A contrastive learning approach leveraging the space of functionally correct programs to improve code models, especially for type inference.&lt;SEP&gt;Contrastive learning model leveraging the large space of functionally correct programs to improve model robustness and inference, especially for type inference.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Early Domain-Specific Datasets">
  <data key="d0">Early Domain-Specific Datasets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Datasets like FlashFill and Hearthstone used to benchmark neural program synthesis systems, focusing on specific tasks or games.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeSearchNet (Husain et al., 2019)">
  <data key="d0">CodeSearchNet (Husain et al., 2019)</data>
  <data key="d1">Datasets</data>
  <data key="d2">A large corpus of code from GitHub across multiple languages for benchmarking code understanding and synthesis models.&lt;SEP&gt;Large corpus of code from GitHub across multiple languages for training and evaluating code understanding and synthesis models.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeXGLUE (Lu et al., 2021)">
  <data key="d0">CodeXGLUE (Lu et al., 2021)</data>
  <data key="d1">Datasets</data>
  <data key="d2">A collection of programming benchmarks, including CodeBLEU for evaluating code generation quality.&lt;SEP&gt;Collection of programming benchmarks, including CodeBLEU, for evaluating code generation and translation tasks.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Tufano et al. (2020)">
  <data key="d0">Tufano et al. (2020)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">Applied transformers to generate unit tests for code, outperforming some commercial tools in testing capabilities.&lt;SEP&gt;Used transformers to generate unit tests for code, outperforming some commercial tools.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Aye et al. (2021)">
  <data key="d0">Aye et al. (2021)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">Developed an internal auto-complete system at Facebook, improving performance by training on user accepted completions.&lt;SEP&gt;Developed an internal auto-complete tool at Facebook, improving code completion performance by training on user accepted completions.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Static or Dynamic Code Analysis">
  <data key="d0">Static or Dynamic Code Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Traditional approaches to locate and fix bugs in code, involving running code against test suites to evaluate correctness and identify issues.&lt;SEP&gt;Traditional techniques for locating and fixing bugs through static or dynamic analysis, running code against test suites to verify correctness and identify issues.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Genetic Programming">
  <data key="d0">Genetic Programming</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A type of evolutionary algorithm that evolves computer programs to solve problems, used as a benchmark for comparing AI code generators.&lt;SEP&gt;An evolutionary algorithm that automatically evolves computer programs to solve specific problems, used here as a benchmark comparison for AI code generation tools.&lt;SEP&gt;An evolutionary approach to program synthesis and debugging, evolving code solutions through genetic algorithms.&lt;SEP&gt;Evolutionary algorithms used to automatically generate or optimize code, aiding in program synthesis and debugging.&lt;SEP&gt;Genetic programming is an evolutionary algorithm approach for automatic program synthesis, used as a benchmark to compare with AI code generation tools like Copilot.&lt;SEP&gt;Genetic programming is an evolutionary algorithm-based method for automatic program synthesis, used as a benchmark to compare with GitHub Copilot in code synthesis performance.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-ba9b42eee12401dc9fac455e631a8fd8&lt;SEP&gt;chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Abstract Syntax Tree (AST)">
  <data key="d0">Abstract Syntax Tree (AST)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A tree representation of program syntax used for analyzing and generating code structures in program synthesis.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="RobustFill (Devlin et al., 2017)">
  <data key="d0">RobustFill (Devlin et al., 2017)</data>
  <data key="d1">Tools</data>
  <data key="d2">Program synthesis system that samples multiple programs via beam search to find programs consistent with input-output examples.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Early Datasets">
  <data key="d0">Early Datasets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Domain-specific datasets like FlashFill and Hearthstone used to benchmark neural program synthesis systems, focusing on specific tasks or games.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="APPS Benchmark (Hendrycks et al., 2021)">
  <data key="d0">APPS Benchmark (Hendrycks et al., 2021)</data>
  <data key="d1">Datasets</data>
  <data key="d2">Benchmark for measuring functional correctness of code solutions from competitive programming problems like Codeforces.</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Korel &amp; Rilling, 1997">
  <data key="d0">Korel &amp; Rilling, 1997</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A foundational reference discussing learned association rules, which are patterns derived from data that help in understanding relationships in programming and AI.&lt;SEP&gt;A foundational reference that discusses learned association rules, indicating the importance of pattern recognition in programming and AI.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="learned association rules">
  <data key="d0">learned association rules</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A set of rules derived from data that identify relationships between variables, used in debugging and program analysis.&lt;SEP&gt;Rules learned from data that identify relationships and dependencies among variables, used in debugging and program analysis.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="genetic programming">
  <data key="d0">genetic programming</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An evolutionary algorithm technique that automatically generates or optimizes programs, applied here to debug faulty code.&lt;SEP&gt;An evolutionary algorithm-based approach to automatically generate or improve programs, used here for debugging faulty code.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="test suite">
  <data key="d0">test suite</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A collection of tests designed to evaluate the correctness and functionality of code, used to assess bug fixes.&lt;SEP&gt;A set of predefined tests used to evaluate the correctness and functionality of code, crucial in bug fixing processes.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="execution trace">
  <data key="d0">execution trace</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A record of program execution flow, used to analyze program behavior and identify bugs.&lt;SEP&gt;A record of the sequence of executed instructions in a program, used to analyze program behavior and locate bugs.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="search">
  <data key="d0">search</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process of exploring possible solutions or code modifications to find fixes for bugs.&lt;SEP&gt;A process of exploring possible solutions or modifications to find bug fixes or improvements in code.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="recent works (Tufano et al., 2019; Drain et al., 2021)">
  <data key="d0">recent works (Tufano et al., 2019; Drain et al., 2021)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research efforts that treat bug-fixing as neural machine translation, exploring AI-based automated debugging approaches.&lt;SEP&gt;Research studies that consider bug-fixing as neural machine translation, exploring AI-based approaches to debugging.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="neural machine translation">
  <data key="d0">neural machine translation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A deep learning approach that treats bug fixing as translating buggy code into correct code, applied to program repair.&lt;SEP&gt;A deep learning paradigm that translates buggy code into corrected code, used for automating bug fixes.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="exact match">
  <data key="d0">exact match</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A metric comparing generated code to a reference code snippet to evaluate correctness.&lt;SEP&gt;A metric that compares generated code to a reference code snippet to assess similarity and correctness.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="weak test suites">
  <data key="d0">weak test suites</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Limited coverage test collections that may fail to detect certain bugs or functionality issues in code.&lt;SEP&gt;Test collections with limited coverage that may fail to detect certain bugs or verify functional correctness.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="human developers">
  <data key="d0">human developers</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Practitioners who create and write targeted test suites, representing real-world testing practices.&lt;SEP&gt;Practitioners who write targeted test suites, highlighting real-world testing practices.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="natural language docstrings">
  <data key="d0">natural language docstrings</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Comments in code written in natural language, used as input data for training models to generate code or understand code semantics.&lt;SEP&gt;Descriptive comments in code written in natural language, used as input for training models to generate code.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="large language models">
  <data key="d0">large language models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Deep learning models trained on extensive code datasets to generate or complete code from natural language descriptions.&lt;SEP&gt;Deep learning models trained on extensive code datasets to generate, complete, or understand code based on natural language inputs.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GPT">
  <data key="d0">GPT</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A transformer-based language model fine-tuned on code to produce functionally correct code bodies from natural language.&lt;SEP&gt;A transformer-based large language model fine-tuned on code repositories to produce functionally correct code from natural language descriptions.&lt;SEP&gt;GPT (Generative Pre-trained Transformer) is an autoregressive language model that generates the next word based on previous words, mapping sequences to vector representations for contextually relevant content generation.&lt;SEP&gt;GPT (Generative Pre-trained Transformer) is an autoregressive language model that predicts the next word based on previous words, generating contextually relevant content.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83&lt;SEP&gt;chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="training on code from GitHub">
  <data key="d0">training on code from GitHub</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of fine-tuning GPT models using publicly available code repositories to enhance code generation capabilities.&lt;SEP&gt;The process of fine-tuning GPT models using publicly available code repositories to improve code generation performance.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="dataset of human-written problems">
  <data key="d0">dataset of human-written problems</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of programming problems with difficulty levels used to evaluate model performance.&lt;SEP&gt;A collection of programming problems with varying difficulty levels used to evaluate model performance and generalization.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="reverse task of producing docstrings from code bodies">
  <data key="d0">reverse task of producing docstrings from code bodies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A model training task where the goal is to generate descriptive comments from code, demonstrating bidirectional understanding.&lt;SEP&gt;A training task where models generate descriptive comments from code, demonstrating bidirectional understanding of code and natural language.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="broader impacts of code generating models">
  <data key="d0">broader impacts of code generating models</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Discussion of how automated code generation affects software development, automation, and potential societal or ethical considerations.&lt;SEP&gt;Discussion on how code generation models affect software development, automation, and potential limitations.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model limitations">
  <data key="d0">model limitations</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current shortcomings of large language models in code generation, including issues with correctness, reliability, and scope for significant improvement.&lt;SEP&gt;Recognized shortcomings of current models, indicating significant room for improvement in code correctness, reliability, and generalization.&lt;SEP&gt;Limitations include lack of publicly available training datasets for some models and potential biases from proprietary training data.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="2019.Alon, U., Brody, S., Levy, O., and Yahav, E.">
  <data key="d0">2019.Alon, U., Brody, S., Levy, O., and Yahav, E.</data>
  <data key="d1">Research Study</data>
  <data key="d2">A publication presenting code2seq, a method for generating sequences from structured code representations, presented at the International Conference on Learning Representations in 2018.&lt;SEP&gt;A publication presenting code2seq, a method for generating sequences from structured representations of code, presented at the International Conference on Learning Representations in 2018.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="code2seq">
  <data key="d0">code2seq</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A method for generating sequences from structured representations of code, used to improve code understanding and generation.&lt;SEP&gt;A technique for generating sequences from structured code representations, used to improve understanding and generation of code.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Aye, G. A., Kim, S., and Li, H.">
  <data key="d0">Aye, G. A., Kim, S., and Li, H.</data>
  <data key="d1">Research Study</data>
  <data key="d2">A study on learning autocompletion from real-world datasets, presented at IEEE/ACM 43rd International Conference on Software Engineering in 2021.&lt;SEP&gt;Research on learning autocompletion from real-world datasets, presented at IEEE/ACM 43rd International Conference on Software Engineering in 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Learning autocompletion">
  <data key="d0">Learning autocompletion</data>
  <data key="d1">Research Question/Hypothesis</data>
  <data key="d2">How models can learn to predict code autocompletions based on real-world datasets.&lt;SEP&gt;Investigate how models can learn to predict code autocompletions based on real-world datasets.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Baevski, A., Zhou, H., Mohamed, A., and Auli, M.">
  <data key="d0">Baevski, A., Zhou, H., Mohamed, A., and Auli, M.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Introduction of wav2vec 2.0, a framework for self-supervised learning of speech representations, arXiv preprint in 2020.&lt;SEP&gt;Introduction of wav2vec 2.0, a framework for self-supervised learning of speech representations, published as arXiv preprint in 2020.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="wav2vec 2.0">
  <data key="d0">wav2vec 2.0</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A self-supervised learning framework for speech representations that enables improved speech processing tasks.&lt;SEP&gt;A self-supervised learning framework for speech representations that improves speech understanding and processing.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D.">
  <data key="d0">Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Deepcoder: Learning to write programs, presented at ICLR in 2017.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deepcoder">
  <data key="d0">Deepcoder</data>
  <data key="d1">Methodology</data>
  <data key="d2">A neural network system that learns to generate code snippets and write programs, enabling automatic program synthesis.&lt;SEP&gt;A neural network-based system that learns to generate code snippets and write programs.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bao, H., Dong, L., and Wei, F.">
  <data key="d0">Bao, H., Dong, L., and Wei, F.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Beit: BERT pre-training of image transformers, arXiv preprint in 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Beit">
  <data key="d0">Beit</data>
  <data key="d1">Core Concept</data>
  <data key="d2">An image transformer model pre-trained using BERT-style masked image modeling for improved vision tasks.&lt;SEP&gt;An image transformer pre-trained using BERT-style masked image modeling for vision applications.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Barone, A. V. M. and Sennrich, R.">
  <data key="d0">Barone, A. V. M. and Sennrich, R.</data>
  <data key="d1">Research Study</data>
  <data key="d2">A parallel corpus of Python functions and documentation strings for automated code documentation and code generation, arXiv abs/1707.02275 in 2017.&lt;SEP&gt;A parallel corpus of Python functions and documentation strings for automated code documentation and generation, arXiv abs/1707.02275 in 2017.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Barrington, I. M. and Maciel, A.">
  <data key="d0">Barrington, I. M. and Maciel, A.</data>
  <data key="d1">Study Design</data>
  <data key="d2">Lecture notes on nondeterministic computation, accessed online in 2000.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S.">
  <data key="d0">Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Discussion on the dangers of stochastic parrots and the potential risks of large language models, presented at ACM Conference in 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.">
  <data key="d0">Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Introduction of GPT-Neo, a large-scale autoregressive language model using mesh-tensorflow, 2021.&lt;SEP&gt;Introduction of GPT-Neo, a large-scale autoregressive language model utilizing mesh-tensorflow, 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H.">
  <data key="d0">Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Critical survey of bias in NLP, arXiv preprint in 2020.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.">
  <data key="d0">Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Introduction of GPT-3, a large language model demonstrating few-shot learning capabilities, published as arXiv preprint in 2020.&lt;SEP&gt;Introduction of GPT-3, a state-of-the-art language model demonstrating few-shot learning, published as arXiv preprint in 2020.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bureau of Labor Statistics, U. D. o. L.">
  <data key="d0">Bureau of Labor Statistics, U. D. o. L.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Occupational Outlook Handbook entries for computer programmers and software developers, 2021.&lt;SEP&gt;Occupational data for computer programmers and software developers, 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C.">
  <data key="d0">Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Research on extracting training data from large language models, presented at USENIX Security Symposium 2021.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.">
  <data key="d0">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Generative pretraining from pixels, presented at ICML 2020.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Child, R., Gray, S., Radford, A., and Sutskever, I.">
  <data key="d0">Child, R., Gray, S., Radford, A., and Sutskever, I.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Generating long sequences with sparse transformers, arXiv in 2019.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Christiano, P.">
  <data key="d0">Christiano, P.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Clarification of AI alignment concepts, published on AI Alignment Forum in 2018.&lt;SEP&gt;Clarifying 'AI alignment', published on AI Alignment Forum in 2018.&lt;SEP&gt;Researcher focusing on AI alignment, clarifying its principles and challenges to ensure AI systems behave as intended.&lt;SEP&gt;Researcher focusing on AI alignment, clarifying its principles and challenges.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e&lt;SEP&gt;chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Clarkson, M. R., Finkbeiner">
  <data key="d0">Clarkson, M. R., Finkbeiner</data>
  <data key="d1">Study Design</data>
  <data key="d2">Lecture notes on nondeterministic computation, accessed online in 2000.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Parallel Corpus">
  <data key="d0">Parallel Corpus</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset of paired Python functions and documentation strings used for training and evaluation of code generation models.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Nondeterministic Computation">
  <data key="d0">Nondeterministic Computation</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A theoretical computation model where multiple outcomes are possible, foundational in complexity theory.&lt;SEP&gt;A theoretical framework describing computational models where multiple outcomes are possible, foundational in complexity theory.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Stochastic Parrots">
  <data key="d0">Stochastic Parrots</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A metaphor and critique highlighting the risks of large language models producing biased or ungrounded outputs.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in NLP">
  <data key="d0">Bias in NLP</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The study examines societal biases embedded in natural language processing models and their societal impacts.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Occupational Outlook">
  <data key="d0">Occupational Outlook</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Official statistical reports providing job outlooks, employment projections, and occupational descriptions.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training Data Extraction">
  <data key="d0">Training Data Extraction</data>
  <data key="d1">Methodology</data>
  <data key="d2">Techniques and methods used to analyze and extract training data from large models, relevant for privacy and security.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generative Pretraining from Pixels">
  <data key="d0">Generative Pretraining from Pixels</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A method for training models to generate images directly from pixel data, advancing generative modeling.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sparse Transformers">
  <data key="d0">Sparse Transformers</data>
  <data key="d1">Methodology</data>
  <data key="d2">An efficient transformer architecture designed for generating long sequences with reduced computational cost.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AI Alignment">
  <data key="d0">AI Alignment</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The field focused on ensuring AI systems' behaviors align with human values and intentions.</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="S. Radford, A., and Sutskever, I.">
  <data key="d0">S. Radford, A., and Sutskever, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Authors of a 2019 paper on generating long sequences with sparse transformers, contributing to understanding of sequence generation techniques.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generating long sequences with sparse transformers">
  <data key="d0">Generating long sequences with sparse transformers</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A model architecture designed to efficiently generate long sequences by using sparse attention mechanisms, advancing transformer models.&lt;SEP&gt;A research paper proposing a model for sequence generation using sparse transformer architectures.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AI Alignment Forum">
  <data key="d0">AI Alignment Forum</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An online platform dedicated to discussions and clarifications about AI alignment.&lt;SEP&gt;An online platform dedicated to discussions, research, and dissemination of ideas related to AI alignment and safety.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Clarifying 'AI alignment'">
  <data key="d0">Clarifying 'AI alignment'</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A specific inquiry aimed at elucidating the concept and issues surrounding AI alignment.&lt;SEP&gt;A specific inquiry aimed at elucidating the concept of AI alignment, its challenges, and solutions.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Clarkson, M. R., Finkbeiner, B., Koleini, M., Micinski, K. K., Rabe, M. N., and Sánchez, C.">
  <data key="d0">Clarkson, M. R., Finkbeiner, B., Koleini, M., Micinski, K. K., Rabe, M. N., and Sánchez, C.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Authors of a 2014 conference paper on temporal logics for hyperproperties, contributing to security and trust models.&lt;SEP&gt;Researchers who developed formal methods using temporal logics for specifying hyperproperties, relevant to security and trust in systems.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Temporal logics for hyperproperties">
  <data key="d0">Temporal logics for hyperproperties</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A formal framework that extends temporal logic to specify and reason about hyperproperties in security.&lt;SEP&gt;A formal logic framework that extends temporal logic to specify and reason about hyperproperties, particularly in security and privacy contexts.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Clement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N.">
  <data key="d0">Clement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers involved in developing Pymt5, a multi-mode translation system for natural language and Python code using transformers.&lt;SEP&gt;Researchers who developed Pymt5, a multi-mode translation system that leverages transformers to convert natural language and Python code in multiple modes.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pymt5">
  <data key="d0">Pymt5</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A multi-mode translation system that leverages transformer models to convert natural language to Python code and vice versa.&lt;SEP&gt;A transformer-based system enabling multi-mode translation between natural language and Python code, facilitating code generation and understanding.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Crawford, K.">
  <data key="d0">Crawford, K.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher known for addressing bias in AI, critiquing societal and political impacts of AI technologies.&lt;SEP&gt;Researcher known for discussing bias in AI, authoring works on the societal and political impacts of AI technologies.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The trouble with bias">
  <data key="d0">The trouble with bias</data>
  <data key="d1">Results</data>
  <data key="d2">A keynote addressing issues of bias in AI systems, highlighting societal implications.&lt;SEP&gt;A keynote speech discussing the societal and ethical issues related to bias in AI systems, emphasizing the importance of fairness.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Atlas of AI">
  <data key="d0">Atlas of AI</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A book exploring the power, politics, and environmental costs associated with artificial intelligence.&lt;SEP&gt;A comprehensive book analyzing the societal power, political influence, and environmental costs associated with artificial intelligence.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dai, A. M. and Le, Q. V.">
  <data key="d0">Dai, A. M. and Le, Q. V.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who developed semi-supervised sequence learning methods for neural networks.&lt;SEP&gt;Researchers who proposed semi-supervised sequence learning methods to improve neural network training efficiency with limited labeled data.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Semi-supervised sequence learning">
  <data key="d0">Semi-supervised sequence learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A machine learning approach combining labeled and unlabeled data to enhance sequence modeling performance.&lt;SEP&gt;A training approach that combines labeled and unlabeled data to improve sequence model performance.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., Parikh, D., and Batra, D.">
  <data key="d0">Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., Parikh, D., and Batra, D.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who developed visual dialog systems that combine computer vision and natural language understanding for interactive AI.&lt;SEP&gt;Researchers who developed visual dialog systems, integrating vision and language understanding.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Visual dialog">
  <data key="d0">Visual dialog</data>
  <data key="d1">Study Designs</data>
  <data key="d2">An AI system capable of understanding and generating dialogue about visual content, integrating vision and language modalities.&lt;SEP&gt;An AI system designed to understand and generate dialogue about visual content, combining computer vision and NLP.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Davis, B.">
  <data key="d0">Davis, B.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Expert in application security focusing on automated software diversity for protecting applications.&lt;SEP&gt;Expert in application security, focusing on protecting software applications through automated diversity techniques.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Protecting applications with automated software diversity">
  <data key="d0">Protecting applications with automated software diversity</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A security technique that enhances application resilience by automatically generating diverse program variants to prevent attacks.&lt;SEP&gt;A technique to enhance software security by automatically generating diverse program variants.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Łukasz Kaiser">
  <data key="d0">Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Łukasz Kaiser</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who proposed Universal Transformers, an extension of the Transformer model with shared parameters across layers.&lt;SEP&gt;Researchers who proposed Universal Transformers, an extension of the standard transformer architecture to improve model flexibility and performance.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Universal transformers">
  <data key="d0">Universal transformers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An advanced transformer architecture that improves model flexibility and efficiency by sharing weights across layers.&lt;SEP&gt;An advanced transformer architecture that shares parameters across layers to enable dynamic computation and better generalization in NLP tasks.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A., and Kohli, P.">
  <data key="d0">Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A., and Kohli, P.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers behind BERT, a pre-trained deep bidirectional transformer model for language understanding.&lt;SEP&gt;Researchers who developed BERT, a deeply bidirectional transformer model pre-trained for a wide range of language understanding tasks.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="BERT">
  <data key="d0">BERT</data>
  <data key="d1">Tools</data>
  <data key="d2">A pre-trained language model that captures deep bidirectional context, widely used in NLP for various downstream tasks.&lt;SEP&gt;A pre-training model that enables deep bidirectional understanding of language, widely used in NLP tasks.&lt;SEP&gt;A transformer-based encoder used in DPR to produce dense representations of documents and queries, essential for the retrieval process.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b&lt;SEP&gt;chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I.">
  <data key="d0">Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who developed Jukebox, a generative model capable of producing music with long-term coherence and diversity.&lt;SEP&gt;Researchers who developed Jukebox, a generative model for music production.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jukebox">
  <data key="d0">Jukebox</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural network model capable of generating music with long-term coherence, based on generative modeling techniques.&lt;SEP&gt;A neural network model for generating music, capable of producing high-fidelity audio across genres and styles.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Drain, D., Wu, C., Svyatkovskiy, A., and Sundaresan, N.">
  <data key="d0">Drain, D., Wu, C., Svyatkovskiy, A., and Sundaresan, N.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who applied pretrained transformers to generate bug-fixes automatically, advancing automated software maintenance.&lt;SEP&gt;Researchers who worked on generating bug-fixes using pretrained transformers, advancing automated program repair.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generating bug-fixes using pretrained transformers">
  <data key="d0">Generating bug-fixes using pretrained transformers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A machine learning approach that uses transformer models to automatically generate patches for software bugs, improving developer productivity.&lt;SEP&gt;An approach applying transformer models to automatically generate patches for software bugs.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Eghbal, N.">
  <data key="d0">Eghbal, N.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Author and researcher who wrote about the practices, challenges, and sustainability of open source software development.&lt;SEP&gt;Author of a book on open source software, discussing practices of working and maintaining open source projects.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Working in public: the making and maintenance of open source software">
  <data key="d0">Working in public: the making and maintenance of open source software</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A book analyzing the culture, processes, and challenges of open source software development.&lt;SEP&gt;A book analyzing the social dynamics, community practices, and economic aspects of open source software projects.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al.">
  <data key="d0">Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who created CodeBERT, a pre-trained model for programming and natural language understanding.&lt;SEP&gt;Researchers who created CodeBERT, a transformer-based model for understanding code and natural language, facilitating code search, generation, and comprehension.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeBERT">
  <data key="d0">CodeBERT</data>
  <data key="d1">Tools</data>
  <data key="d2">A pre-trained model designed to understand programming languages and natural language, enabling improved code-related NLP applications.&lt;SEP&gt;A transformer-based model designed to understand code and natural language, facilitating code-related NLP tasks.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Frey, C. B.">
  <data key="d0">Frey, C. B.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher known for analyzing the societal impacts and technological traps associated with innovation.&lt;SEP&gt;Researcher who analyzes societal impacts of technological innovation, emphasizing potential traps and unintended consequences.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The technology trap">
  <data key="d0">The technology trap</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A concept describing how technological advancements can entrap societies into certain paths of development.&lt;SEP&gt;A conceptual framework describing how technological progress can lead societies into dependency or adverse paths, potentially limiting future options.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.">
  <data key="d0">Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who compiled The Pile, an 800GB dataset of diverse text sources used to train large-scale language models.&lt;SEP&gt;Researchers who compiled The Pile, an extensive dataset of diverse text for training language models.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The Pile">
  <data key="d0">The Pile</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A large, diverse dataset consisting of various text sources designed for training and evaluating language models.&lt;SEP&gt;An 800GB dataset comprising diverse text sources used for large-scale language model training.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Goldblum, M., Tsipras, D., Xie, C., Chen, X., Schwarzschild, A., Song, D., Madry, A., Li, B., and Goldstein, T.">
  <data key="d0">Goldblum, M., Tsipras, D., Xie, C., Chen, X., Schwarzschild, A., Song, D., Madry, A., Li, B., and Goldstein, T.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers investigating security vulnerabilities in machine learning datasets, including data poisoning, backdoor attacks, and defenses.&lt;SEP&gt;Researchers studying data poisoning, backdoor attacks, and defenses in machine learning security.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dataset security for machine learning">
  <data key="d0">Dataset security for machine learning</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An area focused on protecting datasets against malicious manipulations such as poisoning, backdoors, and ensuring data integrity.&lt;SEP&gt;An area focusing on safeguarding datasets against malicious attacks such as poisoning and backdoors.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Goues, C. L., Dewey-Vogt, M., Forrest, S., and Weimer, W.">
  <data key="d0">Goues, C. L., Dewey-Vogt, M., Forrest, S., and Weimer, W.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who conducted a systematic study on automated program repair, fixing numerous bugs efficiently.&lt;SEP&gt;Researchers who systematically studied automated program repair, achieving high bug-fix success rates at low cost.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automated program repair">
  <data key="d0">Automated program repair</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A systematic approach to automatically identify and fix software bugs, demonstrated by fixing 55 out of 105 bugs.&lt;SEP&gt;A systematic methodology for automatically diagnosing and fixing software bugs, demonstrating effectiveness on real-world bugs.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Graves, A.">
  <data key="d0">Graves, A.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A researcher renowned for work on neural networks, sequence generation, and neural architectures, contributing foundational models for AI.&lt;SEP&gt;Graves, A. is a researcher known for work on neural networks, sequence generation, and neural architectures.&lt;SEP&gt;Researcher known for sequence generation techniques using recurrent neural networks and neural Turing machines, foundational to neural sequence modeling.&lt;SEP&gt;Researcher known for sequence generation with recurrent neural networks and neural Turing machines.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21&lt;SEP&gt;chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generating sequences with recurrent neural networks">
  <data key="d0">Generating sequences with recurrent neural networks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology demonstrating how recurrent neural networks can generate sequential data, illustrating sequence modeling capabilities.&lt;SEP&gt;A methodology for sequence generation using recurrent neural networks, demonstrating how neural models can produce sequential data.&lt;SEP&gt;A technique employing RNNs to produce coherent sequences, foundational for language modeling and other sequence tasks.&lt;SEP&gt;A technique for sequence modeling using RNNs to produce coherent sequences.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21&lt;SEP&gt;chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Graves, A., Wayne, G., and Danihelka, I.">
  <data key="d0">Graves, A., Wayne, G., and Danihelka, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who introduced Neural Turing Machines, augmenting neural networks with external memory.&lt;SEP&gt;Researchers who introduced Neural Turing Machines, integrating external memory with neural networks to perform complex algorithmic tasks.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Turing Machines">
  <data key="d0">Neural Turing Machines</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural network architecture that combines RNNs with external memory modules, enabling learning of algorithmic behaviors.&lt;SEP&gt;A neural network architecture that combines RNNs with external memory to enable complex algorithmic tasks.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="S., Radford, A., and Sutskever, I.">
  <data key="d0">S., Radford, A., and Sutskever, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Authors of a 2019 paper on generating long sequences with sparse transformers, contributing to sequence modeling and deep learning techniques.</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="2012 34th International Conference on Software Engineering (ICSE)">
  <data key="d0">2012 34th International Conference on Software Engineering (ICSE)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">An international conference focused on software engineering research, presenting papers on methodologies, tools, and advancements in the field.&lt;SEP&gt;The 2012 ICSE is an international conference focused on software engineering research, presenting papers and findings related to software development methodologies, tools, and best practices.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural turing machines">
  <data key="d0">Neural turing machines</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural network architecture with external memory, enabling neural networks to perform complex algorithmic tasks by reading and writing to memory modules.&lt;SEP&gt;A neural network model with external memory, enabling neural networks to perform complex algorithmic tasks by reading and writing to memory.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hybrid computing using a neural network with dynamic external memory">
  <data key="d0">Hybrid computing using a neural network with dynamic external memory</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A model combining neural networks with dynamic external memory to perform computation, as described in Nature.&lt;SEP&gt;A model integrating neural networks with external dynamic memory to perform computation, as detailed in Nature, facilitating advanced problem-solving.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Gulwani, S.">
  <data key="d0">Gulwani, S.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A researcher specializing in automating string processing and data manipulation in spreadsheets through program synthesis techniques.&lt;SEP&gt;Gulwani is a researcher specializing in automating string processing and data manipulation in spreadsheets through program synthesis.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automating string processing in spreadsheets using input-output examples">
  <data key="d0">Automating string processing in spreadsheets using input-output examples</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that leverages input-output examples to automatically synthesize string-processing programs, streamlining data tasks.&lt;SEP&gt;A technique that uses input-output examples to automatically generate string-processing programs in spreadsheets.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Spreadsheet data manipulation using examples">
  <data key="d0">Spreadsheet data manipulation using examples</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An approach that automates data manipulation in spreadsheets by learning from examples, simplifying complex transformations.&lt;SEP&gt;An approach to automate data manipulation tasks in spreadsheets through examples, simplifying complex data transformations.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="He, P., Liu, X., Gao, J., and Chen, W.">
  <data key="d0">He, P., Liu, X., Gao, J., and Chen, W.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers involved in developing DeBERTa, a language model with disentangled attention mechanisms to enhance natural language understanding and decoding.&lt;SEP&gt;Researchers involved in developing DeBERTa, a language model with disentangled attention mechanisms to improve natural language understanding.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deberta: Decoding-enhanced bert with disentangled attention">
  <data key="d0">Deberta: Decoding-enhanced bert with disentangled attention</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A transformer-based language model that enhances BERT with disentangled attention to improve decoding and language comprehension.&lt;SEP&gt;A transformer-based language model that improves upon BERT by incorporating disentangled attention, leading to better decoding and language comprehension.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Helmuth, T. and Spector, L.">
  <data key="d0">Helmuth, T. and Spector, L.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers who created a benchmark suite for general program synthesis, enabling standardized evaluation of synthesis algorithms across tasks.&lt;SEP&gt;Researchers who developed a benchmark suite for general program synthesis to evaluate the performance of different synthesis algorithms.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="General program synthesis benchmark suite">
  <data key="d0">General program synthesis benchmark suite</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A comprehensive benchmark including tasks and metrics to evaluate and compare program synthesis approaches systematically.&lt;SEP&gt;A standardized set of tasks and metrics for evaluating program synthesis approaches across various domains.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hendrycks, D., et al.">
  <data key="d0">Hendrycks, D., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers measuring the competence of coding challenge solutions through apps, assessing software development skills in practical settings.&lt;SEP&gt;Researchers measuring the competence of coding challenge solutions using apps, assessing software development skills through mobile applications.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Measuring coding challenge competence with apps">
  <data key="d0">Measuring coding challenge competence with apps</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology for evaluating programming skills and challenge performance via dedicated mobile applications, enabling large-scale assessment.&lt;SEP&gt;A methodology for evaluating software coding skills and challenge performance via dedicated mobile apps.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hindle, A., Barr, E. T., Su, Z., Gabel, M., and Devanbu, P.">
  <data key="d0">Hindle, A., Barr, E. T., Su, Z., Gabel, M., and Devanbu, P.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers exploring the naturalness of software, analyzing how similar code is to natural language and patterns in codebases.&lt;SEP&gt;Researchers investigating the naturalness of software, analyzing how code resembles natural language and patterns in large codebases.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="On the naturalness of software">
  <data key="d0">On the naturalness of software</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A theory proposing that software exhibits properties similar to natural language, which can be exploited for better code understanding and generation.&lt;SEP&gt;A theory that software code exhibits natural language-like properties, which can be leveraged for better code understanding and generation.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Holtzman, A., et al.">
  <data key="d0">Holtzman, A., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers studying neural text degeneration, examining issues like repetitive or incoherent text generation in neural language models.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The curious case of neural text degeneration">
  <data key="d0">The curious case of neural text degeneration</data>
  <data key="d1">Results</data>
  <data key="d2">An analysis identifying causes of neural text degeneration and proposing potential solutions to improve language model outputs.&lt;SEP&gt;An analysis of problems in neural text generation, including causes and potential solutions to improve output quality.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M.">
  <data key="d0">Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers evaluating semantic code search systems to improve code retrieval and understanding using machine learning models.&lt;SEP&gt;Researchers evaluating semantic code search systems, assessing how well models retrieve and understand code semantically.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Codesearchnet challenge: Evaluating the state of semantic code search">
  <data key="d0">Codesearchnet challenge: Evaluating the state of semantic code search</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A benchmark challenge designed to assess the effectiveness of semantic code search techniques across various models and datasets.&lt;SEP&gt;A benchmark challenge designed to evaluate the effectiveness of semantic code search systems across multiple models and datasets.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J., and Stoica, I.">
  <data key="d0">Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J., and Stoica, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers focusing on contrastive learning approaches to improve code representation and similarity detection in large codebases.&lt;SEP&gt;Researchers working on contrastive learning approaches to improve code representation, similarity detection, and retrieval in large codebases.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Contrastive code representation learning">
  <data key="d0">Contrastive code representation learning</data>
  <data key="d1">Methods</data>
  <data key="d2">A technique that learns to represent code snippets by contrasting similar and dissimilar pairs, enhancing code understanding and retrieval.&lt;SEP&gt;A technique that trains models to distinguish between similar and dissimilar code snippets, enhancing code understanding and retrieval capabilities.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jeffrey, D., Feng, M., Gupta, N., and Gupta, R.">
  <data key="d0">Jeffrey, D., Feng, M., Gupta, N., and Gupta, R.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers developing Bugfix, a learning-based tool designed to assist developers in automatically fixing bugs in code.&lt;SEP&gt;Researchers developing Bugfix, a learning-based tool that assists developers by automatically suggesting bug fixes through machine learning.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bugfix: A learning-based tool to assist developers in fixing bugs">
  <data key="d0">Bugfix: A learning-based tool to assist developers in fixing bugs</data>
  <data key="d1">Tools</data>
  <data key="d2">An automated system leveraging machine learning to generate bug fixes, streamlining debugging and maintenance workflows.&lt;SEP&gt;An automated system that uses machine learning to suggest bug fixes, improving debugging efficiency.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jones, C. and Bonsignour, O.">
  <data key="d0">Jones, C. and Bonsignour, O.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Authors analyzing the economics of software quality, including costs, benefits, and trade-offs associated with software development and maintenance.&lt;SEP&gt;Authors analyzing the economics of software quality, including costs, benefits, and trade-offs associated with software development, testing, and maintenance.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The economics of software quality">
  <data key="d0">The economics of software quality</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A comprehensive analysis of financial impacts, defect costs, and quality improvements in software engineering.&lt;SEP&gt;A comprehensive analysis of the financial aspects related to software quality, including defect costs and quality improvements.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kaiser, Ł. and Sutskever, I.">
  <data key="d0">Kaiser, Ł. and Sutskever, I.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers demonstrating neural GPUs capable of learning algorithms, enabling neural networks to perform complex algorithmic tasks and reasoning.&lt;SEP&gt;Researchers demonstrating neural GPUs that learn algorithms, enabling neural networks to perform complex computational tasks.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural gpus learn algorithms">
  <data key="d0">Neural gpus learn algorithms</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A model where neural networks are trained to execute algorithms, demonstrating the capacity for neural computation of algorithmic processes.&lt;SEP&gt;A model where neural networks are trained to learn and execute algorithms, demonstrating the capacity to perform algorithmic reasoning.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kaplan, J., et al.">
  <data key="d0">Kaplan, J., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers proposing scaling laws for neural language models, providing insights into how model size impacts performance.&lt;SEP&gt;Researchers proposing scaling laws for neural language models, showing how model size and data influence performance and capabilities.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Scaling laws for neural language models">
  <data key="d0">Scaling laws for neural language models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Empirical relationships that describe how increasing model parameters and data size improve language model capabilities.&lt;SEP&gt;Empirical relationships that describe how increasing model parameters and training data improve language model performance.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kenton, Z., et al.">
  <data key="d0">Kenton, Z., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers working on aligning language agents, ensuring that AI systems behave according to intended goals and safety standards.&lt;SEP&gt;Researchers working on aligning language agents, focusing on safety, goal alignment, and reliable behavior of AI systems.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Alignment of language agents">
  <data key="d0">Alignment of language agents</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A hypothesis that proper alignment techniques can ensure AI agents behave safely and according to human values.&lt;SEP&gt;A hypothesis that proper alignment techniques can improve the safety and effectiveness of language-based AI agents.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Keskar, N. S., et al.">
  <data key="d0">Keskar, N. S., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Developers of Ctrl, a conditional transformer language model capable of controllable text generation based on input conditions.&lt;SEP&gt;Developers of Ctrl, a conditional transformer model enabling controllable text generation based on input conditions.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ctrl: A conditional transformer language model for controllable generation">
  <data key="d0">Ctrl: A conditional transformer language model for controllable generation</data>
  <data key="d1">Tools</data>
  <data key="d2">A language model that allows users to guide text generation by specifying control conditions, enabling targeted outputs.&lt;SEP&gt;A language model that allows users to specify conditions to control the style, content, or tone of generated text.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Korel, B. and Rilling, J.">
  <data key="d0">Korel, B. and Rilling, J.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers applying dynamic slicing techniques to program debugging, improving fault localization and program comprehension.&lt;SEP&gt;Researchers applying dynamic slicing techniques to program debugging, improving fault localization and understanding of program execution.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Application of dynamic slicing in program debugging">
  <data key="d0">Application of dynamic slicing in program debugging</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that analyzes execution slices of code to identify bugs and facilitate debugging processes.&lt;SEP&gt;A technique that analyzes program execution slices to identify and isolate bugs, facilitating debugging processes.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Koza, J. R., et al.">
  <data key="d0">Koza, J. R., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Authors pioneering genetic programming, an evolutionary algorithm that automatically generates programs and solutions inspired by biological evolution.&lt;SEP&gt;Authors pioneering genetic programming, an evolutionary algorithm-based approach for automatic problem solving and invention.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Genetic programming III: Darwinian invention and problem solving">
  <data key="d0">Genetic programming III: Darwinian invention and problem solving</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework for evolving programs and solutions through genetic algorithms, enabling automatic invention and problem solving.&lt;SEP&gt;A framework for solving problems through evolution-inspired algorithms that automatically generate solutions.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kulal, S., et al.">
  <data key="d0">Kulal, S., et al.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers developing Spoc, a search-based system that converts pseudocode into executable code, aiding program synthesis.&lt;SEP&gt;Researchers developing Spoc, a search-based system that converts pseudocode into executable code, assisting in program synthesis and code generation.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Spoc: Search-based pseudocode to code">
  <data key="d0">Spoc: Search-based pseudocode to code</data>
  <data key="d1">Tools</data>
  <data key="d2">A system that searches for code snippets matching pseudocode descriptions, facilitating automatic code synthesis from high-level specifications.&lt;SEP&gt;A system that searches for code snippets matching pseudocode descriptions, streamlining code generation from high-level specifications.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.">
  <data key="d0">Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers studying neural text degeneration, focusing on issues like repetitive, incoherent, or nonsensical text generation in neural language models.</data>
  <data key="d3">chunk-603d0ab6656c1e8cc11a81310ec387dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Information Processing Systems">
  <data key="d0">Neural Information Processing Systems</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A conference volume edited by Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch é-Buc, F., Fox, E., and Garnett, R., published in 2019, focusing on advancements in neural information processing technologies and methodologies.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="gvisor">
  <data key="d0">gvisor</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A sandboxed container runtime designed for secure and isolated container execution, open-sourced by Lacasse in 2018.&lt;SEP&gt;Gvisor is an open-source sandboxed container runtime designed to improve security and isolation in containerized environments, released by Lacasse in 2018.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Unsupervised translation of programming languages">
  <data key="d0">Unsupervised translation of programming languages</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates whether unsupervised learning methods can effectively translate programming languages without labeled datasets, as studied by Lachaux et al. in 2020.&lt;SEP&gt;The study investigates whether unsupervised learning techniques can effectively translate programming languages without labeled data, as presented by Lachaux et al. in 2020.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Risk Matrix">
  <data key="d0">Risk Matrix</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A standardized tool for assessing and managing risks, with improvements proposed by Leveson in 2019 to enhance safety and reliability.&lt;SEP&gt;An improved standard risk matrix designed to assess and manage risks more effectively, as discussed by Leveson in 2019.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Empirical Software Engineering">
  <data key="d0">Empirical Software Engineering</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A field applying empirical research methods to study software development practices, as exemplified by Li, Ko, and Begel in 2020.&lt;SEP&gt;A scientific discipline focused on studying software engineering practices through empirical methods, exemplified by Li, Ko, and Begel in 2020.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Latent predictor networks">
  <data key="d0">Latent predictor networks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A neural network architecture for code generation that models latent variables, introduced by Ling et al. in 2016.&lt;SEP&gt;A neural network architecture used for code generation tasks, introduced by Ling et al. in 2016, utilizing latent variables to predict code sequences.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Roberta">
  <data key="d0">Roberta</data>
  <data key="d1">Models/Theories/Models</data>
  <data key="d2">A pretraining approach for BERT that enhances robustness and performance in natural language understanding, developed by Liu et al. in 2019.&lt;SEP&gt;A robustly optimized BERT pretraining approach designed to enhance natural language understanding, developed by Liu et al. in 2019.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="VILBERT">
  <data key="d0">VILBERT</data>
  <data key="d1">Models/Theories/Models</data>
  <data key="d2">A multimodal pretraining model for vision and language understanding, proposed by Lu et al. in 2019.&lt;SEP&gt;Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, proposed by Lu et al. in 2019.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Codexglue">
  <data key="d0">Codexglue</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A benchmark dataset designed for machine learning models to evaluate code understanding and generation, introduced by Lu et al. in 2021.&lt;SEP&gt;A benchmark dataset for machine learning focused on code understanding and generation, created by Lu et al. in 2021.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Structured generative models of natural source code">
  <data key="d0">Structured generative models of natural source code</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Models that generate source code based on structured probabilistic approaches, discussed by Maddison and Tarlow in 2014.&lt;SEP&gt;Models that generate source code based on structured probabilistic frameworks, discussed by Maddison and Tarlow in 2014.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automatic program synthesis">
  <data key="d0">Automatic program synthesis</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Explores the feasibility of automatically generating source code from specifications or models, as examined by Manna and Waldinger in 1971.&lt;SEP&gt;Explores the possibility of automatically generating executable programs from specifications or high-level descriptions, studied by Manna and Waldinger in 1971.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Global data center energy-use estimates">
  <data key="d0">Global data center energy-use estimates</data>
  <data key="d1">Results</data>
  <data key="d2">Refined and recalibrated estimates of global energy consumption attributed to data centers, as presented by Masanet et al. in 2020.&lt;SEP&gt;Refined estimates of global energy consumption by data centers, recalibrated by Masanet et al. in 2020.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Cryptography">
  <data key="d0">Cryptography</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A field focused on secure communication, encryption, and data protection, detailed in the Handbook of Applied Cryptography by Menezes, van Oorschot, and Vanstone in 2018.&lt;SEP&gt;The study of secure communication techniques, detailed in the Handbook of Applied Cryptography by Menezes, van Oorschot, and Vanstone in 2018.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-fidelity image generation">
  <data key="d0">High-fidelity image generation</data>
  <data key="d1">Results</data>
  <data key="d2">Generation of high-quality images using subscale pixel networks and multidimensional upscaling, introduced by Menick and Kalchbrenner in 2018.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Distributed word representations">
  <data key="d0">Distributed word representations</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Distributed representations of words and phrases that capture their meanings and compositionality, developed by Mikolov et al. in 2013.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open source software supply chain attacks">
  <data key="d0">Open source software supply chain attacks</data>
  <data key="d1">Results</data>
  <data key="d2">A comprehensive review of security vulnerabilities and attack vectors in open source software supply chains, authored by Ohm et al. in 2020.&lt;SEP&gt;A review summarizing the types, techniques, and mitigation strategies for supply chain attacks on open source software, by Ohm et al. in 2020.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Intellectual property protection for AI">
  <data key="d0">Intellectual property protection for AI</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Discussion of legal and policy frameworks to safeguard AI innovations, addressed by O’Keefe et al. in 2019.&lt;SEP&gt;Discussion on policies and strategies to safeguard AI innovations through intellectual property rights, addressed by O’Keefe et al. in 2019.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="O*NET 15-1252.00">
  <data key="d0">O*NET 15-1252.00</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A classification and description of software developer roles, skills, and occupational data, published in 2021.&lt;SEP&gt;A standardized classification and description of software developers' roles and skills, available on O*NET in 2021.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pretraining of vision-and-language models">
  <data key="d0">Pretraining of vision-and-language models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for training models like Vilbert to understand and process both visual and linguistic information simultaneously, as described by Lu et al. in 2019.&lt;SEP&gt;Training techniques for models like VILBERT to understand both visual and linguistic data jointly, as described by Lu et al. in 2019.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch é-Buc, F., Fox, E., Garnett, R.">
  <data key="d0">Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch é-Buc, F., Fox, E., Garnett, R.</data>
  <data key="d1">Authors/Editors</data>
  <data key="d2">Editors of the 'Advances in Neural Information Processing Systems' volume, responsible for compiling research on neural information processing.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Advances in Neural Information Processing Systems">
  <data key="d0">Advances in Neural Information Processing Systems</data>
  <data key="d1">Discipline</data>
  <data key="d2">A conference proceedings volume published in 2019, focusing on recent advancements, methodologies, and research in neural information processing and machine learning.&lt;SEP&gt;A leading conference publishing cutting-edge research in neural networks and machine learning.&lt;SEP&gt;A leading conference publishing research on neural networks and machine learning techniques.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c&lt;SEP&gt;chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lacasse">
  <data key="d0">Lacasse</data>
  <data key="d1">Researchers/Developers</data>
  <data key="d2">Author of the 2018 work on open-sourcing gvisor, contributing to container security and virtualization technology.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lachaux, M.-A., Rozière, B., Chanussot, L., Lample, G.">
  <data key="d0">Lachaux, M.-A., Rozière, B., Chanussot, L., Lample, G.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2020 paper exploring unsupervised translation of programming languages using machine learning techniques.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Leveson, N.">
  <data key="d0">Leveson, N.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the 2019 work on improving the standard risk matrix, focusing on safety and risk assessment methodologies.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Li, P. L., Ko, A. J., Begel, A.">
  <data key="d0">Li, P. L., Ko, A. J., Begel, A.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a 2020 empirical study identifying traits of successful software engineers.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ling, W., Blunsom, P., Grefenstette, E., Hermann, K. M., Kočíský, T., Wang, F., Senior, A.">
  <data key="d0">Ling, W., Blunsom, P., Grefenstette, E., Hermann, K. M., Kočíský, T., Wang, F., Senior, A.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2016 paper on latent predictor networks for code generation.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.">
  <data key="d0">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the Roberta model, a significant advancement in NLP pretraining.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lu, J., Batra, D., Parikh, D., Lee, S.">
  <data key="d0">Lu, J., Batra, D., Parikh, D., Lee, S.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of VILBERT, a model for joint vision-and-language tasks.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., Liu, S.">
  <data key="d0">Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., Liu, S.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of Codexglue, a dataset for code understanding and generation tasks.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Maddison, C. J., Tarlow, D.">
  <data key="d0">Maddison, C. J., Tarlow, D.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2014 paper on structured generative models of source code.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Manna, Z., Waldinger, R. J.">
  <data key="d0">Manna, Z., Waldinger, R. J.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 1971 foundational work on program synthesis.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Masanet, E., Shehabi, A., Lei, N., Smith, S., Koomey, J.">
  <data key="d0">Masanet, E., Shehabi, A., Lei, N., Smith, S., Koomey, J.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2020 study on data center energy use.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Menezes, A., van Oorschot, P., Vanstone, S.">
  <data key="d0">Menezes, A., van Oorschot, P., Vanstone, S.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a comprehensive cryptography reference.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Menick, J., Kalchbrenner, N.">
  <data key="d0">Menick, J., Kalchbrenner, N.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2018 work on high-fidelity image synthesis.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Distributed representations of words and phrases">
  <data key="d0">Distributed representations of words and phrases</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Word embeddings that capture semantic and syntactic properties, developed by Mikolov et al. in 2013.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J.">
  <data key="d0">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the foundational work on distributed word representations.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ohm, M., Plate, H., Sykosch, A., Meier, M.">
  <data key="d0">Ohm, M., Plate, H., Sykosch, A., Meier, M.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the review on open source supply chain attacks.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="O’Keefe, C., Lansky, D., Clark, J., Payne, C.">
  <data key="d0">O’Keefe, C., Lansky, D., Clark, J., Payne, C.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors examining intellectual property issues related to AI.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="O*NET">
  <data key="d0">O*NET</data>
  <data key="d1">Tools</data>
  <data key="d2">A comprehensive occupational database providing detailed information about software developer roles and skills.&lt;SEP&gt;An extensive online database providing detailed occupational data, including skills, tasks, and employment statistics for various professions, including software developers.&lt;SEP&gt;An online database providing detailed descriptions of the skills, tasks, and work environments of various occupations, including software development.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628&lt;SEP&gt;chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lu et al.">
  <data key="d0">Lu et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the 2019 paper on pretraining multimodal models like VILBERT.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="15-1252.00">
  <data key="d0">15-1252.00</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The Standard Occupational Classification code for software developers, used to categorize and identify professionals in software development.&lt;SEP&gt;The Standard Occupational Classification code for software developers, used to identify and categorize professionals in the field.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.">
  <data key="d0">Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.</data>
  <data key="d1">Researcher(s)</data>
  <data key="d2">Authors of influential research on neural network architectures, generative models, and representation learning techniques in deep learning.&lt;SEP&gt;Authors of significant research papers on neural network models such as Wavenet, representation learning, and generative models for raw audio.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Wavenet">
  <data key="d0">Wavenet</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A generative neural network model designed for producing raw audio waveforms, capable of capturing complex temporal dependencies.&lt;SEP&gt;A neural network model designed for generating raw audio waveforms, capable of producing high-quality speech and audio content by modeling complex temporal dependencies.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Contrastive Predictive Coding">
  <data key="d0">Contrastive Predictive Coding</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A representation learning technique that learns useful features by predicting future data points in latent space, used in neural network training.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="O’Neill, M. and Spector, L.">
  <data key="d0">O’Neill, M. and Spector, L.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors discussing open issues in automatic programming and genetic programming, exploring the challenges and methodologies in evolving programs automatically.&lt;SEP&gt;Authors exploring open issues in automatic programming, including challenges in evolving programs and automating code generation processes.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Genetic Programming and Evolvable Machines">
  <data key="d0">Genetic Programming and Evolvable Machines</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A journal focusing on research related to automatic program evolution, genetic algorithms, and machine adaptability.&lt;SEP&gt;A scholarly journal publishing research on evolutionary algorithms, automatic program synthesis, and machine adaptability in software engineering.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pantridge, E., Helmuth, T., McPhee, N. F., and Spector, L.">
  <data key="d0">Pantridge, E., Helmuth, T., McPhee, N. F., and Spector, L.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors analyzing the difficulty of benchmarking inductive program synthesis methods, highlighting challenges in evaluating AI-generated code.&lt;SEP&gt;Authors analyzing the environmental impact of training large neural networks, emphasizing carbon footprint and sustainability issues.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Inductive Program Synthesis">
  <data key="d0">Inductive Program Synthesis</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The study investigates how to effectively evaluate and benchmark methods that automatically generate programs from specifications.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J.">
  <data key="d0">Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors examining the environmental impact of training large neural networks, emphasizing carbon emissions.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carbon Emissions from Neural Networks">
  <data key="d0">Carbon Emissions from Neural Networks</data>
  <data key="d1">Results</data>
  <data key="d2">Large-scale neural network training contributes significantly to carbon footprint, raising concerns about environmental sustainability.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L.">
  <data key="d0">Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors developing deep contextualized word representations, advancing natural language understanding.&lt;SEP&gt;Authors developing deep contextualized word representations, which improve natural language understanding.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deep Contextualized Word Embeddings">
  <data key="d0">Deep Contextualized Word Embeddings</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A technique for representing words in context, improving performance on various NLP tasks by capturing nuanced meanings.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N.">
  <data key="d0">Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors exploring learning neural programs with recursive tree search and planning, contributing to program synthesis methodologies.&lt;SEP&gt;Authors investigating neural program learning methods, including recursive tree search and planning strategies for program synthesis.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Program Synthesis">
  <data key="d0">Neural Program Synthesis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques combining neural networks and search algorithms to learn and generate complex programs, enabling automated code creation.&lt;SEP&gt;Techniques that combine neural networks with search algorithms to automatically generate, interpret, and execute programs.&lt;SEP&gt;The automated generation of computer programs using neural networks, often involving inferred execution traces to improve accuracy and reliability.&lt;SEP&gt;The process of automatically generating computer programs using neural networks, often involving inferred execution traces to improve performance.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628&lt;SEP&gt;chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Planning">
  <data key="d0">Planning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A strategic approach used in neural program learning to guide the generation process through structured search and planning algorithms.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The economic impacts of inadequate infrastructure for software testing">
  <data key="d0">The economic impacts of inadequate infrastructure for software testing</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A report analyzing how insufficient infrastructure hampers software testing efficiency and impacts economic productivity.&lt;SEP&gt;A report by the National Institute of Standards and Technology analyzing how poor infrastructure hampers software testing efficiency and economic outcomes.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python Software Foundation and JetBrains">
  <data key="d0">Python Software Foundation and JetBrains</data>
  <data key="d1">Organizations</data>
  <data key="d2">Organizations conducting surveys on Python developers to gather insights on usage, practices, and trends in Python programming.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python Developers Survey 2020">
  <data key="d0">Python Developers Survey 2020</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A comprehensive survey collecting data on Python developers' demographics, tool usage, and development practices.&lt;SEP&gt;A survey report providing data on Python developers' demographics, tools, and practices, informing research on software development trends.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Qi, Z., Long, F., Achour, S., and Rinard, M.">
  <data key="d0">Qi, Z., Long, F., Achour, S., and Rinard, M.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors analyzing the plausibility and correctness of generate-and-validate patch generation systems, contributing to software testing and repair methodologies.&lt;SEP&gt;Authors studying generate-and-validate patching systems, focusing on their plausibility, correctness, and effectiveness in software maintenance.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generate-and-Validate Patch Generation Systems">
  <data key="d0">Generate-and-Validate Patch Generation Systems</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Automated approaches for fixing code by generating patches and validating their correctness, used in software maintenance.&lt;SEP&gt;Automated approaches that generate patches for bugs and validate their correctness to improve software repair processes.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.">
  <data key="d0">Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors developing generative pre-training techniques for improving language understanding, foundational for large language models.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generative Pre-Training">
  <data key="d0">Generative Pre-Training</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach where models learn to generate language from large unlabeled datasets, enhancing NLP capabilities.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.">
  <data key="d0">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors demonstrating that language models can serve as unsupervised multitask learners, broadening AI applications.&lt;SEP&gt;Authors demonstrating that large language models can perform multiple NLP tasks in an unsupervised, multitask setting.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Unsupervised Multitask Learning">
  <data key="d0">Unsupervised Multitask Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A paradigm where models learn multiple tasks without explicit supervision, enabling transferability and generalization.&lt;SEP&gt;A paradigm where models learn to perform multiple tasks without explicit supervision, enabling transferability across tasks.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.">
  <data key="d0">Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors working on multi-modal models like CLIP, combining visual and language data for transfer learning.&lt;SEP&gt;Authors working on transfer learning and multi-modal models, including visual-text models like CLIP.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Transferable Visual Models">
  <data key="d0">Transferable Visual Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Models trained on large datasets to understand and generate visual concepts from natural language supervision.&lt;SEP&gt;Models trained on large-scale data to understand and generate visual information from natural language supervision.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I.">
  <data key="d0">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors developing zero-shot text-to-image generation models, enabling image creation from textual descriptions.&lt;SEP&gt;Authors pioneering zero-shot text-to-image generation, enabling models to generate images from textual descriptions.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Zero-Shot Text-to-Image Generation">
  <data key="d0">Zero-Shot Text-to-Image Generation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A method where models generate images based on textual prompts without task-specific training, expanding AI creative capabilities.&lt;SEP&gt;A method where models generate images based solely on textual prompts without task-specific training, expanding AI creativity.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Reed, S. and de Freitas, N.">
  <data key="d0">Reed, S. and de Freitas, N.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors developing neural programmer-interpreters, systems that learn to interpret and execute programs from data.&lt;SEP&gt;Authors proposing neural interpreters that interpret and execute programs learned from data, advancing program understanding.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Programmer-Interpreters">
  <data key="d0">Neural Programmer-Interpreters</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Models that combine neural networks with program interpretation capabilities to learn and execute code from data.&lt;SEP&gt;Models that combine neural networks with program interpretation capabilities, enabling automatic code understanding and execution.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S.">
  <data key="d0">Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors proposing CodeBLEU, an automatic evaluation metric for code synthesis quality.&lt;SEP&gt;Authors proposing CodeBLEU, an automatic metric for evaluating the quality and correctness of generated code.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeBLEU">
  <data key="d0">CodeBLEU</data>
  <data key="d1">Tools</data>
  <data key="d2">An automatic evaluation metric designed to assess the quality of generated code, facilitating improvements in code synthesis systems.&lt;SEP&gt;An automatic metric designed to evaluate the correctness and quality of generated code, facilitating improvements in code synthesis systems.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al.">
  <data key="d0">Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors studying biological structures and functions emerging from large-scale unsupervised learning of protein sequences.&lt;SEP&gt;Authors studying how large-scale unsupervised learning of protein sequences reveals biological structure and function.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Biological Structure and Function from Protein Sequences">
  <data key="d0">Biological Structure and Function from Protein Sequences</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Research analyzing how scaling unsupervised learning models to vast protein data reveals insights into biological structure and function.&lt;SEP&gt;Research analyzing the emergence of biological insights from large-scale unsupervised learning models trained on protein data.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sourcefinder">
  <data key="d0">Sourcefinder</data>
  <data key="d1">Tools</data>
  <data key="d2">A system designed to identify malware source code from publicly available repositories such as GitHub, aiding cybersecurity and malware analysis.&lt;SEP&gt;A system for identifying malware source code from publicly available repositories such as GitHub, aiding cybersecurity analysis.&lt;SEP&gt;A tool designed to find malware source code from repositories, utilizing analysis techniques to identify malicious code origins in GitHub repositories.&lt;SEP&gt;A tool designed to locate malware source code within repositories, facilitating malware analysis from publicly available sources like GitHub.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628&lt;SEP&gt;chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Representation Learning with Contrastive Predictive Coding">
  <data key="d0">Representation Learning with Contrastive Predictive Coding</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A self-supervised learning framework that enables models to learn useful data representations by predicting future latent variables, used in neural network training.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carbon Emissions from Large Neural Network Training">
  <data key="d0">Carbon Emissions from Large Neural Network Training</data>
  <data key="d1">Results</data>
  <data key="d2">Training state-of-the-art neural networks significantly increases carbon emissions, raising environmental sustainability concerns.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deep Contextualized Word Representations">
  <data key="d0">Deep Contextualized Word Representations</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Advanced word embedding techniques that capture context-dependent meanings, significantly enhancing NLP task performance.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Planning in Neural Program Learning">
  <data key="d0">Planning in Neural Program Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Structured search and planning approaches used to guide neural models in generating complex programs.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python Software Foundation">
  <data key="d0">Python Software Foundation</data>
  <data key="d1">Organizations</data>
  <data key="d2">Non-profit organization supporting Python development, community, and ecosystem growth.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="JetBrains">
  <data key="d0">JetBrains</data>
  <data key="d1">Organizations</data>
  <data key="d2">A software development company known for creating tools and conducting surveys related to programming practices.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.">
  <data key="d0">Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of foundational language models that utilize generative pre-training to improve language understanding.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generative Pre-Training for Language Models">
  <data key="d0">Generative Pre-Training for Language Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Training models on large unlabeled datasets to enable unsupervised learning and transfer learning capabilities.</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Biological Structure and Function">
  <data key="d0">Biological Structure and Function</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Biological structure and function are explored through the application of scaling and unsupervised learning techniques to analyze 250 million protein sequences, revealing fundamental biological insights.&lt;SEP&gt;Biological structure and function are studied through scaling and unsupervised learning applied to protein sequences, revealing fundamental biological insights.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Scaling Unsupervised Learning">
  <data key="d0">Scaling Unsupervised Learning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A machine learning approach that involves training models on large datasets without explicit labels, applied here to protein sequences to uncover biological structures and functions.&lt;SEP&gt;A machine learning approach that involves training models on large, unlabeled datasets to uncover patterns, applied here to analyze protein sequences and infer biological structures and functions.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Protein Sequences">
  <data key="d0">Protein Sequences</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large datasets comprising amino acid sequences used to analyze biological functions and structures via machine learning techniques.&lt;SEP&gt;Large datasets of amino acid sequences used to analyze biological structures and functions through machine learning.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Proceedings of the National Academy of Sciences">
  <data key="d0">Proceedings of the National Academy of Sciences</data>
  <data key="d1">Discipline</data>
  <data key="d2">A prominent scientific journal publishing research across biological and physical sciences, where the study was published.&lt;SEP&gt;A scientific journal publishing research across biological and physical sciences, where the study was published.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Malware Source Code">
  <data key="d0">Malware Source Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Malicious code stored in repositories, which Sourcefinder aims to locate and analyze.&lt;SEP&gt;Malicious software code stored in repositories, which Sourcefinder aims to identify and analyze.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research in Attacks, Intrusions and Defenses (RAID)">
  <data key="d0">Research in Attacks, Intrusions and Defenses (RAID)</data>
  <data key="d1">Discipline</data>
  <data key="d2">An international symposium focusing on cybersecurity threats, attack techniques, and defense mechanisms.&lt;SEP&gt;An international symposium focusing on cybersecurity threats, defenses, and attack methodologies.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Poisoning Vulnerabilities in Neural Code Completion">
  <data key="d0">Poisoning Vulnerabilities in Neural Code Completion</data>
  <data key="d1">Results</data>
  <data key="d2">A study identifying security vulnerabilities where adversarial inputs can manipulate neural code completion systems, pointing to risks in neural code generation.&lt;SEP&gt;A study identifying security vulnerabilities where adversarial inputs can manipulate neural code completion systems.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sequence to Sequence Learning">
  <data key="d0">Sequence to Sequence Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural network architecture for translating or transforming sequences, foundational for tasks like translation, summarization, and code generation.&lt;SEP&gt;A neural network architecture that models the transformation of one sequence into another, foundational for tasks like translation, summarization, and code generation.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="End-to-End Memory Networks">
  <data key="d0">End-to-End Memory Networks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A neural network architecture that incorporates memory components allowing models to reason over sequences and improve tasks like question answering.&lt;SEP&gt;Neural network models that incorporate external memory components to enhance reasoning and contextual understanding, used in question answering and reasoning tasks.&lt;SEP&gt;Neural network models that incorporate memory components to improve reasoning and contextual understanding in tasks like question answering.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Women’s Participation in Open Source Software">
  <data key="d0">Women’s Participation in Open Source Software</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A literature survey analyzing the involvement and participation patterns of women in open source software development.&lt;SEP&gt;A survey of literature examining the involvement, participation, and roles of women in open source software development communities.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Learning Bug-Fixing Patches in the Wild">
  <data key="d0">Learning Bug-Fixing Patches in the Wild</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates whether neural machine translation models can learn to generate bug-fixing patches from real-world data.&lt;SEP&gt;Investigates whether neural machine translation models can learn to generate bug-fixing patches from real-world software data, assessing applicability in software maintenance.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Unit Test Case Generation with Transformers">
  <data key="d0">Unit Test Case Generation with Transformers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Applying transformer-based neural models to automatically generate unit tests, aiming to improve software testing efficiency.&lt;SEP&gt;Using transformer-based models to automatically generate unit tests, enhancing software testing processes.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pixel Recurrent Neural Networks">
  <data key="d0">Pixel Recurrent Neural Networks</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural network architecture designed for modeling images by capturing spatial dependencies through recurrent connections, used in image generation and analysis.&lt;SEP&gt;A type of neural network designed for modeling images by capturing spatial dependencies through recurrent connections.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Attention is All You Need">
  <data key="d0">Attention is All You Need</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A seminal paper introducing the Transformer architecture, emphasizing attention mechanisms as the core component for sequence modeling.&lt;SEP&gt;A seminal paper introducing the Transformer architecture, emphasizing the attention mechanism as the core component for sequence modeling tasks.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="GPT-J-6B">
  <data key="d0">GPT-J-6B</data>
  <data key="d1">Tools</data>
  <data key="d2">A large autoregressive language model trained on code and text data, capable of generating, understanding, and translating natural language and programming code.&lt;SEP&gt;A large autoregressive language model trained on code, capable of generating and understanding natural language and code snippets.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="In-ide Code Generation">
  <data key="d0">In-ide Code Generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using natural language inputs to generate code within development environments, promising improvements in developer productivity.&lt;SEP&gt;Using natural language prompts within integrated development environments to automatically generate code snippets, improving developer productivity and code quality.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Natural Language to Code">
  <data key="d0">Natural Language to Code</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Explores the feasibility and challenges of translating natural language instructions into executable code.&lt;SEP&gt;Explores the feasibility, challenges, and methods for translating natural language instructions into executable code using neural models.</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Xu, F. F., Vasilescu, B., and Neubig, G.">
  <data key="d0">Xu, F. F., Vasilescu, B., and Neubig, G.</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Authors of a paper discussing code generation from natural language, focusing on promise and challenges.&lt;SEP&gt;Authors of a scholarly paper discussing code generation from natural language, highlighting its potential and challenges.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="In-ide code generation from natural language: Promise and challenges">
  <data key="d0">In-ide code generation from natural language: Promise and challenges</data>
  <data key="d1">Research Paper</data>
  <data key="d2">A preprint analyzing the capabilities and difficulties in generating code from natural language inputs.&lt;SEP&gt;A scholarly article analyzing the potential and difficulties of generating code from natural language inputs.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Yin, P. and Neubig, G.">
  <data key="d0">Yin, P. and Neubig, G.</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Authors of a study presenting a neural model utilizing syntax for general-purpose code generation.&lt;SEP&gt;Authors of a study presenting a syntactic neural model for general-purpose code generation.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="A syntactic neural model for general-purpose code generation">
  <data key="d0">A syntactic neural model for general-purpose code generation</data>
  <data key="d1">Research Paper</data>
  <data key="d2">A neural network model leveraging syntax trees to improve code generation across various tasks.&lt;SEP&gt;A study proposing a neural network model that leverages syntax to improve code generation capabilities.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Zaremba, W. and Sutskever, I.">
  <data key="d0">Zaremba, W. and Sutskever, I.</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Authors of a paper focused on learning to execute code, exploring methods for neural execution models.&lt;SEP&gt;Authors of a paper on neural learning approaches to executing code, aiming to teach models to perform code execution.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Learning to execute">
  <data key="d0">Learning to execute</data>
  <data key="d1">Research Paper</data>
  <data key="d2">A scholarly article exploring methods for neural models to learn and perform code execution tasks.&lt;SEP&gt;A scholarly article on training models to execute code, emphasizing neural approaches.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y.">
  <data key="d0">Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y.</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Authors of a paper introducing Merlot, a multimodal neural model designed for script knowledge understanding and generation.&lt;SEP&gt;Authors of a paper introducing Merlot, a multimodal neural script knowledge model.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Merlot: Multimodal neural script knowledge models">
  <data key="d0">Merlot: Multimodal neural script knowledge models</data>
  <data key="d1">Research Paper</data>
  <data key="d2">A study presenting Merlot, a model integrating multiple modalities for understanding and generating script knowledge.&lt;SEP&gt;A study proposing a neural model that integrates multiple modalities to understand and generate script-based knowledge.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S.">
  <data key="d0">Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S.</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Authors of a paper on calibration techniques to enhance few-shot learning performance in language models.&lt;SEP&gt;Authors of a paper on improving few-shot performance of language models through calibration.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Calibrate before use: Improving few-shot performance of language models">
  <data key="d0">Calibrate before use: Improving few-shot performance of language models</data>
  <data key="d1">Research Paper</data>
  <data key="d2">A study proposing calibration techniques to enhance the few-shot learning capabilities of language models.&lt;SEP&gt;A study that introduces calibration methods to improve the few-shot learning capabilities of large language models.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ziegler, A.">
  <data key="d0">Ziegler, A.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a report examining rote learning in GitHub Copilot suggestions.&lt;SEP&gt;Author of a report examining rote learning phenomena in GitHub Copilot suggestions.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="A first look at rote learning in github copilot suggestions">
  <data key="d0">A first look at rote learning in github copilot suggestions</data>
  <data key="d1">Research Report</data>
  <data key="d2">An analysis exploring how GitHub Copilot's suggestions may involve rote learning patterns.&lt;SEP&gt;An analysis exploring whether GitHub Copilot's code suggestions rely on rote memorization patterns.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Estimating pass@k">
  <data key="d0">Estimating pass@k</data>
  <data key="d1">Methodology</data>
  <data key="d2">A statistical estimator designed to evaluate the probability that at least one of k generated code samples passes unit tests, ensuring unbiased assessment of code generation quality.&lt;SEP&gt;An unbiased statistical estimator used to evaluate the success probability of code generation models, ensuring fair comparison.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="n">
  <data key="d0">n</data>
  <data key="d1">Variable</data>
  <data key="d2">Number of samples drawn in the pass@k estimation process.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="c">
  <data key="d0">c</data>
  <data key="d1">Variable</data>
  <data key="d2">Number of correct samples passing unit tests among the total samples.&lt;SEP&gt;Number of correct samples passing unit tests among total samples generated.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="c, the number of correct samples that pass the unit tests, is distributed Binom(n; p)">
  <data key="d0">c, the number of correct samples that pass the unit tests, is distributed Binom(n; p)</data>
  <data key="d1">Variable</data>
  <data key="d2">Statistical distribution describing the number of successful samples in the evaluation of code generation models.&lt;SEP&gt;The binomial distribution modeling the count of successful samples in code evaluation.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Figure 13">
  <data key="d0">Figure 13</data>
  <data key="d1">Figure</data>
  <data key="d2">A figure comparing bias and variance of estimators for pass@k, illustrating estimator performance.&lt;SEP&gt;A visual comparison of bias and variance in estimators of pass@k, illustrating their performance differences.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Random Problems and Solutions from Codex-12B">
  <data key="d0">Random Problems and Solutions from Codex-12B</data>
  <data key="d1">Dataset/Task</data>
  <data key="d2">A collection of randomly selected programming problems with solutions generated by Codex-12B, used for evaluation.&lt;SEP&gt;A set of randomly selected problems with generated solutions from Codex-12B for evaluating code generation models.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="def words_string(s):">
  <data key="d0">def words_string(s):</data>
  <data key="d1">Function</data>
  <data key="d2">A Python function that splits a string into individual words, handling spaces and commas appropriately.&lt;SEP&gt;A function designed to split a string of words into an array of individual words, handling commas and spaces.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="is_prime(n)">
  <data key="d0">is_prime(n)</data>
  <data key="d1">Function</data>
  <data key="d2">A Python function that determines whether a given number n is prime, returning True or False.&lt;SEP&gt;A function to determine if a given number n is prime, returning true or false.</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="words">
  <data key="d0">words</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The string of words being analyzed in the text.</data>
  <data key="d3">chunk-b9d1dd3aac85f5453acff84163bfde5a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Words">
  <data key="d0">Words</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The sequence of words in the string, representing textual data to be analyzed.</data>
  <data key="d3">chunk-b9d1dd3aac85f5453acff84163bfde5a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="y">
  <data key="d0">y</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The letter 'y' functions as a vowel only when it appears at the end of a word, highlighting its contextual role in phonetics and orthography.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="vowels_count">
  <data key="d0">vowels_count</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A function that counts vowels in a string, considering 'y' as a vowel only at the end of words, exemplified by counting vowels in input strings.&lt;SEP&gt;A variable holding the count of vowels found in the input string.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="abcde">
  <data key="d0">abcde</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An example input string used to demonstrate the vowels counting function, containing multiple vowels including 'y'.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ACEDY">
  <data key="d0">ACEDY</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Another example input string to illustrate the counting of vowels, with 'Y' at the end of the word.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="vowels">
  <data key="d0">vowels</data>
  <data key="d1">Variables</data>
  <data key="d2">A set or list of characters representing vowels, including 'a', 'e', 'i', 'o', 'u', and optionally 'y', used in vowel counting functions.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="return v">
  <data key="d0">return v</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">The function returns the total number of vowels counted in the input string, applying conditional logic to include 'y' only at the end of words.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="abcde&quot; and &quot;ACEDY">
  <data key="d0">abcde" and "ACEDY</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sample inputs used to demonstrate how the vowels counting function operates with different strings.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Y&quot; as a vowel">
  <data key="d0">Y" as a vowel</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The letter 'Y' is considered a vowel only when it appears at the end of a word, illustrating its dual role in English phonetics.</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="b_digit">
  <data key="d0">b_digit</data>
  <data key="d1">Variables</data>
  <data key="d2">b_digit represents a digit extracted from variable b, used in calculations.&lt;SEP&gt;b_digit represents a single digit extracted from variable b, used in calculations involving digit manipulation and positional value.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="result">
  <data key="d0">result</data>
  <data key="d1">Results</data>
  <data key="d2">result accumulates the computed value based on digits of a and b, representing the final output of the calculation function.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="a_digit">
  <data key="d0">a_digit</data>
  <data key="d1">Variables</data>
  <data key="d2">a_digit is a single digit extracted from variable a, involved in the calculation of result.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="power">
  <data key="d0">power</data>
  <data key="d1">Variables</data>
  <data key="d2">power tracks the exponent used in the calculation of result, indicating the positional significance of digits.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ifb">
  <data key="d0">ifb</data>
  <data key="d1">Variables</data>
  <data key="d2">ifb is a condition checking whether b is less than 0, influencing the control flow to return 0 minus result.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="number">
  <data key="d0">number</data>
  <data key="d1">Variables</data>
  <data key="d2">number is the product of a and b, used as an input in calculations or further processing.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="string">
  <data key="d0">string</data>
  <data key="d1">Variables</data>
  <data key="d2">string is the string representation of number, enabling digit-by-digit processing for summing or analysis.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="total">
  <data key="d0">total</data>
  <data key="d1">Variables</data>
  <data key="d2">total accumulates the sum of digits in string, used in a separate function to analyze digit properties.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="i">
  <data key="d0">i</data>
  <data key="d1">Variables</data>
  <data key="d2">i is an iterator over each character (digit</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 4">
  <data key="d0">COMPLETION 4</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that returns the last element in a list of strings, with a conditional check for empty input.&lt;SEP&gt;A duplicate of the first snippet, with the same incorrect implementation.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 5">
  <data key="d0">COMPLETION 5</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that compares n with x and y, returning n if less than x, else y, with logical flaws.&lt;SEP&gt;A code snippet that finds and returns the longest string in a list, handling empty input cases.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 6">
  <data key="d0">COMPLETION 6</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that checks if n is divisible by numbers in range, returning y if divisible, else x, with logical errors.&lt;SEP&gt;A code snippet that returns the longest string in a list, using the max function with key=len.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 7">
  <data key="d0">COMPLETION 7</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that finds and returns the longest string in a list, with an incorrect implementation involving variable name errors.&lt;SEP&gt;A recursive implementation that returns x if n is 2, y if 3, else calls itself with n-1, with logical flaws.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 8">
  <data key="d0">COMPLETION 8</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that compares n to x and y, returning x or y if matches, else n, with incorrect logic.&lt;SEP&gt;A code snippet that returns the longest string in a list through iteration, with syntax errors such as missing colons and improper indentation.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 1">
  <data key="d0">COMPLETION 1</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet intended to return x if n is divisible by x, else y, but with incorrect logic.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 2">
  <data key="d0">COMPLETION 2</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet intended to return x if n is an odd number, else y, but with logical errors.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="COMPLETION 3">
  <data key="d0">COMPLETION 3</data>
  <data key="d1">Results</data>
  <data key="d2">A code snippet that returns x if n is divisible by x, else y, with syntax errors.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Building Blocks for Synthetic Tasks">
  <data key="d0">Building Blocks for Synthetic Tasks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A description of 13 modular code and text components used to generate synthetic tasks for model evaluation, including their descriptions and implementations.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Synthetic Tasks">
  <data key="d0">Synthetic Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks created from building blocks to test code synthesis capabilities.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research Communities">
  <data key="d0">Research Communities</data>
  <data key="d1">Disciplines</data>
  <data key="d2">ML and synthesis communities involved in evaluating code generation and synthesis.</data>
  <data key="d3">chunk-31212d7716c4c20f6aabfd8fb9a1905c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="D. Details of Specification-based Evaluation Framework">
  <data key="d0">D. Details of Specification-based Evaluation Framework</data>
  <data key="d1">study_design</data>
  <data key="d2">The framework discusses methods for evaluating code synthesis and generation capabilities, emphasizing the importance of benchmarks and attributes like complexity and expressivity of specifications.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Synthesis and Generation">
  <data key="d0">Code Synthesis and Generation</data>
  <data key="d1">core_concept</data>
  <data key="d2">Processes involving automatic creation of code from specifications, with evaluation metrics focusing on correctness, complexity, and expressivity.&lt;SEP&gt;Processes of automatically creating code from specifications, with evaluation focusing on correctness, complexity, and expressivity.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ML (Xu et al., 2021)">
  <data key="d0">ML (Xu et al., 2021)</data>
  <data key="d1">theory/model</data>
  <data key="d2">A machine learning approach referenced as part of prior research exploring code synthesis capabilities.&lt;SEP&gt;A machine learning approach used in prior studies to evaluate code synthesis capabilities, focusing on correctness and complexity metrics.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Synthesis (Helmuth &amp; Spector, 2015; Pantridge et al., 2017)">
  <data key="d0">Synthesis (Helmuth &amp; Spector, 2015; Pantridge et al., 2017)</data>
  <data key="d1">theory/model</data>
  <data key="d2">Communities and research efforts focused on code synthesis and evaluation methodologies.&lt;SEP&gt;Research communities and models that explore the principles and evaluation of code synthesis and automatic programming.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="McCabe Cyclomatic Complexity (CC)">
  <data key="d0">McCabe Cyclomatic Complexity (CC)</data>
  <data key="d1">tool</data>
  <data key="d2">A metric used to analyze the complexity of code outputs, assessing correctness and structural complexity.&lt;SEP&gt;A metric used to analyze the complexity of code outputs, recommended for evaluating synthesis correctness and complexity.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Specifications">
  <data key="d0">Specifications</data>
  <data key="d1">objects of study</data>
  <data key="d2">Natural language or formal prompts defining what code should accomplish, central to evaluating synthesis models.&lt;SEP&gt;The formal or natural language prompts that define what the synthesized code should accomplish, central to evaluation.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Expressivity and Complexity of Specifications">
  <data key="d0">Expressivity and Complexity of Specifications</data>
  <data key="d1">variables</data>
  <data key="d2">Attributes used to measure the richness and difficulty of specifications, including their ability to reason over computations and states.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Attributes for Measuring Specifications">
  <data key="d0">Attributes for Measuring Specifications</data>
  <data key="d1">variables</data>
  <data key="d2">Features like reasoning over abstractions, variable dependencies, inter-procedural reasoning, and computational interleavings used to evaluate specifications.&lt;SEP&gt;Qualitative metrics such as reasoning over abstractions, variable dependencies, inter-procedural reasoning, and computational interleavings.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-level and Low-level Specifications">
  <data key="d0">High-level and Low-level Specifications</data>
  <data key="d1">variables</data>
  <data key="d2">Different abstraction levels of specifications, with high-level being more ambiguous and low-level being more concrete and well-defined.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Variable Interdependencies">
  <data key="d0">Variable Interdependencies</data>
  <data key="d1">variables</data>
  <data key="d2">Relationships among multiple variables, including their nesting, dependencies, and permutations of states.&lt;SEP&gt;Tracking relationships and nesting among multiple variables, including permutations of states and input-output relationships.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Temporal Reasoning">
  <data key="d0">Temporal Reasoning</data>
  <data key="d1">variables</data>
  <data key="d2">Reasoning about future and past program states, including safety and liveness properties.&lt;SEP&gt;Reasoning about program states over time, including safety properties (bad states) and liveness properties (progress).</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Concurrency and Parallelism">
  <data key="d0">Concurrency and Parallelism</data>
  <data key="d1">variables</data>
  <data key="d2">Properties related to computational interleavings, synchronization, and correctness in concurrent executions.&lt;SEP&gt;Properties related to multiple processes executing simultaneously, including synchronization, atomicity, and interleaving correctness.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Strong Fairness, Weak Fairness, Mutual Exclusion">
  <data key="d0">Strong Fairness, Weak Fairness, Mutual Exclusion</data>
  <data key="d1">variables</data>
  <data key="d2">Properties ensuring proper process execution, avoiding race conditions, and ensuring fairness in concurrent systems.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hyperproperties (Clarkson et al., 2014)">
  <data key="d0">Hyperproperties (Clarkson et al., 2014)</data>
  <data key="d1">theory/model</data>
  <data key="d2">Properties related to information flow and security policies, such as noninterference, requiring programs to behave deterministically across different security levels.&lt;SEP&gt;Properties related to information flow policies and cryptographic algorithms requiring observational determinism, e.g., noninterference.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Nondeterminism">
  <data key="d0">Nondeterminism</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Nondeterminism in computational theory refers to algorithms that can produce different outputs for the same input across different executions, highlighting variability in outcomes depending on execution paths.&lt;SEP&gt;Property of algorithms that may produce different outputs on different runs with the same input, important for assessing non-deterministic behaviors.&lt;SEP&gt;The property of algorithms that can produce different outputs for the same input across different executions, relevant for assessing non-deterministic algorithms.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019&lt;SEP&gt;chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Expressivity">
  <data key="d0">Expressivity</data>
  <data key="d1">Variables</data>
  <data key="d2">Attribute measuring the richness of specifications, including their ability to reason over computations, states, and abstractions.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Complexity">
  <data key="d0">Complexity</data>
  <data key="d1">Variables</data>
  <data key="d2">Attribute measuring the difficulty of specifications, including their structural and semantic intricacies.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-level Specifications">
  <data key="d0">High-level Specifications</data>
  <data key="d1">Variables</data>
  <data key="d2">Abstract, loosely defined specifications that require implicit derivation of internal details for code synthesis.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Low-level Specifications">
  <data key="d0">Low-level Specifications</data>
  <data key="d1">Variables</data>
  <data key="d2">Well-defined, concrete specifications with explicit constraints, reducing ambiguity in code synthesis.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Strong Fairness">
  <data key="d0">Strong Fairness</data>
  <data key="d1">Variables</data>
  <data key="d2">A property ensuring every process enabled infinitely often is executed infinitely often, preventing starvation.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Weak Fairness">
  <data key="d0">Weak Fairness</data>
  <data key="d1">Variables</data>
  <data key="d2">A property ensuring processes that are almost always enabled are executed infinitely often, a weaker fairness condition.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Mutual Exclusion, Atomicity, Synchronization">
  <data key="d0">Mutual Exclusion, Atomicity, Synchronization</data>
  <data key="d1">Variables</data>
  <data key="d2">Properties ensuring correct concurrent execution, avoiding race conditions and ensuring proper resource access.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Noninterference">
  <data key="d0">Noninterference</data>
  <data key="d1">Variables</data>
  <data key="d2">A hyperproperty ensuring that high-security inputs do not influence low-security outputs, preserving confidentiality.</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Low-Security Users">
  <data key="d0">Low-Security Users</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Low-security users are individuals with limited access or permissions within a system, whose observed outputs are consistent regardless of inputs submitted by high-security users, indicating a security boundary or boundary condition in system behavior.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-Security Users">
  <data key="d0">High-Security Users</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">High-security users are individuals with elevated permissions or access rights, whose inputs can influence system outputs, but in this context, their inputs do not alter the observed outputs for low-security users.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Outputs">
  <data key="d0">Outputs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Outputs refer to the results or responses generated by computational systems when processing inputs, which are observed and analyzed to assess system behavior and security properties.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deterministic Algorithm">
  <data key="d0">Deterministic Algorithm</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A deterministic algorithm always produces the same output for a given input across all executions, contrasting with nondeterministic algorithms.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Random Number Generator">
  <data key="d0">Random Number Generator</data>
  <data key="d1">Tools</data>
  <data key="d2">A simple example of a nondeterministic process that produces different outcomes in different runs, often used to illustrate nondeterminism in algorithms.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ML Algorithms">
  <data key="d0">ML Algorithms</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Machine Learning algorithms are advanced computational models that can exhibit nondeterministic behavior, especially in their training and inference processes.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Specification-Independent Coding Practices">
  <data key="d0">Specification-Independent Coding Practices</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Coding practices that are designed to be independent of specific system specifications, promoting attributes like code reuse, automatic architecture determination, and broad applicability.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Genetic Programming Community">
  <data key="d0">Genetic Programming Community</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A research community focused on evolutionary algorithms and programming techniques that evolve programs to solve problems, discussing properties relevant to code synthesis and nondeterminism.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code and Parameterized Reuse">
  <data key="d0">Code and Parameterized Reuse</data>
  <data key="d1">Variables</data>
  <data key="d2">Reusing code segments and parameters to improve efficiency and adaptability in program development.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automatic Determination of Program Architecture">
  <data key="d0">Automatic Determination of Program Architecture</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques that enable systems to infer or optimize program structures without explicit human specification.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Wide Range of Programming Constructs">
  <data key="d0">Wide Range of Programming Constructs</data>
  <data key="d1">Variables</data>
  <data key="d2">The diversity of programming elements and structures available for constructing complex algorithms.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Well-Defined">
  <data key="d0">Well-Defined</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Attributes indicating clear, unambiguous specifications or properties in system design and coding practices.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Increasingly Higher Level Specifications">
  <data key="d0">Increasingly Higher Level Specifications</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The hypothesis that higher-level system specifications should reduce the need for explicit coding constructs, allowing code generation algorithms to infer necessary components.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code Generation Algorithms">
  <data key="d0">Code Generation Algorithms</data>
  <data key="d1">Tools</data>
  <data key="d2">Algorithms designed to automatically generate code based on specifications or higher-level descriptions, aiming to handle complex and high-level tasks.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Alignment Problems">
  <data key="d0">Alignment Problems</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Alignment problems refer to issues where AI models do not perform as intended or desired by users, especially as capabilities improve, potentially leading to misaligned outputs or behaviors.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Capability">
  <data key="d0">Model Capability</data>
  <data key="d1">Variables</data>
  <data key="d2">The potential or ability of a model to perform specific tasks, which can be latent or explicitly trained.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Operational Conditions">
  <data key="d0">Operational Conditions</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Conditions under which a model is tested or evaluated, such as prompt engineering, fine-tuning, or other techniques that reveal capabilities.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Intent Misalignment">
  <data key="d0">Intent Misalignment</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A state where a model produces outputs that do not align with user preferences, despite being capable of producing the desired outputs, indicating a discrepancy between capability and intent.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Capability to Perform Task X">
  <data key="d0">Capability to Perform Task X</data>
  <data key="d1">Variables</data>
  <data key="d2">The model's inherent or latent ability to accomplish a specific task, which can be activated or measured through various methods.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sufficient Conditions for Model Capability">
  <data key="d0">Sufficient Conditions for Model Capability</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Conditions under which a model can be considered capable of performing a task, including prompt engineering, fine-tuning, or inferring capabilities through related tasks.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Detecting Problems with Models">
  <data key="d0">Detecting Problems with Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Evaluation techniques aimed at identifying issues such as misalignment, bias, or security vulnerabilities in AI models.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evaluation of Alignment">
  <data key="d0">Evaluation of Alignment</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Systematic processes to measure how well a model's outputs match user intentions or desired behaviors, especially in complex or subtle scenarios.</data>
  <data key="d3">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="alignment evaluations">
  <data key="d0">alignment evaluations</data>
  <data key="d1">Results</data>
  <data key="d2">The results of alignment evaluations assess the model's ability to produce correct, bug-free code and distinguish between situations where it should or should not output certain code types.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="model">
  <data key="d0">model</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A model is a mathematical or computational representation used to simulate or generate data, such as language or code, often trained on large datasets.&lt;SEP&gt;The model refers to the code-generation models, such as Codex, evaluated for alignment and performance.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="code with bugs">
  <data key="d0">code with bugs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code with bugs refers to intentionally or unintentionally flawed code used to evaluate the model's ability to detect and avoid generating bugs.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="high-quality code">
  <data key="d0">high-quality code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High-quality code is bug-free, well-structured code used as a standard or goal for model output.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="dataset">
  <data key="d0">dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The dataset includes HumanEval problems and other code samples used for evaluating model performance and alignment.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="formal analysis">
  <data key="d0">formal analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Formal analysis involves systematic evaluation of code quality using formal methods or metrics to filter or label datasets.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="human feedback">
  <data key="d0">human feedback</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Explicit evaluations or preferences provided by humans to guide model training and fine-tuning.&lt;SEP&gt;Explicit evaluations, preferences, or corrections provided by humans to guide the training and fine-tuning of models.&lt;SEP&gt;Human feedback involves collecting input from human labelers to guide model training and improve alignment.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="fine-tuning">
  <data key="d0">fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Fine-tuning involves adjusting a pre-trained model on specific datasets to improve its performance on particular tasks, such as parallel code generation.&lt;SEP&gt;Fine-tuning involves further training models on specific datasets or tasks to improve their performance in generating parallel code.&lt;SEP&gt;Fine-tuning involves training language models on specific datasets, such as code corpora, to improve their performance on targeted tasks.&lt;SEP&gt;Fine-tuning is the process of retraining models on curated datasets, such as high-quality code, to improve their output quality and alignment.&lt;SEP&gt;Fine-tuning involves training pre-existing language models on specific datasets to improve their performance on targeted tasks, such as code generation.&lt;SEP&gt;Fine-tuning involves training pre-trained language models on specific datasets to improve their performance on targeted tasks such as code generation, by adjusting model weights.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="RLHF (Reinforcement Learning from Human Feedback)">
  <data key="d0">RLHF (Reinforcement Learning from Human Feedback)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">RLHF is a training approach that uses human feedback as reward signals to align model behavior with desired outcomes.&lt;SEP&gt;Training technique where models are optimized based on human-provided reward signals, enhancing alignment and usefulness.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="metrics for alignment">
  <data key="d0">metrics for alignment</data>
  <data key="d1">Variables</data>
  <data key="d2">Metrics for alignment are quantitative measures used to assess how well a model's outputs conform to desired standards, such as correctness and helpfulness.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="transparency tools">
  <data key="d0">transparency tools</data>
  <data key="d1">Tools</data>
  <data key="d2">Transparency tools help understand and evaluate whether a model is aligned by providing insights into its decision-making processes.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="subtle bugs">
  <data key="d0">subtle bugs</data>
  <data key="d1">Results</data>
  <data key="d2">Subtle bugs are minor, often difficult-to-detect errors intentionally inserted into code to test the model's ability to identify and avoid them.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="evaluation of downstream tasks">
  <data key="d0">evaluation of downstream tasks</data>
  <data key="d1">Results</data>
  <data key="d2">Evaluation of downstream tasks measures how well the model performs on practical applications after alignment efforts.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Alignment Evaluations">
  <data key="d0">Alignment Evaluations</data>
  <data key="d1">Results</data>
  <data key="d2">Assessments of the model's ability to produce correct, bug-free code, and to distinguish between situations where different outputs are appropriate, based on evaluation datasets and methodologies.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model">
  <data key="d0">Model</data>
  <data key="d1">Tools</data>
  <data key="d2">Pre-trained language models such as GPT-2, GPT-Neo, and PolyCoder, which are fine-tuned to generate or classify code pragmas and predict performance changes.&lt;SEP&gt;Refers to the code-generation models, such as Codex, evaluated for their alignment, correctness, and ability to follow instructions.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Alignment">
  <data key="d0">Alignment</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The conceptual framework or properties defining how well a model's outputs match desired standards, including correctness, helpfulness, and safety.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Code with Bugs">
  <data key="d0">Code with Bugs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code snippets intentionally or unintentionally containing errors, used to evaluate the model's ability to detect, avoid, or correct bugs.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-Quality Code">
  <data key="d0">High-Quality Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code that is bug-free, well-structured, and adheres to best practices, serving as a standard for model output quality.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Datasets">
  <data key="d0">Datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Collections of code problems, solutions, and test cases used for evaluating and training models on code correctness and alignment, such as HumanEval.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Formal Analysis">
  <data key="d0">Formal Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Systematic evaluation techniques applying formal methods or metrics to assess code quality, used for filtering or labeling datasets.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Human Feedback">
  <data key="d0">Human Feedback</data>
  <data key="d1">Tools</data>
  <data key="d2">Input from human labelers or reviewers used to guide model training, fine-tuning, and alignment to improve output quality.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Metrics for Alignment">
  <data key="d0">Metrics for Alignment</data>
  <data key="d1">Variables</data>
  <data key="d2">Quantitative measures designed to evaluate how well a model's outputs align with desired qualities such as correctness, helpfulness, and safety.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Transparency Tools">
  <data key="d0">Transparency Tools</data>
  <data key="d1">Tools</data>
  <data key="d2">Software or methods that provide insights into model decision-making processes, aiding in evaluating and understanding alignment.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Subtle Bugs">
  <data key="d0">Subtle Bugs</data>
  <data key="d1">Results</data>
  <data key="d2">Minor errors intentionally inserted into code to test the model's ability to detect and avoid such errors during generation.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Downstream Tasks Evaluation">
  <data key="d0">Downstream Tasks Evaluation</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of model performance on practical applications after training and alignment efforts, measuring real-world usefulness.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evaluation Datasets">
  <data key="d0">Evaluation Datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Specific datasets like HumanEval used to systematically evaluate model performance, correctness, and alignment.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pre-training Dataset">
  <data key="d0">Pre-training Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Large collections of code used to train models, which can be curated to improve quality and reduce bugs.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Automated Testing and Formal Verification Tools">
  <data key="d0">Automated Testing and Formal Verification Tools</data>
  <data key="d1">Tools</data>
  <data key="d2">Software tools used to automatically test, verify, and analyze code quality, assisting in filtering datasets and evaluating model outputs.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Curated Datasets">
  <data key="d0">Curated Datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Datasets selected or labeled to contain high-quality, bug-free code, used for training or evaluation.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Metrics of Code Quality">
  <data key="d0">Metrics of Code Quality</data>
  <data key="d1">Variables</data>
  <data key="d2">Quantitative measures used to assess the quality, correctness, and security of code, guiding dataset filtering and model training.</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Capabilities">
  <data key="d0">Model Capabilities</data>
  <data key="d1">Results</data>
  <data key="d2">Refers to the abilities of models to generate correct, safe, and aligned code, and to distinguish between different instruction types.&lt;SEP&gt;The capabilities and limitations of Codex in generating malicious code, discovering vulnerabilities, and suggesting dependencies influence its potential misuse and security impact.&lt;SEP&gt;The extent to which language models can perform various NLP tasks, including question answering, decision support, and knowledge retrieval.&lt;SEP&gt;The range of tasks and performance levels of language models, including question answering, reasoning, and decision support.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-bf1121c44634b616af61c49cd3adf29a&lt;SEP&gt;chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Methodologies">
  <data key="d0">Methodologies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Systematic procedures and approaches used to conduct research, analyze data, or perform activities within a discipline.&lt;SEP&gt;The process of analyzing and processing data or information using systematic procedures to achieve specific objectives.&lt;SEP&gt;Systematic procedures and techniques used to conduct research, collect data, and analyze results.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Study Designs">
  <data key="d0">Study Designs</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Plans and structures for conducting research studies, such as experimental, observational, or survey designs.&lt;SEP&gt;Structured frameworks and plans that guide how research is conducted, including experimental, observational, and other designs.&lt;SEP&gt;The structured approaches and frameworks used to plan, conduct, and analyze research studies.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Results">
  <data key="d0">Results</data>
  <data key="d1">Results</data>
  <data key="d2">Findings and outcomes derived from data analysis that answer research questions or test hypotheses.&lt;SEP&gt;Findings or outcomes obtained after analyzing data in a research or activity context.&lt;SEP&gt;The outcomes or findings derived from data analysis in a research or activity context.&lt;SEP&gt;Results refer to the outcomes of experiments or evaluations, such as code quality metrics, performance measurements, and accuracy assessments presented in the study.&lt;SEP&gt;The Results section summarizes the findings related to code complexity, code generation challenges, and the potential impact of generative AI on HPC software development, maintenance, and education.&lt;SEP&gt;The findings of the experiments, indicating the accuracy and proficiency levels achieved by code generated in different scenarios.&lt;SEP&gt;The study finds that AI-generated code outputs correlate with the maturity of programming models: high scores for OpenMP and CUDA, lower for HIP; prompts benefit from adding keywords, especially in Fortran and Python; Julia performs well with mature models like Threads and CUDA.jl.&lt;SEP&gt;The study finds that Codex outputs correlate with the maturity of programming models, scoring high on OpenMP and CUDA, but less so on HIP; prompts benefit from adding keywords, especially in Fortran and Python; Julia performs well with mature models.&lt;SEP&gt;Summary of performance metrics and comparative analysis of different compilation strategies and models.&lt;SEP&gt;Findings from experiments demonstrating the performance, throughput, and memory trade-offs among various HPC code generation models.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f&lt;SEP&gt;chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-04aecdc2586d2c7966e70e02cac846b0&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Variables">
  <data key="d0">Variables</data>
  <data key="d1">Variables</data>
  <data key="d2">Attributes or factors that can vary within a study, used to measure or manipulate aspects of the objects of study.&lt;SEP&gt;Elements or factors that can change or vary within a study, used to measure effects or relationships.&lt;SEP&gt;Elements or factors that can vary within a study, used to measure effects or relationships.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Tools">
  <data key="d0">Tools</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External utilities such as retrieval models, multimodal models, APIs, calculators, and other software components that can be integrated into LM workflows to extend their capabilities.&lt;SEP&gt;GitHub Copilot, an AI-powered code generation tool leveraging OpenAI Codex, used to produce implementation suggestions for numerical kernels in various programming models.&lt;SEP&gt;Instruments or software used to collect, analyze, or visualize data in research.&lt;SEP&gt;Instruments or software used to facilitate data collection, analysis, or visualization in research.&lt;SEP&gt;Instruments, software, or resources used to facilitate data collection, analysis, or visualization.&lt;SEP&gt;The evaluation framework and metrics developed, including ParEval, serve as tools for assessing LLM capabilities in parallel code generation.&lt;SEP&gt;The evaluation framework, including ParEval and the novel metrics, serve as tools for systematically assessing the capabilities of language models in parallel code generation.&lt;SEP&gt;Various external utilities such as retrieval models, multimodal models, APIs, and calculators that can be integrated into LM pipelines to extend functionality.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85&lt;SEP&gt;chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Analytical Techniques">
  <data key="d0">Analytical Techniques</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Methods and procedures employed to interpret data, identify patterns, and draw conclusions.&lt;SEP&gt;Methods employed to interpret data, identify patterns, and derive conclusions.&lt;SEP&gt;Methods employed to interpret data, identify patterns, and draw conclusions, such as statistical analyses or computational methods.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Core Concepts">
  <data key="d0">Core Concepts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Fundamental ideas and principles that underpin the subject matter, providing a basis for understanding and further analysis.&lt;SEP&gt;Fundamental ideas or principles that form the basis of a discipline or activity.&lt;SEP&gt;Fundamental ideas or principles that underpin a discipline or activity.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Objects of Study">
  <data key="d0">Objects of Study</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specific entities, phenomena, or subjects that are examined or analyzed in research.&lt;SEP&gt;Subjects or entities being examined in the research, such as phenomena, processes, or systems.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Discipines">
  <data key="d0">Discipines</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Branches of knowledge or academic fields related to the activity or research.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Disciplines">
  <data key="d0">Disciplines</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Academic fields or branches of knowledge related to the research or activity.&lt;SEP&gt;Academic or professional fields contributing to or focusing on the research area, such as computer science, statistics, or engineering.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Taxonomies">
  <data key="d0">Taxonomies</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">Classification systems that organize entities or concepts into hierarchical categories.&lt;SEP&gt;Classifications and categorizations that organize concepts, entities, or phenomena within the field.</data>
  <data key="d3">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Study Populations/Dataset">
  <data key="d0">Study Populations/Dataset</data>
  <data key="d1">&lt;Study Populations/Dataset</data>
  <data key="d2">A dataset consisting of 420 diverse coding tasks related to scientific and parallel computing, used to evaluate the performance of various language models.&lt;SEP&gt;Groups of subjects or data collections used for research analysis.&lt;SEP&gt;The dataset includes 420 diverse coding tasks related to scientific and parallel computing, used to evaluate various state-of-the-art language models.&lt;SEP&gt;The group of subjects or the dataset used for analysis, representing the population or sample under investigation.&lt;SEP&gt;Various datasets and computational workloads used to evaluate performance and scalability of high-performance computing techniques.&lt;SEP&gt;Various datasets and computational workloads used to evaluate performance, scalability, and portability in high-performance computing research.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011&lt;SEP&gt;chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Applications/Implications">
  <data key="d0">Applications/Implications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhancing development tools for high-performance computing (HPC&lt;SEP&gt;Practical uses or consequences derived from research findings or activity outcomes.&lt;SEP&gt;Practical uses or consequences derived from research findings, influencing policy, technology, or practice.&lt;SEP&gt;The research aims to enhance performance portability, optimize scheduling, and improve computational efficiency across heterogeneous architectures, impacting scientific simulations, data analysis, and large-scale computations.&lt;SEP&gt;The research aims to improve performance portability, optimize scheduling, and enhance computational efficiency across heterogeneous architectures, impacting scientific computing, simulation, and data processing.&lt;SEP&gt;Understanding how to optimize fine-tuning and synthetic data generation for improving HPC code LLMs, with implications for accelerating scientific computing workflows.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e&lt;SEP&gt;chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="planet2">
  <data key="d0">planet2</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A variable representing the second planet in a sequence, used in code for planetary calculations.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="planet_names">
  <data key="d0">planet_names</data>
  <data key="d1">Variables</data>
  <data key="d2">A list or array containing names of planets, used for referencing planet identifiers.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="planet1_index">
  <data key="d0">planet1_index</data>
  <data key="d1">Variables</data>
  <data key="d2">An index indicating the position of the first planet in a list or array, used to determine neighboring planets.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="planet2_index">
  <data key="d0">planet2_index</data>
  <data key="d1">Variables</data>
  <data key="d2">An index indicating the position of the second planet in a list or array, used in calculations involving planets.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="anti_shuffle">
  <data key="d0">anti_shuffle</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that takes a string and returns an ordered version where all characters in each word are sorted based on ASCII values, preserving word order and spaces.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="count_up_to">
  <data key="d0">count_up_to</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that takes a non-negative integer n and returns a list of prime numbers less than n, up to the first n primes.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="smallest_change">
  <data key="d0">smallest_change</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that calculates the minimum number of changes needed to make an array palindromic by changing individual elements.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="F. Supplemental Bias Analysis">
  <data key="d0">F. Supplemental Bias Analysis</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An analysis discussing how generative models, including code generation models like Codex, can encode biases present in training data, leading to potential harms such as allocative and representational harms.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generative models">
  <data key="d0">Generative models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Models that generate content such as natural language, images, or code, which can inherit biases from their training data.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in code generation">
  <data key="d0">Bias in code generation</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">The phenomenon where code generated by models like Codex reflects societal biases, potentially causing harm when used in applications.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Allocative harms">
  <data key="d0">Allocative harms</data>
  <data key="d1">Results</data>
  <data key="d2">Harms that occur when a system allocates or withholds opportunities or resources based on biased or stereotypical outputs.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Representational harms">
  <data key="d0">Representational harms</data>
  <data key="d1">Results</data>
  <data key="d2">Harms that reinforce stereotypes or subordinate groups through biased representations in generated content.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Review and verification">
  <data key="d0">Review and verification</data>
  <data key="d1">Tools</data>
  <data key="d2">Processes recommended to assess the accuracy and fairness of code generated by models, to mitigate bias-related harms.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias probes">
  <data key="d0">Bias probes</data>
  <data key="d1">Tools</data>
  <data key="d2">Testing methods developed to detect bias in model outputs, such as biased classification or stereotypical completions.</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Protected Classes Classification Prompts">
  <data key="d0">Protected Classes Classification Prompts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Prompts designed to classify individuals into social categories like gender, race, and age, which may be leading or biased, used to probe model responses.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Language Models">
  <data key="d0">Bias in Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Models such as GPT-3 and Codex exhibit biases learned from training data, which can manifest in harmful or prejudiced outputs.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Harmful Code Generation">
  <data key="d0">Harmful Code Generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Generation of code or comments that reflect social biases, stereotypes, or prejudiced views due to biased training data or prompts.&lt;SEP&gt;Techniques that involve prompts or training data leading models to produce biased, harmful, or stereotypical code or text.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Analysis in Text">
  <data key="d0">Bias Analysis in Text</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Methods like co-occurrence tests to evaluate biases in generated comments or code, measuring word associations related to social groups.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Code Generation">
  <data key="d0">Bias in Code Generation</data>
  <data key="d1">Results</data>
  <data key="d2">Models may reinforce social biases present in training data, leading to harmful stereotypes or prejudice in code comments or outputs.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Social Biases">
  <data key="d0">Social Biases</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Prejudiced representations of gender, race, religion, and other social categories in AI-generated text and code.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dataset of Code and Text">
  <data key="d0">Dataset of Code and Text</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Corpora used for training models like Codex and GPT-3, which encode social biases.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Mitigation Strategies">
  <data key="d0">Bias Mitigation Strategies</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Approaches aimed at reducing harmful biases in language models, including dataset curation and prompt design.&lt;SEP&gt;Approaches like fine-tuning, prompt design, and dataset curation aimed at reducing biases in model outputs.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Reinforcement">
  <data key="d0">Bias Reinforcement</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges in fully eliminating biases due to the models' reliance on biased training data and the complexity of social categories.&lt;SEP&gt;Models may reinforce societal biases present in training datasets, leading to harmful stereotypes in generated content.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Prompts for Classification">
  <data key="d0">Bias in Prompts for Classification</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Prompts designed to classify individuals into social categories like gender, race, and age, which may be leading or biased, used to probe model responses.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Biased Prompts">
  <data key="d0">Biased Prompts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Prompts that inadvertently reinforce harmful stereotypes or biases when used to test or train language models.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Text Generated by Codex">
  <data key="d0">Bias in Text Generated by Codex</data>
  <data key="d1">Results</data>
  <data key="d2">Codex can produce biased or prejudiced comments or code snippets, reflecting learned societal biases.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Detection Methods">
  <data key="d0">Bias Detection Methods</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Co-occurrence tests and other statistical measures used to identify biases in model outputs, such as biased word associations.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Responses">
  <data key="d0">Model Responses</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Outputs generated by language models in response to prompts, which can include biased or prejudiced content.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Model Responses">
  <data key="d0">Bias in Model Responses</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The phenomenon where language models produce outputs that reflect societal biases, stereotypes, or prejudices.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Evaluation Procedures">
  <data key="d0">Bias Evaluation Procedures</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Procedures such as co-occurrence tests to evaluate and quantify biases in model-generated text or code.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias Impact">
  <data key="d0">Bias Impact</data>
  <data key="d1">Results</data>
  <data key="d2">The potential societal harms caused by biased outputs, including reinforcement of stereotypes and prejudice.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Out-of-Distribution Usage">
  <data key="d0">Bias in Out-of-Distribution Usage</data>
  <data key="d1">Limitations</data>
  <data key="d2">Models may behave differently when prompted with out-of-distribution inputs, potentially exacerbating biases.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in Comment Generation">
  <data key="d0">Bias in Comment Generation</data>
  <data key="d1">Results</data>
  <data key="d2">Codex's comments can reproduce biases similar to GPT-3, especially when explicitly prompted about sensitive groups.</data>
  <data key="d3">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="fashion">
  <data key="d0">fashion</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Fashion refers to the prevailing styles and trends in clothing, accessories, and personal appearance, influencing social and cultural expression.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Supplemental Security Analysis">
  <data key="d0">Supplemental Security Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A detailed evaluation of security threats, vulnerabilities, and potential misuse of AI models like Codex, including threat actor profiling and application assessments.&lt;SEP&gt;A detailed examination of security threats, vulnerabilities, and potential misuse of AI models like Codex, involving threat actor profiling and application assessment.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Threat Actors">
  <data key="d0">Threat Actors</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Threat actors include a range of malicious entities from low-skilled individuals to well-resourced organized groups, with strategic objectives such as financial gain, chaos, information theft, or operational goals.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Malicious Code">
  <data key="d0">Malicious Code</data>
  <data key="d1">Results</data>
  <data key="d2">Code generated by Codex can be incorporated into complex systems, with limited proficiency in generating standalone malicious payloads but capable of recursive encryption scripts.&lt;SEP&gt;Codex can generate code snippets that may be incorporated into complex malicious systems; it struggles with standalone malicious payloads but can produce recursive encryption scripts.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Vulnerability Discovery">
  <data key="d0">Vulnerability Discovery</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Experiments comparing Codex's ability to identify vulnerabilities against static analysis tools, assessing its effectiveness and limitations in security contexts.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Supply Chain Attack">
  <data key="d0">Supply Chain Attack</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Risks involve Codex suggesting malicious or typosquatted software dependencies, potentially leading to vulnerabilities in downstream applications.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Phishing Pretext">
  <data key="d0">Phishing Pretext</data>
  <data key="d1">Potential Misuse Applications</data>
  <data key="d2">Codex models trained on source code are tested for their suitability in generating phishing scenarios, with findings indicating no significant advantage over conventional models.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Insecure Code">
  <data key="d0">Insecure Code</data>
  <data key="d1">Results</data>
  <data key="d2">Codex may suggest insecure or malicious code, such as compromised dependencies or insecure functions, posing supply chain risks if widely adopted.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fashion">
  <data key="d0">Fashion</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Fashion refers to the prevailing styles, trends, and social expressions related to clothing, accessories, and personal appearance, influencing cultural identity and social dynamics.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pre-training and Fine-tuning Processes">
  <data key="d0">Pre-training and Fine-tuning Processes</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The training procedures for Codex, involving pre-training on large datasets and fine-tuning, which affect trust boundaries and susceptibility to adversarial inputs.</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Model Limitations">
  <data key="d0">Model Limitations</data>
  <data key="d1">Limitations</data>
  <data key="d2">Constraints such as lack of pre-training on code (e.g., GPT-2) and effects of catastrophic forgetting impacting performance.&lt;SEP&gt;Current limitations of Codex in generating effective malicious payloads or discovering complex vulnerabilities, which may improve with future research.&lt;SEP&gt;Specific models like GPT-4 show weaknesses on certain parallel problems such as Kokkos search problems, indicating limitations in handling complex parallel code.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-bf1121c44634b616af61c49cd3adf29a&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="tuning processes">
  <data key="d0">tuning processes</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Processes involved in adjusting and optimizing models, generally considered untrusted due to security and reliability risks.&lt;SEP&gt;Tuning processes refer to methods and practices involved in adjusting and optimizing models, which are generally considered untrusted due to inherent risks and uncertainties.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="interest of potential attackers">
  <data key="d0">interest of potential attackers</data>
  <data key="d1">Variables</data>
  <data key="d2">The interest of potential attackers indicates the level of malicious intent or focus on exploiting model vulnerabilities, affecting security risks.&lt;SEP&gt;The level of malicious interest or focus from actors seeking to exploit model vulnerabilities, impacting security risks.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Codex model">
  <data key="d0">Codex model</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An AI system capable of generating code, which may suggest insecure or malicious code, posing supply chain and security risks.&lt;SEP&gt;The Codex model is an AI system capable of generating code, which may suggest insecure or malicious code and pose supply chain risks.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="supply chain risk">
  <data key="d0">supply chain risk</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Risks to software supply chains due to insecure code generated by models like Codex, leading to potential widespread vulnerabilities.&lt;SEP&gt;Supply chain risk refers to the potential for compromised or insecure code generated by models like Codex to affect software infrastructure.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security-relevant behaviors">
  <data key="d0">security-relevant behaviors</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Security-relevant behaviors involve actions like generating insecure code, which can lead to vulnerabilities in software systems.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="insecure code">
  <data key="d0">insecure code</data>
  <data key="d1">Results</data>
  <data key="d2">Code that contains vulnerabilities or insecure configurations, often produced due to training on untrusted data containing insecure code snippets.&lt;SEP&gt;Insecure code is code that contains vulnerabilities or unsafe configurations, often produced by models trained on untrusted data.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="cryptographic libraries">
  <data key="d0">cryptographic libraries</data>
  <data key="d1">Tools</data>
  <data key="d2">Cryptographic libraries are software components used to implement encryption, which can be misused if insecure code is generated.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="RSA keys">
  <data key="d0">RSA keys</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">RSA keys are cryptographic keys used for encryption; their proper configuration is critical for security, and insecure configurations can be generated by models.&lt;SEP&gt;RSA keys are cryptographic keys used in RSA encryption, with security depending on length and implementation.&lt;SEP&gt;RSA keys are cryptographic keys used in RSA encryption, with security depending on their length; keys shorter than 2048 bits are considered insecure.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AES contexts">
  <data key="d0">AES contexts</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">AES contexts are cryptographic configurations for encryption; improper setups like ECB mode are insecure and can be produced by models.&lt;SEP&gt;AES contexts involve the use of AES encryption modes, such as ECB, which impact security.&lt;SEP&gt;AES contexts involve the use of AES encryption modes, such as ECB, which impact the security of encrypted data.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="cryptographic vulnerabilities">
  <data key="d0">cryptographic vulnerabilities</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">Categories of weaknesses or insecure configurations in cryptographic implementations, which models may inadvertently produce.&lt;SEP&gt;Categorization of specific weaknesses in cryptographic implementations, relevant for evaluating generated code.&lt;SEP&gt;Cryptographic vulnerabilities categorize specific weaknesses or insecure configurations in cryptographic implementations.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="economic and labor market implications">
  <data key="d0">economic and labor market implications</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Implications refer to the potential economic impacts of code generation technology, including effects on employment and productivity.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="automating coding tasks">
  <data key="d0">automating coding tasks</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Automating coding tasks can increase efficiency but also introduce security vulnerabilities if insecure code is produced.&lt;SEP&gt;Using AI to automate programming can increase productivity but may also lead to insecure code if not properly managed.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="cryptography experts">
  <data key="d0">cryptography experts</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Cryptography experts are specialists in encryption and security protocols who provide consensus on cryptographic configurations and their security implications.&lt;SEP&gt;Cryptography experts are specialists in securing communication and data through encryption, providing consensus on insecure configurations.&lt;SEP&gt;Cryptography experts are specialists who assess cryptographic security and evaluate the safety of generated cryptographic code.</data>
  <data key="d3">chunk-aa0dc077f44be79e1c2829e5cb3b62c2&lt;SEP&gt;chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ECB">
  <data key="d0">ECB</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">ECB (European Central Bank) is a key financial institution responsible for monetary policy within the Eurozone, often less desired in certain contexts due to its policies or actions.&lt;SEP&gt;ECB (European Central Bank) is a key institution responsible for monetary policy in the Eurozone, often regarded as undesirable in certain contexts due to its policies or influence.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="conﬁgurations">
  <data key="d0">conﬁgurations</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Configurations refer to specific parameter settings in cryptographic systems, such as key lengths and cipher modes, influencing security.&lt;SEP&gt;Configurations refer to specific parameter settings in cryptographic systems, such as key lengths and cipher modes, which determine security or insecurity.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security standards">
  <data key="d0">security standards</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Security standards are evolving guidelines and best practices that define secure cryptographic configurations over time, influencing what is considered insecure.&lt;SEP&gt;Security standards are evolving guidelines and best practices that define secure cryptographic configurations over time.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="insecurity">
  <data key="d0">insecurity</data>
  <data key="d1">Results</data>
  <data key="d2">Insecurity refers to cryptographic configurations that are vulnerable, such as RSA keys shorter than 2048 bits or AES in ECB mode, which can be exploited.&lt;SEP&gt;Insecurity refers to the vulnerability of cryptographic outputs, such as short RSA keys or ECB mode AES, which can be exploited by attackers.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="code generation tools">
  <data key="d0">code generation tools</data>
  <data key="d1">Tools</data>
  <data key="d2">Automated systems that generate code, which may influence workforce skills, productivity, and accessibility in programming.&lt;SEP&gt;Code generation tools assist programmers by automatically producing code snippets, documentation, and tests, impacting software development efficiency.&lt;SEP&gt;Code generation tools assist programmers by automatically producing code snippets, documentation, and tests, impacting software development workflows.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9&lt;SEP&gt;chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="programmers and engineers">
  <data key="d0">programmers and engineers</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Programmers and engineers are primary users of code generation tools, with their productivity and roles potentially affected by AI-generated code.&lt;SEP&gt;Programmers and engineers are the primary users and beneficiaries of code generation tools, whose productivity and roles may be affected.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="software development tasks">
  <data key="d0">software development tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks in software development include writing code, documentation, testing, reviews, and collaboration, which can be influenced by AI tools.&lt;SEP&gt;Tasks include writing code, documentation, testing, reviews, and collaboration, which can be influenced by AI tools.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="economic implications">
  <data key="d0">economic implications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Economic implications involve how AI tools like Codex influence productivity, costs, labor markets, and new work opportunities in software engineering.&lt;SEP&gt;Economic implications involve how AI tools like Codex influence productivity, costs, labor markets, and the emergence of new work roles.&lt;SEP&gt;Variations in package import patterns can alter market dominance of certain packages, affecting the software supply chain and economic rewards.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9&lt;SEP&gt;chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security risks">
  <data key="d0">security risks</data>
  <data key="d1">Limitations</data>
  <data key="d2">Security risks are limitations of AI code generation, such as producing insecure cryptographic configurations, which can lead to vulnerabilities.&lt;SEP&gt;Security risks are limitations of current AI code generation, such as producing insecure cryptographic configurations, which pose potential vulnerabilities.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security vulnerabilities">
  <data key="d0">security vulnerabilities</data>
  <data key="d1">Results</data>
  <data key="d2">Security vulnerabilities are weaknesses in cryptographic configurations, such as insecure key lengths or modes, that compromise security.</data>
  <data key="d3">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Stack Overflow">
  <data key="d0">Stack Overflow</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Stack Overflow is an online community where users share knowledge, and data from it suggests patterns in user participation and role representation.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Women">
  <data key="d0">Women</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Women are comparatively more represented in data science and analysis roles than in DevOps, system administration, and site reliability roles, indicating gender disparities in technical roles.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Data Science and Analysis Roles">
  <data key="d0">Data Science and Analysis Roles</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Roles involving data analysis and scientific computing, with women being more represented in these areas compared to other engineering roles.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="DevOps, System Administrator, Site Reliability">
  <data key="d0">DevOps, System Administrator, Site Reliability</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Engineering roles with comparatively lower female representation.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Large Language Models Trained on Code">
  <data key="d0">Large Language Models Trained on Code</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Large language models like Codex are trained on vast code datasets to generate code, impacting programming workflows and labor dynamics.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Programming Languages">
  <data key="d0">Programming Languages</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Languages like C++, CUDA, and HIP are used to write code for different parallel execution models, supporting diverse hardware architectures.&lt;SEP&gt;Languages such as Python, used in educational and professional contexts, with growth in Python use influencing accessibility.&lt;SEP&gt;Languages like Python, C++, and others are analyzed for their popularity and usage in open source projects.&lt;SEP&gt;Programming languages like C++, Fortran, Python, and Julia are tools used in HPC, with their popularity and accessibility affecting code generation results.&lt;SEP&gt;Programming languages like C++, Fortran, Python, and Julia are tools used in HPC, with their popularity and accessibility influencing results.&lt;SEP&gt;Programming languages like C++, Python, and Julia are fundamental tools for software development, each with specific syntax, semantics, and applications in scientific computing.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python">
  <data key="d0">Python</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A dominant programming language in education and industry, known for high readability, with increasing use potentially broadening participation in programming.&lt;SEP&gt;A dominant, highly readable programming language used widely in education and industry, with increasing use potentially broadening programming participation.&lt;SEP&gt;Python is a high-level programming language used extensively in scientific computing, data analysis, and machine learning.&lt;SEP&gt;Python is a high-level programming language widely used in scientific computing, data analysis, and machine learning.&lt;SEP&gt;Python is a high-level, interpreted programming language favored for its ease of use, versatility, and extensive scientific libraries like NumPy and SciPy.&lt;SEP&gt;Python is a widely-used, high-level programming language valued for its simplicity, versatility, and extensive ecosystem, especially prominent in AI, research, and education.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Generation Tools">
  <data key="d0">Code Generation Tools</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tools like GitHub Copilot powered by large language models are studied for usability, performance, and security implications in software development.&lt;SEP&gt;Tools that automate code writing, which may affect workforce skills, productivity, and accessibility in programming.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Effects on Non-Engineers">
  <data key="d0">Effects on Non-Engineers</data>
  <data key="d1">Results</data>
  <data key="d2">Code generation tools can widen access to programming and automate repetitive tasks for non-engineering roles, influencing skill distribution.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Differential Package Import Rates">
  <data key="d0">Differential Package Import Rates</data>
  <data key="d1">Variables</data>
  <data key="d2">Patterns in how Codex imports packages vary based on training data, affecting software development practices and economic implications.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Package Import Patterns">
  <data key="d0">Package Import Patterns</data>
  <data key="d1">Variables</data>
  <data key="d2">The rates at which different packages are imported by Codex, influencing code robustness, security, and economic effects.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Economic Implications">
  <data key="d0">Economic Implications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Patterns of package import may impact the dominance of certain packages, affecting the software supply chain and related economic factors.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Safety and Security">
  <data key="d0">Safety and Security</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Import patterns can have safety and security implications, such as errors or vulnerabilities in code due to package choices.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research Questions/Hypotheses">
  <data key="d0">Research Questions/Hypotheses</data>
  <data key="d1">Questions</data>
  <data key="d2">Investigate how individual choices in data, model architecture, and prompt configuration impact the effectiveness of HPC code LLMs.&lt;SEP&gt;Main inquiry into societal and economic effects of AI code tools.&lt;SEP&gt;Specific inquiries or predictions that guide the research process and aim to address particular aspects of the study.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-706ca7a8d3fc8406a24b8328b849606b&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="users (Stack Overflow, 2020)">
  <data key="d0">users (Stack Overflow, 2020)</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Data source from Stack Overflow, indicating user participation patterns and role distributions in programming communities.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="women">
  <data key="d0">women</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Women are comparatively more represented in data science and analysis roles than in DevOps, system administrator, and site reliability roles, highlighting gender disparities in tech employment.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="data science and analysis roles">
  <data key="d0">data science and analysis roles</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Roles involving data analysis, scientific computing, and related tasks, with higher female representation compared to other engineering roles.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="DevOps, system administrator, site reliability">
  <data key="d0">DevOps, system administrator, site reliability</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Engineering roles with comparatively lower female representation, indicating occupational gender gaps.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="programming languages">
  <data key="d0">programming languages</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Languages such as Python, used in education and industry, whose adoption and characteristics influence accessibility and productivity.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="effects on non-engineers">
  <data key="d0">effects on non-engineers</data>
  <data key="d1">Results</data>
  <data key="d2">Code generation tools could democratize programming, enabling non-engineers to perform coding tasks and automate repetitive processes, affecting skill distribution.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="differential package import rates">
  <data key="d0">differential package import rates</data>
  <data key="d1">Variables</data>
  <data key="d2">Patterns in how Codex imports third-party packages vary based on training data, influencing software development practices and economic effects.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="package import patterns">
  <data key="d0">package import patterns</data>
  <data key="d1">Variables</data>
  <data key="d2">The frequency and choice of imported packages by Codex, which affect code robustness, security, and economic implications.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="safety and security">
  <data key="d0">safety and security</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Import patterns may introduce errors or vulnerabilities, impacting the safety and security of generated code.</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="research questions/hypotheses">
  <data key="d0">research questions/hypotheses</data>
  <data key="d1">Research Questions</data>
  <data key="d2">How do AI code generation models influence societal inequality, workforce dynamics, and economic structures?</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="prompt engineering">
  <data key="d0">prompt engineering</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Prompt engineering involves crafting prompts to guide Codex's code generation, affecting the suggestions and outputs produced by the model.&lt;SEP&gt;The manual crafting of prompts to guide language models, which DSPy aims to reduce or eliminate through modularization.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="machine learning packages">
  <data key="d0">machine learning packages</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Machine learning packages like TensorFlow and PyTorch are libraries used for developing machine learning models, with their selection influenced by Codex's suggestions.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="open-source developers">
  <data key="d0">open-source developers</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Open-source developers maintain machine learning packages, and their resources and practices are impacted by the need for backward compatibility and model biases.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="biases in model outputs">
  <data key="d0">biases in model outputs</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Biases in Codex's suggestions may reflect training data limitations, influencing package popularity and developer incentives.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="long-run labor market">
  <data key="d0">long-run labor market</data>
  <data key="d1">Discipline</data>
  <data key="d2">The long-term effects of AI code generation on labor markets, wages, and worker productivity are areas requiring further research.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="deployment scenarios">
  <data key="d0">deployment scenarios</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Different deployment scenarios of Codex can be studied to understand impacts on code quality, productivity, and economic outcomes.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="research on code generation and automation">
  <data key="d0">research on code generation and automation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future research aims to measure Codex's economic value, impact on documentation, testing practices, worker productivity, barriers to entry, and broader societal effects.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="potential impacts on the labor market">
  <data key="d0">potential impacts on the labor market</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Understanding how AI code generation influences employment, wages, and skill requirements in programming and engineering fields.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Time">
  <data key="d0">Time</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Time refers to the temporal aspect of user adaptation and system interactions, indicating how users learn and change behaviors over periods.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Users">
  <data key="d0">Users</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Users are individuals interacting with systems like Codex, learning prompt engineering, and adapting their workflows.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Prompt engineering">
  <data key="d0">Prompt engineering</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Designing and crafting input prompts to guide the model’s inference process towards domain-specific outputs.&lt;SEP&gt;Designing specific input prompts to steer the model’s inference process toward domain-relevant outputs, often at the input level.&lt;SEP&gt;Prompt engineering involves designing input prompts to guide AI models like Codex to produce desired outputs, influencing the suggestions and their relevance.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1&lt;SEP&gt;chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Decision-making tool">
  <data key="d0">Decision-making tool</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Codex functions as a decision-making tool, aiding users in selecting machine learning packages and coding methods, impacting workflow efficiency.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Search engine">
  <data key="d0">Search engine</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Codex also acts as a search engine, replacing traditional internet searches for code-related queries, affecting information retrieval practices.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Machine learning package">
  <data key="d0">Machine learning package</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Machine learning packages such as TensorFlow and PyTorch are libraries used for developing machine learning models, and their suggestions influence user choices and market dynamics.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open-source developers">
  <data key="d0">Open-source developers</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Developers maintain open-source packages, and their practices are impacted by the suggestions made by Codex, especially regarding backward compatibility.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training data">
  <data key="d0">Training data</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Training data includes datasets used to train Codex, which may contain biases or outdated information affecting its suggestion accuracy and relevance.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Biases in model outputs">
  <data key="d0">Biases in model outputs</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Biases in Codex's suggestions may reflect biases in training data, leading to skewed recommendations and entrenchment in certain packages.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Market">
  <data key="d0">Market</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The market for machine learning packages and code generation tools, influenced by Codex suggestions and user adoption patterns.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open-source projects">
  <data key="d0">Open-source projects</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Open-source projects are community-maintained codebases potentially affected by Codex's suggestions for deprecated methods and backward compatibility.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Future directions">
  <data key="d0">Future directions</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future research aims to assess Codex's economic impact, biases, influence on documentation and testing practices, worker productivity, and societal effects.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Labor market">
  <data key="d0">Labor market</data>
  <data key="d1">Discipline</data>
  <data key="d2">The long-term labor market implications of AI code generation technologies, including effects on wages, employment, and skill requirements.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Deployment scenarios">
  <data key="d0">Deployment scenarios</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Different scenarios of deploying Codex are studied to understand impacts on code quality, productivity, and economic outcomes.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research on code generation and automation">
  <data key="d0">Research on code generation and automation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research focuses on measuring the benefits and risks of code generation, automation impacts, and societal implications.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Impacts on worker productivity, wages, and quality of life">
  <data key="d0">Impacts on worker productivity, wages, and quality of life</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Understanding how AI tools like Codex affect employment conditions, wages, and overall worker well-being.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in suggestions">
  <data key="d0">Bias in suggestions</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Biases in Codex's suggestions may lead to preferential entrenchment of certain packages and methods, impacting innovation and diversity.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open-source development practices">
  <data key="d0">Open-source development practices</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Practices of maintaining, updating, and ensuring backward compatibility in open-source projects are influenced by Codex suggestions.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Educational and career progression">
  <data key="d0">Educational and career progression</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">AI code generation tools could lower barriers to entry for new programmers and influence educational pathways.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Societal effects">
  <data key="d0">Societal effects</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Broader societal impacts include shifts in employment, skill requirements, and economic inequality due to AI-driven automation.</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Models">
  <data key="d0">Models</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d2">Large Language Models (LLMs) such as GPT-3.5, GPT-4, Phind-V2, and others evaluated for their ability to generate correct parallel code.&lt;SEP&gt;Models refer to large language models (LLMs) like GPT-3.5, GPT-4, Phind-V2, and others evaluated for code generation capabilities.&lt;SEP&gt;The evaluation framework provides standardized metrics and datasets to systematically compare model performance in code generation."|&gt;"benchmarking, assessment&lt;SEP&gt;Various LLMs such as CodeLlama, StarCoder, GPT-3.5, and GPT-4 are evaluated for their ability to generate accurate code based on prompts.&lt;SEP&gt;Various LLMs such as CodeLlama, StarCoder, GPT-3.5, and GPT-4 are evaluated for their code generation capabilities, with details on architecture, size, and licensing.&lt;SEP&gt;Various models like CodeLlama and StarCoderBase are based on Llama 2 architecture, fine-tuned specifically for code generation and translation tasks.&lt;SEP&gt;Models are structured computational frameworks, like DeepSeek-Coder variants, used to generate or understand code in HPC environments.</data>
  <data key="d1">Objects of Study</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Kulal et al. (2019)">
  <data key="d0">Kulal et al. (2019)</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d2">Kulal et al. (2019) evaluate functional correctness using the pass@k metric, which estimates the probability that generated code samples pass unit tests.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="secure code execution">
  <data key="d0">secure code execution</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d2">A sandbox environment was developed to safely run untrusted programs against unit tests, preventing malicious activity and resource compromise.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="security boundary">
  <data key="d0">security boundary</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d2">gVisor provides a security boundary by emulating host resources, protecting the host from malicious containers during code execution.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="cloud infrastructure">
  <data key="d0">cloud infrastructure</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d2">Kubernetes is used as part of the infrastructure for training and deploying models, addressing limitations of cloud environments.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="results">
  <data key="d0">results</data>
  <data key="d3">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d2">The evaluation process assesses the performance and correctness of code generation models like Codex using datasets and metrics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evaluation">
  <data key="d0">Evaluation</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-fd545f17fc1dff68e5c09ae892fd670e&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-4bb8bb2fc231e52a13ed0e91512e55eb&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b&lt;SEP&gt;chunk-5f6ee49c82aaaf22c167cac791d43a0f&lt;SEP&gt;chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19&lt;SEP&gt;chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d2">Evaluation assesses large language models' (LLMs) ability to generate code based on prompts, measuring accuracy and performance across models and tasks.&lt;SEP&gt;Evaluation involves assessing the performance of large language models (LLMs) in code generation and translation tasks across different models and prompts.&lt;SEP&gt;Pass@k measures the likelihood that at least one generated sample passes unit tests, reflecting the model's problem-solving ability.&lt;SEP&gt;The final phase where the model's performance is assessed against benchmarks, feedback is collected, and refinements are made.&lt;SEP&gt;The final step of testing the specialized model against benchmarks, gathering feedback, and refining it accordingly.&lt;SEP&gt;Evaluation involves systematically testing the generated kernels across different programming models and hardware (CPU, GPU) to assess correctness, performance, and suitability for HPC applications.&lt;SEP&gt;Evaluation involves systematically testing the generated kernels across various programming models and hardware (CPU, GPU) to assess quality, correctness, and performance.&lt;SEP&gt;Methods for categorizing suggestion correctness and analyzing the influence of prompt structure and language.&lt;SEP&gt;The RAG models outperform parametric-only seq2seq models and task-specific retrieve-and-extract architectures on three open domain QA tasks, generating more specific, diverse, and factual language.&lt;SEP&gt;Evaluation involves assessing the effectiveness of code optimization tools through benchmarks and real-world examples, measuring performance improvements.&lt;SEP&gt;The process of assessing the program’s performance using metrics such as score, to determine effectiveness.&lt;SEP&gt;Evaluation involves assessing the performance of fine-tuned code LLMs against benchmarks like ParEval to measure their effectiveness in real parallel code generation tasks.</data>
  <data key="d1">Study Designs</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model size and performance">
  <data key="d0">Model size and performance</data>
  <data key="d3">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d2">Performance metrics such as test loss and pass@k scale according to a power law with model size, indicating predictable improvements.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Study Population/Dataset">
  <data key="d0">Study Population/Dataset</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d2">The APPS dataset provides a set of coding problems and unit tests for evaluating language models' coding abilities."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evidence Types">
  <data key="d0">Evidence Types</data>
  <data key="d3">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d2">BLEU scores are used as evidence to gauge the similarity and potential correctness of generated solutions."|&lt;SEP&gt;Categories of data and proof used to support scientific claims, including qualitative, quantitative, or mixed evidence.</data>
  <data key="d1">Evidence Types</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Metrics">
  <data key="d0">Metrics</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d2">Evaluation metrics include correctness, accuracy, and performance of generated code, used to compare models and translation quality.&lt;SEP&gt;Metrics like pass@1 and performance scores (e.g., HumanEval, MBPP) quantify the effectiveness of models in code generation tasks.&lt;SEP&gt;Metrics like pass@1 and performance scores (e.g., HumanEval, MBPP) quantify the success rate of models in generating correct code solutions.&lt;SEP&gt;Pass@k metrics are used to evaluate the success rate of models in passing code tests at various thresholds."|&gt;"performance metrics, evaluation standards&lt;SEP&gt;Two novel metrics introduced for assessing runtime performance and scaling behavior of generated code.&lt;SEP&gt;Metrics are evaluation standards, such as accuracy and trustworthiness, that are crucial for assessing AI performance in code generation.&lt;SEP&gt;Metrics are standards such as accuracy and trustworthiness used to evaluate AI-generated code, essential for assessing performance and reliability.</data>
  <data key="d1">Variables</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Study Design">
  <data key="d0">Study Design</data>
  <data key="d3">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d2">Designs the comparative evaluation of models using standardized metrics."|&gt;"study design, comparative analysis</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Tracing Methodology">
  <data key="d0">Tracing Methodology</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">CI employs tracing methodologies like sys.setprofile to monitor function calls and collect input-output data during testing.&lt;SEP&gt;CI workflows employ tracing methodologies, such as sys.setprofile, to collect function inputs and outputs during testing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CI Frameworks">
  <data key="d0">CI Frameworks</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">GitHub repositories often use CI frameworks like Travis and Tox to automate build and test processes.&lt;SEP&gt;GitHub repositories often use CI tools such as Travis and Tox to automate build, test, and deployment processes.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Python Package Index">
  <data key="d0">Python Package Index</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">PyPI provides code modules and packages used as sources for problem objects and training data.&lt;SEP&gt;PyPI provides code objects that can be used for problem creation and model training.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Source of Code">
  <data key="d0">Source of Code</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">PyPI provides code modules and packages used as sources for problem objects and training data.&lt;SEP&gt;PyPI provides code objects that can be used for problem creation and model training.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Fine-tuned GPT-Neo">
  <data key="d0">Fine-tuned GPT-Neo</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">GPT-Neo models are fine-tuned on curated problems and reference solutions to improve code generation accuracy.&lt;SEP&gt;GPT-Neo models are fine-tuned using curated problems and reference solutions to improve code generation performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Performance Metric">
  <data key="d0">Performance Metric</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">pass@k measures the percentage of test cases passed within top k solutions, evaluating model accuracy.&lt;SEP&gt;pass@k measures the success rate of passing test cases within top k solutions, serving as an evaluation metric.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Sampling Parameter">
  <data key="d0">Sampling Parameter</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">Adjusting temperature influences the diversity of model outputs, affecting the likelihood of passing test cases.&lt;SEP&gt;Temperature influences the diversity of model outputs, affecting the likelihood of passing test cases.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Filtering Problems">
  <data key="d0">Filtering Problems</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">Filtering applies criteria to remove problematic or low-quality problems, ensuring dataset integrity.&lt;SEP&gt;Filtering ensures only high-quality, deterministic problems are used for training or evaluation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training Problems">
  <data key="d0">Training Problems</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">Training problems serve as datasets for fine-tuning language models like Codex or GPT-Neo.&lt;SEP&gt;Training problems serve as datasets to fine-tune models like Codex and GPT-Neo for better code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Reference Solution">
  <data key="d0">Reference Solution</data>
  <data key="d3">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d2">Reference solutions are used as target outputs during supervised fine-tuning of models.&lt;SEP&gt;Reference solutions are used as targets during model fine-tuning to guide correct output generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="docstring generation">
  <data key="d0">docstring generation</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Codex-D is trained to generate descriptive docstrings from code, aiming to describe the function's purpose and behavior.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Manual Grading">
  <data key="d0">Manual Grading</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Manual grading of generated docstrings assesses their correctness and completeness, compensating for the lack of automatic metrics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="evaluation">
  <data key="d0">evaluation</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d2">Manual grading of generated docstrings assesses their correctness and completeness, compensating for the lack of automatic metrics.&lt;SEP&gt;The process of assessing DSPy across different case studies, including math word problems and multi-hop question answering, to measure performance improvements.&lt;SEP&gt;The process of systematically assessing the performance of language model pipelines across various metrics and configurations, including accuracy improvements from module compilation.</data>
  <data key="d1">Study Designs</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Performance">
  <data key="d0">Performance</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Codex-S outperforms Codex in pass@k metrics, demonstrating improved efficiency and accuracy.&lt;SEP&gt;The effectiveness of fine-tuned models, measured through metrics such as code generation accuracy or ability to handle specific tasks, varies based on model size, data amount, and data quality.&lt;SEP&gt;The effectiveness of the fine-tuned HPC models in generating parallel code, with results indicating they outperform other open-source models in accuracy and efficiency.&lt;SEP&gt;The effectiveness of the fine-tuned HPC models in generating parallel code, with the study finding these models to be the best open-source options currently.</data>
  <data key="d1">Results</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Generated Samples">
  <data key="d0">Generated Samples</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Sample ranking methods select the best samples based on log probabilities or back-translation scores.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generated Docstrings">
  <data key="d0">Generated Docstrings</data>
  <data key="d3">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Manual grading assesses the correctness of generated docstrings, compensating for the lack of automatic evaluation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generated Code">
  <data key="d0">Generated Code</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d2">Code produced by the model in response to prompts, evaluated for correctness and functionality.&lt;SEP&gt;The quality of generated code is measured by pass@k and manual evaluation, reflecting correctness and usefulness.</data>
  <data key="d1">Results</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Metrics">
  <data key="d0">Performance Metrics</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-7d9c04128411fd2962a424d26641c6ad&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-9ff3c43f969888578a1b963a36023834&lt;SEP&gt;chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7&lt;SEP&gt;chunk-5ff9371dcd9f0dcbeb25583bbba6123c&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d2">Metrics like Bleu, Rouge-L, and accuracy quantify the success of models on various QA and classification tasks.&lt;SEP&gt;Metrics such as Rouge-L and Bleu-1 used to evaluate the quality of responses based on the number of retrieved documents in RAG models.&lt;SEP&gt;Quantitative measures such as task performance scores (ϕ, ϕ_D) used to evaluate the effectiveness of models and adapters.&lt;SEP&gt;Quantitative measures such as task performance scores (ϕ, ϕ_D) used to evaluate the success of models and adaptation strategies.&lt;SEP&gt;The quality of generated code is measured by pass@k and manual evaluation, reflecting correctness and usefulness.&lt;SEP&gt;Quantitative measures used to evaluate the correctness, efficiency, and quality of the code generated by LLMs in the benchmark.&lt;SEP&gt;Quantitative measures used to evaluate and compare the effectiveness of different DSP Y pipelines, such as accuracy improvements.&lt;SEP&gt;Performance metrics like pass@1 measure the accuracy and effectiveness of models in solving specific problems within a set of test cases.&lt;SEP&gt;Quantitative measures used to evaluate the effectiveness of models in generating accurate and efficient parallel code.</data>
  <data key="d1">Results</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Models like Codex">
  <data key="d0">Models like Codex</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d2">Implementation of safety strategies&lt;SEP&gt;Risk mitigation strategies</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Disciplinary Approach">
  <data key="d0">Disciplinary Approach</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d2">Ongoing engagement with policymakers, ethical standards, and safety practices to ensure responsible AI deployment.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Algorithms">
  <data key="d0">Algorithms</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d2">Neural program learning encompasses methods like program induction, synthesis, and interpreters, enabling models to generate or execute code directly.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Theories/Models">
  <data key="d0">Theories/Models</data>
  <data key="d3">chunk-a17c97bd5849d0ccac2f6a28eab01a8e&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d2">Led to advances in neural networks that underpin program induction and code generation capabilities.&lt;SEP&gt;Structured frameworks or conceptual models that explain phenomena or guide research within the field.</data>
  <data key="d1">Theories/Models</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Probabilistic Context-Free Grammar">
  <data key="d0">Probabilistic Context-Free Grammar</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">PCFGs are classical formal models used to generate syntactic structures of programs, facilitating probabilistic program generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AST">
  <data key="d0">AST</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">ASTs are used to represent program syntax structures, and models like Maddison &amp; Tarlow (2014) improve synthesis by conditioning on AST-related features.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hindle et al. (2012)">
  <data key="d0">Hindle et al. (2012)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Investigated predictability of code using n-gram models, showing code is more predictable than natural language, aiding in code modeling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Language Models">
  <data key="d0">Language Models</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3&lt;SEP&gt;chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-3c24b46980eea930fd6590d61f0b0f25&lt;SEP&gt;chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-ba9b42eee12401dc9fac455e631a8fd8&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d2">Investigated predictability of code using n-gram models, showing code is more predictable than natural language, aiding in code modeling.&lt;SEP&gt;Language models are statistical models trained to predict sequences of tokens, including causal (left-to-right), masked, and encoder-decoder architectures.&lt;SEP&gt;Language models predict sequences of tokens, trained on natural language or code, and can be causal, masked, or encoder-decoder types.&lt;SEP&gt;Language models are complex neural network architectures designed to understand, generate, and process human language, often used in NLP tasks.&lt;SEP&gt;Language models are computational systems designed to understand, generate, and process human language, encompassing various architectures and training techniques.&lt;SEP&gt;Language models are computational systems designed to understand, generate, and process human language, often built with deep learning architectures.&lt;SEP&gt;Language models are models trained to understand and generate human language, including models like GPT and T5, used for tasks such as translation, summarization, and text generation.&lt;SEP&gt;Language models are computational systems designed to understand and generate human language, often trained using unsupervised learning techniques and capable of multitask learning.</data>
  <data key="d1">Core Concepts</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ling et al. (2016)">
  <data key="d0">Ling et al. (2016)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Developed latent predictor networks that generate code for specific tasks, such as Magic: The Gathering cards, using character-level models.&lt;SEP&gt;Developed latent predictor networks that generate code for tasks like Magic: The Gathering, using character-level models with latent modes."|&lt;"code generation, latent modes</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="DeepCoder">
  <data key="d0">DeepCoder</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">DeepCoder predicts functions in source code to guide program search, improving synthesis efficiency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Large Transformers">
  <data key="d0">Large Transformers</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Large-scale transformer models are applied to program synthesis, leveraging extensive datasets for improved code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Models/Techniques">
  <data key="d0">Models/Techniques</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Large-scale transformer models are applied to program synthesis, leveraging extensive datasets for improved code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="PyMT5">
  <data key="d0">PyMT5</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">PyMT5 translates between different code languages and natural language, enabling multilingual code understanding.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Models/Methods">
  <data key="d0">Models/Methods</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">SPoC produces functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="SPoC">
  <data key="d0">SPoC</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">SPoC produces functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="TransCoder">
  <data key="d0">TransCoder</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">TransCoder translates code between languages, emphasizing functional correctness over traditional metrics like BLEU.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="ContraCode">
  <data key="d0">ContraCode</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">ContraCode uses contrastive learning over the space of correct programs to improve model robustness and inference.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="RobustFill">
  <data key="d0">RobustFill</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">RobustFill synthesizes multiple candidate programs via beam search to find programs consistent with input-output examples.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeSearchNet">
  <data key="d0">CodeSearchNet</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">CodeSearchNet provides a large corpus of code for training and evaluating code understanding and synthesis models across multiple languages.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="CodeXGLUE">
  <data key="d0">CodeXGLUE</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">CodeXGLUE aggregates multiple benchmarks and uses metrics like CodeBLEU to evaluate code generation quality.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="APPS Benchmark">
  <data key="d0">APPS Benchmark</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">APPS measures functional correctness of code solutions from competitive programming problems, serving as a standard benchmark.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research Studies">
  <data key="d0">Research Studies</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Applied transformer models to generate unit tests, outperforming some commercial tools in code testing."|&lt;"unit test generation, model performance&lt;SEP&gt;Applied transformers to generate unit tests, outperforming some commercial tools in code testing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Recurrent Neural Program Interpreter">
  <data key="d0">Recurrent Neural Program Interpreter</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Recurrent neural program interpreters are related to neural program interpretation models, such as those by Reed &amp; de Freitas (2016), Shin et al. (2018), and Pierrot et al. (2021).</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dehghani et al. (2019)">
  <data key="d0">Dehghani et al. (2019)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Dehghani et al. (2019) introduced the Universal Transformer, which applies recurrence to transformer models, relevant for sequence modeling and program understanding.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="PCFG">
  <data key="d0">PCFG</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Probabilistic Context-Free Grammar (PCFG) is a classical formalism used to generate syntactic program structures in synthesis."|&lt;"formal grammar, syntax</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Balog et al. (2017)">
  <data key="d0">Balog et al. (2017)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">DeepCoder trains neural models to predict functions in source code, guiding program synthesis."|&lt;"function prediction, program search</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Devlin et al. (2018); Radford et al. (2019); Liu et al. (2019); Raffel et al. (2020); Brown et al. (2020)">
  <data key="d0">Devlin et al. (2018); Radford et al. (2019); Liu et al. (2019); Raffel et al. (2020); Brown et al. (2020)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Large-scale transformer models trained on vast datasets for code understanding and generation, leveraging natural language processing advances."|&lt;"transformer models, large-scale training</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Feng et al. (2020)">
  <data key="d0">Feng et al. (2020)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">CodeBERT, trained on paired docstrings and code, achieves strong performance in code search and understanding."|&lt;"code understanding, search</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Clement et al. (2020)">
  <data key="d0">Clement et al. (2020)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">PyMT5, a multilingual transformer model trained with T5 objectives, translates between code and natural language."|&lt;"multilingual translation, code understanding</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Methods/Models">
  <data key="d0">Methods/Models</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">SPoC system produces functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics."|&lt;"program correctness, benchmarking</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lachaux et al. (2020)">
  <data key="d0">Lachaux et al. (2020)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">TransCoder translates code between languages, emphasizing functional correctness over BLEU scores."|&lt;"code translation, unsupervised learning</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jain et al. (2020)">
  <data key="d0">Jain et al. (2020)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">ContraCode uses contrastive learning to leverage the space of correct programs, improving robustness and inference."|&lt;"contrastive learning, program robustness</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Devlin et al. (2017)">
  <data key="d0">Devlin et al. (2017)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">RobustFill synthesizes multiple program samples via beam search to find programs consistent with input-output examples."|&lt;"program synthesis, beam search</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Gulwani (2011); Gulwani et al. (2012)">
  <data key="d0">Gulwani (2011); Gulwani et al. (2012)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Early datasets like FlashFill and Hearthstone used to benchmark neural program synthesis methods, focusing on specific tasks."|&lt;"benchmark datasets, domain-specific</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Husain et al. (2019)">
  <data key="d0">Husain et al. (2019)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">CodeSearchNet dataset, a large corpus from GitHub for training and evaluating code understanding models."|&lt;"large code corpus, multi-language</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lu et al. (2021)">
  <data key="d0">Lu et al. (2021)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">CodeXGLUE benchmark suite, aggregating multiple programming benchmarks and metrics like CodeBLEU."|&lt;"benchmark suite, evaluation metrics</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hendrycks et al. (2021)">
  <data key="d0">Hendrycks et al. (2021)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">APPS benchmark for assessing functional correctness of code solutions from competitive programming problems."|&lt;"benchmark, competitive programming</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Agrawal et al. (1995); Korel &amp; Rilling (1997)">
  <data key="d0">Agrawal et al. (1995); Korel &amp; Rilling (1997)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Static or dynamic code analysis techniques used to locate bugs, verify correctness, and fix faulty code."|&lt;"bug detection, code debugging</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Jeffrey et al. (2009)">
  <data key="d0">Jeffrey et al. (2009)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Learned association rules for code analysis, aiding in bug detection and code fixing."|&lt;"association rules, bug fixing</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Goues et al. (2012)">
  <data key="d0">Goues et al. (2012)</data>
  <data key="d3">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d2">Genetic programming approaches for code synthesis and debugging, evolving code solutions."|&lt;"genetic algorithms, program evolution</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural machine translation is used to translate buggy code into correct code, aiming to automate bug fixing.">
  <data key="d0">Neural machine translation is used to translate buggy code into correct code, aiming to automate bug fixing.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d2">AI, program repair</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="limitations">
  <data key="d0">limitations</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d2">Current LLMs struggle with generating correct and efficient parallel code, especially for complex or unstructured problems, and often lack scalability and transferability between models.&lt;SEP&gt;High cost of multiple samples and hardware constraints limit the scope and number of evaluations.&lt;SEP&gt;The evaluation highlights limitations such as low efficiency of generated parallel code and varying performance across different models and execution contexts, suggesting areas for future research.&lt;SEP&gt;Weak test suites often fail to detect bugs or verify functional correctness, limiting the effectiveness of automated evaluation.&lt;SEP&gt;Weak test suites often fail to detect bugs or verify functional correctness, posing challenges for evaluation.&lt;SEP&gt;Limitations refer to the constraints and potential shortcomings of the new technology, including issues with accuracy, generalizability, and dependency on datasets.</data>
  <data key="d1">Limitations</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="test suites">
  <data key="d0">test suites</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d2">Human developers create targeted test suites, but these may not be sufficient for evaluating AI-generated code.&lt;SEP&gt;Human developers write targeted test suites, but these may not be sufficient for automated evaluation of AI-generated code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural machine translation models are used to translate buggy code into corrected code, facilitating automated bug fixing.">
  <data key="d0">Neural machine translation models are used to translate buggy code into corrected code, facilitating automated bug fixing.</data>
  <data key="d3">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d2">AI, bug fixing</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="2019.Alon, U., Brody, S., Levy, O., and Yahav, E">
  <data key="d0">2019.Alon, U., Brody, S., Levy, O., and Yahav, E</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">The publication introduces code2seq, a method for generating sequences from structured code representations, which is applied to code understanding and generation tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Aye, G. A., Kim, S., and Li, H">
  <data key="d0">Aye, G. A., Kim, S., and Li, H</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">The study explores models that learn to perform autocompletion based on real-world datasets, contributing to software engineering practices.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Baevski, A., Zhou, H., Mohamed, A., and Auli, M">
  <data key="d0">Baevski, A., Zhou, H., Mohamed, A., and Auli, M</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">wav2vec 2.0 is a framework for self-supervised learning of speech representations that advances speech processing technologies.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D">
  <data key="d0">Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Deepcoder is a neural system that learns to write programs, representing a methodology for program synthesis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bao, H., Dong, L., and Wei, F">
  <data key="d0">Bao, H., Dong, L., and Wei, F</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Beit is a pre-trained image transformer model that applies BERT-style masked image modeling for vision tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Barone, A. V. M. and Sennrich, R">
  <data key="d0">Barone, A. V. M. and Sennrich, R</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">A dataset of Python functions and documentation strings used for training and evaluating code generation models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="parallel corpus">
  <data key="d0">parallel corpus</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">A dataset of Python functions and documentation strings used for training and evaluating code generation models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Barrington, I. M. and Maciel, A">
  <data key="d0">Barrington, I. M. and Maciel, A</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Educational material explaining nondeterministic computation concepts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="lecture on nondeterministic computation">
  <data key="d0">lecture on nondeterministic computation</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Educational material explaining nondeterministic computation concepts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S">
  <data key="d0">Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">The paper discusses risks associated with large language models, including biases and ethical concerns.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dangers of stochastic parrots">
  <data key="d0">Dangers of stochastic parrots</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">The paper discusses risks associated with large language models, including biases and ethical concerns.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S">
  <data key="d0">Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Introduction of GPT-Neo, a large-scale autoregressive language model utilizing mesh-tensorflow, highlighting tools for NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H">
  <data key="d0">Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">The survey critically examines biases in NLP models and their societal impacts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bureau of Labor Statistics, U. D. o. L">
  <data key="d0">Bureau of Labor Statistics, U. D. o. L</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Provides official statistics and occupational outlooks for roles like computer programmers and software developers.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Occupational data">
  <data key="d0">Occupational data</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Provides official statistics and occupational outlooks for roles like computer programmers and software developers.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C">
  <data key="d0">Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Research on techniques for extracting training data from large language models, relevant to model interpretability and security.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Training data extraction">
  <data key="d0">Training data extraction</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Research on techniques for extracting training data from large language models, relevant to model interpretability and security.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I">
  <data key="d0">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Pretraining from pixels enables models to generate images, demonstrating advances in generative modeling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generative pretraining">
  <data key="d0">Generative pretraining</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Pretraining from pixels enables models to generate images, demonstrating advances in generative modeling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Child, R., Gray, S., Radford, A., and Sutskever, I">
  <data key="d0">Child, R., Gray, S., Radford, A., and Sutskever, I</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Sparse transformers facilitate efficient long sequence generation, impacting NLP and sequence modeling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Long sequence generation">
  <data key="d0">Long sequence generation</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Sparse transformers facilitate efficient long sequence generation, impacting NLP and sequence modeling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AI alignment">
  <data key="d0">AI alignment</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Provides clarification on AI alignment issues, contributing to the discourse on safe and aligned AI development.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Christiano, P">
  <data key="d0">Christiano, P</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Provides clarification on AI alignment issues, contributing to the discourse on safe and aligned AI development.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Nondeterministic computation">
  <data key="d0">Nondeterministic computation</data>
  <data key="d3">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d2">Educational material explaining nondeterministic computation concepts, relevant to theoretical computer science.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Bias in AI">
  <data key="d0">Bias in AI</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Crawford's work addresses societal biases in AI, highlighting issues in technology and ethics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Application security">
  <data key="d0">Application security</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Davis's work on automated software diversity aims to improve application security by generating diverse program variants.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generating bug-fixes">
  <data key="d0">Generating bug-fixes</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">This research applies pretrained transformers to automatically generate bug fixes, improving software maintenance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Open source software">
  <data key="d0">Open source software</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Eghbal's work discusses the practices and maintenance of open source projects, highlighting community and sustainability.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Technology trap">
  <data key="d0">Technology trap</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Frey discusses how technological advancements can lead societies into traps, influencing policy and development paths.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Data poisoning and defenses">
  <data key="d0">Data poisoning and defenses</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Their research focuses on security threats like poisoning and backdoors in datasets, and strategies to defend against them.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Generating sequences with RNNs">
  <data key="d0">Generating sequences with RNNs</data>
  <data key="d3">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d2">Graves pioneered sequence generation techniques using recurrent neural networks, foundational for many NLP applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Lachaux et al.">
  <data key="d0">Lachaux et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">The study by Lachaux et al. (2020) explores the application of unsupervised learning techniques to translate programming languages.&lt;SEP&gt;Their research explores unsupervised machine learning approaches to translate programming languages, indicating a focus on AI and code understanding.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Leveson">
  <data key="d0">Leveson</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Leveson (2019) discusses improvements to the standard risk matrix to better assess and manage risks.&lt;SEP&gt;Leveson (2019) proposes improvements to the risk matrix to better assess safety and operational risks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Li, Ko, and Begel">
  <data key="d0">Li, Ko, and Begel</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Their 2020 study identifies traits that distinguish great software engineers, contributing to the discipline of empirical software engineering.&lt;SEP&gt;Their research investigates the characteristics that distinguish great software engineers within the discipline of empirical software engineering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ling et al.">
  <data key="d0">Ling et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Ling et al. (2016) introduce latent predictor networks as a methodology for code generation tasks, leveraging deep learning techniques.&lt;SEP&gt;Ling et al. (2016) introduced latent predictor networks as a methodology for code generation leveraging deep learning techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Liu et al.">
  <data key="d0">Liu et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Liu et al. (2019) developed Roberta, a pretraining approach to improve natural language understanding models.&lt;SEP&gt;Liu et al. (2019) developed Roberta, an improved pretraining approach for NLP models, enhancing language understanding.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Maddison and Tarlow">
  <data key="d0">Maddison and Tarlow</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">They discuss models that generate natural source code based on structured probabilistic frameworks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Structured generative models">
  <data key="d0">Structured generative models</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">They discuss models that generate natural source code based on structured probabilistic frameworks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Manna and Waldinger">
  <data key="d0">Manna and Waldinger</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Their 1971 work explores early concepts and approaches toward automatic program synthesis.&lt;SEP&gt;Their work from 1971 explores early concepts and approaches toward automatic synthesis of programs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Data center energy-use estimates">
  <data key="d0">Data center energy-use estimates</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Masanet et al. (2020) recalibrated estimates of energy consumption by data centers on a global scale.&lt;SEP&gt;Masanet et al. (2020) recalibrated estimates of global energy consumption by data centers, providing updated insights.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Masanet et al.">
  <data key="d0">Masanet et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Masanet et al. (2020) recalibrated estimates of energy consumption by data centers on a global scale.&lt;SEP&gt;Masanet et al. (2020) recalibrated estimates of global energy consumption by data centers, providing updated insights.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Menezes, van Oorschot, and Vanstone">
  <data key="d0">Menezes, van Oorschot, and Vanstone</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Their book covers principles and applications of cryptography for secure communication.&lt;SEP&gt;Their book provides comprehensive coverage of applied cryptography principles and methods.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="High-fidelity images">
  <data key="d0">High-fidelity images</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">They introduced models for generating high-quality images using subscale pixel networks and upscaling techniques.&lt;SEP&gt;They introduced techniques for generating high-quality images with subscale pixel networks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Menick and Kalchbrenner">
  <data key="d0">Menick and Kalchbrenner</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">They introduced models for generating high-quality images using subscale pixel networks and upscaling techniques.&lt;SEP&gt;They introduced techniques for generating high-quality images with subscale pixel networks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Mikolov et al.">
  <data key="d0">Mikolov et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Mikolov et al. (2013) developed distributed word and phrase representations capturing semantic and syntactic properties.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ohm et al.">
  <data key="d0">Ohm et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">They review and analyze supply chain attacks affecting open source software.&lt;SEP&gt;They review security vulnerabilities and attack techniques affecting open source software supply chains.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="O’Keefe et al.">
  <data key="d0">O’Keefe et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">O’Keefe et al. (2019) discuss legal frameworks and policies for protecting AI innovations.&lt;SEP&gt;O’Keefe et al. (2019) discuss policies and legal considerations for protecting AI innovations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Software developers">
  <data key="d0">Software developers</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">O*NET classifies and describes the roles, skills, and job requirements of software developers, as documented in 2021.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Wallach et al.">
  <data key="d0">Wallach et al.</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">The volume edited by Wallach et al. (2019) includes research on neural systems, methodologies, and applications in AI.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Distributed representations of words">
  <data key="d0">Distributed representations of words</data>
  <data key="d3">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d2">Mikolov et al. (2013) pioneered distributed word embeddings capturing semantic and syntactic properties.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Software Developers">
  <data key="d0">Software Developers</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">O*NET provides occupational data that helps define and categorize software developers, including skills and tasks involved.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Raw Audio">
  <data key="d0">Raw Audio</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Wavenet models generate or analyze raw audio waveforms, enabling applications in speech synthesis and audio generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Representation Learning">
  <data key="d0">Representation Learning</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">This methodology is used within neural network training to learn useful data representations by predicting future data points.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K">
  <data key="d0">Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors of foundational neural network models and learning techniques, contributing to AI advancements."|&lt;SEP&gt;Authors of foundational neural network models and representation learning techniques, contributing to AI research."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research">
  <data key="d0">Research</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628&lt;SEP&gt;chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d2">Authors of foundational neural network models and learning techniques, contributing to AI advancements."|&lt;SEP&gt;Authors of foundational neural network models and representation learning techniques, contributing to AI research."|&lt;SEP&gt;Large language models like GPT-3 and GPT-4, which are advanced AI systems capable of natural language understanding and generation.</data>
  <data key="d1">Objects of Study</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="O’Neill, M. and Spector, L">
  <data key="d0">O’Neill, M. and Spector, L</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors discussing open issues in automatic programming and the challenges in evolving programs automatically."|&lt;SEP&gt;Authors discussing open issues in automatic programming, highlighting challenges and methodologies in evolving programs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Study">
  <data key="d0">Study</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">A research effort to benchmark and assess the performance of large language models on code-related tasks."|&lt;SEP&gt;A study examining the challenges of benchmarking inductive program synthesis methods, emphasizing evaluation difficulties.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J">
  <data key="d0">Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors analyzing the environmental impact of large neural network training, emphasizing carbon footprint concerns.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Carbon Emissions and Large Neural Network Training">
  <data key="d0">Carbon Emissions and Large Neural Network Training</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Training large neural networks significantly contributes to carbon emissions, raising sustainability concerns."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L">
  <data key="d0">Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors developing deep contextualized word representations to enhance NLP models."|&lt;SEP&gt;Developers of deep contextualized word representations, advancing NLP understanding."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N">
  <data key="d0">Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors exploring neural program learning via recursive tree search and planning, contributing to program synthesis methodologies."|&lt;SEP&gt;Authors exploring neural program learning, including recursive tree search and planning techniques."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Organizations">
  <data key="d0">Organizations</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Organizations conducting surveys to gather data on Python developers' practices and trends."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I">
  <data key="d0">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors demonstrating that large language models can perform multiple NLP tasks without supervision, in an unsupervised, multitask manner."|&lt;SEP&gt;Authors showing that large language models can perform multiple NLP tasks in an unsupervised manner."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I">
  <data key="d0">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Developers of zero-shot text-to-image generation models, enabling image creation from textual prompts."|&lt;SEP&gt;Developers of zero-shot text-to-image models that generate images from textual prompts."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I">
  <data key="d0">Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I</data>
  <data key="d3">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d2">Authors of large-scale language models trained via generative pre-training, greatly advancing NLP."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Poisoning Vulnerabilities">
  <data key="d0">Poisoning Vulnerabilities</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d2">Identifying security vulnerabilities in neural code completion relates to neural program synthesis by highlighting potential risks and security challenges in automated code generation systems."|&lt;SEP&gt;The study of poisoning vulnerabilities in neural code completion systems relates to neural program synthesis by highlighting security challenges in automated code generation."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Transformers">
  <data key="d0">Transformers</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63&lt;SEP&gt;chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-861fc8218eac993fbe13b418ec4b837c&lt;SEP&gt;chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d2">A neural network architecture that is state-of-the-art for natural language processing tasks, enabling efficient handling of large-scale language understanding.&lt;SEP&gt;A neural network architecture that relies on attention mechanisms to process sequential data efficiently and effectively, revolutionizing NLP.&lt;SEP&gt;The Transformer architecture introduced in the paper forms the basis for many modern models used in code generation, natural language processing, and sequence modeling."|&lt;SEP&gt;The Transformer architecture introduced in the paper underpins many modern neural models, including those for code generation and language understanding."|&lt;SEP&gt;Transformer architecture is a deep learning model that underpins modern NLP systems, enabling effective language understanding and generation.&lt;SEP&gt;An architecture for neural networks that is the basis for most state-of-the-art NLP models, enabling efficient processing of language data.</data>
  <data key="d1">Tools</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Open Source">
  <data key="d0">Open Source</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d2">The survey examines women's participation in open source projects, relating gender participation to open source development."|&lt;SEP&gt;The survey investigates women's roles and participation patterns in open source projects, relating gender diversity to community development."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Learning Bug-Fixing Patches">
  <data key="d0">Learning Bug-Fixing Patches</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d2">Applying neural machine translation techniques to learn bug-fixing patches from real-world data connects neural models with practical software maintenance and automation."|&lt;SEP&gt;The study applies neural machine translation techniques to learn bug-fixing patches from real-world data, connecting neural models with practical software maintenance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Neural Machine Translation">
  <data key="d0">Neural Machine Translation</data>
  <data key="d3">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d2">Applying neural machine translation techniques to learn bug-fixing patches from real-world data connects neural models with practical software maintenance and automation."|&lt;SEP&gt;The study applies neural machine translation techniques to learn bug-fixing patches from real-world data, connecting neural models with practical software maintenance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Research Paper">
  <data key="d0">Research Paper</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">Authors of a paper discussing code generation from natural language, focusing on promise and challenges."|&gt;"authorship&lt;SEP&gt;Authors of a paper discussing code generation from natural language, highlighting its promise and challenges."|&gt;"authorship</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Researcher">
  <data key="d0">Researcher</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">Author of a report examining rote learning in GitHub Copilot suggestions."|&gt;"authorship&lt;SEP&gt;Author of a report on rote learning tendencies in GitHub Copilot suggestions."|&gt;"authorship</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Methodology">
  <data key="d0">Methodology</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">An unbiased estimator for evaluating the success probability of code generation models."|&gt;"methodology&lt;SEP&gt;An unbiased statistical estimator used to evaluate the success probability of code generation models, ensuring fair comparison."|&gt;"methodology&lt;SEP&gt;The approach of evaluating AI-generated code suggestions across multiple programming languages and models, focusing on prompt structure and correctness metrics.&lt;SEP&gt;The approach of evaluating prompt outputs for parallel programming models in C++, Fortran, Python, and Julia, including code suggestion generation and correctness metrics.</data>
  <data key="d1">Methodologies</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Variable">
  <data key="d0">Variable</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">A metric indicating the probability that at least one of k generated samples passes unit tests, used to assess code generation quality."|&gt;"variable&lt;SEP&gt;A success probability metric for code samples, indicating if at least one passes."|&gt;"variable</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Figure">
  <data key="d0">Figure</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">A figure comparing bias and variance of estimators for pass@k, illustrating estimator performance."|&gt;"illustration&lt;SEP&gt;Graph comparing bias and variance of pass@k estimators."|&gt;"illustration</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Dataset/Task">
  <data key="d0">Dataset/Task</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">A set of randomly selected problems with generated solutions from Codex-12B for evaluating code generation models."|&gt;"dataset&lt;SEP&gt;A set of randomly selected programming problems with solutions generated by Codex-12B for evaluation."|&gt;"dataset</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Function">
  <data key="d0">Function</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">A function designed to split a string of words into an array of individual words, handling commas and spaces."|&gt;"function&lt;SEP&gt;A function to split strings into words, handling spaces and commas."|&gt;"function</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Distribution">
  <data key="d0">Distribution</data>
  <data key="d3">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d2">Statistical distribution modeling the number of successful samples."|&gt;"distribution</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Vowels Count">
  <data key="d0">Vowels Count</data>
  <data key="d3">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d2">The count of vowels in 'ACEDY' is 3, including 'Y' because it is at the end of the word, demonstrating the rule for 'Y' as a vowel.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="return 0 - result">
  <data key="d0">return 0 - result</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">The condition ifb &lt; 0 triggers the return of the negated result, indicating flow control based on the value of b.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="The function returns the computed result when the condition ifb &lt; 0 is false, providing the main output.">
  <data key="d0">The function returns the computed result when the condition ifb &lt; 0 is false, providing the main output.</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">output, calculation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="return result">
  <data key="d0">return result</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">output, calculation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="product">
  <data key="d0">product</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">number is the product of a and b, which is used in further calculations or as an input for other operations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="digit-wise operation">
  <data key="d0">digit-wise operation</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">The string representation of number allows digit-by-digit processing for summing digits or other manipulations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="digit sum">
  <data key="d0">digit sum</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">total accumulates the sum of individual digits in the string, used in a separate function to analyze digit properties.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="digit">
  <data key="d0">digit</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">i iterates over each character (digit) in the string, enabling digit-wise operations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="range">
  <data key="d0">range</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">n defines the upper limit for the range of numbers evaluated in the function, setting the scope of analysis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="positive integer n">
  <data key="d0">positive integer n</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">n is the main object of study, defining the range for counting palindromic numbers.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="tuple">
  <data key="d0">tuple</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">The output tuple contains counts of even and odd palindromes, summarizing the analysis within the range.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Evidence">
  <data key="d0">Evidence</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">Evidence refers to the information retrieved from Wikipedia that supports or refutes a claim in the FEVER task.&lt;SEP&gt;Provides a specific case demonstrating the function's expected output for input 3, validating correctness.</data>
  <data key="d1">Objects of Study</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Example 1">
  <data key="d0">Example 1</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">Provides a specific case demonstrating the function's expected output for input 3, validating correctness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Example 2">
  <data key="d0">Example 2</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">Shows the function's output for input 12, further supporting the function's accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Note">
  <data key="d0">Note</data>
  <data key="d3">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d2">Clarifies the constraints and expectations for the function, such as input limits and output format, guiding correct implementation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Properties">
  <data key="d0">Properties</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d2">Concurrency properties such as mutual exclusion and fairness are essential for correct parallel code synthesis."|&gt;"correctness, properties&lt;SEP&gt;Concurrency properties such as mutual exclusion, fairness, and synchronization are essential for correct parallel code synthesis."|&gt;"correctness, properties</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Hyperproperties">
  <data key="d0">Hyperproperties</data>
  <data key="d3">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d2">Hyperproperties include nondeterminism as a property, relating to the behavior of algorithms in different executions."|&gt;"property, algorithm behavior</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="RLHF">
  <data key="d0">RLHF</data>
  <data key="d3">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d2">Reinforcement Learning from Human Feedback is applied to improve the model's alignment with human preferences and correctness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="prime numbers">
  <data key="d0">prime numbers</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d2">The count_up_to function generates a list of prime numbers less than a given number, used in mathematical computations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="array">
  <data key="d0">array</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d2">The smallest_change function computes the minimum edits required to make an array palindromic, relevant in array analysis and correction.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="generative models">
  <data key="d0">generative models</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d2">The analysis describes how models like Codex can encode societal biases, impacting generated code and content.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Harms">
  <data key="d0">Harms</data>
  <data key="d3">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d2">Biases in generated code can lead to societal harms such as stereotypes and unfair resource allocation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Prompts">
  <data key="d0">Prompts</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-10b08670a9cf75866c6b05fa5b5cfc12&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d2">Prompts are input instructions or comments used to guide the model in generating code or annotations, such as function descriptions or pragma requests.&lt;SEP&gt;Prompts for classifying protected classes can be leading and may reinforce harmful stereotypes.&lt;SEP&gt;Prompts are structured instructions given to large language models (LLMs) to generate code or responses, critical in evaluating model performance in code synthesis.</data>
  <data key="d1">Tools</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Adversarial Inputs">
  <data key="d0">Adversarial Inputs</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d2">Adversarial inputs can manipulate Codex's outputs, especially if the training data contains biases or vulnerabilities, increasing misuse potential.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Security Risks">
  <data key="d0">Security Risks</data>
  <data key="d3">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d2">Current limitations of Codex in generating effective malicious payloads or discovering complex vulnerabilities, which may improve with future research."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="How do AI code generation models influence societal inequality and economic structures?">
  <data key="d0">How do AI code generation models influence societal inequality and economic structures?</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d2">societal impact, economic influence</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="data source">
  <data key="d0">data source</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d2">Provides data on user participation and role distribution, serving as a basis for analyzing representation and role differences.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="societal impact">
  <data key="d0">societal impact</data>
  <data key="d3">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d2">Investigates how AI code generation influences societal inequality, labor markets, and economic power structures.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="AI code generation technologies">
  <data key="d0">AI code generation technologies</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d2">Advancements in AI code generation could substantially affect the labor market, wages, and job quality for programmers and engineers.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="future research">
  <data key="d0">future research</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d2">Future research aims to investigate the economic value, impacts on documentation, worker productivity, barriers to entry, and societal implications of Codex and similar systems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="Machine learning packages">
  <data key="d0">Machine learning packages</data>
  <data key="d3">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d2">Codex suggests popular machine learning libraries like TensorFlow and PyTorch, affecting their adoption and market share.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</node>
<node id="HPC-Coder">
  <data key="d0">HPC-Coder</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A fine-tuned large language model designed specifically to model, generate, and analyze HPC and scientific source code, including labeling OpenMP pragmas and predicting performance.&lt;SEP&gt;A specialized large language model (LLM) fine-tuned for tasks related to high performance computing (HPC) and scientific codes, capable of code auto-completion, decorating loops with OpenMP pragmas, and modeling performance changes.&lt;SEP&gt;An LLM fine-tuned specifically to model HPC and scientific source code, used for code generation, pragma labeling, and performance prediction.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Modeling Parallel Programs">
  <data key="d0">Modeling Parallel Programs</data>
  <data key="d1">Tools</data>
  <data key="d2">The approach of representing and understanding parallel programs using models, particularly leveraging large language models to automate tasks and analyze performance.&lt;SEP&gt;This methodology involves using large language models to simulate and analyze parallel programming constructs.&lt;SEP&gt;This methodology involves using large language models to simulate, analyze, and optimize parallel programming constructs and behaviors.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Modeling">
  <data key="d0">Performance Modeling</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigate whether domain-specific language models can accurately predict and analyze code performance with limited data.&lt;SEP&gt;Investigating whether HPC-specific language models can accurately predict and analyze code performance with limited data.&lt;SEP&gt;The investigation into whether large language models can effectively predict execution times and performance changes in scientific and parallel codes.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Parallel Programs">
  <data key="d0">Parallel Programs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Programs designed to run computations concurrently across multiple processing units, which are increasingly complex and diverse in hardware and programming models in HPC environments.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenMP Pragmas">
  <data key="d0">OpenMP Pragmas</data>
  <data key="d1">Tools</data>
  <data key="d2">Annotations added to code loops to enable parallel execution, a common HPC coding task tested for model performance.&lt;SEP&gt;Compiler directives used to specify parallel regions and control parallel execution in code, which the model can automatically decorate in loops.&lt;SEP&gt;Directives in parallel programming that specify how loops and code sections should be executed concurrently, enabling correct and efficient parallelization.&lt;SEP&gt;OpenMP pragmas are compiler directives used to specify parallel regions in code, particularly for decorating loops with directives like '#pragma omp parallel for' to enable shared-memory parallelism.&lt;SEP&gt;OpenMP pragmas are compiler directives used to specify parallel regions in code, particularly for decorating loops with parallel execution instructions, such as '#pragma omp parallel for'.&lt;SEP&gt;OpenMP pragmas are directives used in parallel programming to specify how loops and code sections should be executed concurrently, ensuring correct parallelization and optimizing performance.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Dataset of HPC and Scientific Codes">
  <data key="d0">Dataset of HPC and Scientific Codes</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of source code used to train and fine-tune the large language models for HPC-related tasks, including parallel codes and performance data.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Changes in Scientific Applications">
  <data key="d0">Performance Changes in Scientific Applications</data>
  <data key="d1">Results</data>
  <data key="d2">The observed variations in execution time and efficiency of scientific codes as modeled by the LLMs, demonstrating their capability to predict performance impacts.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Completion">
  <data key="d0">Code Completion</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Code completion is a benchmark task where models generate code snippets based on prompts, assessing their ability to produce syntactically and functionally correct code.&lt;SEP&gt;The use of LLMs to automatically generate or complete HPC functions, reducing developer effort and errors.&lt;SEP&gt;Code completion is a task where models predict and generate code snippets based on prompts, crucial for assisting developers in writing correct and efficient code.&lt;SEP&gt;The task of predicting and generating source code snippets based on prompts, aiding developers by providing syntactically and semantically relevant code suggestions.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fine-tuning of Pre-trained Models">
  <data key="d0">Fine-tuning of Pre-trained Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adjusting existing large language models on specific datasets of HPC codes to improve their performance on specialized tasks like code auto-completion and performance prediction.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Transfer Learning">
  <data key="d0">Transfer Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A machine learning methodology where knowledge gained from one task is applied to improve performance on another related task, often used in NLP models.&lt;SEP&gt;A machine learning technique where a model trained on one task is adapted to perform a different but related task, often used to improve performance with limited data.&lt;SEP&gt;A technique where a model trained on one task is adapted to perform related tasks, improving efficiency and performance.&lt;SEP&gt;A technique where models trained on related tasks or data are adapted to new, data-scarce tasks such as performance modeling of HPC codes.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Code Development and Analysis Tools">
  <data key="d0">Code Development and Analysis Tools</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Automated tools that assist developers in writing, optimizing, and maintaining complex parallel codebases.&lt;SEP&gt;Automated tools that assist developers in writing, optimizing, and maintaining parallel HPC software, enhanced by language modeling advancements.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Automated Performance Analysis Tools">
  <data key="d0">Automated Performance Analysis Tools</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Tools powered by LLMs that perform complex performance analysis and optimization tasks for HPC codes.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Open-source Code Data">
  <data key="d0">Open-source Code Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large repositories of publicly available source code used for training and fine-tuning LLMs for coding and performance tasks.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Parallel Programming Models">
  <data key="d0">Parallel Programming Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Different approaches such as MPI, OpenMP, CUDA, HIP, and Kokkos that define how computations are parallelized and executed.&lt;SEP&gt;Different parallel programming models (e.g., MPI, OpenMP, CUDA) are used as frameworks within which the generated code is evaluated for correctness and performance.&lt;SEP&gt;Frameworks such as MPI, OpenMP, CUDA, and others, within which the generated code is evaluated for correctness, efficiency, and suitability.&lt;SEP&gt;Parallel programming models like MPI, OpenMP, CUDA, HIP, and Kokkos define different approaches to parallelism, with varying complexity and similarity to serial code, affecting LLM performance.&lt;SEP&gt;Theoretical frameworks such as OpenMP, MPI, CUDA, etc., used to implement parallel code in the prompts.&lt;SEP&gt;Various models such as OpenMP, MPI, and others used to develop parallel programs in HPC environments.&lt;SEP&gt;Models such as OpenMP used to facilitate parallel computing in languages like C++, Fortran, Python, and Julia.&lt;SEP&gt;Models such as OpenMP, used to facilitate parallel computing in different programming languages.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Hardware Diversity in HPC">
  <data key="d0">Hardware Diversity in HPC</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The variety of hardware architectures (CPUs, GPUs, accelerators) that influence parallel program development and performance modeling.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Data">
  <data key="d0">Performance Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Datasets containing code snippets paired with performance metrics, used to train models to predict performance characteristics.&lt;SEP&gt;Datasets pairing code snippets with performance metrics, used for training models to predict performance characteristics.&lt;SEP&gt;Quantitative measurements of execution time, resource usage, and efficiency used to train and evaluate performance models.&lt;SEP&gt;Real performance data from code execution, used to train models like PolyCoder+HPC for HPC code generation and performance modeling.&lt;SEP&gt;Real-world data collected from code performance measurements, used for training and evaluating models like PolyCoder+HPC.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Prediction">
  <data key="d0">Performance Prediction</data>
  <data key="d1">Results</data>
  <data key="d2">The outcome of the models' ability to forecast execution times and performance impacts of code modifications.&lt;SEP&gt;The task of predicting relative performance changes between code versions, such as whether code will be slower or faster after modifications.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Automated Code Decoration">
  <data key="d0">Automated Code Decoration</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The process of automatically adding OpenMP pragmas to loops to facilitate parallelization, enabled by LLMs.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Optimization">
  <data key="d0">Code Optimization</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using LLMs to suggest or implement code modifications that improve performance or maintainability.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Language Modeling Advancements">
  <data key="d0">Language Modeling Advancements</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Recent developments in language modeling that enable better understanding and generation of source code, including for HPC applications.</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC and scientific codes">
  <data key="d0">HPC and scientific codes</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High Performance Computing (HPC) and scientific codes are specialized software used in computational science to perform complex calculations efficiently, and are the focus of the performance modeling in this research.&lt;SEP&gt;High Performance Computing and scientific source codes used for computational tasks, which are modeled and analyzed for performance and code behavior.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenMP pragmas">
  <data key="d0">OpenMP pragmas</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Annotations in parallel code that guide multi-threaded execution, used as a target for LLMs to identify and generate in HPC code.&lt;SEP&gt;Compiler directives used in parallel programming to specify shared memory parallelism, which the model can label with 97% accuracy.&lt;SEP&gt;OpenMP pragmas are directives used to parallelize code, evaluated here to assess models' capability to generate correct parallel annotations.&lt;SEP&gt;OpenMP pragmas are parallelization directives evaluated to assess models' ability to generate correct parallel code annotations.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-107fb981331e6facca90c9849f179ee2&lt;SEP&gt;chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="performance modeling">
  <data key="d0">performance modeling</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">The hypothesis that LLMs can be effectively used to predict the performance of source code changes, especially in HPC and scientific computing contexts.&lt;SEP&gt;The hypothesis that LLMs can effectively predict and analyze the performance of source code, especially in HPC and scientific applications, using fewer samples and specialized datasets.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="large curated dataset">
  <data key="d0">large curated dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of HPC and scientific codes from open-source repositories used to train and evaluate the models.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="code generation tasks">
  <data key="d0">code generation tasks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Specific tasks designed to evaluate the model's ability to generate HPC-related code, with HPC-Coder outperforming others by passing these tasks at a 53% higher rate.&lt;SEP&gt;Specific tasks designed to evaluate the model's ability to generate HPC-related code, with the model passing at a 53% higher rate than others.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="predicting relative performance">
  <data key="d0">predicting relative performance</data>
  <data key="d1">Results</data>
  <data key="d2">The model can predict the relative performance of source code modifications with up to 92% accuracy.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="self-attention mechanism">
  <data key="d0">self-attention mechanism</data>
  <data key="d1">Theoretical Model</data>
  <data key="d2">A core component of transformer architectures that allows models to weigh different parts of input sequences differently, crucial for modeling sequential data like source code.&lt;SEP&gt;A core component of transformer models that assigns importance weights to different parts of input sequences, enabling effective modeling of sequential data like source code.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="transformer-based language models">
  <data key="d0">transformer-based language models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A class of models that utilize self-attention mechanisms to process sequential data, foundational to the LLMs discussed.&lt;SEP&gt;A class of models utilizing self-attention mechanisms to process sequential data, foundational to the LLMs used for code modeling and performance prediction.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="perplexity">
  <data key="d0">perplexity</data>
  <data key="d1">Variables</data>
  <data key="d2">A metric used to evaluate how well a language model predicts a sequence of tokens; lower perplexity indicates better predictive performance.&lt;SEP&gt;A metric used to evaluate the predictive quality of language models, where lower perplexity indicates better performance in modeling source code or text.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="transfer learning">
  <data key="d0">transfer learning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A machine learning approach where knowledge gained from training on one task or domain is applied to improve learning efficiency and performance on a different but related task, often reducing the number of samples needed.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Large Language Models (LLMs)">
  <data key="d0">Large Language Models (LLMs)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Advanced neural models trained on vast textual data, capable of various NLP tasks, with emphasis on their domain disruption capabilities.&lt;SEP&gt;Advanced neural network models based on transformer architectures trained on large-scale textual data, capable of modeling and generating source code, and performing performance predictions in HPC and scientific computing.&lt;SEP&gt;Advanced neural network models trained on massive datasets capable of performing a wide range of NLP tasks, with emphasis on their potential to be disruptive when domain-specific.&lt;SEP&gt;LLMs are advanced AI models capable of understanding and generating human-like text, with recent growth in their applications and effectiveness across various NLP tasks.&lt;SEP&gt;Large Language Models are extensive neural network models trained on vast text corpora, capable of few-shot learning and various NLP tasks.&lt;SEP&gt;LLMs are advanced AI models trained on large datasets to perform tasks such as code generation, explanation, and automation in programming.&lt;SEP&gt;LLMs are advanced AI models trained on vast datasets to generate human-like text, used for automated programming exercises, code explanations, and code generation.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="dataset of HPC and scientific codes">
  <data key="d0">dataset of HPC and scientific codes</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A curated collection of HPC and scientific codes from open-source repositories used to train, validate, and evaluate the models.&lt;SEP&gt;The large, curated collection of HPC and scientific codes used for training, testing, and evaluating the models.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="performance prediction of source code">
  <data key="d0">performance prediction of source code</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The activity of estimating the relative performance impacts of code changes, with the model achieving up to 92% accuracy in predictions.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="labeling OpenMP pragmas">
  <data key="d0">labeling OpenMP pragmas</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of identifying and annotating OpenMP directives in source code, with the model achieving 97% accuracy in this task.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="performance prediction">
  <data key="d0">performance prediction</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Estimating the relative performance of code modifications, with the model reaching up to 92% accuracy across datasets.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="training on open-source repositories">
  <data key="d0">training on open-source repositories</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of collecting and using publicly available HPC and scientific code for training the models.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="state-of-the-art models">
  <data key="d0">state-of-the-art models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Existing leading models in code modeling and generation, against which HPC-Coder's performance is compared.</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Perplexity">
  <data key="d0">Perplexity</data>
  <data key="d1">Results</data>
  <data key="d2">Perplexity measures how well a language model predicts a sample; it is calculated as the exponential of the training loss and indicates model confidence and performance.&lt;SEP&gt;Perplexity measures model confidence and is used to evaluate language model performance, indicating how well a model predicts a sample.&lt;SEP&gt;Perplexity measures the confidence of language models, indicating how well the model predicts data, and is used as an evaluation metric.&lt;SEP&gt;Perplexity measures the uncertainty of language models in predicting text, with lower values indicating better performance after fine-tuning.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Confidence">
  <data key="d0">Model Confidence</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model confidence reflects the certainty of a language model in its predictions, influencing downstream task performance.&lt;SEP&gt;Model confidence reflects the certainty of predictions made by the language model, impacting downstream task performance.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Accuracy">
  <data key="d0">Accuracy</data>
  <data key="d1">Results</data>
  <data key="d2">A metric used to evaluate the correctness of generated pragmas, calculated as the ratio of correct pragmas to total tested pragmas, considering both syntactic and functional correctness.&lt;SEP&gt;A quantitative metric measuring the correctness of generated pragmas, calculated as the ratio of correct pragmas to total tested, considering both syntactic and functional correctness.&lt;SEP&gt;Accuracy measures the correctness of the model's predictions, distinguished from confidence metrics like perplexity.&lt;SEP&gt;Accuracy measures the proportion of correct predictions made by the model on validation data, used alongside perplexity to assess performance.&lt;SEP&gt;Accuracy refers to the correctness of a model's predictions, though it is distinguished from confidence measures like perplexity.&lt;SEP&gt;The measure of how correctly the indices match actual leaders; low accuracy indicates mismatches.&lt;SEP&gt;The percentage measure indicating how correctly the indices match the actual leaders, with low accuracy indicating mismatches.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Text Generation">
  <data key="d0">Text Generation</data>
  <data key="d1">Process</data>
  <data key="d2">Text generation involves using trained models to produce new textual content based on learned patterns.&lt;SEP&gt;Text generation is the activity of producing new text sequences using trained models, often involving sampling methods to enhance diversity.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Token Probability">
  <data key="d0">Token Probability</data>
  <data key="d1">Variables</data>
  <data key="d2">Token probability is the likelihood assigned by the model to the next token in sequence, fundamental to language modeling.&lt;SEP&gt;Token probability refers to the likelihood assigned by the model to each token during prediction, fundamental to sampling and generation processes.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Sampling Methods">
  <data key="d0">Sampling Methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Sampling methods such as temperature, top-k, and nucleus sampling are techniques used to generate diverse and contextually relevant text from language models.&lt;SEP&gt;Sampling methods such as temperature, top-k, and nucleus sampling are techniques used to select tokens during generation, balancing diversity and relevance.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Top-k Sampling">
  <data key="d0">Top-k Sampling</data>
  <data key="d1">Tools</data>
  <data key="d2">Top-k sampling involves selecting tokens from the top k most probable tokens, reducing randomness and focusing on high-probability options.&lt;SEP&gt;Top-k sampling selects from the most probable k tokens, aiming to balance diversity and coherence in generated text.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Nucleus Sampling">
  <data key="d0">Nucleus Sampling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A probabilistic sampling technique that selects tokens from the cumulative distribution up to a cutoff probability p, promoting diversity and more representative outputs in text generation.&lt;SEP&gt;A sampling technique that selects tokens from a probability distribution up to a cumulative probability cutoff, promoting diversity and representativeness in generated text.&lt;SEP&gt;Nucleus (top-p) sampling dynamically selects tokens based on cumulative probability p, aiming to produce more coherent and diverse outputs.&lt;SEP&gt;Nucleus sampling (top-p) chooses tokens based on cumulative probability, offering a dynamic cutoff to improve text diversity.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Left-to-Right Models">
  <data key="d0">Left-to-Right Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Left-to-right models generate sequences sequentially, predicting the next token based on preceding context, suitable for text and code generation.&lt;SEP&gt;Left-to-right models generate text sequentially from start to end, predicting the next token based on prior context, suitable for text generation.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Masked Models">
  <data key="d0">Masked Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Masked models predict tokens at masked positions within sequences, using bidirectional context for more accurate predictions.&lt;SEP&gt;Masked models predict tokens at randomly masked positions within sequences, utilizing bidirectional context for improved accuracy.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Encoder-Decoder Models">
  <data key="d0">Encoder-Decoder Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Encoder-decoder models process input sequences through an encoder and generate output via a decoder, often used in sequence-to-sequence tasks.&lt;SEP&gt;Encoder-decoder models process input sequences through an encoder and generate output with a decoder, often used in sequence-to-sequence tasks like translation.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Training on HPC Code">
  <data key="d0">Training on HPC Code</data>
  <data key="d1">Study Design</data>
  <data key="d2">Training models on large datasets of high-performance computing source code to adapt language models for HPC-specific tasks.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Downstream Tasks">
  <data key="d0">Downstream Tasks</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Downstream tasks involve applying trained language models to specific applications such as text generation, code completion, or labeling.&lt;SEP&gt;Downstream tasks refer to applications like code generation, labeling, or prediction that utilize the trained language models.&lt;SEP&gt;Specific tasks designed to evaluate the performance of the trained language model, including code generation, OpenMP pragma labeling, and relative performance prediction.&lt;SEP&gt;Specific tasks such as code generation used to evaluate the fine-tuned models' practical performance.&lt;SEP&gt;Tasks designed to evaluate the trained model's performance in code generation, OpenMP pragma labeling, and relative performance prediction.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Large Dataset of HPC Code">
  <data key="d0">Large Dataset of HPC Code</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of high-performance computing source code used to train and fine-tune language models for HPC applications.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Sampling Techniques">
  <data key="d0">Sampling Techniques</data>
  <data key="d1">Tools</data>
  <data key="d2">Sampling techniques like temperature, top-k, and nucleus sampling are used to generate diverse, relevant text by controlling randomness during sampling.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Training">
  <data key="d0">Model Training</data>
  <data key="d1">Study Design</data>
  <data key="d2">Training involves adjusting model parameters using datasets, including pre-training and fine-tuning, to optimize performance for specific tasks.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Selection">
  <data key="d0">Model Selection</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Choosing an appropriate pre-trained model trained for code tasks is crucial for effective fine-tuning and achieving optimal performance in HPC code generation.&lt;SEP&gt;Selecting the best performing model involves evaluating multiple trained models based on validation metrics for downstream tasks.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Application in HPC">
  <data key="d0">Application in HPC</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Applying trained language models to generate, label, or analyze HPC source code to enhance computational workflows.</data>
  <data key="d3">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Training Setup">
  <data key="d0">Training Setup</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Details the process of training the language model, including fine-tuning pre-trained models on HPC code and downstream task evaluation.&lt;SEP&gt;Details the process of training the language model, including fine-tuning pre-trained models on HPC code and the evaluation of downstream tasks.&lt;SEP&gt;Training setup includes the hardware, software, and hyperparameters used to perform model training, such as multi-GPU configurations and precision settings.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Fine-Tuning">
  <data key="d0">Model Fine-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of adapting pre-trained language models specifically for HPC code understanding and generation.&lt;SEP&gt;The process of adapting pre-trained language models specifically for understanding and generating HPC code by training on domain-specific datasets.&lt;SEP&gt;The process of training the combined retriever and generator components end-to-end to optimize performance on specific tasks.&lt;SEP&gt;The process of further training pre-trained language models on domain-specific data, such as HPC-INSTRUCT, to improve their capabilities in generating parallel code.&lt;SEP&gt;The process of training pre-existing models on specific datasets, such as HPC-INSTRUCT, to improve their performance on specialized tasks like parallel code generation.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Source Code Dataset">
  <data key="d0">HPC Source Code Dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large collection of HPC-related C/C++ source code files collected from GitHub repositories, used for training and evaluating the language model.&lt;SEP&gt;A large collection of HPC-related C/C++ source code files collected from GitHub, used to train and evaluate the language model.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenMP Pragma Labeling">
  <data key="d0">OpenMP Pragma Labeling</data>
  <data key="d1">Results</data>
  <data key="d2">A task where the model labels OpenMP directives in code, testing its understanding of parallel programming constructs.&lt;SEP&gt;A task where the model labels OpenMP pragmas in code, assessing its ability to understand parallelization directives.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Relative Performance Prediction">
  <data key="d0">Relative Performance Prediction</data>
  <data key="d1">Results</data>
  <data key="d2">A task testing the model's ability to predict the performance of HPC code snippets, demonstrating language comprehension.&lt;SEP&gt;A task where the model predicts the relative performance of HPC code snippets, demonstrating language comprehension in performance modeling.&lt;SEP&gt;Experiments assessing the models' ability to predict code performance changes, specifically whether code will slow down or not, based on prior training.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Dataset Collection">
  <data key="d0">Dataset Collection</data>
  <data key="d1">Methodology</data>
  <data key="d2">The process of collecting HPC source code from GitHub repositories filtered by language and HPC relevance, including deduplication and filtering.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Data Pre-processing">
  <data key="d0">Data Pre-processing</data>
  <data key="d1">Methodology</data>
  <data key="d2">Steps to remove duplicate files, filter out small and large files, and tokenize code for model training.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Tokenization">
  <data key="d0">Tokenization</data>
  <data key="d1">Tools</data>
  <data key="d2">Using GPT-2 based Byte-Pair Encoding (BPE) tokenizers to convert code into integer sequences suitable for model input.&lt;SEP&gt;Using GPT-2 based Byte-Pair Encoding (BPE) tokenizers to convert source code into sequences of integers suitable for model input.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Datasets">
  <data key="d0">Performance Datasets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Two datasets containing pairs of code with associated performance data: one capturing performance regressions over commits, and another from programming contests.&lt;SEP&gt;Two datasets pairing code with performance data: one capturing performance regressions via version control, and another from programming contest solutions.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Deduplication">
  <data key="d0">Deduplication</data>
  <data key="d1">Methodology</data>
  <data key="d2">Removing duplicate files based on SHA-256 hash to prevent bias during training.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Filtering">
  <data key="d0">Filtering</data>
  <data key="d1">Methodology</data>
  <data key="d2">Removing files larger than 1MB or containing fewer than 15 tokens to ensure quality and relevance of the dataset.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Regression Dataset">
  <data key="d0">Performance Regression Dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset created from version control history of HPC applications (Kripke and Laghos), capturing code changes and performance metrics across 830 commits.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Programming Contests Dataset">
  <data key="d0">Programming Contests Dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset of solutions from online programming contests (Aizu, AtCoder, CodeChef, CodeForces, HackerEarth), used to create code pairs solving the same problem with different implementations.</data>
  <data key="d3">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="830 Commits">
  <data key="d0">830 Commits</data>
  <data key="d1">Results</data>
  <data key="d2">The total number of commits in the dataset is 830, indicating extensive code contribution analyzed.&lt;SEP&gt;The total number of commits in the dataset is 830, indicating the extent of code contributions analyzed.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Programming Competition Solutions">
  <data key="d0">Programming Competition Solutions</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Solutions submitted in various online programming contests from platforms such as Aizu, AtCoder, CodeChef, CodeForces, and HackerEarth, used for performance and implementation analysis.&lt;SEP&gt;Solutions submitted in various online programming contests, aggregated from multiple platforms, used for analysis of code performance and implementation differences.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Contests Dataset">
  <data key="d0">Code Contests Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of data from online programming competitions including Aizu, AtCoder, CodeChef, CodeForces, and HackerEarth, used as the primary dataset for analysis.&lt;SEP&gt;A comprehensive dataset aggregating solutions from multiple online programming competitions, serving as the primary data source for analysis.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Run Time">
  <data key="d0">Run Time</data>
  <data key="d1">Variables</data>
  <data key="d2">The execution time recorded for each solution when run against test cases, used to compare solution efficiency and group solutions into faster and slower pairs.&lt;SEP&gt;The recorded execution time of each solution when run against contest test cases, used to compare solution performance.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Solution Pairs">
  <data key="d0">Solution Pairs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Pairs of solutions that solve the same problem but differ in implementation, labeled as slower or faster based on run time.&lt;SEP&gt;Pairs of solutions that solve the same problem but differ in implementation, labeled based on their run times as slower or faster.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="V. Fine-Tuning Methodology">
  <data key="d0">V. Fine-Tuning Methodology</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework describing the process of selecting, training, and optimizing large language models for specific tasks, including model selection and hyperparameter tuning.&lt;SEP&gt;A framework describing the selection, training, and optimization of large language models for specific tasks, including model selection and hyperparameter tuning processes.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Models Selected for Fine-tuning">
  <data key="d0">Models Selected for Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A set of large language models based on GPT-2 and GPT-3 architectures, chosen for their balance between performance and deployability, including GPT-2, GPT-Neo, and PolyCoder.&lt;SEP&gt;A set of large language models based on GPT-2 and GPT-3 architectures, chosen for their balance between performance, size, and deployability, including GPT-2, GPT-Neo, and PolyCoder.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Pre-Training Data">
  <data key="d0">Pre-Training Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Datasets used to pre-train the models, such as WebText, Pile, and source code from GitHub, providing diverse language and code data for model training.&lt;SEP&gt;Datasets used to pre-train the models, such as WebText, Pile, and source code repositories from GitHub, providing diverse natural language and source code data.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="DeepSpeed">
  <data key="d0">DeepSpeed</data>
  <data key="d1">Tools</data>
  <data key="d2">A framework for distributed training and memory optimization that enables efficient fine-tuning of large models on GPU hardware.&lt;SEP&gt;A framework for distributed training and memory optimization used to facilitate fine-tuning large models efficiently on GPU hardware.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="AdamW Optimizer">
  <data key="d0">AdamW Optimizer</data>
  <data key="d1">Tools</data>
  <data key="d2">An optimization algorithm used during model training to update weights and minimize loss, facilitating effective fine-tuning.&lt;SEP&gt;An optimization algorithm used to update model weights during fine-tuning, minimizing loss and improving model performance.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Hyperparameters">
  <data key="d0">Hyperparameters</data>
  <data key="d1">Variables</data>
  <data key="d2">Hyperparameters are adjustable training settings like batch size, sequence length, and optimizer parameters that influence training efficiency and model performance.&lt;SEP&gt;Hyperparameters are settings like learning rate and training duration that influence the performance of model training processes.&lt;SEP&gt;Parameters such as learning rate (5×10^-5), beta1 (0.9), beta2 (0.999), and floating-point precision (16-bit), controlling training dynamics.&lt;SEP&gt;Parameters such as learning rate (set to 5x10^-5), Adam parameters (beta1=0.9, beta2=0.999), and floating point precision (16-bit) used to control training dynamics and efficiency.&lt;SEP&gt;Hyperparameters are the adjustable settings during model training, such as batch size, sequence length, and optimizer choice, which influence training efficiency and model performance.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Training Hardware">
  <data key="d0">Training Hardware</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The hardware setup used for training includes an AMD EPYC 7763 CPU, 512 GB RAM, and four NVIDIA A100 GPUs with 40 GB memory each, enabling large-scale model fine-tuning.</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Training Process">
  <data key="d0">Training Process</data>
  <data key="d1">Methodology</data>
  <data key="d2">A joint training approach for retriever and generator components, minimizing negative marginal log-likelihood using stochastic gradient descent, with the document encoder fixed to avoid costly updates.&lt;SEP&gt;The process involves fine-tuning pre-trained models on the dataset using DeepSpeed with data parallelism, optimizer updates, and hyperparameter settings.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b&lt;SEP&gt;chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Validation Dataset">
  <data key="d0">Validation Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A subset of data, 5% in this case, used to evaluate model performance during training to prevent overfitting.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Downstream Inference Tasks">
  <data key="d0">Downstream Inference Tasks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Tasks where trained models are applied to generate outputs, such as code generation or predicting pragmas, to evaluate real-world performance.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HumanEval Benchmark">
  <data key="d0">HumanEval Benchmark</data>
  <data key="d1">Tools</data>
  <data key="d2">A benchmark dataset of 164 Python problems used to evaluate code generation models based on functional correctness.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Code Generation Problems">
  <data key="d0">HPC Code Generation Problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Custom HPC code problems designed to evaluate models' ability to generate parallel code, including numerics, OpenMP, and MPI routines.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Prompt">
  <data key="d0">Prompt</data>
  <data key="d1">Variables</data>
  <data key="d2">A prompt is a structured instruction or question provided to an LLM to generate specific code or responses, used in evaluating code generation capabilities.&lt;SEP&gt;A sequence of tokens or instructions designed to elicit a specific response from an LLM, including task descriptions and examples.&lt;SEP&gt;Input descriptions or comments provided to the model to generate code or annotations, such as a function header for saxpy.&lt;SEP&gt;The prompt asks models to use MPI to compute an average across ranks, testing their ability to generate distributed memory code.&lt;SEP&gt;The prompt asks models to use MPI to compute an average across ranks, testing their ability to generate distributed memory parallel code.&lt;SEP&gt;The prompt instructs models to generate code computing a sum in parallel using OpenMP, serving as a test of their code generation capabilities for parallelism.&lt;SEP&gt;The prompt instructs the models to compute a sum in parallel using OpenMP, serving as a test for their code generation capabilities.&lt;SEP&gt;In AI code generation, prompts are input instructions or queries that guide the model to produce specific code outputs, with language and syntax adapted to the community.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-7c9dc700be1265a346cc35786ad23072&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parallelization">
  <data key="d0">Parallelization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallelization involves dividing tasks into concurrent units to improve computational efficiency, often achieved via directives like OpenMP pragmas.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Functionality">
  <data key="d0">Functionality</data>
  <data key="d1">Variables</data>
  <data key="d2">Functionality refers to the correctness and operational behavior of generated code, measured through correctness tests and pass@k metrics.</data>
  <data key="d3">chunk-7c9dc700be1265a346cc35786ad23072</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="MPI">
  <data key="d0">MPI</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">MPI (Message Passing Interface) facilitates communication and data transfer between distributed processes or devices in high-performance computing environments.&lt;SEP&gt;MPI (Message Passing Interface) is a parallel programming model that enables communication between processes in distributed computing, known for its complexity.&lt;SEP&gt;MPI (Message Passing Interface) is a standardized and portable message-passing system designed to function on parallel computing architectures, enabling processes to communicate with one another during computations.&lt;SEP&gt;MPI (Message Passing Interface) is a standardized protocol for communication among processes in parallel computing environments, enabling distributed memory parallelism.&lt;SEP&gt;MPI (Message Passing Interface) is a standardized and portable message-passing system designed to function on parallel computing architectures, noted for its complexity and difficulty for LLMs to generate correct code.&lt;SEP&gt;Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on parallel computing architectures, used as an execution model in the study.&lt;SEP&gt;MPI (Message Passing Interface) is a standardized and portable message-passing system designed for parallel computing, allowing processes to communicate across distributed systems.</data>
  <data key="d3">chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-b943bff32a36cd0ce9ab43893da92b5b&lt;SEP&gt;chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Parallel Framework">
  <data key="d0">Parallel Framework</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Parallel frameworks are conceptual models and systems that facilitate the execution of computations across multiple processing units, such as MPI and OpenMP, providing structured approaches to parallel programming.&lt;SEP&gt;Parallel frameworks provide structured models and methods for executing computations across multiple processors or cores, facilitating efficient parallel processing.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC coding task">
  <data key="d0">HPC coding task</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High-Performance Computing (HPC) coding tasks include activities like decorating loops with OpenMP pragmas to enable parallel execution and optimize performance.&lt;SEP&gt;High-Performance Computing (HPC) coding tasks include writing and optimizing code for parallel execution, such as decorating loops with OpenMP pragmas.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Dataset of for loops with OpenMP pragmas">
  <data key="d0">Dataset of for loops with OpenMP pragmas</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of code samples where for loops are annotated with OpenMP pragmas, used for training and fine-tuning models.&lt;SEP&gt;A curated collection of code snippets where for loops are annotated with OpenMP pragmas, used for training and evaluating models in code generation tasks.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluation Metric">
  <data key="d0">Evaluation Metric</data>
  <data key="d1">Variables</data>
  <data key="d2">A measure used to assess the correctness and proficiency of generated code, based on predefined levels from non-knowledge to expert.&lt;SEP&gt;Metrics like accuracy used to assess model performance in code pragma generation and performance prediction tasks.&lt;SEP&gt;Metrics such as accuracy, used to assess the success of models in generating correct pragmas or predicting code performance.&lt;SEP&gt;A specific measure, such as score, used to assess the performance of the program during evaluation.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Git commit data">
  <data key="d0">Git commit data</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Data consisting of code before and after Git commits, used to train models for performance change prediction.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code contest dataset">
  <data key="d0">Code contest dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A dataset of code pairs from programming contests, used for training models to classify performance differences.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance class">
  <data key="d0">Performance class</data>
  <data key="d1">Variables</data>
  <data key="d2">Binary labels indicating whether code performance is slower (positive) or same/faster (negative) after changes.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Validation perplexity">
  <data key="d0">Validation perplexity</data>
  <data key="d1">Results</data>
  <data key="d2">A measure of how well a language model predicts a sample, with lower perplexity indicating better performance after fine-tuning.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Prediction accuracy">
  <data key="d0">Prediction accuracy</data>
  <data key="d1">Results</data>
  <data key="d2">The percentage of correct predictions made by models in tasks such as pragma correctness or performance change classification.</data>
  <data key="d3">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="GPT-2">
  <data key="d0">GPT-2</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GPT-2 is a language model used as a baseline in code generation tasks, with its performance evaluated after fine-tuning on HPC data.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="PolyCoder">
  <data key="d0">PolyCoder</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A language model trained for code generation and analysis, achieving 94% accuracy in generating OpenMP pragmas.&lt;SEP&gt;A language model trained for code generation and analysis, capable of producing OpenMP pragmas with high accuracy, but slightly less than PolyCoder+HPC.&lt;SEP&gt;PolyCoder demonstrates high accuracy in generating correct OpenMP pragmas, indicating strong performance in parallel annotation tasks.&lt;SEP&gt;PolyCoder is a state-of-the-art language model for code generation, fine-tuned on HPC data, and used as a benchmark in the study.&lt;SEP&gt;PolyCoder's ability to generate correct OpenMP pragmas with high accuracy is evaluated, demonstrating its competence in parallel code annotation.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="PolyCoder+HPC">
  <data key="d0">PolyCoder+HPC</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A fine-tuned language model specialized for HPC code generation and performance analysis, outperforming other LLMs in relevant tasks.&lt;SEP&gt;A fine-tuned large language model tailored for HPC code generation and performance modeling, outperforming other models in relevant tasks.&lt;SEP&gt;A high-performance language model trained to generate and predict OpenMP pragmas with high accuracy, specifically tailored for HPC code.&lt;SEP&gt;An advanced language model trained for high-performance computing code generation and prediction tasks, achieving 97% accuracy in generating functionally correct OpenMP pragmas.&lt;SEP&gt;PolyCoder model further fine-tuned on HPC data, showing improved code generation performance, especially in HPC-specific tasks.&lt;SEP&gt;PolyCoder+HPC also demonstrates high accuracy in generating OpenMP pragmas, indicating its proficiency in parallel code prediction.&lt;SEP&gt;PolyCoder+HPC also shows high accuracy in generating OpenMP pragmas, confirming its proficiency in parallel code prediction.&lt;SEP&gt;PolyCoder+HPC is an enhanced version of Poly-Coder incorporating high-performance computing features, showing higher success rates in code compilation and parallel code generation.&lt;SEP&gt;PolyCoder+HPC is an enhanced version of Poly-Coder, incorporating high-performance computing features, showing improved compilation success rates and ability to generate parallel code with OpenMP and MPI pragmas.&lt;SEP&gt;PolyCoder+HPC is identified as the best performing model based on training scores and code generation tests, used further for comparison in the study.&lt;SEP&gt;PolyCoder+HPC is identified as the top-performing model based on training scores and code generation tests, used for further comparisons in the study.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="GPT-Neo+HPC">
  <data key="d0">GPT-Neo+HPC</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GPT-Neo model fine-tuned on HPC data, evaluated for code generation, with performance metrics provided.&lt;SEP&gt;GPT-Neo+HPC is a model evaluated alongside others, performing slightly worse than PolyCoder+HPC, likely due to less code in its training dataset.&lt;SEP&gt;GPT-Neo+HPC is another model evaluated in the study, performing slightly worse than PolyCoder+HPC in sample compilation, likely due to dataset limitations.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="GPT2+HPC">
  <data key="d0">GPT2+HPC</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GPT-2 model fine-tuned on HPC data, tested for code generation accuracy, with noted limitations due to lack of pre-training on code.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluation Performance">
  <data key="d0">Evaluation Performance</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics such as pass@k rates and compilation success percentages used to assess model effectiveness in code generation.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Catastrophic Forgetting">
  <data key="d0">Catastrophic Forgetting</data>
  <data key="d1">Limitations</data>
  <data key="d2">A challenge in continual learning where neural networks forget previously acquired knowledge upon learning new information, addressed by techniques like elastic weight consolidation.&lt;SEP&gt;A challenge where fine-tuning on new tasks causes the model to lose previously acquired knowledge, impacting overall performance.&lt;SEP&gt;A challenge where neural networks forget previously learned information when trained on new data, addressed by lifelong learning prompts that retain prior knowledge.&lt;SEP&gt;A phenomenon where fine-tuning causes models to lose previously learned information, impacting downstream task performance.&lt;SEP&gt;A phenomenon where neural networks forget previously learned information upon learning new data, addressed by lifelong learning prompts.&lt;SEP&gt;Catastrophic forgetting is a phenomenon where models lose previously acquired knowledge after fine-tuning on new data, impacting performance.&lt;SEP&gt;Catastrophic forgetting occurs when an LLM loses previously learned knowledge after fine-tuning on new data, impacting its overall performance.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Samples">
  <data key="d0">Samples</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Samples refer to individual code snippets or outputs generated by models during evaluation.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluation Figures">
  <data key="d0">Evaluation Figures</data>
  <data key="d1">Tools</data>
  <data key="d2">Figures 4, 5, 6, and 7 are used to visualize and compare model performance metrics such as perplexity, pass@k, and compilation rates.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Parallelism in HPC Code">
  <data key="d0">Parallelism in HPC Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">OpenMP and MPI are parallel programming paradigms evaluated in the generated code for HPC tasks.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Syntactic Correctness">
  <data key="d0">Syntactic Correctness</data>
  <data key="d1">Results</data>
  <data key="d2">The percentage of generated code samples that successfully compile, indicating syntactic validity.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Capacity">
  <data key="d0">Model Capacity</data>
  <data key="d1">Variables</data>
  <data key="d2">Model sizes (1.3B and 6.7B) used to analyze how model size influences the benefit gained from additional training data.&lt;SEP&gt;The size and complexity of language models, influencing their ability to generate accurate HPC code.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Pre-Training">
  <data key="d0">Pre-Training</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Initial training phase of language models on large general datasets, which impacts their ability to learn code during fine-tuning.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Domain Adaptation">
  <data key="d0">Domain Adaptation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Fine-tuning models specifically on HPC source code to improve their performance in HPC tasks.&lt;SEP&gt;The process of adjusting an LLM to perform effectively in specific unseen domains by using prompts or specialized training.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sample Size">
  <data key="d0">Sample Size</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of code snippets generated during evaluation, affecting statistical reliability of performance metrics.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Trends">
  <data key="d0">Performance Trends</data>
  <data key="d1">Results</data>
  <data key="d2">Larger models generally perform better on structured, dense problems like graph and dense linear algebra but struggle with sparse and geometry problems.&lt;SEP&gt;Observation that models perform best on serial and OpenMP, with performance decreasing on more complex models like CUDA/HIP and MPI, and that larger models tend to perform better overall.&lt;SEP&gt;Observation that perplexity continues to improve past a certain point, while downstream performance declines due to catastrophic forgetting.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Training Iterations">
  <data key="d0">Training Iterations</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of training steps or samples processed during fine-tuning, influencing model performance and overfitting risk.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Comparison of Models">
  <data key="d0">Comparison of Models</data>
  <data key="d1">Results</data>
  <data key="d2">Evaluation of PolyCoder+HPC, GPT-Neo+HPC, and GPT2+HPC on code generation and syntactic correctness.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Figures 4, 5, 6, 7">
  <data key="d0">Figures 4, 5, 6, 7</data>
  <data key="d1">Tools</data>
  <data key="d2">Visual representations of model evaluation results, including perplexity, pass@k, and compile success rates.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluation Tasks">
  <data key="d0">Evaluation Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code generation tasks, including openMP and MPI-specific code, used to assess models' practical HPC code generation abilities.</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Poly-Coder">
  <data key="d0">Poly-Coder</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Poly-Coder is a code generation model evaluated for its ability to produce correct code, with performance measured by sample compilation success.&lt;SEP&gt;Poly-Coder is a code generation model that aims to produce correct code based on prompts, with performance evaluated by its ability to compile samples successfully.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="GPT2-HPC">
  <data key="d0">GPT2-HPC</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GPT2-HPC is a model with significantly lower compilation success rate, attributed to having less code in its pre-training dataset.&lt;SEP&gt;GPT2-HPC is a model with significantly lower performance in code compilation, attributed to having less code in its pre-training dataset.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fig. 7">
  <data key="d0">Fig. 7</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Figure 7 illustrates the correlation between build rates and correctness across models, emphasizing the importance of build success for functional correctness.&lt;SEP&gt;Figure 7 illustrates the correlation between build rates and correctness for models, emphasizing the importance of build success in code correctness.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Figure 8">
  <data key="d0">Figure 8</data>
  <data key="d1">Results</data>
  <data key="d2">Figure 8 depicts expected maximum speedup and efficiency across resource counts, analyzing how parallel code performance scales with resource availability and identifying plateauing behaviors.&lt;SEP&gt;Figure 8 depicts expected maximum speedup and efficiency across resource counts, demonstrating trends in parallel code performance.&lt;SEP&gt;Figure 8 shows example outputs from PolyCoder and PolyCoder+HPC for generating OpenMP code, demonstrating the models' ability to produce parallel code with appropriate pragmas.&lt;SEP&gt;Figure 8 shows example outputs from PolyCoder and PolyCoder+HPC for generating OpenMP code, demonstrating their ability to produce parallel code with pragmas.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenMP code">
  <data key="d0">OpenMP code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">OpenMP code is used as an example to evaluate the models' ability to generate correct parallel code annotations, with PolyCoder+HPC successfully adding pragmas.&lt;SEP&gt;OpenMP code is used as an example to evaluate the models' capacity to generate parallel programming constructs, with PolyCoder+HPC correctly adding pragmas, unlike PolyCoder.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="PolyCoder output">
  <data key="d0">PolyCoder output</data>
  <data key="d1">Results</data>
  <data key="d2">PolyCoder produces correct sequential code but fails to add OpenMP pragmas, indicating limitations in parallel code generation.&lt;SEP&gt;PolyCoder produces correct sequential code but fails to include OpenMP pragmas, indicating limited parallel code generation ability.&lt;SEP&gt;PolyCoder produces long, often incorrect MPI code, indicating limited understanding of MPI routines and usage.&lt;SEP&gt;PolyCoder produces long, often incorrect MPI code, indicating limited understanding of MPI routines.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="PolyCoder+HPC output">
  <data key="d0">PolyCoder+HPC output</data>
  <data key="d1">Results</data>
  <data key="d2">PolyCoder+HPC generates correct MPI code for averaging, demonstrating improved comprehension of MPI programming.&lt;SEP&gt;PolyCoder+HPC successfully adds OpenMP pragmas, demonstrating improved ability to generate parallel code.&lt;SEP&gt;PolyCoder+HPC successfully adds OpenMP pragmas, demonstrating improved parallel code generation.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Figure 9">
  <data key="d0">Figure 9</data>
  <data key="d1">Results</data>
  <data key="d2">Figure 9 compares pass@1 scores for translating code between execution models, assessing the models' ability to accurately convert code from serial to OpenMP, MPI, and CUDA to Kokkos, thereby evaluating translation effectiveness.&lt;SEP&gt;Figure 9 compares pass@1 scores for translating code between execution models, showing the effectiveness of LLMs in code translation tasks.&lt;SEP&gt;Figure 9 presents examples of MPI code generated by PolyCoder and PolyCoder+HPC, with the latter correctly implementing MPI routines for parallel averaging.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="MPI code">
  <data key="d0">MPI code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A type of parallel code involving message passing interface, which LLMs struggle with most.&lt;SEP&gt;MPI code examples are used to evaluate the models' understanding of distributed memory parallelism, with PolyCoder+HPC showing better performance in generating correct MPI routines.&lt;SEP&gt;MPI code examples are used to evaluate the models' understanding of distributed memory parallelism, with PolyCoder+HPC showing better performance.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Figure 10">
  <data key="d0">Figure 10</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A visual illustration or diagram demonstrating the process of generating search queries from prompts, serving as an example of methodology.&lt;SEP&gt;A visual or graphical representation illustrating the process or example of generating search queries from prompts.&lt;SEP&gt;Figure 10 shows speedup comparisons for code generated by PolyCoder+HPC, indicating that the generated parallel code outperforms sequential baselines.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Speedup">
  <data key="d0">Speedup</data>
  <data key="d1">Results</data>
  <data key="d2">A metric that quantifies how much faster a model or process is compared to the sequential baseline, indicating performance improvement.&lt;SEP&gt;A quantitative metric that measures how much faster a process or model performs relative to the sequential baseline, indicating efficiency gains.&lt;SEP&gt;Speedup metrics demonstrate that PolyCoder+HPC produces code faster than sequential implementations, confirming its ability to generate efficient parallel code.&lt;SEP&gt;Speedup metrics demonstrate that PolyCoder+HPC produces code faster than sequential implementations, indicating effective parallelization.&lt;SEP&gt;Performance metrics indicating the ratio of runtime improvement achieved by the PPL approach over the original code, with observed values from 1.20 to 12.43.&lt;SEP&gt;Speedup measures the improvement in runtime performance achieved by the PPL approach compared to the original code, with values ranging from 1.20 to 12.43 in the study.&lt;SEP&gt;Speedup quantifies the improvement in execution time achieved through optimization, with up to 12.43x speedups reported.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-aebb652b359c14534612e14559754bbc&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Figure 11">
  <data key="d0">Figure 11</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A visual diagram or illustration demonstrating the process of generating a second-hop search query in multi-hop question-answering systems, illustrating methodology.&lt;SEP&gt;A visual illustration related to the process of prompt generation in multi-hop question-answering systems.&lt;SEP&gt;Figure 11 displays results from OpenMP pragma prediction tests, showing high accuracy in generating functionally correct pragmas.&lt;SEP&gt;Figure 11 shows high accuracy in models' ability to generate functionally correct OpenMP pragmas in the prediction tests.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953&lt;SEP&gt;chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Fig. 10">
  <data key="d0">Fig. 10</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Figure 10 shows the speedups achieved by PolyCoder+HPC over sequential baselines, indicating the effectiveness of the generated parallel code.</data>
  <data key="d3">chunk-3e334e5860963a6ca1939b6719cae953</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="LLMs (Large Language Models)">
  <data key="d0">LLMs (Large Language Models)</data>
  <data key="d1">Tools</data>
  <data key="d2">Advanced artificial intelligence models trained on large datasets to analyze, predict, and assist in biological and medical tasks.&lt;SEP&gt;Large Language Models are extensive neural network models trained on vast text corpora, capable of few-shot learning and various NLP tasks.&lt;SEP&gt;Large language models trained on source code to understand dependencies, syntax, and semantics of programming languages, enabling tasks like code generation and performance prediction.&lt;SEP&gt;Large pre-trained models trained on source code to understand syntax, dependencies, and semantics, enabling tasks like code generation, prediction, and performance modeling.&lt;SEP&gt;LLMs are advanced AI models trained on large datasets to generate human-like text, including code, and are evaluated for their ability to produce correct parallel code.&lt;SEP&gt;Large Language Models like GPT-3.5 and GPT-4 are used to generate code outputs based on prompts, evaluated for performance and correctness.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d&lt;SEP&gt;chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4&lt;SEP&gt;chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="OpenMP Prediction Tests">
  <data key="d0">OpenMP Prediction Tests</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Experiments designed to evaluate the ability of models to generate correct OpenMP pragmas and reproduce them exactly, assessing functional correctness and syntactic accuracy.&lt;SEP&gt;Experiments designed to evaluate the models' ability to generate correct OpenMP pragmas and reproduce them exactly, assessing functional correctness and syntactic accuracy.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Functionally Correct OpenMP Pragmas">
  <data key="d0">Functionally Correct OpenMP Pragmas</data>
  <data key="d1">Results</data>
  <data key="d2">The models achieved 97% (PolyCoder+HPC) and 94% (PolyCoder) accuracy in generating pragmas that are functionally correct, indicating strong understanding of dependencies and clauses.&lt;SEP&gt;The models achieved high accuracy (97% for PolyCoder+HPC and 94% for PolyCoder) in generating pragmas that are correct in functionality, indicating strong understanding of dependencies and clauses.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Exact Reproduction of Pragmas">
  <data key="d0">Exact Reproduction of Pragmas</data>
  <data key="d1">Results</data>
  <data key="d2">Models correctly reproduce the exact clauses and order in 67% (PolyCoder+HPC) and 61% (PolyCoder) of cases, reflecting learned trends in pragma construction.&lt;SEP&gt;The models correctly reproduce the exact clauses and order in 67% (PolyCoder+HPC) and 61% (PolyCoder) of cases, demonstrating learned trends in pragma construction.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Proxy Applications and Programming Competition Dataset">
  <data key="d0">Proxy Applications and Programming Competition Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Data repositories used to evaluate the models' accuracy in predicting performance impacts of code changes, with larger datasets leading to higher accuracy for PolyCoder+HPC.&lt;SEP&gt;Data repositories used to evaluate the models' accuracy in predicting performance impacts of code changes, with the larger programming dataset leading to higher accuracy (92%) for PolyCoder+HPC.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Model Performance Prediction Accuracy">
  <data key="d0">Model Performance Prediction Accuracy</data>
  <data key="d1">Results</data>
  <data key="d2">Both models achieved high classification accuracy (86-92%), with PolyCoder+HPC slightly outperforming the baseline, demonstrating the models' ability to correlate language understanding with performance properties.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Machine Learning in Source Code Performance">
  <data key="d0">Machine Learning in Source Code Performance</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Application of machine learning techniques to model and predict code performance, including performance slowdown and optimization, extending beyond traditional code generation tasks.</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="code2vec">
  <data key="d0">code2vec</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Code2vec is a neural network-based approach that maps source code into vector representations, used to analyze or model code performance.&lt;SEP&gt;Code2vec is a technique that maps source code into an embedded vector space for analysis and modeling, used in performance-related studies.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="ir2vec">
  <data key="d0">ir2vec</data>
  <data key="d1">Methodologies</data>
  <data key="d2">ir2vec is a method similar to code2vec that converts source code into embeddings for performance analysis, applied in specific contexts like OpenCL kernel placement.&lt;SEP&gt;ir2vec is a technique that converts intermediate representations of source code into embeddings, facilitating performance analysis and related tasks.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenCL Kernel Device Placement">
  <data key="d0">OpenCL Kernel Device Placement</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The task of assigning OpenCL kernels to suitable hardware devices to optimize performance, relevant in performance modeling.&lt;SEP&gt;This refers to the task of assigning OpenCL kernels to hardware devices optimally, related to performance modeling.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="DeepDevPERF">
  <data key="d0">DeepDevPERF</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">DeepDevPERF is a BART-based large language model designed to suggest performance improvements to C# code, based on data from code changes with performance keywords.&lt;SEP&gt;DeepDevPERF is a BART-based large language model designed to suggest performance improvements to C# code, trained on performance-related data from code changes.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="BART">
  <data key="d0">BART</data>
  <data key="d1">Tools</data>
  <data key="d2">A transformer-based model used for text generation, compared to RAG in diversity and factuality.&lt;SEP&gt;BART is a denoising sequence-to-sequence pre-training model designed for natural language generation, translation, and comprehension, representing an advanced NLP model.&lt;SEP&gt;BART is a denoising sequence-to-sequence pre-training model used for natural language generation, translation, and comprehension, representing a significant model in NLP.&lt;SEP&gt;BART is a transformer-based language model architecture used as the backbone for DeepDevPERF, enabling code understanding and transformation.&lt;SEP&gt;BART is a transformer-based language model architecture used as the backbone for DeepDevPERF, enabling performance suggestions.&lt;SEP&gt;BART is a sequence-to-sequence transformer model used for text generation, known for hallucination issues and less factual accuracy compared to RAG.&lt;SEP&gt;BART is a transformer-based model used for text generation, compared with RAG in diversity and factuality.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-25ebcde4744e2b19ba1d5d1fd25807b0&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd&lt;SEP&gt;chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Git Commits">
  <data key="d0">Git Commits</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Git commits are version control records that include code changes and messages, used as noisy data sources for performance-related datasets.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Code Generation">
  <data key="d0">HPC Code Generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Automated generation of high-performance computing code to improve efficiency and correctness in scientific computing.&lt;SEP&gt;The process of automatically generating high-performance computing code, which the fine-tuned LLM aims to improve in correctness and efficiency.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Transformations">
  <data key="d0">Code Transformations</data>
  <data key="d1">Methods</data>
  <data key="d2">Code transformations involve modifying source code to improve performance, as suggested by models like DeepDevPERF.&lt;SEP&gt;Modifications applied to source code aimed at improving performance, suggested by models like DeepDevPERF.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance Improvements">
  <data key="d0">Performance Improvements</data>
  <data key="d1">Results</data>
  <data key="d2">The study demonstrates that fine-tuned models can generate correct HPC code with higher success rates and label loops with high accuracy, indicating effective performance analysis.&lt;SEP&gt;The study shows that fine-tuned models can generate correct HPC code with up to 53% higher pass@k rates and label loops with 97% success, indicating effective performance analysis.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Future Work">
  <data key="d0">Future Work</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Explore further analyses, improving code performance, and developing practical tools for HPC developers using language models.&lt;SEP&gt;Exploring further analyses, generating performant code, and developing practical tools for HPC developers using language models.&lt;SEP&gt;Future work aims to improve MPI utilization, reduce memory overhead, enhance scalability, support C/C++, and extend dynamic load balancing capabilities.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Git commits with performance keywords">
  <data key="d0">Git commits with performance keywords</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Git commits containing performance-related keywords serve as data sources for training datasets in performance modeling.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC-specific language models">
  <data key="d0">HPC-specific language models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Language models trained specifically on HPC code to assist in code generation, performance analysis, and optimization tasks.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Performance properties of source code">
  <data key="d0">Performance properties of source code</data>
  <data key="d1">Variables</data>
  <data key="d2">Attributes of code such as efficiency, speed, and resource usage that can be studied and predicted using models.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Labeling for loops with OpenMP pragmas">
  <data key="d0">Labeling for loops with OpenMP pragmas</data>
  <data key="d1">Results</data>
  <data key="d2">High success rate (97%) in identifying loops with OpenMP pragmas, demonstrating the model's capability in code annotation for parallelization.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code generation and performance study">
  <data key="d0">Code generation and performance study</data>
  <data key="d1">Study Designs</data>
  <data key="d2">The approach of using fine-tuned language models to generate code and analyze its performance, validated through success metrics.</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Llama">
  <data key="d0">Code Llama</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An open foundation model designed for code generation and understanding, aimed at supporting various programming tasks.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattaﬁori, W. Xiong, A. D´efossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, G. Synnaeve">
  <data key="d0">B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattaﬁori, W. Xiong, A. D´efossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, G. Synnaeve</data>
  <data key="d1">Research Team or Authors</data>
  <data key="d2">A group of researchers and authors who contributed to the publication titled 'Code Llama: Open foundation models for code' in 2023, focusing on foundational models for code-related AI tasks.&lt;SEP&gt;A group of researchers who authored the publication titled 'Code Llama: Open foundation models for code' in 2023, focusing on foundational models for code understanding and generation.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Android Mobile Malware Detection">
  <data key="d0">Android Mobile Malware Detection</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A study or review focusing on methods for identifying malicious software in Android devices, utilizing machine learning techniques.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Systematic Review">
  <data key="d0">Systematic Review</data>
  <data key="d1">Study Design</data>
  <data key="d2">A comprehensive analysis of existing literature on ChatGPT's utility in healthcare, assessing future potential and limitations.&lt;SEP&gt;A comprehensive review synthesizing existing literature on ChatGPT's utility in healthcare, assessing future prospects and limitations.&lt;SEP&gt;A research methodology that systematically compiles and analyzes existing studies on Android malware detection using machine learning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Machine Learning">
  <data key="d0">Machine Learning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A set of algorithms and statistical models enabling computers to perform tasks without explicit instructions, used here for malware detection.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Electronics">
  <data key="d0">Electronics</data>
  <data key="d1">Discipline</data>
  <data key="d2">An academic discipline related to electronic systems and devices, within which the systematic review on malware detection is situated.&lt;SEP&gt;An academic field related to electronic devices and systems, within which the review on malware detection is situated.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="ML4Code">
  <data key="d0">ML4Code</data>
  <data key="d1">Tools</data>
  <data key="d2">A platform or resource related to machine learning applications in code analysis, accessible via https://ml4code.github.io/.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Summarization">
  <data key="d0">Code Summarization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of generating concise descriptions of source code functionality, often using machine learning models.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Gu, P. Salza, H. C. Gall">
  <data key="d0">J. Gu, P. Salza, H. C. Gall</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Authors of a 2022 IEEE conference paper on assembling foundation models for automatic code summarization, focusing on leveraging large models for this task.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Ahmed, P. Devanbu">
  <data key="d0">T. Ahmed, P. Devanbu</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Authors who researched learning code summarization from small and local datasets, published as an arXiv preprint in 2022.&lt;SEP&gt;Researchers who explored learning code summarization from small and local datasets, published as an arXiv preprint in 2022.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Haque, Z. Eberhart, A. Bansal, C. McMillan">
  <data key="d0">S. Haque, Z. Eberhart, A. Bansal, C. McMillan</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers who developed semantic similarity metrics for evaluating source code summarization, presented at ICPC 2022.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="W. U. Ahmad, S. Chakraborty, B. Ray, K.-W. Chang">
  <data key="d0">W. U. Ahmad, S. Chakraborty, B. Ray, K.-W. Chang</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers who proposed a transformer-based approach for source code summarization, published as an arXiv preprint in 2020.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Richter, H. Wehrheim">
  <data key="d0">C. Richter, H. Wehrheim</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers studying how to learn from developer mistakes, specifically localizing and repairing real bugs from actual bug fixes, published as an arXiv preprint in 2022.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Kharkar, R. Z. Moghaddam, M. Jin, X. Liu, X. Shi, C. B. Clement, N. Sundaresan">
  <data key="d0">A. Kharkar, R. Z. Moghaddam, M. Jin, X. Liu, X. Shi, C. B. Clement, N. Sundaresan</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers working on reducing false positives in analytic bug detectors, presented at ICSE 2022.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin">
  <data key="d0">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Authors of the influential 2017 paper 'Attention is all you need', which introduces the Transformer architecture foundational for modern neural models.&lt;SEP&gt;Authors of the seminal 2017 paper 'Attention is all you need', introducing the Transformer architecture.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="F. F. Xu, U. Alon, G. Neubig, V. J. Hellendoorn">
  <data key="d0">F. F. Xu, U. Alon, G. Neubig, V. J. Hellendoorn</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers who conducted a systematic evaluation of large language models of code, published in 2022.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Allamanis">
  <data key="d0">M. Allamanis</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher who discussed the adverse effects of code duplication in machine learning models of code, presented at ACM SIGPLAN 2019.&lt;SEP&gt;Researcher who discussed the adverse effects of code duplication on machine learning models trained on code, highlighting data quality issues in 2019.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Radford, Wu, Child, Luan, Amodei, Sutskever">
  <data key="d0">Radford, Wu, Child, Luan, Amodei, Sutskever</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Authors of the 2019 paper 'Language models are unsupervised multitask learners', foundational in developing large-scale language models.&lt;SEP&gt;Authors of the 2019 paper 'Language models are unsupervised multitask learners', which introduced large-scale language models capable of multiple tasks.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="KRIPKE">
  <data key="d0">KRIPKE</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A massively parallel transport mini-application used in high-performance computing research, developed by Lawrence Livermore National Laboratory.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="V. A. Dobrev, T. V. Kolev, R. N. Rieben">
  <data key="d0">V. A. Dobrev, T. V. Kolev, R. N. Rieben</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Researchers who developed high-order curvilinear finite element methods for Lagrangian hydrodynamics, published in SIAM Journal on Scientific Computing in 2012.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert">
  <data key="d0">Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">A list of researchers involved in various scientific and computational studies, possibly related to scientific computing and AI.&lt;SEP&gt;A list of researchers involved in various scientific and computational studies, possibly related to the broader scientific context.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Senanayake, H. Kalutarage, M. O. Al-Kadri">
  <data key="d0">J. Senanayake, H. Kalutarage, M. O. Al-Kadri</data>
  <data key="d1">Researchers/Authors</data>
  <data key="d2">Authors of a systematic review published in 2021 on Android mobile malware detection using machine learning techniques.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="ml4code">
  <data key="d0">ml4code</data>
  <data key="d1">Tools</data>
  <data key="d2">A platform or resource dedicated to machine learning applications in code analysis, accessible online at https://ml4code.github.io/.</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Computing">
  <data key="d0">Computing</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Computing refers to the process of using algorithms and computational systems to perform tasks, analyze data, and generate outputs, often involving programming, data processing, and algorithmic workflows.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="vol. 34, no. 5, pp. B606–B641, 2012">
  <data key="d0">vol. 34, no. 5, pp. B606–B641, 2012</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A scholarly article published in a scientific journal, representing a research study or review conducted and documented in 2012, volume 34, issue 5, pages B606–B641.&lt;SEP&gt;This reference indicates a published article in a scientific journal, representing a scholarly study or review published in volume 34, issue 5, pages B606–B641, in 2012.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. d. M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals">
  <data key="d0">Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. d. M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">This list includes multiple researchers involved in the study titled “Competition-level code generation with alphacode,” indicating the primary authors contributing to this research publication.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Competition-level code generation with alphacode”">
  <data key="d0">“Competition-level code generation with alphacode”</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This research aims to develop and evaluate an advanced code generation system (AlphaCode) capable of competing at a high level in programming competitions, testing hypotheses about its performance and capabilities.&lt;SEP&gt;This research hypothesizes that an advanced AI system (AlphaCode) can perform competitively in programming competitions, demonstrating high-level code generation capabilities.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://arxiv.org/abs/2203.07814">
  <data key="d0">https://arxiv.org/abs/2203.07814</data>
  <data key="d1">Tools</data>
  <data key="d2">This is the URL link to the online preprint of the research paper “Competition-level code generation with alphacode,” serving as a digital tool for accessing the study.&lt;SEP&gt;URL link to the preprint of the AlphaCode research paper, providing access to methodology, data, and results.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Gokaslan and V. Cohen">
  <data key="d0">A. Gokaslan and V. Cohen</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the “Openwebtext corpus,” a large dataset of web text used for training language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Openwebtext corpus”">
  <data key="d0">“Openwebtext corpus”</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large dataset composed of web text data, designed to facilitate research in natural language processing and training language models.&lt;SEP&gt;A large, diverse dataset of web text used for training and evaluating language models in NLP research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="http://Skylion007.github.io/OpenWebTextCorpus">
  <data key="d0">http://Skylion007.github.io/OpenWebTextCorpus</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to access the Openwebtext corpus dataset, providing researchers a resource for language modeling tasks.&lt;SEP&gt;URL link to access the “Openwebtext corpus” dataset for NLP and language modeling research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Black, G. Leo, P. Wang, C. Leahy, and S. Biderman">
  <data key="d0">S. Black, G. Leo, P. Wang, C. Leahy, and S. Biderman</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the paper on GPT-Neo, a large-scale autoregressive language model.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorﬂow”">
  <data key="d0">“GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorﬂow”</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This research investigates the performance and scalability of GPT-Neo as a large-scale autoregressive language model, testing its ability to generate coherent, high-quality text.&lt;SEP&gt;This work investigates the development and performance of GPT-Neo, a large-scale language model, testing its effectiveness and scalability in natural language processing.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://doi.org/10.5281/zenodo.5297715">
  <data key="d0">https://doi.org/10.5281/zenodo.5297715</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to resources and code for GPT-Neo model for NLP research.&lt;SEP&gt;URL link to the GPT-Neo model and related resources for implementation and research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy">
  <data key="d0">L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of “The pile,” a large, diverse text dataset for training language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“The pile: An 800gb dataset of diverse text for language modeling”">
  <data key="d0">“The pile: An 800gb dataset of diverse text for language modeling”</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An extensive dataset comprising 800GB of diverse textual data used to train and evaluate language models, enhancing their performance and generalization.&lt;SEP&gt;An extensive, 800GB dataset comprising diverse textual sources for training and evaluating language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://arxiv.org/abs/2101.00027">
  <data key="d0">https://arxiv.org/abs/2101.00027</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to access “The pile” dataset for research purposes.&lt;SEP&gt;URL link to access “The pile” dataset, supporting large-scale NLP training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei">
  <data key="d0">T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the seminal paper “Language models are few-shot learners,” which explores the capabilities of large-scale language models in few-shot learning scenarios.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Language models are few-shot learners”">
  <data key="d0">“Language models are few-shot learners”</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This study hypothesizes that large language models can perform effectively in few-shot learning tasks, demonstrating versatility and efficiency in natural language understanding.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://arxiv.org/abs/2005.14165">
  <data key="d0">https://arxiv.org/abs/2005.14165</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the paper detailing the “Language models are few-shot learners” study, providing access to methodology and results.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="OpenAI">
  <data key="d0">OpenAI</data>
  <data key="d1">Researcher/Organization</data>
  <data key="d2">OpenAI is an organization that developed advanced language models, including GPT-4, and conducts research on artificial intelligence and natural language processing.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Gpt-4 technical report”">
  <data key="d0">“Gpt-4 technical report”</data>
  <data key="d1">Research Results</data>
  <data key="d2">A detailed technical report published in 2023 describing the architecture, capabilities, and advancements of GPT-4, a state-of-the-art language model.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush">
  <data key="d0">T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the paper “Transformers: State-of-the-Art Natural Language Processing,” which discusses the transformer architecture foundational to modern NLP models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Transformers: State-of-the-Art Natural Language Processing”">
  <data key="d0">“Transformers: State-of-the-Art Natural Language Processing”</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">This publication explains the transformer architecture, a deep learning model that underpins many modern NLP systems, enabling effective language understanding and generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://www.aclweb.org/anthology/2020.emnlp-demos.6">
  <data key="d0">https://www.aclweb.org/anthology/2020.emnlp-demos.6</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the paper and resources on transformer models for NLP.&lt;SEP&gt;URL link to the transformer architecture paper and related resources.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He">
  <data key="d0">J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the paper on “Zero-offload,” a framework for democratizing billion-scale model training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Zero-offload: Democratizing billion-scale model training”">
  <data key="d0">“Zero-offload: Democratizing billion-scale model training”</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology that offloads computation and memory to enable training of billion-scale models on accessible hardware.&lt;SEP&gt;This work introduces a methodology for training extremely large models by offloading computation, making billion-scale training feasible on accessible hardware.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://arxiv.org/abs/2101.06840">
  <data key="d0">https://arxiv.org/abs/2101.06840</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the Zero-offload framework for implementing large-scale model training.&lt;SEP&gt;URL link to the Zero-offload framework for large-scale model training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Ben-Nun and T. Hoeffler">
  <data key="d0">T. Ben-Nun and T. Hoeffler</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the paper “Demystifying parallel and distributed deep learning,” which analyzes concurrency in large-scale deep learning systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Demystifying parallel and distributed deep learning: An in-depth concurrency analysis”">
  <data key="d0">“Demystifying parallel and distributed deep learning: An in-depth concurrency analysis”</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">This publication applies concurrency analysis techniques to understand and optimize parallel and distributed deep learning systems, enhancing efficiency and scalability.&lt;SEP&gt;This publication applies concurrency analysis techniques to understand and optimize parallel and distributed deep learning systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://doi.org/10.1145/3320060">
  <data key="d0">https://doi.org/10.1145/3320060</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the paper on parallel and distributed deep learning for further reading.&lt;SEP&gt;URL link to the paper on parallel and distributed deep learning for further study.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. Nichols, S. Singh, S.-H. Lin, and A. Bhatele">
  <data key="d0">D. Nichols, S. Singh, S.-H. Lin, and A. Bhatele</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the survey and empirical evaluation of parallel deep learning frameworks, providing insights into their performance and design.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“A survey and empirical evaluation of parallel deep learning frameworks”">
  <data key="d0">“A survey and empirical evaluation of parallel deep learning frameworks”</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A systematic review and empirical evaluation assessing the performance, scalability, and efficiency of various parallel deep learning frameworks.&lt;SEP&gt;This work systematically reviews and empirically evaluates various parallel deep learning frameworks to assess their performance and scalability.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="http://arxiv.org/abs/1711.05101">
  <data key="d0">http://arxiv.org/abs/1711.05101</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the paper on weight decay regularization in Adam optimizer, relevant for training stability in deep learning.&lt;SEP&gt;URL link to the paper on weight decay regularization in Adam optimizer, relevant for training stability.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="I. Loshchilov and F. Hutter">
  <data key="d0">I. Loshchilov and F. Hutter</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors of the paper “Fixing weight decay regularization in adam,” which proposes improvements for regularization techniques in deep learning optimizers.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="“Fixing weight decay regularization in adam”">
  <data key="d0">“Fixing weight decay regularization in adam”</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology to improve weight decay regularization in Adam optimizer, enhancing training stability and model performance.&lt;SEP&gt;This publication presents a methodology for improving weight decay regularization in the Adam optimizer to enhance training stability and model performance.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Bur">
  <data key="d0">M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Bur</data>
  <data key="d1">Researcher Names</data>
  <data key="d2">Authors involved in ongoing research on large language models and their training techniques, indicating active research in model scaling and optimization.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="COMPUTING">
  <data key="d0">COMPUTING</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Computing involves the use of algorithms, data processing, and computational systems to perform tasks, analyze information, and generate outputs across various applications and disciplines.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Li">
  <data key="d0">Y. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">One of the authors of the study titled “Competition-level code generation with alphacode,” involved in research on advanced code generation systems.&lt;SEP&gt;Y. Li is a researcher working on large language models and code understanding.&lt;SEP&gt;Y. Li is a researcher working on large language models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. Choi">
  <data key="d0">D. Choi</data>
  <data key="d1">Researcher</data>
  <data key="d2">One of the authors contributing to the research on code generation, focusing on competitive programming models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Chung">
  <data key="d0">J. Chung</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in the study on code generation and AI systems for programming tasks.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Kushman">
  <data key="d0">N. Kushman</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of AI models for code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Schrittwieser">
  <data key="d0">J. Schrittwieser</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in research on reinforcement learning and AI systems, contributing to the study on code generation models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="R. Leblond">
  <data key="d0">R. Leblond</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the research on AI and code generation systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Eccles">
  <data key="d0">T. Eccles</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in AI research related to code generation and machine learning models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Keeling">
  <data key="d0">J. Keeling</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the study on AI systems for code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="F. Gimeno">
  <data key="d0">F. Gimeno</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI research and code generation systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. D. Lago">
  <data key="d0">A. D. Lago</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of AI models for code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Hubert">
  <data key="d0">T. Hubert</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI research, particularly in code generation and machine learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P. Choy">
  <data key="d0">P. Choy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the study on AI-driven code generation systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. d. M. d’Autume">
  <data key="d0">C. d. M. d’Autume</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI research related to code generation and reinforcement learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="I. Babuschkin">
  <data key="d0">I. Babuschkin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research, including code generation models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="X. Chen">
  <data key="d0">X. Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI and machine learning research for code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P.-S. Huang">
  <data key="d0">P.-S. Huang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI systems for code generation and reinforcement learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Welbl">
  <data key="d0">J. Welbl</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI research related to code generation and natural language processing.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Gowal">
  <data key="d0">S. Gowal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research, including code generation models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Cherepanov">
  <data key="d0">A. Cherepanov</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI and machine learning research focused on code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Molloy">
  <data key="d0">J. Molloy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research on code generation systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. J. Mankowitz">
  <data key="d0">D. J. Mankowitz</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI and reinforcement learning research for code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="E. S. Robson">
  <data key="d0">E. S. Robson</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research, including code generation models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P. Kohli">
  <data key="d0">P. Kohli</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI and machine learning research related to code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. de Freitas">
  <data key="d0">N. de Freitas</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research, particularly in reinforcement learning and code generation.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="K. Kavukcuoglu">
  <data key="d0">K. Kavukcuoglu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI and deep learning research, including code generation models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="O. Vinyals">
  <data key="d0">O. Vinyals</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research, including development of code generation systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Gokaslan">
  <data key="d0">A. Gokaslan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of “Openwebtext corpus,” involved in compiling a large web text dataset for NLP research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="V. Cohen">
  <data key="d0">V. Cohen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Co-author of the “Openwebtext corpus,” contributing to dataset creation for language modeling.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Black">
  <data key="d0">S. Black</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in compiling “The pile” dataset for language modeling.&lt;SEP&gt;Author of the paper on GPT-Neo, a large-scale language model, involved in NLP model development.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="G. Leo">
  <data key="d0">G. Leo</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to GPT-Neo research, focusing on large-scale autoregressive language modeling.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P. Wang">
  <data key="d0">P. Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">P. Wang is a researcher working on code intelligence and large language models.&lt;SEP&gt;P. Wang is a researcher working on code models, AI for programming, and large language models.&lt;SEP&gt;Researcher involved in NLP and language model research, including GPT-Neo development.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Leahy">
  <data key="d0">C. Leahy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to large language model research and NLP systems.&lt;SEP&gt;Researcher involved in dataset development for NLP training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Biderman">
  <data key="d0">S. Biderman</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the GPT-Neo project, focusing on large-scale language modeling with Mesh-Tensorﬂow.&lt;SEP&gt;Co-author of “The pile,” contributing to dataset creation for training NLP models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Black">
  <data key="d0">M. Black</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to the GPT-Neo project, involved in large-scale autoregressive language modeling.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Gao">
  <data key="d0">L. Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of “The pile,” a large dataset of diverse text used for training language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Golding">
  <data key="d0">L. Golding</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to “The pile” dataset, focusing on dataset diversity.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Hoppe">
  <data key="d0">T. Hoppe</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in dataset curation for NLP training datasets.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Foster">
  <data key="d0">C. Foster</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of large datasets for language modeling.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Phang">
  <data key="d0">J. Phang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in NLP datasets and large-scale training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="H. He">
  <data key="d0">H. He</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to dataset curation for NLP models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Thite">
  <data key="d0">A. Thite</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in dataset development for language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Nabeshima">
  <data key="d0">N. Nabeshima</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to large NLP datasets.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Presser">
  <data key="d0">S. Presser</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in dataset curation for language modeling.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Brown">
  <data key="d0">T. Brown</data>
  <data key="d1">Researcher</data>
  <data key="d2">Lead author of “Language models are few-shot learners,” focusing on large language model capabilities.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="B. Mann">
  <data key="d0">B. Mann</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the study on few-shot learning with large language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Ryder">
  <data key="d0">N. Ryder</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in large-scale language model research, including GPT-3.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Subbiah">
  <data key="d0">M. Subbiah</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of GPT models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Kaplan">
  <data key="d0">J. Kaplan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of research on the capabilities of large language models in few-shot settings.&lt;SEP&gt;See above (duplicate entry, same as earlier</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P. Dhariwal">
  <data key="d0">P. Dhariwal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in the development and evaluation of GPT models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Neelakantan">
  <data key="d0">A. Neelakantan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to large language model research and few-shot learning studies.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="P. Shyam">
  <data key="d0">P. Shyam</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in GPT model research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="G. Sastry">
  <data key="d0">G. Sastry</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of large-scale language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Askell">
  <data key="d0">A. Askell</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI safety and large language model research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Agarwal">
  <data key="d0">S. Agarwal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to GPT research and NLP capabilities.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Herbert-Voss">
  <data key="d0">A. Herbert-Voss</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in large language model research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="G. Krueger">
  <data key="d0">G. Krueger</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI and NLP research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Henighan">
  <data key="d0">T. Henighan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in scaling large language models and few-shot learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="R. Child">
  <data key="d0">R. Child</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to the development of GPT models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Ramesh">
  <data key="d0">A. Ramesh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in AI research including GPT models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. M. Ziegler">
  <data key="d0">D. M. Ziegler</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to large language model development.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Wu">
  <data key="d0">J. Wu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in NLP research and GPT model training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Winter">
  <data key="d0">C. Winter</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to AI research and language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Hesse">
  <data key="d0">C. Hesse</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in large-scale NLP model research.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Chen">
  <data key="d0">M. Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in research on large language models, training techniques, and optimization.&lt;SEP&gt;Researcher contributing to large language model development and training techniques.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Tworek">
  <data key="d0">J. Tworek</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher focusing on large-scale NLP models and training methodologies.&lt;SEP&gt;Researcher involved in scaling and training large language models, including GPT-3.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="H. Jun">
  <data key="d0">H. Jun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to NLP model scaling and training.&lt;SEP&gt;Researcher contributing to NLP model training and large-scale language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Q. Yuan">
  <data key="d0">Q. Yuan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in NLP research, focusing on model scaling.&lt;SEP&gt;Researcher involved in large language model training and scaling techniques.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="H. P. de Oliveira Pinto">
  <data key="d0">H. P. de Oliveira Pinto</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to NLP and large language model training.&lt;SEP&gt;Researcher working on NLP training and optimization.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="H. Edwards">
  <data key="d0">H. Edwards</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in large-scale NLP model training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Bur">
  <data key="d0">Y. Bur</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to NLP research and large language models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J. Ren">
  <data key="d0">J. Ren</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of “Zero-offload” framework, involved in large-scale model training methodologies.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Rajbhandari">
  <data key="d0">S. Rajbhandari</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to scalable training techniques for billion-scale models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="R. Y. Aminabadi">
  <data key="d0">R. Y. Aminabadi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in distributed deep learning and model training frameworks.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="O. Ruwase">
  <data key="d0">O. Ruwase</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to large-scale model training and parallelism techniques.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Yang">
  <data key="d0">S. Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in model training efficiency and scalability.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Zhang">
  <data key="d0">M. Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to large model training methodologies.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. Li">
  <data key="d0">D. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in deep learning frameworks for large models.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. He">
  <data key="d0">Y. He</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to scalable and efficient model training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Ben-Nun">
  <data key="d0">T. Ben-Nun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of “Demystifying parallel and distributed deep learning,” analyzing concurrency in large-scale training.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="T. Hoeffler">
  <data key="d0">T. Hoeffler</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the same paper, focusing on parallelism and efficiency in distributed deep learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="D. Nichols">
  <data key="d0">D. Nichols</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of empirical studies evaluating parallel deep learning frameworks.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Singh">
  <data key="d0">S. Singh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in performance evaluation of deep learning frameworks.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S.-H. Lin">
  <data key="d0">S.-H. Lin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to empirical evaluation and performance analysis of parallel deep learning systems.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Bhatele">
  <data key="d0">A. Bhatele</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in the empirical assessment of deep learning frameworks.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="I. Loshchilov">
  <data key="d0">I. Loshchilov</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the paper proposing improvements to weight decay regularization in Adam optimizer.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="F. Hutter">
  <data key="d0">F. Hutter</data>
  <data key="d1">Researcher</data>
  <data key="d2">Co-author of the same paper, focusing on optimization techniques in deep learning.</data>
  <data key="d3">chunk-171366f30e20f3d5db56b06a48c19e63</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Loshchilov and Hutter 2017">
  <data key="d0">Loshchilov and Hutter 2017</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A research study proposing a method to fix weight decay regularization in the Adam optimizer, aiming to improve regularization techniques in deep learning.&lt;SEP&gt;A research study that proposes a method for fixing weight decay regularization in the Adam optimization algorithm.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fixing weight decay regularization in Adam">
  <data key="d0">Fixing weight decay regularization in Adam</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A methodological approach to improve regularization in the Adam optimizer, focusing on weight decay.&lt;SEP&gt;A methodological approach to improve the application of weight decay regularization specifically within the Adam optimization algorithm.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Chen et al. 2021">
  <data key="d0">M. Chen et al. 2021</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A comprehensive evaluation of large language models trained on code, including performance and capabilities.&lt;SEP&gt;An extensive evaluation of large language models trained on code, assessing their performance, capabilities, and limitations across various programming tasks.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Evaluating large language models trained on code">
  <data key="d0">Evaluating large language models trained on code</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A research study assessing the performance and capabilities of large language models in coding tasks.&lt;SEP&gt;Investigates how large language models trained on source code perform on different tasks, including understanding, generation, and code comprehension.&lt;SEP&gt;Investigates the performance, capabilities, and limitations of large language models trained on source code.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68&lt;SEP&gt;chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="J. Devlin et al. 2019">
  <data key="d0">J. Devlin et al. 2019</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Development and pre-training of BERT, a deep bidirectional transformer model designed for natural language understanding tasks.&lt;SEP&gt;Development and pre-training of BERT, a deep bidirectional transformer model for language understanding.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="BERT: Pre-training of deep bidirectional transformers for language understanding">
  <data key="d0">BERT: Pre-training of deep bidirectional transformers for language understanding</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A foundational NLP model that uses deep bidirectional transformers pre-trained on large corpora to improve language understanding.&lt;SEP&gt;A foundational model architecture for natural language processing, utilizing deep bidirectional transformers pre-trained on large corpora.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Liu et al. 2019">
  <data key="d0">Y. Liu et al. 2019</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Development of RoBERTa, an optimized BERT pretraining approach for improved language understanding.&lt;SEP&gt;Development of RoBERTa, an optimized version of BERT that enhances pretraining techniques for better NLP performance.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Roberta: A robustly optimized BERT pretraining approach">
  <data key="d0">Roberta: A robustly optimized BERT pretraining approach</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An improved transformer-based model that achieves better results in NLP tasks through optimized training strategies.&lt;SEP&gt;An improved version of BERT that enhances pretraining techniques for better performance in NLP tasks.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Wei et al. 2023">
  <data key="d0">Y. Wei et al. 2023</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Introduction of Magicoder, a large language model specialized for source code understanding and generation, aiming to facilitate programming tasks.&lt;SEP&gt;Introduction of Magicoder, a source code-focused model leveraging large language models for programming tasks.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Magicoder: Source code is all you need">
  <data key="d0">Magicoder: Source code is all you need</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A large language model designed specifically for source code tasks, leveraging advanced training to assist in programming and code comprehension.&lt;SEP&gt;A large language model specialized for source code understanding and generation, aiming to streamline programming workflows.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="J.-B. Doderlein et al. 2022">
  <data key="d0">J.-B. Doderlein et al. 2022</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Analysis of how prompt temperature and strategies influence the behavior and effectiveness of code generation models like Copilot and Codex.&lt;SEP&gt;Evaluation of copilot and codex models under different prompt conditions to understand their behavior and effectiveness.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Piloting copilot and codex: Hot temperature, cold prompts, or black magic?">
  <data key="d0">Piloting copilot and codex: Hot temperature, cold prompts, or black magic?</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Examines how prompt temperature and prompting strategies influence code generation performance of AI models like Copilot and Codex.&lt;SEP&gt;Examines the impact of prompt temperature settings and prompt engineering strategies on the quality and reliability of AI-generated code.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Barke et al. 2022">
  <data key="d0">S. Barke et al. 2022</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Analysis of how programmers interact with grounded code-generating AI models, focusing on usability and effectiveness.&lt;SEP&gt;Investigation into how programmers interact with grounded AI code-generation models, focusing on usability, effectiveness, and user experience.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Grounded copilot: How programmers interact with code-generating models">
  <data key="d0">Grounded copilot: How programmers interact with code-generating models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A study analyzing user interactions, challenges, and behaviors when working with AI-powered coding tools in real-world programming environments.&lt;SEP&gt;A study on user interaction patterns and challenges when working with AI-powered code generation tools.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Sarkar et al. 2022">
  <data key="d0">A. Sarkar et al. 2022</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Qualitative exploration of programmers' experiences and perceptions when programming with artificial intelligence.&lt;SEP&gt;Qualitative exploration of programmers' experiences, perceptions, and challenges when programming with artificial intelligence assistance.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="What is it like to program with artificial intelligence?">
  <data key="d0">What is it like to program with artificial intelligence?</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates programmers' subjective experiences, challenges, and benefits of using AI in coding.&lt;SEP&gt;Seeks to understand programmers' subjective experiences, benefits, and difficulties when integrating AI into coding workflows.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Chen et al. 2023">
  <data key="d0">L. Chen et al. 2023</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Development of data race detection techniques utilizing large language models for software analysis.&lt;SEP&gt;Development of techniques utilizing large language models to detect data races in concurrent software, enhancing software reliability.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Data race detection using large language models">
  <data key="d0">Data race detection using large language models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Application of large language models to identify and analyze data races in concurrent programming environments.&lt;SEP&gt;Application of large language models to identify data races in concurrent programming environments.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Munley et al. 2023">
  <data key="d0">C. Munley et al. 2023</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Development of LLM-driven test suites for compiler validation, aiming to improve compiler reliability.&lt;SEP&gt;Development of an LLM-driven test suite for compiler validation, aimed at improving compiler correctness and reliability.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Llm4vv: Developing llm-driven testsuite for compiler validation">
  <data key="d0">Llm4vv: Developing llm-driven testsuite for compiler validation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A methodology employing large language models to generate and validate compiler test cases.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="U. Alon et al. 2018">
  <data key="d0">U. Alon et al. 2018</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Introduction of code2vec, a model that learns distributed representations of source code to facilitate code similarity, classification, and analysis tasks.&lt;SEP&gt;Proposal of code2vec, a model for learning distributed representations of source code for various code analysis tasks.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="code2vec: Learning distributed representations of code">
  <data key="d0">code2vec: Learning distributed representations of code</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A machine learning model that encodes source code into vector representations to facilitate code similarity, classification, and analysis.&lt;SEP&gt;A machine learning technique that encodes source code into vector embeddings, enabling various code analysis applications.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. VenkataKeerthy et al. 2020">
  <data key="d0">S. VenkataKeerthy et al. 2020</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Development of IR2V, a scalable program embedding method based on LLVM IR for large-scale code analysis.&lt;SEP&gt;Development of IR2V, a scalable program embedding method based on LLVM IR, designed for large-scale, language-agnostic code analysis.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="IR2V: LLVM IR-based scalable program embeddings">
  <data key="d0">IR2V: LLVM IR-based scalable program embeddings</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A technique for creating scalable, language-agnostic program embeddings using LLVM intermediate representation.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Garg et al. 2022">
  <data key="d0">S. Garg et al. 2022</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Application of deep learning models to improve software performance through performance profiling, analysis, and optimization techniques.&lt;SEP&gt;Application of deep learning techniques to improve software performance, focusing on performance profiling and optimization.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Deepdev-perf: a deep learning-based approach for improving software performance">
  <data key="d0">Deepdev-perf: a deep learning-based approach for improving software performance</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A methodology that employs deep learning models to analyze and enhance software performance.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Llm4vv">
  <data key="d0">Llm4vv</data>
  <data key="d1">Tools</data>
  <data key="d2">A methodology employing large language models to generate, execute, and validate compiler test cases for software correctness.&lt;SEP&gt;A test suite development framework driven by large language models for compiler validation, described by Munley et al. in 2023.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb&lt;SEP&gt;chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="IR2V">
  <data key="d0">IR2V</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A scalable program embedding approach that uses LLVM intermediate representation to generate embeddings suitable for large-scale code analysis.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Deepdev-perf">
  <data key="d0">Deepdev-perf</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A deep learning-based approach for analyzing software performance and suggesting optimizations to improve efficiency.&lt;SEP&gt;Deepdev-perf is a deep learning-based approach aimed at improving software performance, representing a specific technical methodology in software engineering.</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68&lt;SEP&gt;chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Software Performance">
  <data key="d0">Software Performance</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Software performance refers to how efficiently software operates, which is the focus of the Deepdev-perf methodology.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering">
  <data key="d0">ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</data>
  <data key="d1">Study Design</data>
  <data key="d2">This conference is a venue where research like Deepdev-perf is presented, indicating a platform for disseminating methodologies and results in software engineering.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Garg, R. Z. Moghaddam, C. B. Clement, N. Sundaresan, C. Wu">
  <data key="d0">S. Garg, R. Z. Moghaddam, C. B. Clement, N. Sundaresan, C. Wu</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Researchers who authored the study presenting Deepdev-perf, contributing to the scientific literature on software performance improvement.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L. Zettlemoyer">
  <data key="d0">M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L. Zettlemoyer</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Researchers who authored the study on BART, a pre-training model for natural language processing, contributing to NLP methodologies.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Natural Language Generation, Translation, and Comprehension">
  <data key="d0">Natural Language Generation, Translation, and Comprehension</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The study on BART investigates how denoising pre-training can enhance performance across various NLP tasks.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="dec 2020">
  <data key="d0">dec 2020</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The date December 2020 indicates the time period during which the referenced studies or data were published or relevant.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="https://doi.org/10.1145/3418463">
  <data key="d0">https://doi.org/10.1145/3418463</data>
  <data key="d1">Tools</data>
  <data key="d2">A digital object identifier (DOI) link providing direct access to the referenced publication or dataset.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Garg">
  <data key="d0">S. Garg</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of Deepdev-perf, contributing to methodologies for improving software performance.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="R. Z. Moghaddam">
  <data key="d0">R. Z. Moghaddam</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of Deepdev-perf, contributing to methodologies for improving software performance.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. B. Clement">
  <data key="d0">C. B. Clement</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of Deepdev-perf, contributing to methodologies for improving software performance.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Sundaresan">
  <data key="d0">N. Sundaresan</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of Deepdev-perf, contributing to methodologies for improving software performance.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="C. Wu">
  <data key="d0">C. Wu</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of Deepdev-perf, contributing to methodologies for improving software performance.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering">
  <data key="d0">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</data>
  <data key="d1">Study Design</data>
  <data key="d2">Conference proceedings where Deepdev-perf and related studies are presented, serving as a platform for dissemination of research in software engineering.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Lewis">
  <data key="d0">M. Lewis</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Liu">
  <data key="d0">Y. Liu</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Goyal">
  <data key="d0">N. Goyal</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Ghazvininejad">
  <data key="d0">M. Ghazvininejad</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Mohamed">
  <data key="d0">A. Mohamed</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="O. Levy">
  <data key="d0">O. Levy</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="V. Stoyanov">
  <data key="d0">V. Stoyanov</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Zettlemoyer">
  <data key="d0">L. Zettlemoyer</data>
  <data key="d1">Authors</data>
  <data key="d2">Researcher involved in the development of BART, a pre-training model for NLP, contributing to theories/models of language processing.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Natural Language Generation">
  <data key="d0">Natural Language Generation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">One of the primary NLP tasks BART aims to improve through pre-training, testing hypotheses about enhanced language modeling capabilities.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Translation">
  <data key="d0">Translation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">BART is evaluated for its effectiveness in translation tasks, exploring hypotheses about transfer learning in multilingual contexts.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Comprehension">
  <data key="d0">Comprehension</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">BART is intended to enhance language comprehension, testing hypotheses about understanding and reasoning in NLP.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Decorate Loops with OpenMP Pragmas">
  <data key="d0">Decorate Loops with OpenMP Pragmas</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d2">HPC-Coder automatically decorates loops with OpenMP pragmas to facilitate parallel execution.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fine-tuned on HPC and scientific code datasets">
  <data key="d0">Fine-tuned on HPC and scientific code datasets</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d2">The model is trained on a curated dataset of HPC and scientific source codes to enhance its domain-specific capabilities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Codes">
  <data key="d0">HPC Codes</data>
  <data key="d3">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d2">Fine-tuning adjusts pre-trained language models on HPC-specific code datasets to improve task performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="source code">
  <data key="d0">source code</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d2">Transformer models are applied to source code to enable tasks like code generation, labeling, and performance prediction."|&lt;SEP&gt;Transformer models underpin the activity of modeling, generating, and analyzing source code in this research."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="training and evaluation">
  <data key="d0">training and evaluation</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d2">The curated dataset is used to train, validate, and test the models' capabilities in code generation and performance prediction."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="model's accuracy">
  <data key="d0">model's accuracy</data>
  <data key="d3">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d2">The activity of predicting code performance achieves up to 92% accuracy, demonstrating effectiveness."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Fine-tuning Setup">
  <data key="d0">Fine-tuning Setup</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d2">The selected models are fine-tuned using specific datasets, hyperparameters, and hardware configurations to optimize their performance for the task.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Training and Validation">
  <data key="d0">Training and Validation</data>
  <data key="d3">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d2">Metrics like perplexity and token accuracy are used to evaluate the effectiveness of the fine-tuned models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Dataset">
  <data key="d0">HPC Dataset</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d2">PolyCoder is fine-tuned on the HPC dataset, which improves its ability to generate HPC-specific code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="GPT2">
  <data key="d0">GPT2</data>
  <data key="d3">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d2">GPT-2, lacking pre-training on code, performs poorly even after fine-tuning on HPC data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="LLMs">
  <data key="d0">LLMs</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-c48289d50367366b4b4faa00d871ebb4&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d2">Large Language Models (LLMs) are advanced AI systems designed to generate code and solve computational problems across various programming models, with performance varying based on problem complexity and model size.&lt;SEP&gt;Large Language Models (LLMs) are advanced artificial intelligence models trained on vast datasets to understand and generate human-like text, used here for evaluating code generation and translation capabilities.&lt;SEP&gt;Large Language Models (LLMs) are advanced artificial intelligence systems designed to generate code and solve computational problems, with varying effectiveness depending on the programming model and problem type.&lt;SEP&gt;Large Language Models (LLMs) are artificial intelligence models designed to understand and generate human-like text, evaluated here for their ability to produce correct and efficient parallel code.&lt;SEP&gt;Large Language Models understand the dependencies and syntax of OpenMP pragmas, enabling them to generate functionally correct directives.</data>
  <data key="d1">Core Concepts</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Functionally Correct Pragmas">
  <data key="d0">Functionally Correct Pragmas</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d2">High accuracy in generating functionally correct pragmas indicates strong comprehension of parallelization requirements.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Exact Reproduction">
  <data key="d0">Exact Reproduction</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d2">Models' ability to reproduce pragmas exactly reflects learned patterns in pragma construction and ordering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Proxy Applications and Dataset">
  <data key="d0">Proxy Applications and Dataset</data>
  <data key="d3">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d2">The larger programming dataset allows PolyCoder+HPC to better learn performance trends, resulting in higher prediction accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Code Changes Dataset">
  <data key="d0">Code Changes Dataset</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d2">DeepDevPERF uses code changes from Git commits with performance keywords to train and evaluate performance improvement suggestions.&lt;SEP&gt;DeepDevPERF utilizes code changes from Git commits with performance keywords to train and evaluate the model for performance improvement suggestions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="HPC Tasks">
  <data key="d0">HPC Tasks</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d2">Fine-tuning a language model on HPC code improves its ability to generate correct and performant code.&lt;SEP&gt;Fine-tuning enhances the model's ability to generate correct and performant HPC code from limited data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Analysis and Tool Development">
  <data key="d0">Analysis and Tool Development</data>
  <data key="d3">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d2">Future plans include analyzing code performance further and creating practical tools for HPC developers.&lt;SEP&gt;Future research aims to develop tools and further analyze code performance using the specialized language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Research Team or Authors">
  <data key="d0">Research Team or Authors</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">The listed researchers and authors contributed to the publication about Code Llama, indicating their role in developing or analyzing the foundation model for code."|&gt;"research contribution, authorship</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Assemble Foundation Models for Automatic Code Summarization">
  <data key="d0">Assemble Foundation Models for Automatic Code Summarization</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">These authors developed models to automate code summarization, advancing AI capabilities in code understanding."|&gt;"research focus, model development&lt;SEP&gt;These authors developed models to automate code summarization, advancing the field of AI-driven code understanding."|&gt;"research focus, model development</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Learning Code Summarization">
  <data key="d0">Learning Code Summarization</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">Their research explores how to effectively learn code summaries from limited datasets, contributing to model training strategies."|&gt;"research question, data efficiency&lt;SEP&gt;Their work explores how to effectively learn code summaries from limited datasets, contributing to model training strategies."|&gt;"research question, data efficiency</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Semantic Similarity Metrics">
  <data key="d0">Semantic Similarity Metrics</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">Metrics used to evaluate the semantic quality of source code summaries based on their meaning and similarity.&lt;SEP&gt;They created metrics to evaluate the quality of source code summaries, aiding in assessment of models."|&gt;"evaluation methodology, metrics&lt;SEP&gt;They created metrics to evaluate the quality of source code summaries, aiding in model assessment."|&gt;"evaluation methodology, metrics</data>
  <data key="d1">Tools</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Transformer-Based Approach">
  <data key="d0">Transformer-Based Approach</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">Developed a transformer-based method for source code summarization, leveraging advanced neural architectures."|&gt;"model architecture, technical innovation&lt;SEP&gt;Developed a transformer-based method for source code summarization, leveraging neural architectures."|&gt;"model architecture, technical innovation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Learning from Developer Mistakes">
  <data key="d0">Learning from Developer Mistakes</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">Their research involves localizing and repairing real bugs from actual developer fixes, contributing to debugging techniques."|&gt;"application, bug repair&lt;SEP&gt;Their research involves localizing and repairing real bugs from actual developer fixes, informing debugging techniques."|&gt;"application, bug repair</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Reducing False Positives in Bug Detectors">
  <data key="d0">Reducing False Positives in Bug Detectors</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">They aim to improve analytic bug detectors by reducing false positives, enhancing bug detection accuracy."|&gt;"application, bug detection&lt;SEP&gt;They aim to improve bug detection accuracy by reducing false positives in analytic bug detectors."|&gt;"application, bug detection</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Large Language Models of Code">
  <data key="d0">Large Language Models of Code</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">They systematically evaluate how large language models perform on coding tasks, providing benchmarks and insights."|&gt;"evaluation, benchmarking&lt;SEP&gt;They systematically evaluate large language models' performance on coding tasks, providing benchmarks."|&gt;"evaluation, benchmarking</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Adverse Effects of Code Duplication">
  <data key="d0">Adverse Effects of Code Duplication</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">He discussed how code duplication negatively impacts machine learning models trained on code, highlighting data quality issues."|&gt;"data quality, model performance&lt;SEP&gt;He discusses how code duplication negatively impacts machine learning models trained on code, affecting data quality and model performance."|&gt;"data quality, model performance</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Language Models Are Unsupervised Multitask Learners">
  <data key="d0">Language Models Are Unsupervised Multitask Learners</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">These authors introduced large-scale language models capable of multiple tasks without supervised training."|&gt;"model capability, multitask learning&lt;SEP&gt;They introduced large-scale language models capable of multiple tasks, foundational for AI language understanding."|&gt;"model capability, multitask learning</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="High-Order Finite Element Methods">
  <data key="d0">High-Order Finite Element Methods</data>
  <data key="d3">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d2">Their work advances numerical methods for Lagrangian hydrodynamics, impacting computational physics."|&gt;"methodology, computational modeling&lt;SEP&gt;Their work advances numerical techniques for Lagrangian hydrodynamics in scientific computing."|&gt;"methodology, computational physics</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="BERT: Pre-training of deep bidirectional transformers">
  <data key="d0">BERT: Pre-training of deep bidirectional transformers</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d2">Introduces BERT, a pre-trained transformer model for natural language understanding.&lt;SEP&gt;Introduces BERT, a transformer-based pretraining model that significantly advances NLP tasks through deep bidirectional context learning."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Roberta: A robustly optimized BERT">
  <data key="d0">Roberta: A robustly optimized BERT</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d2">Presents RoBERTa, an improved pretraining approach that enhances BERT's performance by optimizing training strategies and data usage."|&lt;SEP&gt;Presents RoBERTa, an optimized version of BERT for better NLP task performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Piloting copilot and codex">
  <data key="d0">Piloting copilot and codex</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d2">Analyzes how prompt temperature and prompt strategies influence the behavior, quality, and reliability of AI code generation models like Copilot and Codex."|&lt;SEP&gt;Analyzes how prompt temperature and strategies affect AI code generation models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Grounded copilot">
  <data key="d0">Grounded copilot</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d2">Studies how programmers interact with AI code-generation tools, focusing on usability, effectiveness, and user behavior."|&lt;SEP&gt;Studies programmer interactions with AI code-generation tools, focusing on usability and effectiveness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Programming with artificial intelligence">
  <data key="d0">Programming with artificial intelligence</data>
  <data key="d3">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d2">Explores programmers' experiences and perceptions when working with AI-assisted programming.&lt;SEP&gt;Explores programmers' subjective experiences, perceptions, and challenges when working with AI-assisted programming tools."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="S. Garg et al.">
  <data key="d0">S. Garg et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors introduced Deepdev-perf as a methodology for enhancing software performance.&lt;SEP&gt;Authors introduced Deepdev-perf as a methodology for improving software performance, impacting software efficiency and effectiveness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Lewis et al.">
  <data key="d0">M. Lewis et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors developed BART as a pre-training model to improve NLP tasks like generation, translation, and comprehension.&lt;SEP&gt;Authors developed BART, a pre-training NLP model designed to enhance language generation, translation, and comprehension.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="R. Z. Moghaddam et al.">
  <data key="d0">R. Z. Moghaddam et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors developed Deepdev-perf, a deep learning approach aimed at optimizing software performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Y. Liu et al.">
  <data key="d0">Y. Liu et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the development and evaluation of BART as a denoising sequence-to-sequence model.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="N. Goyal et al.">
  <data key="d0">N. Goyal et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the development of BART, focusing on its capabilities in NLP tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="M. Ghazvininejad et al.">
  <data key="d0">M. Ghazvininejad et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the design and evaluation of BART for language tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="A. Mohamed et al.">
  <data key="d0">A. Mohamed et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the research on BART's performance in NLP tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="O. Levy et al.">
  <data key="d0">O. Levy et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the development and testing of BART for NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="V. Stoyanov et al.">
  <data key="d0">V. Stoyanov et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the development and evaluation of BART for language understanding.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="L. Zettlemoyer et al.">
  <data key="d0">L. Zettlemoyer et al.</data>
  <data key="d3">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d2">Authors contributed to the theoretical foundation and application of BART in NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</node>
<node id="Domain Specialization">
  <data key="d0">Domain Specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A set of principles and techniques aimed at tailoring large language models (LLMs) to perform effectively within specific application domains, addressing challenges like limited domain expertise and model complexity.&lt;SEP&gt;Applying adapters to LLMs can improve their performance and stability in specific domains, but challenges remain regarding stability, resource requirements, and fine-tuning impacts.&lt;SEP&gt;Domain specialization refers to tailoring large language models (LLMs) to perform effectively within specific application domains by employing specialized techniques, addressing unique challenges, and enhancing performance for sector-specific tasks.&lt;SEP&gt;Domain specialization refers to techniques aimed at tailoring large language models to specific application domains to overcome heterogeneity, knowledge complexity, and constraints, making them more effective and disruptive in targeted fields.&lt;SEP&gt;Focusing on tailoring models to specific domains to enhance their disruptive potential in NLP applications.&lt;SEP&gt;Techniques and strategies aimed at tailoring large language models (LLMs) to specific application domains to address data heterogeneity, complex domain knowledge, unique objectives, and various constraints, thereby enhancing their effectiveness and disruptive potential.&lt;SEP&gt;The focus on tailoring models to specific domains to enhance their disruptive potential, as discussed in a comprehensive survey.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Specification Techniques">
  <data key="d0">Domain Specification Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">These are systematic methods and approaches used to adapt and tailor LLMs for specific domains, including techniques that modify, fine-tune, or guide models based on domain data, knowledge, objectives, and constraints.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Heterogeneity of Domain Data">
  <data key="d0">Heterogeneity of Domain Data</data>
  <data key="d1">Variables</data>
  <data key="d2">The diversity and variability in data types, formats, and sources within specific domains that challenge the application of general-purpose LLMs.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Knowledge">
  <data key="d0">Domain Knowledge</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Comprehensive understanding of a specific field, including concepts, principles, facts, and patterns, which can be explicit or implicit, used to enhance model performance.&lt;SEP&gt;Specialized, complex information, norms, and conventions relevant to particular fields that models need to incorporate for effective domain adaptation.&lt;SEP&gt;The collection of relevant facts, principles, and information that define a particular domain, essential for understanding and modeling within that field.&lt;SEP&gt;The specialized, often complex, body of information, norms, and conventions relevant to a particular domain that models need to understand and incorporate.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Objectives">
  <data key="d0">Domain Objectives</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The specific goals or tasks within a domain that models are expected to achieve, which guide the design and adaptation of LLMs.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Constraints in Domain Applications">
  <data key="d0">Constraints in Domain Applications</data>
  <data key="d1">Variables</data>
  <data key="d2">Social norms, cultural norms, religious beliefs, ethical standards, and other constraints that influence how models should operate within a domain.&lt;SEP&gt;Social norms, cultural values, religious beliefs, ethical standards, and operational constraints that influence how models should operate within a domain.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Systematic Taxonomy of Domain Specialization Techniques">
  <data key="d0">Systematic Taxonomy of Domain Specialization Techniques</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">A structured classification system that categorizes various methods for domain-specific adaptation of LLMs based on their accessibility and other attributes.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Application Domains">
  <data key="d0">Application Domains</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Application domains are specific fields or sectors where domain-specialized LLMs can be applied to enhance performance and utility.&lt;SEP&gt;Application domains are specific fields where domain-specialized LLMs are deployed, such as healthcare, finance, or legal sectors.&lt;SEP&gt;Fields such as healthcare, social sciences, formal sciences, where domain-specific LLMs are applied to perform specialized tasks.&lt;SEP&gt;Specific fields or sectors such as healthcare, finance, legal, etc., that can benefit from specialized LLMs for improved performance and relevance.&lt;SEP&gt;Specific sectors such as healthcare, legal, finance, and other industry-specific fields where tailored LLMs are employed to improve task performance and accuracy.&lt;SEP&gt;Various fields such as natural sciences, social sciences, and formal sciences where specialized LLMs can be applied to perform domain-specific tasks.&lt;SEP&gt;Various sectors such as medical, legal, and financial fields where LLMs are tailored for improved task performance and domain-specific accuracy.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Open Challenges">
  <data key="d0">Open Challenges</data>
  <data key="d1">Limitations</data>
  <data key="d2">Main challenges include stability and universality of adapters across architectures, resource demands due to enlarged model size, and training memory requirements, especially for large language models.&lt;SEP&gt;Open Challenges refer to ongoing issues and limitations associated with prompt tuning methods for large language models, including interpretability, access restrictions, and domain adaptation challenges.&lt;SEP&gt;Open challenges include keeping LLMs updated with the latest knowledge, addressing domain-specific constraints, and ensuring ethical and culturally appropriate responses.&lt;SEP&gt;Open challenges include the lack of systematic evaluation standards, cross-domain adaptability issues, and obstacles for non-AI experts in applying domain-specific techniques.&lt;SEP&gt;Significant difficulties in maintaining up-to-date, domain-specific, and ethically compliant LLMs, including knowledge updating, domain constraints, and social norms.&lt;SEP&gt;Unresolved issues and difficulties in effectively applying domain specialization techniques, including data heterogeneity, knowledge integration, and ethical considerations.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5&lt;SEP&gt;chunk-bc782afdd6c18f4e206e464038047556&lt;SEP&gt;chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Future Trends">
  <data key="d0">Future Trends</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Emerging directions and innovations in domain-specific LLM research, including new methodologies and applications.&lt;SEP&gt;Emerging directions and potential developments in domain-specific LLM research, including new methodologies, applications, and challenges.&lt;SEP&gt;Future research aims to address bottlenecks, improve domain adaptation methods, and develop standardized evaluation frameworks for LLMs.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Data Heterogeneity">
  <data key="d0">Domain Data Heterogeneity</data>
  <data key="d1">Variables</data>
  <data key="d2">The diversity in data types, formats, sources, and quality within specific domains that pose challenges for applying general-purpose LLMs effectively.</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Specialization of LLMs">
  <data key="d0">Domain Specialization of LLMs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Domain specialization involves adapting large language models to perform effectively within specific fields or domains.&lt;SEP&gt;Domain specialization involves customizing general-purpose LLMs with domain-specific data, knowledge, objectives, and constraints to improve performance in specific fields.&lt;SEP&gt;Domain specialization involves tailoring large language models to perform optimally within specific fields or domains.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Law">
  <data key="d0">Scaling Law</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The scaling law describes how increasing model size and training data can lead to continuous improvements in model capacity and performance.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pre-trained Language Models (PLMs)">
  <data key="d0">Pre-trained Language Models (PLMs)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">PLMs are models trained on large corpora to learn universal language representations, which can be fine-tuned for various downstream NLP tasks.&lt;SEP&gt;PLMs are neural network models trained on large text corpora to learn linguistic patterns, structures, and semantics, forming the basis for LLMs.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Cut-off">
  <data key="d0">Knowledge Cut-off</data>
  <data key="d1">Variables</data>
  <data key="d2">Knowledge cut-off refers to the fixed point in time up to which the LLM's training data is current, limiting its awareness of recent events and discoveries.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Update Mechanisms">
  <data key="d0">Knowledge Update Mechanisms</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques and approaches used to keep LLMs current with the latest information, such as continuous learning or retraining processes.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Disciplinary Domains">
  <data key="d0">Disciplinary Domains</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Fields such as social sciences, natural sciences, formal sciences, and biomedical sciences where LLMs are applied for specialized tasks.&lt;SEP&gt;Various fields such as medicine, law, social sciences, and others where LLMs are applied for domain-specific tasks.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-Specific Knowledge">
  <data key="d0">Domain-Specific Knowledge</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Domain-specific knowledge refers to specialized information relevant to particular fields, which can be under-represented in general-purpose LLMs.&lt;SEP&gt;Knowledge tailored to particular fields like biomedicine, finance, or law, distilled from large models to improve relevance and performance in those domains.&lt;SEP&gt;Knowledge tailored to specific fields like biomedicine, finance, or law, distilled from large models to improve relevance and performance in those areas.&lt;SEP&gt;Specialized information, data, and expertise relevant to particular fields that enhance LLM performance within those domains.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-130e4012ca037ccaf70ac4ffd0928777&lt;SEP&gt;chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Customization Techniques">
  <data key="d0">Customization Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Methods used to adapt general LLMs for specific domains, including fine-tuning, prompt engineering, and incorporation of domain knowledge.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Social Norms and Cultural Constraints">
  <data key="d0">Social Norms and Cultural Constraints</data>
  <data key="d1">Variables</data>
  <data key="d2">Parameters reflecting societal, cultural, religious, and legal considerations that influence LLM responses in different contexts.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ethical and Legal Regulations">
  <data key="d0">Ethical and Legal Regulations</data>
  <data key="d1">Variables</data>
  <data key="d2">Rules and standards that govern the development and deployment of LLMs to ensure compliance and social acceptability.</data>
  <data key="d3">chunk-130e4012ca037ccaf70ac4ffd0928777</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Discoveries">
  <data key="d0">Discoveries</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Discoveries refer to new knowledge or findings in specialized domains that continuously emerge, impacting practices, regulations, and knowledge bases.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Regulations">
  <data key="d0">Regulations</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Regulations are rules or directives that govern practices within various domains, often evolving with new discoveries and best practices.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Best Practices">
  <data key="d0">Best Practices</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Best practices are established methods or techniques that are recognized as effective within specific fields, frequently updated as new discoveries and regulations develop.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="News Articles">
  <data key="d0">News Articles</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">News articles are written reports covering current events, including mainstream news that contribute to information dissemination and knowledge accumulation.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Social Media Analysis">
  <data key="d0">Social Media Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Social media analysis involves examining content from social media platforms to extract insights, trends, and factual information, often used in fact-checking and opinion monitoring.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fact-Checking">
  <data key="d0">Fact-Checking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Fact-checking is the process of verifying information accuracy, crucial for maintaining knowledge integrity, especially in dynamic fields.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Re-Training">
  <data key="d0">Re-Training</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Re-training involves updating an LLM with new data to improve relevance and accuracy, requiring resource-intensive processes.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Continuous Learning Mechanisms">
  <data key="d0">Continuous Learning Mechanisms</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Continuous learning mechanisms allow models to adapt over time by incorporating new data, essential for staying current in evolving domains.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Re-Training">
  <data key="d0">Model Re-Training</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Model re-training involves updating an existing language model with new data to improve its relevance, accuracy, and knowledge currency.&lt;SEP&gt;Model re-training is the process of updating a language model with new or domain-specific data to improve performance and relevance.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Extraction">
  <data key="d0">Knowledge Extraction</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Knowledge extraction involves deriving structured information from unstructured data sources, such as training corpora, to inform model updates.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Specialized Terminology">
  <data key="d0">Specialized Terminology</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specialized terminology includes domain-specific vocabulary that models need to understand for accurate task performance.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Complex Concepts">
  <data key="d0">Complex Concepts</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Complex concepts are intricate ideas unique to specific domains, often challenging for models to learn and represent accurately.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hallucination">
  <data key="d0">Hallucination</data>
  <data key="d1">Results</data>
  <data key="d2">Hallucination describes instances where LLMs generate plausible-sounding but incorrect or unsupported information, often due to lack of structured knowledge.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task-Specific Demonstrations">
  <data key="d0">Task-Specific Demonstrations</data>
  <data key="d1">Tools</data>
  <data key="d2">Task-specific demonstrations are examples provided to guide LLMs in producing relevant and accurate responses for particular tasks.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Vague User Instructions">
  <data key="d0">Vague User Instructions</data>
  <data key="d1">Limitations</data>
  <data key="d2">Vague instructions hinder effective guidance of LLMs, leading to less relevant or inaccurate outputs.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Finite Context Window">
  <data key="d0">Finite Context Window</data>
  <data key="d1">Variables</data>
  <data key="d2">The finite context window limits the amount of input tokens an LLM can process at once, impacting its ability to handle long or complex inputs.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Downstream Task Learning">
  <data key="d0">Downstream Task Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Downstream task learning involves adapting pre-trained LLMs to specific tasks using additional data and fine-tuning techniques.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="High-Performance GPUs/TPUs">
  <data key="d0">High-Performance GPUs/TPUs</data>
  <data key="d1">Tools</data>
  <data key="d2">High-performance GPUs and TPUs are specialized hardware required to train and fine-tune large-scale LLMs due to their extensive computational demands.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Specialization Techniques">
  <data key="d0">Domain Specialization Techniques</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Conceptual approaches aimed at adapting and optimizing LLMs for particular application fields, improving their effectiveness and relevance.&lt;SEP&gt;Domain specialization techniques are methods designed to adapt LLMs for effective performance in specific application domains, often categorized by accessibility levels.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Black-Box, Grey-Box, White-Box">
  <data key="d0">Black-Box, Grey-Box, White-Box</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">These categories represent different levels of accessibility to LLMs for domain adaptation, influencing the choice of techniques.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Specialized Domains">
  <data key="d0">Specialized Domains</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specialized domains are specific fields or areas of knowledge that require tailored approaches and understanding, often involving complex concepts and terminology.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Bases">
  <data key="d0">Knowledge Bases</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Knowledge bases are structured repositories of information used to support learning, reasoning, and information retrieval within various domains.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Extraction Methods">
  <data key="d0">Knowledge Extraction Methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Knowledge extraction methods are techniques used to derive structured information from unstructured data sources, crucial for updating and maintaining knowledge bases.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Training Corpus">
  <data key="d0">Training Corpus</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A training corpus is a large collection of text data used to train language models, containing domain-specific and general knowledge.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Continuous Learning">
  <data key="d0">Continuous Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Continuous learning mechanisms enable models to adapt over time by incorporating new information, maintaining relevance in dynamic fields.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Relevance">
  <data key="d0">Knowledge Relevance</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Knowledge relevance refers to the importance and applicability of information for specific tasks or domains, influencing model updates and training focus.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Resource-Intensive Processes">
  <data key="d0">Resource-Intensive Processes</data>
  <data key="d1">Limitations</data>
  <data key="d2">Re-training and continuous learning require significant computational resources, including high-performance hardware and extensive data processing.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Complexity">
  <data key="d0">Model Complexity</data>
  <data key="d1">Variables</data>
  <data key="d2">Model complexity pertains to the architecture and number of parameters in LLMs, affecting training difficulty, resource needs, and susceptibility to issues like catastrophic forgetting.&lt;SEP&gt;The degree of intricacy in a model's structure and parameters, influencing its interpretability and performance.&lt;SEP&gt;The intricacy and resource demands of large language models, influencing their deployment feasibility and interpretability.&lt;SEP&gt;The intricacy and sophistication of large language models, which impact their interpretability, training, and deployment in specialized domains.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hyperparameter Tuning">
  <data key="d0">Hyperparameter Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Hyperparameter tuning involves adjusting settings like learning rate and batch size to optimize training and fine-tuning processes.&lt;SEP&gt;The process of adjusting hyperparameters in the program to improve performance, often using tools like Optuna.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Cross-Domain Adaptation">
  <data key="d0">Cross-Domain Adaptation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Cross-domain adaptation involves techniques to transfer knowledge from one domain to another, addressing challenges in applying models across diverse fields.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation Standards">
  <data key="d0">Evaluation Standards</data>
  <data key="d1">Limitations</data>
  <data key="d2">Lack of systematic evaluation standards hampers consistent assessment of domain adaptation techniques for LLMs.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Standardization">
  <data key="d0">Standardization</data>
  <data key="d1">Limitations</data>
  <data key="d2">Absence of standardized methods for evaluating domain specialization techniques creates obstacles for benchmarking and comparison.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Taxonomy of Techniques">
  <data key="d0">Taxonomy of Techniques</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">Taxonomies classify and organize various domain specialization techniques based on accessibility levels and methodological approaches.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Gaps">
  <data key="d0">Knowledge Gaps</data>
  <data key="d1">Limitations</data>
  <data key="d2">Existing research faces gaps in cross-referencing techniques across domains and in establishing comprehensive evaluation frameworks.&lt;SEP&gt;Insufficient or incomplete domain knowledge that hinders effective training and application of specialized LLMs.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Future Research Directions">
  <data key="d0">Future Research Directions</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future research aims to address current limitations, develop standardized evaluation methods, and improve cross-domain adaptation strategies.</data>
  <data key="d3">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Transformer Architecture">
  <data key="d0">Transformer Architecture</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The foundational neural network architecture used in most code LLMs, enabling modeling of sequential data like code and natural language.&lt;SEP&gt;The neural network framework underlying most modern code LLMs, enabling sequence modeling and contextual understanding.&lt;SEP&gt;Transformer architecture is a neural network design that enables efficient processing of sequential data, underpinning most modern LLMs and PLMs.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Scaling">
  <data key="d0">Model Scaling</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Model scaling involves increasing the size (parameters) of models like 1.3b, 6.7b, and 16b to improve their capacity and performance, often utilizing architectures like MOE.&lt;SEP&gt;Model scaling involves increasing the size (parameters) of models like 1.3b, 6.7b, and 16b to improve their capacity and performance, often with architectural modifications like MOE.&lt;SEP&gt;Scaling pre-trained language models by increasing model size or data size to enhance capacity and performance on downstream tasks.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Adaptation Techniques">
  <data key="d0">Domain Adaptation Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Methods used to adapt general-purpose language models to specific domains, such as adding layers or updating parameters, though challenging for LLMs due to architecture inaccessibility.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Updating">
  <data key="d0">Knowledge Updating</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies for updating or infusing new knowledge into LLMs, which are computationally costly and complex due to the models' size and architecture.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Taxonomy of Domain Specialization">
  <data key="d0">Taxonomy of Domain Specialization</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A systematic classification of existing techniques aimed at customizing LLMs for various domains to facilitate sector-specific applications.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Challenges in Domain Specialization">
  <data key="d0">Challenges in Domain Specialization</data>
  <data key="d1">Limitations</data>
  <data key="d2">Open problems include architecture inaccessibility, high computational costs, knowledge updating difficulties, and risks of inaccuracies in specialized applications.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Future Trends in LLM Research">
  <data key="d0">Future Trends in LLM Research</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Future research directions involve developing more efficient domain adaptation techniques, improving model scalability, enhancing knowledge updating strategies, and addressing current bottlenecks to make LLMs more effective and accessible across sectors.&lt;SEP&gt;Predicted directions include developing more efficient adaptation techniques, better understanding of domain-specific fine-tuning, and addressing open challenges in model scalability and knowledge management.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research in NLP and AI">
  <data key="d0">Research in NLP and AI</data>
  <data key="d1">Disciplines</data>
  <data key="d2">The scientific disciplines underpinning the development, evaluation, and application of large language models (LLMs) and domain specialization techniques, including computational linguistics, machine learning, and artificial intelligence.&lt;SEP&gt;The scientific fields underpinning the development, evaluation, and application of LLMs and domain specialization techniques.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Open Challenges and Bottlenecks">
  <data key="d0">Open Challenges and Bottlenecks</data>
  <data key="d1">Results</data>
  <data key="d2">Current limitations in domain specialization include inaccessibility of model architecture, high computational costs, difficulty in knowledge updating, and risks of inaccuracies when applying LLMs to specific domains.&lt;SEP&gt;Current limitations include inaccessibility of model architecture, high computational costs, difficulty in knowledge updating, and risks of inaccuracies in domain-specific deployments.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Systematic Survey and Investigation">
  <data key="d0">Systematic Survey and Investigation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Methodical reviews and investigations of existing domain specialization techniques and applications to understand current status, trends, and challenges.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Technical Taxonomy of Domain Specialization">
  <data key="d0">Technical Taxonomy of Domain Specialization</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">A comprehensive classification framework that categorizes the various techniques used to adapt and specialize LLMs for different fields.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Open Problems">
  <data key="d0">Open Problems</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Unresolved issues such as architecture inaccessibility, high computational costs, knowledge update limitations, and risks of inaccuracies that hinder effective domain specialization.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Increased Model Capacity">
  <data key="d0">Increased Model Capacity</data>
  <data key="d1">Variables</data>
  <data key="d2">Enhancing the ability of LLMs to perform well on downstream tasks by scaling model size and training data.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Performance on Downstream Tasks">
  <data key="d0">Performance on Downstream Tasks</data>
  <data key="d1">Results</data>
  <data key="d2">Improved task-specific performance achieved through domain adaptation and model scaling techniques.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Risks of Applying Generic LLMs">
  <data key="d0">Risks of Applying Generic LLMs</data>
  <data key="d1">Limitations</data>
  <data key="d2">Potential issues such as lack of originality and inaccuracies when deploying generic LLMs in specialized fields like medicine or law.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal Domain-Specific LLMs">
  <data key="d0">Legal Domain-Specific LLMs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Language models tailored for legal applications to improve legal document analysis, contract review, and legal research.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Financial Sector-Specific LLMs">
  <data key="d0">Financial Sector-Specific LLMs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Language models customized for financial tasks, showing improved performance without compromising general benchmarks.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Infusion Techniques">
  <data key="d0">Knowledge Infusion Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Methods designed to incorporate new or domain-specific knowledge into existing LLMs, despite challenges posed by model size and architecture.</data>
  <data key="d3">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ms">
  <data key="d0">Ms</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Ms (Massively Multilingual models) are large-scale language models used for sequence-to-sequence tasks like machine translation and summarization, exemplified by T5.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="T5">
  <data key="d0">T5</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">T5 is a notable example of a sequence-to-sequence language model designed for various NLP tasks.&lt;SEP&gt;T5 is a sequence-to-sequence language model designed for various NLP tasks, notable for its versatility and performance.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Autoregressive Language Models">
  <data key="d0">Autoregressive Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Autoregressive language models generate text by predicting the next token in a sequence based on preceding tokens, suitable for text generation tasks.&lt;SEP&gt;Autoregressive language models generate text by predicting the next token in a sequence based on preceding tokens, suitable for text-generation tasks.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="External Augmentation">
  <data key="d0">External Augmentation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">External augmentation uses external resources or tools to incorporate domain knowledge into LLMs without changing internal model parameters, often via retrieval or knowledge bases.&lt;SEP&gt;External augmentation uses external resources or tools to incorporate domain-specific knowledge into LLMs without modifying internal model parameters, often via retrieval or external knowledge integration.&lt;SEP&gt;Using external sources or datasets to enhance LLM capabilities for domain-specific applications.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc&lt;SEP&gt;chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Crafting">
  <data key="d0">Prompt Crafting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Prompt crafting involves designing input prompts to elicit domain knowledge from LLMs, especially under limited model access, to improve performance.&lt;SEP&gt;Prompt crafting involves designing input prompts to guide LLMs to produce domain-specific or improved outputs, especially under limited model access.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Black Box">
  <data key="d0">Black Box</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Black box refers to a scenario where only the model's outputs are accessible via API, with no internal details available, used for external augmentation approaches.&lt;SEP&gt;Black box refers to models where only the output is accessible via API, with no internal details available, used in external augmentation approaches.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Grey Box">
  <data key="d0">Grey Box</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Grey box indicates limited access to model internals, such as gradient or loss values, enabling more refined prompt crafting.&lt;SEP&gt;Grey box indicates limited internal access, such as gradient or loss values, enabling more refined prompt engineering.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="White Box">
  <data key="d0">White Box</data>
  <data key="d1">Study Designs</data>
  <data key="d2">White box denotes full access to model architecture and parameters, necessary for internal modifications like fine-tuning.&lt;SEP&gt;White box denotes full access to the model's architecture and parameters, necessary for direct fine-tuning or internal modifications.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Training Strategies">
  <data key="d0">Training Strategies</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Training strategies categorize approaches like pre-training, fine-tuning, or mixed training to adapt LLMs to specific domains.&lt;SEP&gt;Training strategies categorize methods like pre-training, fine-tuning, or mixed training to adapt models to specific domains.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pre-training Intervention">
  <data key="d0">Pre-training Intervention</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Pre-training intervention involves modifying the initial training process to embed domain-specific knowledge into the model.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuning Intervention">
  <data key="d0">Fine-tuning Intervention</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Fine-tuning intervention updates the model’s parameters during the fine-tuning stage with domain-specific data to improve performance.&lt;SEP&gt;Fine-tuning involves further training the model with domain-specific data during the adaptation phase.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Inference-time Intervention">
  <data key="d0">Inference-time Intervention</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Inference-time intervention modifies model behavior during deployment to produce domain-specific outputs.&lt;SEP&gt;Inference-time intervention modifies the model's behavior during actual deployment to generate more domain-specific outputs.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation and Feedback Mechanism">
  <data key="d0">Evaluation and Feedback Mechanism</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Mechanisms like fixed evaluation sets, dynamic evaluation, or user feedback assess and guide the domain adaptation process.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sequence-to-Sequence Tasks">
  <data key="d0">Sequence-to-Sequence Tasks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Sequence-to-sequence tasks involve transforming one sequence of data into another, such as translation or summarization, often performed by models like T5.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Machine Translation">
  <data key="d0">Machine Translation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Machine translation is the process of automatically translating text from one language to another, a key application of sequence-to-sequence models.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Summarization">
  <data key="d0">Summarization</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Summarization involves condensing lengthy text into concise summaries, a common task for models like T5.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation and Feedback Mechanisms">
  <data key="d0">Evaluation and Feedback Mechanisms</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Mechanisms such as fixed evaluation sets, dynamic evaluation, or user feedback are used to assess and guide model adaptation to domains.</data>
  <data key="d3">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="parameters to incorporate">
  <data key="d0">parameters to incorporate</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parameters to incorporate refer to the process of integrating domain-specific knowledge directly into language models to enhance their performance in specialized fields.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Approaches in different categories">
  <data key="d0">Approaches in different categories</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Relations between various approaches (black box, grey box, white box) for domain-specific modeling, highlighting their differences and interactions.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Levels of specialization">
  <data key="d0">Levels of specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Different levels such as black box, grey box, and white box, describing the scope and depth of model customization and their respective methods of knowledge integration.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="External knowledge augmentation">
  <data key="d0">External knowledge augmentation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Using external domain-specific information sources to enhance language model performance without modifying internal parameters, through retrieval and integration.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neural adapters">
  <data key="d0">Neural adapters</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adding auxiliary modules or components to a language model to incorporate domain knowledge during training or inference.&lt;SEP&gt;Adding auxiliary modules to a model to incorporate domain knowledge, enabling performance improvements while maintaining base model integrity.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Common Framework">
  <data key="d0">Common Framework</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A structured process consisting of four stages—Definition, Augmentation, Optimization, and Evaluation—for domain specialization of LLMs.&lt;SEP&gt;A structured, four-stage process (Definition, Augmentation, Optimization, Evaluation) for domain-specific adaptation of language models.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Definition">
  <data key="d0">Definition</data>
  <data key="d1">Study Design</data>
  <data key="d2">The initial phase where the domain, objectives, and constraints are explicitly defined to guide subsequent customization and training efforts.&lt;SEP&gt;The initial step where the specific domain, objectives, and constraints are clearly defined to guide subsequent customization efforts.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Augmentation">
  <data key="d0">Augmentation</data>
  <data key="d1">Study Design</data>
  <data key="d2">Stage involving the incorporation of domain-specific knowledge into the model or its inputs/outputs, such as fine-tuning or prompt crafting.&lt;SEP&gt;The stage involving the incorporation of domain-specific knowledge into models or their inputs/outputs, such as via fine-tuning, prompt crafting, or external tools.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Optimization">
  <data key="d0">Optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Optimization involves performing global transformations on the APT, such as reordering and explicit hardware mapping, to enhance performance before code generation.&lt;SEP&gt;Optimization involves tuning code to better utilize hardware resources, such as cores and GPUs, and rewriting kernels to improve parallelism.&lt;SEP&gt;Optimization refers to the process of improving code performance, stability, and parallelism, including global optimization steps to eliminate local copies and enhance efficiency.&lt;SEP&gt;Techniques applied to improve code performance, including tuning core counts, rewriting kernels, and reducing synchronization overhead.&lt;SEP&gt;The process of refining and improving model performance through methods like gradient descent, prompt tuning, or output filtering after augmentation.&lt;SEP&gt;The process of refining model performance through methods like gradient descent, prompt engineering, or output filtering after augmentation.&lt;SEP&gt;Techniques employed by DSP Y's compiler to enhance pipeline performance according to specific metrics.&lt;SEP&gt;The process of systematically adjusting program parameters to maximize or minimize an objective function.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b&lt;SEP&gt;chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1&lt;SEP&gt;chunk-5ff9371dcd9f0dcbeb25583bbba6123c&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="External Knowledge">
  <data key="d0">External Knowledge</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External knowledge encompasses domain-specific information sourced externally to enhance the depth, accuracy, and contextual relevance of language models.&lt;SEP&gt;Information stored outside the model that can be retrieved and used to improve factual accuracy and relevance of generated responses.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Knowledge Augmentation">
  <data key="d0">Domain Knowledge Augmentation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A method of enriching language models with explicit or implicit external domain-specific information to improve performance.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Explicit Knowledge">
  <data key="d0">Explicit Knowledge</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Structured, clearly defined domain knowledge that can be directly retrieved and used to inform model outputs.&lt;SEP&gt;Structured, clearly defined domain knowledge that can be directly retrieved and utilized by the model.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Implicit Knowledge">
  <data key="d0">Implicit Knowledge</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Latent, embedded domain knowledge within data or systems that influences model behavior without direct expression.&lt;SEP&gt;Latent, embedded domain knowledge within data or systems that is not directly expressed but influences model behavior.&lt;SEP&gt;Latent, non-obvious information embedded within data or models, often represented as vectorized embeddings learned during pre-training.&lt;SEP&gt;Latent, non-obvious information embedded within models, often represented as vectorized embeddings learned during pre-training, which encode intricate data patterns.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd&lt;SEP&gt;chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Retrieval augmentation">
  <data key="d0">Retrieval augmentation</data>
  <data key="d1">Tools</data>
  <data key="d2">Technique involving fetching relevant external information or domain knowledge to supplement and improve model responses without changing internal model parameters.&lt;SEP&gt;Technique involving fetching relevant external information to supplement model responses without altering internal model parameters.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameters to incorporate">
  <data key="d0">Parameters to incorporate</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parameters to incorporate refer to the process of integrating domain-specific knowledge directly into language models to enhance their performance in specialized fields.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Relations between approaches in different categories">
  <data key="d0">Relations between approaches in different categories</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Relations describing how different modeling approaches (black box, grey box, white box) interrelate and differ in their methods and applications.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Different levels of specialization">
  <data key="d0">Different levels of specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Levels such as black box, grey box, and white box, representing varying degrees and methods of domain-specific adaptation in language models.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Augmenting with external knowledge">
  <data key="d0">Augmenting with external knowledge</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Using external domain-specific information sources to enhance language model capabilities without modifying internal parameters, via retrieval or integration techniques.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge graphs">
  <data key="d0">Knowledge graphs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Structured representations of domain knowledge, such as relationships and concepts, used to enhance external knowledge sources.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge base">
  <data key="d0">Knowledge base</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A repository of domain-specific information, facts, and concepts used for retrieval augmentation.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge source">
  <data key="d0">Knowledge source</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Any external repository or resource (documents, databases, graphs) providing domain-specific information for augmentation.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation benchmarks">
  <data key="d0">Evaluation benchmarks</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Standardized datasets and metrics used to assess the performance of domain-adapted language models.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Overfitting">
  <data key="d0">Overfitting</data>
  <data key="d1">Limitations</data>
  <data key="d2">A challenge where models become too tailored to training data, reducing their ability to generalize to new data, especially relevant in fine-tuning.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Generalization">
  <data key="d0">Generalization</data>
  <data key="d1">Limitations</data>
  <data key="d2">The ability of a model to perform well on unseen data, which can be compromised by overfitting during domain adaptation.</data>
  <data key="d3">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Explicit Knowledge with LLM">
  <data key="d0">Explicit Knowledge with LLM</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A method that involves retrieving domain-specific information from external sources to enhance language model performance on specialized tasks.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Retrieval">
  <data key="d0">Retrieval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Retrieval is the process of fetching relevant evidence from Wikipedia to support claim classification in FEVER.&lt;SEP&gt;Retrieval is the process of identifying and fetching relevant evidence documents or information from a large corpus to support tasks like question answering and knowledge inference.&lt;SEP&gt;Retrieval refers to the process of identifying and fetching relevant information or evidence documents from a large corpus to support tasks like question answering and knowledge inference.&lt;SEP&gt;The process of accessing external information sources to obtain task-relevant data for language models.&lt;SEP&gt;The process of accessing external knowledge sources, such as large corpora or knowledge bases, to supply relevant information to language models.&lt;SEP&gt;The process of searching for relevant information within external knowledge sources based on similarity metrics.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neural Retriever">
  <data key="d0">Neural Retriever</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural network-based system that vectorizes queries and information to find relevant external data based on similarity metrics like cosine similarity.&lt;SEP&gt;A neural network-based system that vectorizes queries and knowledge to identify relevant information based on similarity metrics like cosine similarity.&lt;SEP&gt;A pre-trained neural network that retrieves relevant documents or passages from a large corpus, such as Wikipedia, to support knowledge retrieval in RAG.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb&lt;SEP&gt;chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Base">
  <data key="d0">Knowledge Base</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Structured repositories like Wikidata or Wikipedia used as external sources for retrieval.&lt;SEP&gt;Structured repositories of domain-specific information, such as Wikidata, used to supply context to language models.&lt;SEP&gt;Structured repositories such as Wikidata or Wikipedia used as external sources of domain-specific information.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Explicit Knowledge Source">
  <data key="d0">Explicit Knowledge Source</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External data repositories containing task-relevant information that can be retrieved to inform model predictions.&lt;SEP&gt;External data sources containing task-relevant information that can be retrieved to improve model predictions.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Predictions">
  <data key="d0">Model Predictions</data>
  <data key="d1">Results</data>
  <data key="d2">Outputs generated by language models that can be refined or corrected using retrieved external knowledge.&lt;SEP&gt;Outputs generated by language models, which can be refined or corrected by incorporating external knowledge.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pretrained Language Models (PLMs)">
  <data key="d0">Pretrained Language Models (PLMs)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Language models trained on large datasets that can store implicit domain knowledge and generate responses based on learned patterns.&lt;SEP&gt;Large-scale language models trained on vast datasets, capable of storing implicit domain knowledge and generating contextually relevant responses.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Attention Mechanisms">
  <data key="d0">Attention Mechanisms</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques that enable models to focus on relevant parts of input or knowledge, facilitating retrieval of task-related information from implicit knowledge through scoring and weighting.&lt;SEP&gt;Techniques that enable models to retrieve task-related information from implicit knowledge by calculating attention scores between query vectors and knowledge entries.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Latent Embeddings">
  <data key="d0">Latent Embeddings</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Vector representations of domain knowledge learned during pre-training, capturing complex data patterns and semantic relationships.&lt;SEP&gt;Vector representations of domain knowledge learned during pre-training, capturing intricate data patterns.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Retrieval">
  <data key="d0">Knowledge Retrieval</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Methods involving scoring, similarity measurement, and weighted summation to extract relevant information from explicit or implicit sources.&lt;SEP&gt;The process of extracting relevant information from implicit or explicit sources using attention scores, softmax functions, and weighted sums.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instruction Cycle">
  <data key="d0">Instruction Cycle</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process involving retrieving, parsing, and storing information from implicit knowledge to solve complex domain-specific problems and improve model performance.&lt;SEP&gt;A process involving retrieving, parsing, and storing information within models to handle complex domain-specific problems.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="External Knowledge Integration">
  <data key="d0">External Knowledge Integration</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhances model performance and adaptability by supplementing training data with relevant external information, enabling lifelong learning.&lt;SEP&gt;Enhances model performance, supports lifelong learning, and allows models to adapt to new or changing information without extensive retraining.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Seamless Integration">
  <data key="d0">Seamless Integration</data>
  <data key="d1">Limitations</data>
  <data key="d2">The challenge of effectively incorporating external knowledge into LLMs, especially managing conflicts, incomplete data, and rejection options.&lt;SEP&gt;The challenge of effectively incorporating external knowledge into LLMs, especially when information is conflicting or incomplete.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scalability and Adaptability">
  <data key="d0">Scalability and Adaptability</data>
  <data key="d1">Limitations</data>
  <data key="d2">Difficulties in scaling systems to handle large knowledge bases and updating models efficiently as knowledge expands.&lt;SEP&gt;The difficulty in managing large knowledge bases and updating models efficiently as information expands.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Tool Augmentation">
  <data key="d0">Domain Tool Augmentation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Use of specialized software or frameworks designed for specific fields to improve task performance in domain-specific applications.&lt;SEP&gt;Utilization of specialized software, libraries, or frameworks designed for specific fields to improve domain-specific task performance.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Tools">
  <data key="d0">Domain Tools</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Software or frameworks tailored for particular domains, such as APIs for genomics or theorem provers for mathematics.&lt;SEP&gt;Software or frameworks tailored for particular domains, such as APIs for genomics, theorem provers, or social behavior simulators.&lt;SEP&gt;Software or hardware components tailored for specific domains, used by LLMs to perform domain-specific tasks such as theorem proving, solution outlining, or robotic control.&lt;SEP&gt;Specialized software or hardware components used by LLMs to perform domain-specific functions, such as theorem proving, solution outlining, or robotic control.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc&lt;SEP&gt;chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Limitations of LLMs">
  <data key="d0">Limitations of LLMs</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current constraints include result variability, format instability, and difficulty in handling highly specialized tasks.&lt;SEP&gt;Current constraints include result variability, unstable result formats depending on seeds and hyperparameters, and difficulty in handling highly specialized or variable tasks.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-Specific Tasks">
  <data key="d0">Domain-Specific Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks that require specialized knowledge or data, such as medical diagnosis, legal analysis, or scientific research, which benefit from external knowledge retrieval.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="External Context">
  <data key="d0">External Context</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External information sources, such as knowledge bases or corpora, used to supply relevant information during model inference.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Similarity Metrics">
  <data key="d0">Similarity Metrics</data>
  <data key="d1">Variables</data>
  <data key="d2">Quantitative measures such as cosine similarity used to evaluate the relevance of retrieved information in the retrieval process.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Source">
  <data key="d0">Knowledge Source</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External repositories like Wikipedia or Wikidata that provide domain-specific information for retrieval.&lt;SEP&gt;Wikipedia is a primary external knowledge source used by RAG models to ground responses in factual information.&lt;SEP&gt;Wikipedia serves as a primary external knowledge source for grounding factual responses in RAG models.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Explainability">
  <data key="d0">Explainability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The extent to which a model's decision-making process and retrieval steps can be understood and interpreted by humans.&lt;SEP&gt;The extent to which a model's decision-making process can be understood and interpreted by humans, crucial for trust and transparency.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Transparency">
  <data key="d0">Transparency</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The degree to which the retrieval and reasoning processes within models are open and interpretable.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Management">
  <data key="d0">Knowledge Management</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Handling, updating, and scaling external knowledge sources to improve model performance and relevance.</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-Specific Tools">
  <data key="d0">Domain-Specific Tools</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specialized software, algorithms, or APIs designed to perform tasks within specific domains, which can be called upon by LLMs to enhance performance and overcome their inherent limitations.&lt;SEP&gt;Tools and functionalities designed for specific domains to enhance task performance, often called upon by LLMs to overcome their limitations.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Collaborative Integration Approach">
  <data key="d0">Collaborative Integration Approach</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A strategy that combines LLMs with domain-specific tools, allowing the models to call external tools, utilize domain knowledge, and improve performance on complex, domain-specific tasks by leveraging the strengths of both.&lt;SEP&gt;A strategy that combines the strengths of LLMs and domain-specific tools, utilizing LLMs for user interaction and tool calling, to improve performance on complex domain-specific tasks.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-Stage Pipeline">
  <data key="d0">Multi-Stage Pipeline</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process involving eliciting commands from LLMs, executing them in domain tools, and post-processing outputs to solve complex tasks.&lt;SEP&gt;A process involving multiple steps: LLMs generate commands or prompts, execute domain tools, and post-process outputs to solve complex problems effectively.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Executable Commands">
  <data key="d0">Executable Commands</data>
  <data key="d1">Tools</data>
  <data key="d2">Commands generated by LLMs to invoke domain tools, such as scripts or API calls, for task execution.&lt;SEP&gt;Scripts or API calls generated by LLMs that invoke domain tools to perform specific operations, such as calculations or data retrieval.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Python Script">
  <data key="d0">Python Script</data>
  <data key="d1">Tools</data>
  <data key="d2">A programming code snippet used by LLMs to perform calculations or solve problems, like the example solving the chicken and rabbit problem.&lt;SEP&gt;A programming code snippet, often used by LLMs to perform computational tasks like solving arithmetic problems, exemplified by the chicken and rabbit problem solution.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Arithmetic Tasks">
  <data key="d0">Arithmetic Tasks</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Tasks requiring precise calculations, often challenging for LLMs due to their lack of numerical accuracy.&lt;SEP&gt;Tasks requiring precise numerical calculations, which pose challenges for LLMs due to their lack of numerical accuracy and potential for hallucination.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="APIs">
  <data key="d0">APIs</data>
  <data key="d1">Tools</data>
  <data key="d2">Application Programming Interfaces that enable access to domain-specific functionalities, such as travel booking or database querying, often invoked by LLMs.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task Planners">
  <data key="d0">Task Planners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Frameworks conceptualizing LLMs as systems that decompose complex tasks into subtasks, coordinate multiple tools, and generate executable commands, often called 'API selectors' or 'controllers'.&lt;SEP&gt;Frameworks that conceptualize LLMs as systems that decompose complex tasks into subtasks, coordinate multiple domain tools, and generate executable commands, often referred to as 'API selectors' or 'controllers'.&lt;SEP&gt;Models that coordinate multiple domain tools and decompose complex tasks into subtasks, guiding the execution flow.&lt;SEP&gt;Models that decompose complex tasks into smaller subtasks, coordinate multiple domain tools, and guide the overall execution flow to accomplish complex objectives.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74&lt;SEP&gt;chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Result">
  <data key="d0">Result</data>
  <data key="d1">Results</data>
  <data key="d2">Outputs generated after executing commands or tools, such as numerical solutions, data, or processed information, used to derive conclusions or further steps.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tool Calling">
  <data key="d0">Tool Calling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process where LLMs generate commands to invoke external domain tools, enabling hybrid workflows that combine language understanding with specialized functionalities.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Engineering">
  <data key="d0">Prompt Engineering</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Designing prompts to elicit specific responses from LLMs, such as generating executable code or commands to call domain tools.&lt;SEP&gt;The process of designing and optimizing prompts to elicit desired responses from language models, crucial for effective zero-shot and few-shot learning.&lt;SEP&gt;The process of designing and refining prompts to optimize LLM performance on specific tasks.&lt;SEP&gt;The process of designing prompts to effectively elicit desired responses from language models, crucial for zero-shot and few-shot tasks.&lt;SEP&gt;Prompt engineering refers to designing input prompts to effectively guide AI models like Codex in generating accurate, optimized HPC kernels.&lt;SEP&gt;Prompt engineering refers to the process of designing input prompts to guide AI models like Codex to generate accurate and optimized code for HPC kernels.&lt;SEP&gt;The practice of designing and structuring prompts to guide language models towards desired outputs, often involving extensive templates and manual tuning.&lt;SEP&gt;The practice of designing, refining, and optimizing prompts to improve language model responses, often involving manual prompt crafting and iterative testing.&lt;SEP&gt;The process of designing and refining prompts to improve model responses, often involving hand-crafted templates and iterative adjustments.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-d8772ec3475b4e5b1b113032265f9d74&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zero-Shot and Few-Shot Prompting">
  <data key="d0">Zero-Shot and Few-Shot Prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques used to prompt LLMs with minimal examples or instructions to generate accurate executable commands or responses.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Complex Tasks">
  <data key="d0">Complex Tasks</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Tasks that involve multiple steps, diverse tools, or domain-specific knowledge, requiring coordinated efforts between LLMs and domain tools for successful completion.</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-Shot or Few-Shot Prompting Techniques">
  <data key="d0">Zero-Shot or Few-Shot Prompting Techniques</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Prompting methods enabling LLMs to perform tasks with minimal examples, leveraging pre-trained knowledge to generalize to new tasks without extensive retraining.&lt;SEP&gt;Prompting techniques that enable large language models (LLMs) to perform tasks with minimal examples or instructions, leveraging their pre-trained knowledge to generalize to new tasks without extensive retraining.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Automated Theorem Proving">
  <data key="d0">Automated Theorem Proving</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques involving formal proof systems and LLMs to generate and verify mathematical proofs, often utilizing informal drafts, formal sketches, and off-the-shelf provers within a structured framework like DSP.&lt;SEP&gt;Techniques involving formal proof systems and LLMs to generate informal and formal proofs, utilizing frameworks like DSP that draft, sketch, and prove within a structured process.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Solution Outline Derivation">
  <data key="d0">Solution Outline Derivation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Processes where LLMs generate high-level plans or outlines for solving domain-specific tasks, then match subtasks to existing models or systems for execution, exemplified by TaskMatrix.AI.&lt;SEP&gt;Processes where LLMs produce high-level solution outlines for tasks, then match subtasks to existing models or systems for execution, exemplified by TaskMatrix.AI.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Controller Management">
  <data key="d0">Controller Management</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches where LLMs act as controllers to manage and coordinate existing domain models or systems to address complex problems, as exemplified by HuggingGPT.&lt;SEP&gt;Strategies where LLMs act as controllers to manage existing domain models or systems, coordinating their functions to solve complex problems, as seen with HuggingGPT.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task Decomposition and Dynamic Adjustment">
  <data key="d0">Task Decomposition and Dynamic Adjustment</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches where LLMs break down complex tasks into subtasks, adjust plans dynamically, and select appropriate tools for each subtask, as proposed by Qin et al.&lt;SEP&gt;Strategies employed by LLMs to break down complex tasks into subtasks, dynamically modify execution plans, and utilize appropriate tools for each subtask, as proposed by Qin et al.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLMs Embodied in Domain Tools">
  <data key="d0">LLMs Embodied in Domain Tools</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The concept of integrating LLMs into interactive environments, such as robots, where they serve as decision-making modules for domain-specific applications, enabling situated actions and multi-agent collaboration.&lt;SEP&gt;The integration of LLMs into interactive environments, such as robots, where they serve as decision-making modules for domain-specific tasks, enabling situated actions and multi-agent collaboration.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLMs in Robotics">
  <data key="d0">LLMs in Robotics</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Application of large language models embedded in robots or agents to assist in perception, decision-making, action generation, and multi-party interactions.&lt;SEP&gt;Application of large language models embedded in robots or interactive agents to assist in tasks like perception, action generation, and multi-party conversation identification.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Grounded Rationale">
  <data key="d0">Grounded Rationale</data>
  <data key="d1">Results</data>
  <data key="d2">Findings demonstrating how LLMs interacting with physics engines or environment simulators can produce physics-aligned, grounded reasoning in tasks.&lt;SEP&gt;Findings that demonstrate how LLMs benefit from interaction with physics engines or environment simulators to produce grounded, physics-aligned reasoning in tasks.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-Agent Collaboration Frameworks">
  <data key="d0">Multi-Agent Collaboration Frameworks</data>
  <data key="d1">Tools</data>
  <data key="d2">Systems like CAMEL that assign roles to multiple LLM agents, enabling communication and collaboration to solve tasks collectively in instruction-following scenarios.&lt;SEP&gt;Systems like CAMEL that assign roles to multiple LLM agents, enabling them to communicate, share information, and collaboratively solve tasks in instruction-following scenarios.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Game-Based Simulation of Human Behavior">
  <data key="d0">Game-Based Simulation of Human Behavior</data>
  <data key="d1">Tools</data>
  <data key="d2">Use of multiple LLMs as generative agents within sandbox environments to produce believable human-like behaviors for interactive applications.&lt;SEP&gt;Use of multiple LLMs as generative agents within sandbox environments to simulate human-like behaviors for interactive applications.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Open Challenges in Domain Tool Augmentation">
  <data key="d0">Open Challenges in Domain Tool Augmentation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Key issues include automated integration of tools via standardized protocols and the development of AGI models that do not rely on external tools, aiming to enhance efficiency and generality of LLMs.&lt;SEP&gt;Key issues include automating integration via standardized protocols and developing AGI models that do not depend on external tools, aiming to improve efficiency and generality.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Standardized Protocols for Integration">
  <data key="d0">Standardized Protocols for Integration</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Conceptual frameworks that enable seamless communication and interaction among diverse domain-specific applications and services through common interfaces.&lt;SEP&gt;Frameworks that enable seamless communication among diverse domain-specific applications and services through common interfaces.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Artificial General Intelligence (AGI)">
  <data key="d0">Artificial General Intelligence (AGI)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Hypothetical advanced AI systems capable of performing complex tasks independently of external tools, representing a future goal for AI development.&lt;SEP&gt;Hypothetical advanced AI systems capable of performing complex tasks without external tools, representing a future goal for AI development.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Crafting for Domain Specialization">
  <data key="d0">Prompt Crafting for Domain Specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The practice of designing task-specific prompts to guide LLM responses, enhancing domain adaptation, accuracy, and safety.&lt;SEP&gt;The process of designing task-specific input prompts to guide LLMs in generating accurate, relevant, and less toxic responses, thereby enhancing their domain adaptation capabilities.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Discrete Prompt">
  <data key="d0">Discrete Prompt</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework for eliciting desired outputs from large language models (LLMs) using specific instructions or examples, without updating the model's internal parameters.&lt;SEP&gt;A technique involving manually crafted natural language instructions that elicit domain-specific knowledge from LLMs without changing their internal parameters, used for zero-shot or few-shot learning.&lt;SEP&gt;Discrete prompts are manually designed text instructions or templates used to guide LLMs, with challenges related to sensitivity and optimization complexity.&lt;SEP&gt;Manually crafted natural language instructions that prompt LLMs to perform unseen tasks with zero-shot or few-shot learning, without modifying internal parameters.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-13de013c89659019025ef3c655d8a436&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Continuous Prompt">
  <data key="d0">Continuous Prompt</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique employing learnable vectors as prompts, replacing manual instructions, to adapt LLMs to specific domains more efficiently.&lt;SEP&gt;Learnable vector prompts that replace manual instructions, allowing more flexible and efficient domain adaptation for LLMs.</data>
  <data key="d3">chunk-13de013c89659019025ef3c655d8a436</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-shot Discrete Prompts">
  <data key="d0">Zero-shot Discrete Prompts</data>
  <data key="d1">Methodology</data>
  <data key="d2">A prompting approach where only task descriptions are provided to LLMs without illustrative examples, testing the model's ability to perform unseen tasks based solely on instructions.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Few-shot Discrete Prompts">
  <data key="d0">Few-shot Discrete Prompts</data>
  <data key="d1">Methodology</data>
  <data key="d2">A prompting approach where a few annotated examples are included in the prompt to guide LLMs in performing specific tasks with limited data.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLM (Large Language Model)">
  <data key="d0">LLM (Large Language Model)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A type of artificial intelligence model trained on vast text data to understand and generate human language, capable of performing various language tasks.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task Description">
  <data key="d0">Task Description</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Natural language instructions that specify the task to be performed by an LLM, such as entailment detection or reasoning.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Test Query">
  <data key="d0">Test Query</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A specific input or question provided to the LLM to generate an output based on the task description and prompt.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-shot-CoT (Chain-of-Thoughts)">
  <data key="d0">Zero-shot-CoT (Chain-of-Thoughts)</data>
  <data key="d1">Methodology</data>
  <data key="d2">A prompting technique that encourages LLMs to generate multi-step reasoning processes by adding a prompt like 'Let's think step by step' before answering, enhancing reasoning performance.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instruction Alignment Pre-training">
  <data key="d0">Instruction Alignment Pre-training</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A pre-training strategy that aligns model instructions with human intentions, improving zero-shot task performance across diverse applications.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Semantic Segmentation">
  <data key="d0">Semantic Segmentation</data>
  <data key="d1">Study Design</data>
  <data key="d2">A computer vision task where the goal is to classify each pixel in an image into a category, often used in zero-shot domain adaptation research.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Manuscript">
  <data key="d0">Manuscript</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of academic research articles, including the current study on discrete prompts and LLMs.&lt;SEP&gt;The research paper discussing GingGPT's methodology and applications in AI task solving.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Performance Evaluation">
  <data key="d0">Performance Evaluation</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of how well LLMs perform on tasks like reasoning, classification, and domain adaptation using different prompting strategies.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Logical Reasoning Tasks">
  <data key="d0">Logical Reasoning Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks designed to evaluate the reasoning capabilities of LLMs, such as arithmetic and symbolic reasoning.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Unseen Domains">
  <data key="d0">Unseen Domains</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Domains or areas not encountered during the training of the LLM, targeted in domain adaptation and zero-shot learning.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Natural Language Processing (NLP)">
  <data key="d0">Natural Language Processing (NLP)</data>
  <data key="d1">Discipline</data>
  <data key="d2">A field of artificial intelligence focused on the interaction between computers and human language, encompassing tasks like prompting and reasoning.&lt;SEP&gt;NLP is a field of artificial intelligence focused on enabling computers to understand, interpret, and generate human language.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Token Sequences">
  <data key="d0">Token Sequences</data>
  <data key="d1">Variables</data>
  <data key="d2">Sequences of words or symbols used as inputs or prompts for LLMs, representing instructions, examples, or queries.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Parameters">
  <data key="d0">Model Parameters</data>
  <data key="d1">Variables</data>
  <data key="d2">The internal settings of an LLM that are learned during training, such as weights, which are kept frozen during inference in prompt-based tasks.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Reasoning Abilities">
  <data key="d0">Reasoning Abilities</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The capacity of LLMs to perform logical, multi-step, or symbolic reasoning processes, often evaluated through specialized prompting techniques like Chain-of-Thoughts.</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="APGAR Scores">
  <data key="d0">APGAR Scores</data>
  <data key="d1">Results</data>
  <data key="d2">APGAR scores are a clinical measurement used to assess the health of newborns immediately after birth, with low scores indicating potential health issues.</data>
  <data key="d3">chunk-03fe9b9ee91cb6d7d1e6d86d23f22ae9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hypothesis">
  <data key="d0">Hypothesis</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A proposed statement suggesting that she had low APGAR scores, used to guide investigation or analysis.&lt;SEP&gt;The more complex the parallel programming model's code relative to serial code, the more difficult it is for LLMs to generate correct code across all models.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-03fe9b9ee91cb6d7d1e6d86d23f22ae9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="She">
  <data key="d0">She</data>
  <data key="d1">Study Population/Dataset</data>
  <data key="d2">Refers to the individual whose APGAR scores are being evaluated, forming the subject of the hypothesis.</data>
  <data key="d3">chunk-03fe9b9ee91cb6d7d1e6d86d23f22ae9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Tuning">
  <data key="d0">Prompt Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for adjusting prompts to guide language models towards specific tasks, including continuous, soft, prefix, and instance-dependent prompt methods.&lt;SEP&gt;A technique to adapt large language models by optimizing prompts, including various approaches such as continuous, soft, prefix, and instance-dependent prompt tuning.&lt;SEP&gt;Prompt tuning is a technique for adapting pre-trained language models (PLMs) and large language models (LLMs) to specific tasks by optimizing continuous prompts, which can be task-dependent or instance-dependent, enabling parameter-efficient and controllable model adaptation.&lt;SEP&gt;Prompt tuning is a technique that involves adjusting prompts to improve the performance of large language models (LLMs) on specific tasks by utilizing their broad language understanding capabilities.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task-dependent Prompt Tuning">
  <data key="d0">Task-dependent Prompt Tuning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A category of prompt tuning that optimizes a shared prompt for all instances within a specific task, capturing task-specific information from large datasets.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Content Enhancement">
  <data key="d0">Prompt Content Enhancement</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Enhancements in prompt content involve task-specific initialization and prior knowledge transfer, such as initializing prompts with embeddings of task-relevant words or labels to improve optimization and performance.&lt;SEP&gt;Techniques to improve prompt effectiveness by adding external knowledge, external ontologies, or dynamically adjusting prompt position, length, and content for each instance.&lt;SEP&gt;Techniques to improve prompt effectiveness by adding instance-related knowledge, external ontology information, or dynamic configuration of position, length, and values.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="WARP">
  <data key="d0">WARP</data>
  <data key="d1">Tools</data>
  <data key="d2">A prompt initialization method that uses the embedding of the special token '[MASK]' to start prompt optimization, improving convergence and performance.&lt;SEP&gt;A prompt-based classification method that uses all intersections with a '[MASK]' token for tasks like classification, demonstrating prompt design techniques.&lt;SEP&gt;A prompt-based classification method using all intersections with a '[MASK]' token for tasks like classification.&lt;SEP&gt;WARP is a study demonstrating that soft prompts are non-interpretable and lack meaningful content, highlighting interpretability issues.&lt;SEP&gt;WARP is a study that found soft prompts to be non-interpretable and lacking meaningful content, highlighting interpretability limitations of prompt tuning.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="KnowPrompt">
  <data key="d0">KnowPrompt</data>
  <data key="d1">Tools</data>
  <data key="d2">A knowledge-aware prompt-tuning approach that uses synergistic optimization to improve relation extraction performance.&lt;SEP&gt;A knowledge-aware prompt-tuning method that uses synergistic optimization to improve relation extraction performance.&lt;SEP&gt;A method that appends templates with '[MASK]' tokens and trainable virtual type words to facilitate relation extraction with entity type conditioning.&lt;SEP&gt;A prompt design approach that creates learnable virtual tokens initialized from aggregated representations of label words and frequent dataset words, enhancing prompt effectiveness.&lt;SEP&gt;A prompt design that appends templates with '[MASK]' tokens and incorporates trainable 'virtual type words' to improve relation extraction conditioned on entity types.&lt;SEP&gt;KnowPrompt discovers that prompt tokens are often close to domain-related terms, but the interpretability of the entire prompt sequence remains unclear.&lt;SEP&gt;KnowPrompt is a method that finds prompt tokens close to domain-related terms, but the overall interpretability of prompts remains uncertain.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt-tuning">
  <data key="d0">Prompt-tuning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique that employs pre-trained models with minimal additional parameters, often showing robustness to initialization strategies, especially in large models.&lt;SEP&gt;Prompt-tuning involves training prompts close to domain-specific terms to improve task performance, but faces challenges in interpretability of the whole prompt.&lt;SEP&gt;Prompt-tuning involves training prompts to improve task-specific performance, but interpretability of the entire prompt sequence remains an open issue.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Transfer">
  <data key="d0">Prompt Transfer</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The transferability of prompts trained on source domains to target domains enhances performance in unseen tasks, enabling domain adaptation and cross-task generalization.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-supervised Prompt Pretraining">
  <data key="d0">Self-supervised Prompt Pretraining</data>
  <data key="d1">Methodology</data>
  <data key="d2">Pre-training prompts on large unlabeled corpora using self-supervised learning to create effective initial prompts for downstream tasks.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LFPT5">
  <data key="d0">LFPT5</data>
  <data key="d1">Methodology</data>
  <data key="d2">An advanced soft prompt technique that employs transfer learning to improve prompt initialization and model adaptation across tasks and models.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝒆(𝜔𝑛)">
  <data key="d0">𝒆(𝜔𝑛)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A mathematical expression representing a component or element in a sequence or system, possibly related to neural or computational models.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝒆(𝜏𝑖+1)">
  <data key="d0">𝒆(𝜏𝑖+1)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A mathematical notation indicating the (i+1)-th element or token in a sequence, used in the context of computational modeling or language processing.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝒆(𝜏𝑚)">
  <data key="d0">𝒆(𝜏𝑚)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A mathematical notation indicating the m-th token or element in a sequence, relevant to models processing tokenized input.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜔𝑛">
  <data key="d0">𝜔𝑛</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable representing a frequency, weight, or parameter associated with a specific component in a model, possibly related to neural or embedding spaces.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜏𝑖">
  <data key="d0">𝜏𝑖</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable denoting the i-th token or position in a sequence, used for indexing or positional referencing in models.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜏𝑖+1">
  <data key="d0">𝜏𝑖+1</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable indicating the position immediately after the i-th token in a sequence, used in sequential modeling.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜏𝑚">
  <data key="d0">𝜏𝑚</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable indicating the m-th token or position in a sequence, used for indexing in models.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="T′𝑒(𝝉,𝒄)">
  <data key="d0">T′𝑒(𝝉,𝒄)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A modified embedding sequence incorporating trainable prompt tokens and input tokens, used in prompt tuning for language models.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝑓Θ(·)">
  <data key="d0">𝑓Θ(·)</data>
  <data key="d1">Tools</data>
  <data key="d2">A function representing the language model (LLM) parameterized by Θ, used for processing embedding sequences and generating outputs.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Las">
  <data key="d0">Las</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A loss function used during training or optimization of language models or prompts, guiding the adjustment of parameters.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜽𝜏★">
  <data key="d0">𝜽𝜏★</data>
  <data key="d1">Variables</data>
  <data key="d2">The optimized parameters of the prompt tokens after training, representing the best prompt configuration for a specific task.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝒚">
  <data key="d0">𝒚</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable representing the target output or label in a task, such as sentiment class or classification label.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝑃[𝑀𝐴𝑆𝐾]">
  <data key="d0">𝑃[𝑀𝐴𝑆𝐾]</data>
  <data key="d1">Tools</data>
  <data key="d2">The probability distribution over vocabulary tokens at the masked position, used for prediction in masked language modeling.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="𝜙(𝒚)">
  <data key="d0">𝜙(𝒚)</data>
  <data key="d1">Tools</data>
  <data key="d2">A verbalizer mapping class labels to specific words in the vocabulary, facilitating classification tasks.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="task-dependent prompt tuning">
  <data key="d0">task-dependent prompt tuning</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A research approach aiming to optimize prompts for specific tasks, capturing task-specific information to improve model performance.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt content">
  <data key="d0">prompt content</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Refers to the embedding values of continuous prompts, which can be enhanced through task-specific initialization and transfer learning.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt initialization">
  <data key="d0">prompt initialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of setting initial values for prompt embeddings, which significantly influences optimization and performance.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt transfer">
  <data key="d0">prompt transfer</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The transfer of prompts trained on source domains to target domains enhances model generalization and cross-task performance.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="self-supervised learning">
  <data key="d0">self-supervised learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach where models learn from unlabeled data by predicting parts of input, used here to pre-train prompts for downstream tasks.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt pretraining">
  <data key="d0">prompt pretraining</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Pre-training prompts on large unlabeled corpora using self-supervised methods to generate effective starting points for task-specific tuning.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt robustness">
  <data key="d0">prompt robustness</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The property of prompt tuning methods, especially in large models, to be less sensitive to initialization strategies, leading to consistent performance.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt optimization">
  <data key="d0">prompt optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for refining prompts through gradient-based updates to improve task performance.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt transferability">
  <data key="d0">prompt transferability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The ability of prompts to be effective across different tasks or domains, enabling efficient adaptation.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt initialization strategies">
  <data key="d0">prompt initialization strategies</data>
  <data key="d1">Variables</data>
  <data key="d2">Different approaches to setting initial prompt embeddings, such as random, label-based, or learned representations.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt tuning advantages">
  <data key="d0">prompt tuning advantages</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Parameter-efficient tuning methods that allow large models to be adapted to specific tasks with minimal additional parameters.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="training convergence">
  <data key="d0">training convergence</data>
  <data key="d1">Results</data>
  <data key="d2">The process and speed at which prompt tuning methods reach optimal performance, often accelerated by good initialization and transfer learning.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="large-scale PLMs">
  <data key="d0">large-scale PLMs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large pre-trained language models with billions of parameters, which serve as the foundation for prompt tuning methods.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="domain adaptation">
  <data key="d0">domain adaptation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using prompt transfer techniques to adapt models trained on one domain to perform well in another domain.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="cross-task learning">
  <data key="d0">cross-task learning</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Adapting prompts across different tasks to improve generalization and reduce training data needs.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="optimization algorithms">
  <data key="d0">optimization algorithms</data>
  <data key="d1">Tools</data>
  <data key="d2">Algorithms like SGD used to update prompt parameters during training.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt design">
  <data key="d0">prompt design</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process and principles involved in creating effective prompts, including initialization, transfer, and content enhancement.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="prompt transfer learning">
  <data key="d0">prompt transfer learning</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using transfer learning principles to initialize and adapt prompts across different tasks and models, improving efficiency.</data>
  <data key="d3">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-Supervised Learning">
  <data key="d0">Self-Supervised Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A training approach where models learn to predict parts of data from other parts without labeled data, used to pre-train language models on unlabeled corpora.&lt;SEP&gt;A training paradigm where models learn to predict parts of data from other parts without labeled supervision, used to pre-train language models on large unlabeled corpora.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Transferability of Prompts">
  <data key="d0">Transferability of Prompts</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The concept that prompts trained or designed in one context can be effectively transferred to other tasks or models, enabling cross-task and cross-model adaptation.&lt;SEP&gt;The concept that prompts trained or designed in one setting can be effectively applied to other tasks or models, facilitating cross-task and cross-model adaptation.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Construction">
  <data key="d0">Prompt Construction</data>
  <data key="d1">Variables</data>
  <data key="d2">Design strategies for prompts, including positioning, length, templates, and combinations with additional language components to improve task performance.&lt;SEP&gt;The process of designing and optimizing prompts, including positioning, length, templates, and combinations with additional language components.&lt;SEP&gt;Prompt construction refers to how input instructions are formulated to guide LLMs in generating diverse and accurate parallel code samples, impacting data quality and model performance.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prefix-tuning">
  <data key="d0">Prefix-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A prompt tuning approach where trainable prompts are prepended to sentence embeddings and attention activations to influence model output.&lt;SEP&gt;A prompt tuning approach where tunable prompts are prepended to sentence embeddings and attention activations, leveraging autoregressive model properties for efficient influence.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Template">
  <data key="d0">Template</data>
  <data key="d1">Tools</data>
  <data key="d2">Predefined task-specific formats that reformulate NLP tasks into masked word prediction, enhancing model adaptation.&lt;SEP&gt;Predefined task-specific formats that transform NLP tasks into masked word prediction tasks, facilitating model adaptation during pretraining and fine-tuning.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="KiPT">
  <data key="d0">KiPT</data>
  <data key="d1">Tools</data>
  <data key="d2">A knowledge extractor designed for event detection that identifies trigger words based on semantic similarity to event concepts and reformulates sequence tagging into generative event record outputs.&lt;SEP&gt;A knowledge extractor for event detection that identifies trigger words based on semantic similarity and reformulates sequence tagging as generative tasks.&lt;SEP&gt;KiPT (Knowledge-injected Prompt Tuning) is a technique for enhancing event detection by injecting external knowledge into prompt tuning processes.&lt;SEP&gt;KiPT (Knowledge-injected Prompt Tuning) is a technique for improving event detection by integrating external knowledge into prompt-based tuning methods.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9&lt;SEP&gt;chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instance-dependent prompt tuning">
  <data key="d0">Instance-dependent prompt tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A prompt tuning approach that generates prompts tailored to each individual input instance, considering its specific context and knowledge, to improve task performance.&lt;SEP&gt;A prompt tuning strategy that generates prompts tailored to individual input instances, incorporating contextual and task-specific information.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="IDPG">
  <data key="d0">IDPG</data>
  <data key="d1">Tools</data>
  <data key="d2">A prompt generator that employs a two-layer perceptron to produce adaptive soft prompts based on sentence embeddings, enabling instance-specific prompt generation.&lt;SEP&gt;A prompt generator using a two-layer perceptron to produce adaptive soft prompts based on sentence embeddings.&lt;SEP&gt;Instance-Dependent Prompt Generation method aimed at improving prompt quality and adaptability in language models.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3&lt;SEP&gt;chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ATTEMPT">
  <data key="d0">ATTEMPT</data>
  <data key="d1">Tools</data>
  <data key="d2">A framework that trains multiple prompts across large-scale source tasks and combines them via attention mechanisms to create instance-specific prompts for target tasks.&lt;SEP&gt;A framework that trains multiple prompts and combines them via attention mechanisms to produce instance-specific prompts.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dynamic Prompting">
  <data key="d0">Dynamic Prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A unified framework that dynamically learns to define prompt position, length, and content for each input instance, optimizing prompt structure adaptively.&lt;SEP&gt;A unified framework that learns to define prompt position, length, and content dynamically for each input instance, optimizing prompt structure in real-time.</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Continuous prompt tuning">
  <data key="d0">Continuous prompt tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Continuous prompt tuning is a technique that leverages soft prompts to utilize the broad language understanding capacity of LLMs for specific tasks across different domains, aiming to improve performance and efficiency.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Discreet prompt methods">
  <data key="d0">Discreet prompt methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Discreet prompt methods involve manually designing prompts or templates to guide LLMs, but are limited by sensitivity to wording and high search complexity.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Interpretability of soft prompts">
  <data key="d0">Interpretability of soft prompts</data>
  <data key="d1">Limitations</data>
  <data key="d2">Interpretability issues arise because soft prompts are often non-interpretable token vectors, making it difficult to understand what the model is leveraging in the prompt.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="OPTIMA">
  <data key="d0">OPTIMA</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">OPTIMA uses adversarial learning to adapt prompts for domain-specific data by regularizing decision boundaries, representing a domain adaptation approach.&lt;SEP&gt;OPTIMA uses adversarial learning to regularize decision boundaries for domain adaptation of prompts, improving transferability.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Limited access to LLMs">
  <data key="d0">Limited access to LLMs</data>
  <data key="d1">Limitations</data>
  <data key="d2">Limited access, especially in models with only API or massive size, restricts the ability to perform gradient-based prompt optimization.&lt;SEP&gt;Limited access, especially models with API-only access or enormous size, hampers the ability to perform gradient-based optimization on prompts.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Black-box tuning (BBT)">
  <data key="d0">Black-box tuning (BBT)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Black-box tuning is a gradient-free approach that searches for optimal prompts without gradient information, using strategies like CMA-ES for non-convex optimization.&lt;SEP&gt;Black-box tuning is a gradient-free approach that searches for optimal prompts without requiring access to model gradients, suitable for API-only models.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Covariance Matrix Adaptation Evolution Strategy (CMA-ES)">
  <data key="d0">Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</data>
  <data key="d1">Tools</data>
  <data key="d2">CMA-ES is an optimization algorithm used in black-box tuning to find optimal prompts in a smaller intrinsic space for non-convex problems.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Clip-Tuning">
  <data key="d0">Clip-Tuning</data>
  <data key="d1">Tools</data>
  <data key="d2">A prompt learning methodology that aims to optimize models without derivatives by using a mixture of rewards to improve prompt performance.&lt;SEP&gt;A prompt learning methodology that aims to optimize models without derivatives, using a mixture of rewards to improve prompt-based tasks.&lt;SEP&gt;Clip-Tuning employs deterministic clipping of model instances to optimize intrinsic prompts, especially when model access is limited to textual queries.&lt;SEP&gt;Clip-Tuning employs deterministic clipping of model outputs to optimize prompts in settings with limited model access, focusing on textual queries.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Discrete prompt search">
  <data key="d0">Discrete prompt search</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Discrete prompt search aims to optimize prompts in settings where only textual input is accessible, focusing on derivative-free methods.&lt;SEP&gt;Discrete prompt search involves derivative-free optimization over text prompts, focusing on search strategies that do not require gradient information.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model fine-tuning for domain specialization">
  <data key="d0">Model fine-tuning for domain specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model fine-tuning involves adjusting a pre-trained LLM to improve performance on specific domains or tasks, either via adapters or full parameter updates.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapter-based Fine-tuning">
  <data key="d0">Adapter-based Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adapter-based fine-tuning adds small, trainable modules (adapters) within the existing LLM architecture to enhance domain-specific performance without changing the entire model.&lt;SEP&gt;Adapter-based fine-tuning introduces small, trainable modules (adapters) into the pre-trained LLM, enabling efficient domain adaptation with minimal parameter updates.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task-oriented Fine-tuning">
  <data key="d0">Task-oriented Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process where LLMs are further trained on specific domain datasets to improve performance on targeted tasks, balancing knowledge retention and task adaptation.&lt;SEP&gt;Task-oriented fine-tuning involves modifying or updating the internal parameters of an LLM to better align with specific tasks or domains, often requiring extensive retraining.&lt;SEP&gt;Task-oriented fine-tuning involves modifying the inner parameters of the LLM to better align with specific tasks, often requiring more extensive updates.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5&lt;SEP&gt;chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-specific Knowledge">
  <data key="d0">Domain-specific Knowledge</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Domain-specific knowledge encompasses specialized information within a particular field that can be incorporated into LLMs to improve domain performance through fine-tuning.&lt;SEP&gt;Domain-specific knowledge refers to specialized information within a particular field that can be incorporated into LLMs through fine-tuning.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapters">
  <data key="d0">Adapters</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adapters are modular, trainable components inserted into LLMs to facilitate efficient domain adaptation and task-specific tuning.&lt;SEP&gt;Adapters are modules inserted into language models to enable task-specific or domain-specific adaptation without retraining the entire model.&lt;SEP&gt;Adapters are modules integrated into large language models to enable domain-specific adaptation with fewer parameters and less retraining.&lt;SEP&gt;Adapters are trainable modules inserted into pre-trained language models to enable efficient domain-specific fine-tuning without altering the original model parameters.&lt;SEP&gt;Adapters are trainable modules inserted into pre-trained language models to enable efficient domain-specific fine-tuning without modifying the entire model.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556&lt;SEP&gt;chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Soft Prompt">
  <data key="d0">Soft Prompt</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Soft prompts are trainable, continuous embeddings used to steer LLM outputs without changing the model's internal parameters, often criticized for lack of interpretability.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Interpretability">
  <data key="d0">Interpretability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Interpretability refers to how well humans can understand and trace the reasoning behind a model's output, an advantage of RAG models.&lt;SEP&gt;Interpretability refers to the ability to understand and explain how prompts influence LLM outputs; soft prompts often lack clear interpretability.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="CMA-ES">
  <data key="d0">CMA-ES</data>
  <data key="d1">Tools</data>
  <data key="d2">CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is an optimization algorithm used in black-box tuning to find optimal prompts in non-convex spaces without gradient information.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model fine-tuning">
  <data key="d0">Model fine-tuning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model fine-tuning adjusts the parameters of large language models to improve their performance on specific domains or tasks, either via full retraining or modular approaches.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neural Adapters">
  <data key="d0">Neural Adapters</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Adapters that utilize neural network architectures, such as feed-forward layers and attention mechanisms, to facilitate domain adaptation.&lt;SEP&gt;Adapters with neural network architectures, often involving components like feed-forward layers and attention mechanisms, used for domain adaptation.&lt;SEP&gt;Neural adapters are small modules inserted into LLM layers to enable efficient, parameter-efficient domain-specific fine-tuning without modifying the entire model.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter-efficient Fine-tuning">
  <data key="d0">Parameter-efficient Fine-tuning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parameter-efficient fine-tuning strategies aim to adapt LLMs to new tasks or domains with minimal additional parameters, balancing performance and resource use.</data>
  <data key="d3">chunk-03d4474943225c24f9a066079ecfcaf5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameters">
  <data key="d0">Parameters</data>
  <data key="d1">Variables</data>
  <data key="d2">Parameters are adjustable elements within language models and adapters that influence model performance and are tuned during training.&lt;SEP&gt;Parameters refer to the adjustable elements within language models and adapters that influence model behavior and performance.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter-Efficient Fine-Tuning">
  <data key="d0">Parameter-Efficient Fine-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach that fine-tunes models by updating only a subset of parameters or modules, such as adapters, to reduce computational cost and improve flexibility.&lt;SEP&gt;A training approach that optimizes a subset of parameters or modules, such as adapters, to adapt large models to specific tasks or domains efficiently.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Unsupervised Domain Adaptation (UDA)">
  <data key="d0">Unsupervised Domain Adaptation (UDA)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A technique to adapt models to new domains without labeled data, often involving specialized training strategies like adapters to improve cross-domain performance.&lt;SEP&gt;A technique to adapt models to new domains without labeled data, often involving specialized training strategies like adapters.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Cross-Lingual Learning">
  <data key="d0">Cross-Lingual Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework enabling language models trained on one language to perform well on others, often facilitated by domain adaptation techniques.&lt;SEP&gt;A framework enabling models trained in one language to perform well in others, often facilitated by domain adaptation and shared representations.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-Task Learning">
  <data key="d0">Multi-Task Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A learning paradigm where models are trained simultaneously on multiple tasks to promote generalization and shared feature representations.&lt;SEP&gt;A paradigm where models are trained on multiple tasks simultaneously, promoting generalization and shared representations.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-Invariant Representations">
  <data key="d0">Domain-Invariant Representations</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Feature representations that are robust across different domains, crucial for effective domain adaptation and transfer learning.&lt;SEP&gt;Feature representations that are robust across different domains, crucial for successful domain adaptation.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-Instruct Demonstrations">
  <data key="d0">Self-Instruct Demonstrations</data>
  <data key="d1">Tools</data>
  <data key="d2">A technique involving model-generated instructions to enhance instruction-following and reasoning capabilities in large language models, often used for multi-modal tasks.&lt;SEP&gt;A technique involving model-generated instructions to facilitate instruction-following and multi-modal reasoning tasks in large language models.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AdapterFusion">
  <data key="d0">AdapterFusion</data>
  <data key="d1">Methodologies</data>
  <data key="d2">AdapterFusion is a transfer learning technique that combines multiple task-specific adapters in transformer models without destructive interference.&lt;SEP&gt;AdapterFusion trains multiple adapters on different tasks and combines their embeddings via a fusion layer to improve performance.&lt;SEP&gt;AdapterFusion's effectiveness depends on the task composition and transfer learning context.&lt;SEP&gt;An architecture that combines multiple adapters' outputs, enabling better domain adaptation and multi-task learning.&lt;SEP&gt;An architecture that combines multiple adapters' outputs, enhancing domain adaptation capabilities.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLaMA-adapter">
  <data key="d0">LLaMA-adapter</data>
  <data key="d1">Tools</data>
  <data key="d2">An adapter designed for efficient adaptation on large language models like LLaMA, incorporating self-instruct demonstrations.&lt;SEP&gt;An adapter designed for efficient adaptation on large language models like LLaMA, incorporating self-instruction demonstrations for improved instruction-following.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Invertible Adapters">
  <data key="d0">Invertible Adapters</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A type of neural adapter capable of reversing its transformations, inspired by autoencoders, allowing reversible domain adaptation.&lt;SEP&gt;A type of neural adapter that can invert their transformations, inspired by autoencoders, allowing for reversible domain adaptation.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hypercomplex Multiplication Layers">
  <data key="d0">Hypercomplex Multiplication Layers</data>
  <data key="d1">Tools</data>
  <data key="d2">Parameterization layers used in some adapters (e.g., Compacters) that learn Kronecker products, enabling parameter efficiency.&lt;SEP&gt;Parameterization technique used in some adapters (e.g., Compacters) that learns Kronecker products for parameter efficiency.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pruning">
  <data key="d0">Pruning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique to reduce model parameters by removing less important weights or connections, applied here in SparseAdapter for efficiency.&lt;SEP&gt;A technique to reduce the number of parameters in models by removing less important weights, used in SparseAdapter for efficiency.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Residual Connections">
  <data key="d0">Residual Connections</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Connections that add the input of a layer to its output, used in certain adapter architectures like MAD-X to improve training stability.&lt;SEP&gt;Connections that add the input of a layer to its output, used in certain adapter architectures like MAD-X to stabilize training and improve learning.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter Sharing">
  <data key="d0">Parameter Sharing</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The practice of using the same parameters across different modules or layers to improve efficiency and promote generalization.&lt;SEP&gt;The practice of using the same parameters across different parts of a model or modules like adapters to improve efficiency and generalization.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evaluation Strategies">
  <data key="d0">Evaluation Strategies</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Methods such as testing on downstream tasks, domain transfer scenarios, and performance metrics to assess adapter effectiveness.&lt;SEP&gt;Methods such as testing on various downstream tasks, domain transfer scenarios, and performance metrics to assess adapter effectiveness.</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kronecker Products">
  <data key="d0">Kronecker Products</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Kronecker products are mathematical operations used to construct parameter-efficient modules in neural networks, specifically in low-rank adapters.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Network Pruning">
  <data key="d0">Network Pruning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Network pruning involves reducing model parameters by removing less important weights, inspired by techniques like SparseAdapter to improve efficiency.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="SparseAdapter">
  <data key="d0">SparseAdapter</data>
  <data key="d1">Methodologies</data>
  <data key="d2">SparseAdapter is a technique that reduces training parameters by pruning at initialization, applicable to neural adapters for efficiency.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="K-adapters">
  <data key="d0">K-adapters</data>
  <data key="d1">Methodologies</data>
  <data key="d2">K-adapters train multiple adapters on different knowledge domains and inject learned knowledge into language models via concatenation.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ladder Side-Tuning">
  <data key="d0">Ladder Side-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Ladder side-tuning adds small modules to the side of language models connected via shortcuts, reducing memory requirements during training.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LoRA (Low-Rank Adaptation)">
  <data key="d0">LoRA (Low-Rank Adaptation)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">LoRA implants learnable SVD-based modules in parallel to pre-trained weights, significantly reducing parameters and inference latency.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="DyLora">
  <data key="d0">DyLora</data>
  <data key="d1">Methodologies</data>
  <data key="d2">DyLora addresses fixed block size and search for optimal rank in LoRA using dynamic search techniques.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kronecker Adapter (KronA)">
  <data key="d0">Kronecker Adapter (KronA)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">KronA replaces SVD modules in LoRA with Kronecker product modules of smaller matrices to enhance representation power while maintaining efficiency.&lt;SEP&gt;KronA replaces SVD modules with Kronecker product modules of smaller matrices to enhance representation power while maintaining efficiency.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="UniPELT">
  <data key="d0">UniPELT</data>
  <data key="d1">Methodologies</data>
  <data key="d2">UniPELT activates different combinations of adapter methods using a gating mechanism to adapt to current data or task setups.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Serial Adapter, LoRA, Prefix-tuning, Bitfit">
  <data key="d0">Serial Adapter, LoRA, Prefix-tuning, Bitfit</data>
  <data key="d1">Tools</data>
  <data key="d2">These are different adapter architectures used to fine-tune language models for various tasks, offering modular and flexible adaptation options.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AdaMix">
  <data key="d0">AdaMix</data>
  <data key="d1">Methodologies</data>
  <data key="d2">AdaMix stacks multiple adapters of the same type and uses stochastic routing to avoid additional computational costs, enabling efficient multi-adapter integration.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Polytropon">
  <data key="d0">Polytropon</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Polytropon jointly learns an inventory of adapters and a routing function to re-combine adapters across multiple tasks, enhancing multi-task learning.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AdapterHub">
  <data key="d0">AdapterHub</data>
  <data key="d1">Tools</data>
  <data key="d2">AdapterHub is a comprehensive library integrating multiple mainstream adapters, facilitating easy deployment and experimentation.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLM-adapters">
  <data key="d0">LLM-adapters</data>
  <data key="d1">Tools</data>
  <data key="d2">LLM-adapters framework supports open-access large language models, including components like Serial adapter, MAD-X, Parallel adapter, and LoRA, enabling extensible domain-specific adaptation.</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Backpropagation">
  <data key="d0">Backpropagation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Backpropagation is a fundamental algorithm used to train neural networks by propagating errors backward through the model to update parameters.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Stability and Universality">
  <data key="d0">Stability and Universality</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">This refers to the ability of adapters to consistently perform across different architectures and tasks, indicating the robustness and generalizability of the approach.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Computational Resources">
  <data key="d0">Computational Resources</data>
  <data key="d1">Variables</data>
  <data key="d2">Resources such as memory, processing power, and parameters required for training or fine-tuning LLMs with adapters, influencing the feasibility and scalability.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="In-context Learning">
  <data key="d0">In-context Learning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A paradigm where language models learn to perform tasks by conditioning on examples within the prompt, without explicit retraining or fine-tuning.&lt;SEP&gt;A process where language models learn to perform tasks by conditioning on examples or instructions provided within the prompt, without explicit retraining.&lt;SEP&gt;The ability of LLMs to perform tasks based on few-shot or zero-shot prompts without extensive fine-tuning, which can be compromised during aggressive parameter updates.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="High-quality Domain-specific Datasets">
  <data key="d0">High-quality Domain-specific Datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Specialized datasets used to fine-tune LLMs for improved performance in particular fields or tasks.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instruction-based Fine-tuning">
  <data key="d0">Instruction-based Fine-tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique involving training LLMs on datasets with explicit instructions to enhance their ability to generalize to unseen tasks.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Reinforcement Learning from Human Feedback (RLHF)">
  <data key="d0">Reinforcement Learning from Human Feedback (RLHF)</data>
  <data key="d1">Tools</data>
  <data key="d2">A method where human preferences guide the fine-tuning of LLMs to generate safer, more aligned content through iterative feedback and reward modeling.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-shot and Few-shot Performance">
  <data key="d0">Zero-shot and Few-shot Performance</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics indicating LLMs' ability to perform tasks with minimal or no task-specific training, which can be improved through instruction tuning.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuning Strategies">
  <data key="d0">Fine-tuning Strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Various approaches to updating LLM parameters, including instruction-based learning and partial knowledge updates, aimed at optimizing task performance and resource use.</data>
  <data key="d3">chunk-891622908a24bf5367903ed1b9780d8b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="reward model">
  <data key="d0">reward model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An external reward model assigns scores to content based on rankings, capturing evaluator preferences, and is used to guide content optimization.&lt;SEP&gt;An external reward model assigns scores to content based on rankings, capturing evaluator preferences; used to guide content evaluation and optimization.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="model policy">
  <data key="d0">model policy</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A strategy or set of rules used to update the model, often via reinforcement learning, to maximize expected reward and improve content generation.&lt;SEP&gt;Model policy refers to the strategy or set of rules used to update and optimize the model, often using reinforcement learning techniques to maximize expected rewards.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="reinforcement learning techniques">
  <data key="d0">reinforcement learning techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Algorithms that utilize reward signals to train models to produce desired outputs, such as aligning with human preferences.&lt;SEP&gt;Techniques that involve training models through reward signals to improve performance, such as in fine-tuning models to align with human preferences.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="content generation">
  <data key="d0">content generation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The process of creating content by language models, which is subject to optimization via reward models and policy updates.&lt;SEP&gt;The process of producing text or content via language models, which can be optimized through reward-based methods.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ranking">
  <data key="d0">ranking</data>
  <data key="d1">Variables</data>
  <data key="d2">The order or priority assigned to generated content, influenced by reward models to guide model training.&lt;SEP&gt;The ordering or prioritization of content, which is influenced by the reward model to guide model training.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="reward modeling">
  <data key="d0">reward modeling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of constructing models that predict or assign scores to content based on desired criteria, used to guide reinforcement learning.&lt;SEP&gt;The process of creating models that predict scores or rankings for content, used to reinforce desired outputs during training.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="policy optimization">
  <data key="d0">policy optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques to improve the model's decision-making policy to generate higher-quality content based on reward feedback.&lt;SEP&gt;Techniques used to improve the model's decision-making policy to generate higher-quality content according to reward signals.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="natural language understanding tasks">
  <data key="d0">natural language understanding tasks</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Tasks involving comprehension and interpretation of language, where knowledge updates based on explicit instructions are tested.&lt;SEP&gt;Tasks involving comprehension, reasoning, and interpretation of language, where knowledge updates and instruction-based methods are evaluated.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="catastrophic forgetting">
  <data key="d0">catastrophic forgetting</data>
  <data key="d1">Limitations</data>
  <data key="d2">A challenge where models lose previously acquired knowledge when trained on new tasks or data, limiting continual learning and adaptation.&lt;SEP&gt;A phenomenon where models lose previously acquired knowledge when trained on new data, hindering continual learning.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="knowledge updates">
  <data key="d0">knowledge updates</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Processes aimed at enhancing or modifying the knowledge embedded within language models, either through explicit instructions or parameter editing.&lt;SEP&gt;Processes aimed at modifying or enhancing the knowledge embedded within language models, either through explicit instructions or parameter editing techniques.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="explicit instructions">
  <data key="d0">explicit instructions</data>
  <data key="d1">Theoretical/Model</data>
  <data key="d2">Guidelines or commands provided to models to direct their behavior or knowledge updates, often used in instruction-based learning.&lt;SEP&gt;Guidelines or commands provided to models to steer their behavior or knowledge updates, often used in instruction tuning.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="general reasoning">
  <data key="d0">general reasoning</data>
  <data key="d1">Results</data>
  <data key="d2">Models' capacity to perform broad, flexible reasoning tasks without relying on ground-truth labels, improved through approaches like rationale-augmented answers.&lt;SEP&gt;The ability of models to perform broad, flexible reasoning without ground truth labels, improved by methods like rationale-augmented answers.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="rationale-augmented answers">
  <data key="d0">rationale-augmented answers</data>
  <data key="d1">Tools</data>
  <data key="d2">Answers that include reasoning steps or explanations to enhance model reasoning and interpretability.&lt;SEP&gt;Answers that include reasoning steps or explanations to improve model reasoning capabilities.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="continual learning">
  <data key="d0">continual learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies that enable models to learn continuously over time from new data while retaining previous knowledge, often using rehearsal or regularization.&lt;SEP&gt;Strategies to enable models to learn continuously from new data without forgetting previous knowledge, often using rehearsal techniques.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="rehearsal">
  <data key="d0">rehearsal</data>
  <data key="d1">Tools</data>
  <data key="d2">A method in continual learning where previous data or knowledge is replayed to prevent forgetting during training.&lt;SEP&gt;A technique in continual learning where previous data or model states are revisited during training to prevent forgetting.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="partial knowledge update">
  <data key="d0">partial knowledge update</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Techniques that involve updating only specific parts or parameters of a language model to incorporate new information efficiently.&lt;SEP&gt;Techniques that involve updating only specific parts or parameters of a language model to incorporate new knowledge without retraining the entire model.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="parameter-efficient knowledge update">
  <data key="d0">parameter-efficient knowledge update</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies that modify a small subset of model parameters to update knowledge without retraining the entire model.&lt;SEP&gt;Strategies to modify a small subset of model parameters to update knowledge efficiently, avoiding full retraining.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="knowledge editing">
  <data key="d0">knowledge editing</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques aimed at directly locating and modifying specific parameters in a model to update facts or domain knowledge.&lt;SEP&gt;Techniques for directly modifying specific facts or knowledge within a model's parameters to correct or update information.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="hyper-network">
  <data key="d0">hyper-network</data>
  <data key="d1">Tools</data>
  <data key="d2">A network trained to update LLM parameters with minimal fine-tuning, used for knowledge editing.&lt;SEP&gt;A neural network trained to facilitate knowledge editing by updating model parameters with minimal fine-tuning, avoiding performance degradation.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="retrieval-based methods">
  <data key="d0">retrieval-based methods</data>
  <data key="d1">Tools</data>
  <data key="d2">Approaches that store and reason over explicit memory of edits to update model knowledge.&lt;SEP&gt;Approaches that store knowledge edits explicitly and reason over them to update model predictions, improving factual accuracy.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="localizing internal mechanisms">
  <data key="d0">localizing internal mechanisms</data>
  <data key="d1">Research Area</data>
  <data key="d2">Investigating specific neurons, attention mechanisms, or internal representations to facilitate targeted knowledge updates.&lt;SEP&gt;Understanding how specific neurons or attention mechanisms contribute to model predictions, facilitating targeted updates.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="knowledge encodings">
  <data key="d0">knowledge encodings</data>
  <data key="d1">Tools</data>
  <data key="d2">Internal representations within models that encode factual or domain knowledge, which can be manipulated for editing or probing.&lt;SEP&gt;Representations learned within models that can be manipulated to edit or probe knowledge.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="gradient masking">
  <data key="d0">gradient masking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that selectively masks gradients during backpropagation to update only certain parameters, reducing computational load and mitigating catastrophic forgetting.&lt;SEP&gt;Technique to selectively update parts of a model during fine-tuning by masking gradients, reducing computation and mitigating catastrophic forgetting.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="masking function">
  <data key="d0">masking function</data>
  <data key="d1">Tools</data>
  <data key="d2">A function that determines which model parameters' gradients are updated during training, based on relevance or importance.&lt;SEP&gt;Function that determines which model parameters' gradients are updated during training, based on relevance or importance.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="regularization techniques">
  <data key="d0">regularization techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Methods to prevent overfitting and improve training efficiency, often used in fine-tuning smaller models.&lt;SEP&gt;Methods used during training to prevent overfitting and control parameter updates, supporting efficient fine-tuning.</data>
  <data key="d3">chunk-ce8332c6d0b3d23f38189ff80650d4bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Updating">
  <data key="d0">Model Updating</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of modifying parts of a model to improve or adapt its performance, often involving masking gradients and selectively updating parameters.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Gradient Masking">
  <data key="d0">Gradient Masking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique used to selectively prevent certain model parameters from updating during training, enabling focused fine-tuning on relevant parts of the model.&lt;SEP&gt;A technique used to selectively prevent certain parameters' gradients from updating during training, to control which parts of the model are fine-tuned.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter Relevance">
  <data key="d0">Parameter Relevance</data>
  <data key="d1">Variables</data>
  <data key="d2">Criteria or measures used to determine how important specific parameters are for a given task or model update, guiding masking and fine-tuning decisions.&lt;SEP&gt;Criteria used to determine which parameters are important for a specific task or model update, guiding masking and fine-tuning decisions.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Regularization Techniques">
  <data key="d0">Regularization Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Methods such as penalties or constraints added during training to improve generalization and prevent overfitting, especially in small models.&lt;SEP&gt;Methods to prevent overfitting and improve training efficiency, especially in small language models, by adding constraints or penalties during training.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Child-Tuning">
  <data key="d0">Child-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A fine-tuning approach that uses downstream task data to identify task-related parameters as a 'child network' and freeze others to pre-trained weights, improving task adaptation.&lt;SEP&gt;A fine-tuning approach that uses downstream task data to identify task-relevant parameters as a 'child network' and freezes others to pre-trained weights.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dynamic Parameter Selection">
  <data key="d0">Dynamic Parameter Selection</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An adaptive algorithm that selects sub-networks or parameters based on gradients during back-propagation to enhance domain-specific fine-tuning, especially under low-resource scenarios.&lt;SEP&gt;An algorithm that adaptively selects sub-networks based on gradients during back-propagation to improve domain-specific fine-tuning, especially under low-resource scenarios.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Distillation">
  <data key="d0">Knowledge Distillation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), often to improve inference speed and domain adaptation.&lt;SEP&gt;A technique to transfer knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), often used to reduce inference latency and improve domain adaptation.&lt;SEP&gt;A technique where a smaller model learns from a larger 'teacher' model to improve efficiency and performance, with diminishing returns as model size increases.&lt;SEP&gt;A training technique where a smaller 'student' model learns from a larger 'teacher' model, used to improve efficiency and performance, with performance plateauing at larger sizes.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Compression">
  <data key="d0">Model Compression</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Reducing the size of large language models while maintaining performance, often through knowledge distillation or parameter reduction.&lt;SEP&gt;Techniques aimed at reducing the size and computational requirements of large language models while maintaining their performance, often via knowledge distillation or pruning.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-Tuning Challenges">
  <data key="d0">Fine-Tuning Challenges</data>
  <data key="d1">Limitations</data>
  <data key="d2">Open issues such as ensuring compliance with regulations, high computational resource demands, and maintaining relevance with rapidly changing data.&lt;SEP&gt;Open issues such as ensuring compliance with regulations, high computational resource requirements, and maintaining model relevance with evolving data.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Applications of Domain-Specific LLMs">
  <data key="d0">Applications of Domain-Specific LLMs</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Utilization of specialized large language models in various fields for tasks like information extraction, text generation, prediction, conversational agents, and code analysis.&lt;SEP&gt;Utilizing specialized large language models for tasks like information extraction, text generation, prediction, conversational agents, and code analysis across diverse fields.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Biomedical Applications">
  <data key="d0">Biomedical Applications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using LLMs for analyzing biomedical data, predicting biological functions, aiding in drug discovery, and supporting clinical healthcare.&lt;SEP&gt;Using LLMs for analyzing genomic data, predicting protein structures, and supporting clinical healthcare, advancing biomedical research and treatment.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Natural Science Applications">
  <data key="d0">Natural Science Applications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Employing LLMs in earth science and biomedicine for data analysis, hypothesis generation, and knowledge discovery.&lt;SEP&gt;Employing LLMs in earth science and biomedicine for data analysis, hypothesis generation, and scientific discovery.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Social Science Applications">
  <data key="d0">Social Science Applications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Applying LLMs in social sciences such as education, finance, and law for tasks like information extraction, summarization, and decision support.&lt;SEP&gt;Applying LLMs to education, finance, and law for tasks such as information extraction, summarization, and decision support.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Regulations and Compliance">
  <data key="d0">Regulations and Compliance</data>
  <data key="d1">Study Designs</data>
  <data key="d2">The process and criteria for updating or fine-tuning LLMs to ensure adherence to legal, ethical, and industry-specific standards.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="High-Performance Hardware">
  <data key="d0">High-Performance Hardware</data>
  <data key="d1">Tools</data>
  <data key="d2">Specialized computing hardware such as GPUs required for efficient fine-tuning and updating of large language models.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Data Augmentation">
  <data key="d0">Data Augmentation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques used to enhance training datasets with additional or synthetic data to improve model performance and domain adaptation.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instruction Crafting">
  <data key="d0">Instruction Crafting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Designing specific prompts or instructions to guide LLMs in performing domain-specific tasks effectively.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Update">
  <data key="d0">Knowledge Update</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Processes involved in refreshing LLMs with new data or domain knowledge to maintain relevance and accuracy.&lt;SEP&gt;The ability to modify RAG's knowledge base by swapping indices, demonstrating how non-parametric memory can be refreshed with new information.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Data Analysis">
  <data key="d0">Data Analysis</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Methods for examining domain-specific data with LLMs to derive insights, make predictions, or automate tasks.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Entity Recognition">
  <data key="d0">Entity Recognition</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Identifying and classifying entities such as genes, legal clauses, or biological terms from domain-specific texts using LLMs.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Text Summarization">
  <data key="d0">Text Summarization</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Generating concise summaries of complex domain-specific texts to facilitate understanding and decision-making.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prediction and Recommendation">
  <data key="d0">Prediction and Recommendation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using LLMs to forecast trends or suggest actions in fields like finance, medicine, and law based on domain data.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Conversational Agents">
  <data key="d0">Conversational Agents</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Deploying LLMs in chatbots or virtual assistants to provide domain-specific guidance and support.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Code Generation and Analysis">
  <data key="d0">Code Generation and Analysis</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using LLMs to generate, analyze, or improve code, identify bugs, or assist in software engineering tasks.</data>
  <data key="d3">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Useful in the field of biology">
  <data key="d0">Useful in the field of biology</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The text discusses the application of large language models (LLMs) across various aspects of biology, including fundamental biomedical research and clinical healthcare support, emphasizing their role in analyzing biological data, predicting functions, disease mechanisms, and assisting in medical record processing and medical image analysis.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fundamental biomedical research">
  <data key="d0">Fundamental biomedical research</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research focused on basic biological processes, utilizing data analysis and prediction of biological functions, disease mechanisms, and drug discovery.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Clinical healthcare support">
  <data key="d0">Clinical healthcare support</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Application of LLMs in analyzing medical records, assisting in diagnosis, personalized treatment, and medical image analysis to improve patient outcomes.&lt;SEP&gt;Use of LLMs in analyzing medical records, diagnosing, and providing personalized treatment, as well as aiding in medical image analysis to improve healthcare outcomes.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Genomic and proteomic data">
  <data key="d0">Genomic and proteomic data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Biological data types used to train LLMs for analyzing cellular functions, disease mechanisms, and drug discovery.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Protein structures and interactions">
  <data key="d0">Protein structures and interactions</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Critical biological features predicted by LLMs to understand cellular processes and assist in drug design.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Medical records">
  <data key="d0">Medical records</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Data sources for LLMs to identify patterns, assist in diagnoses, and inform treatment plans.&lt;SEP&gt;Electronic health records used by LLMs to identify patterns, assist diagnoses, and support treatment planning.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Medical image analysis">
  <data key="d0">Medical image analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Application of LLMs in analyzing medical images like X-rays and MRI scans using multi-modality learning to identify features.&lt;SEP&gt;Techniques involving LLMs and multi-modality learning to analyze X-rays, MRI scans for feature identification.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Earth Science">
  <data key="d0">Earth Science</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An interdisciplinary domain studying interactions between physical and human systems across spatial and temporal scales, involving methods like Earth observation, spatial analysis, and simulation modeling to investigate phenomena such as climate change, land-use change, and natural disasters.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Spatial information">
  <data key="d0">Spatial information</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Data critical to Earth science research, involving geographic and environmental datasets used for analysis and modeling.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Geographic information science tools">
  <data key="d0">Geographic information science tools</data>
  <data key="d1">Tools</data>
  <data key="d2">Technologies and methods used to analyze spatial data in Earth science studies.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Large language models like ChatGPT">
  <data key="d0">Large language models like ChatGPT</data>
  <data key="d1">Tools</data>
  <data key="d2">AI models capable of answering questions, providing code examples, and generating ideas related to Earth science research.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Climate scenario">
  <data key="d0">Climate scenario</data>
  <data key="d1">Results</data>
  <data key="d2">Generated predictions about future climate conditions using models like LLMs.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Earth observation dataset">
  <data key="d0">Earth observation dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Satellite or aerial data used for environmental monitoring and research.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Methods">
  <data key="d0">Methods</data>
  <data key="d1">Fine-tuning, few-shot, or zero-shot learning</data>
  <data key="d2">Methodologies</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Finance and Law">
  <data key="d0">Finance and Law</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Domains requiring specialized language understanding, including financial trends, regulatory norms, legal language, and court rulings, for accurate content generation and analysis.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Financial domain">
  <data key="d0">Financial domain</data>
  <data key="d1">Discipline</data>
  <data key="d2">Field focused on economic activities, requiring comprehension of financial terminologies, trends, and regulations for tasks like report generation and risk assessment.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal domain">
  <data key="d0">Legal domain</data>
  <data key="d1">Discipline</data>
  <data key="d2">Field involving understanding of laws, legal codes, and court rulings, demanding high precision and formal tone in language models.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model specialization">
  <data key="d0">Model specialization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Fine-tuning models with domain-specific datasets, incorporating explicit knowledge, and optimizing for specific objectives like compliance and accuracy.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ethical guardrails">
  <data key="d0">Ethical guardrails</data>
  <data key="d1">Limitations</data>
  <data key="d2">Safeguards implemented to ensure AI models operate ethically, especially in high-stakes domains like finance and law.&lt;SEP&gt;Safeguards to ensure AI models operate within ethical boundaries, especially in high-stakes domains like finance and law.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Human-computer interaction (HCI)">
  <data key="d0">Human-computer interaction (HCI)</data>
  <data key="d1">Discipline</data>
  <data key="d2">Study of designing and improving user interfaces and interactions, with LLMs tailored to understand and respond to user inputs effectively.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Software engineering">
  <data key="d0">Software engineering</data>
  <data key="d1">Discipline</data>
  <data key="d2">Field involving code generation, bug detection, and documentation, where LLMs are trained on large codebases and related data to assist developers.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Biomedical research">
  <data key="d0">Biomedical research</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Fundamental biomedical research involves studying biological processes and mechanisms, utilizing data analysis, prediction models, and experimental approaches to understand health and disease.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Genomic data">
  <data key="d0">Genomic data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Genomic data encompasses DNA sequences used to train models for understanding biological functions and disease mechanisms.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Proteomic data">
  <data key="d0">Proteomic data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Proteomic data includes protein expression and interaction information used in biological analysis and drug discovery.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Protein structures">
  <data key="d0">Protein structures</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">3D conformations of proteins predicted to understand cellular functions and aid in drug design.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Earth observation">
  <data key="d0">Earth observation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Satellite and aerial data used to monitor environmental changes, climate phenomena, land-use, and natural disasters.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Spatial analysis">
  <data key="d0">Spatial analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for analyzing geographic and environmental spatial data to understand physical and human system interactions.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Climate change">
  <data key="d0">Climate change</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A significant environmental phenomenon studied across spatial and temporal scales, involving data analysis, modeling, and scenario development.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Land-use change">
  <data key="d0">Land-use change</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Transformation of land resources over time, impacting ecosystems and human development, studied through spatial and environmental data.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Natural disasters">
  <data key="d0">Natural disasters</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Events like earthquakes, floods, and storms analyzed to understand causes, impacts, and mitigation strategies.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Environmental development">
  <data key="d0">Environmental development</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Study of human impacts on the environment, including urbanization, pollution, and resource management.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Urbanization">
  <data key="d0">Urbanization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The expansion and development of urban areas, studied through spatial data and environmental impact assessments.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Google Earth Engine">
  <data key="d0">Google Earth Engine</data>
  <data key="d1">Tools</data>
  <data key="d2">A cloud-based platform for processing and analyzing satellite data, used in Earth science research.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Question-answer systems">
  <data key="d0">Question-answer systems</data>
  <data key="d1">Tools</data>
  <data key="d2">AI systems like ChatGPT that provide relevant information, code examples, and knowledge support in Earth science.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Climate scenario generation">
  <data key="d0">Climate scenario generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Use of LLMs to develop future climate models and scenarios for environmental planning.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Few-shot learning">
  <data key="d0">Few-shot learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Training approach where LLMs learn from limited examples to adapt to specific tasks in Earth science and other domains.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-shot learning">
  <data key="d0">Zero-shot learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Technique enabling LLMs to perform tasks without explicit prior training on specific examples, useful in Earth science applications.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Financial trends">
  <data key="d0">Financial trends</data>
  <data key="d1">Variables</data>
  <data key="d2">Economic indicators and market movements analyzed by LLMs for financial modeling and risk assessment.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Regulatory norms">
  <data key="d0">Regulatory norms</data>
  <data key="d1">Variables</data>
  <data key="d2">Legal and financial regulations that models must understand and incorporate for accurate content generation.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal language">
  <data key="d0">Legal language</data>
  <data key="d1">Variables</data>
  <data key="d2">Complex legal terminology and syntax that require precise understanding for legal domain modeling.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Laws and court rulings">
  <data key="d0">Laws and court rulings</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Legal documents and precedents used to train models for understanding legal contexts.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="User interaction logs">
  <data key="d0">User interaction logs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Data capturing user inputs and interactions used to train and refine HCI models.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Codebases, issue trackers, documentation">
  <data key="d0">Codebases, issue trackers, documentation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Software-related data used to train LLMs for code generation, bug detection, and documentation.</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Balancing General and Domain Knowledge">
  <data key="d0">Balancing General and Domain Knowledge</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The need for large language models (LLMs) to maintain a balance between understanding specific domain details and retaining broad general knowledge to provide contextually appropriate responses.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Explainability and Trust">
  <data key="d0">Explainability and Trust</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The importance of making LLM decision-making processes transparent to build user trust, especially in high-stakes domains, and the trade-off between model complexity and explainability.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapting to Domain Evolution">
  <data key="d0">Adapting to Domain Evolution</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The necessity for LLMs to evolve with their respective domains by incorporating new terminologies, concepts, and trends to stay relevant.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scalability">
  <data key="d0">Scalability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Scalability measures how well the system or algorithm performs as the workload or resources increase, critical for large applications.&lt;SEP&gt;The ability of generated parallel code to perform effectively as problem size or resources increase.&lt;SEP&gt;The challenge of efficiently scaling domain-specific training or fine-tuning of LLMs across many domains, considering computational resources, data availability, and expertise.&lt;SEP&gt;The study hypothesizes that language models can generate and translate parallel code that scales effectively across different resource counts and execution models.&lt;SEP&gt;The study investigates how well language models can translate and generate scalable parallel code across different resource counts and execution models.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hybrid Approaches">
  <data key="d0">Hybrid Approaches</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Combining multiple domain specialization methods (black-box, grey-box, white-box) at different stages to optimize performance and resource use, especially with scarce data.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Meta-Learning or AutoML Techniques">
  <data key="d0">Meta-Learning or AutoML Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Automating the selection of optimal strategies for domain adaptation in LLMs using meta-learning or AutoML, reducing resource and expertise requirements.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Incorporating More Explicit World Knowledge">
  <data key="d0">Incorporating More Explicit World Knowledge</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Leveraging structured knowledge sources like knowledge graphs and graph neural networks to enhance LLM understanding of domain relationships, e.g., medical ontologies.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Human-in-the-loop Learning">
  <data key="d0">Human-in-the-loop Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An iterative training approach where human feedback continuously guides model updates, correction, and refinement.&lt;SEP&gt;Continuous interaction with human experts to guide and update LLMs, ensuring relevance and accuracy through feedback and expert input.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Active Learning">
  <data key="d0">Active Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A strategy where models actively query users or sources for information on uncertain or unfamiliar concepts to improve learning and accuracy.&lt;SEP&gt;Allowing models to query for clarification or additional information on unfamiliar domain concepts, improving handling of specialized topics.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Future Techniques in Domain Specialization">
  <data key="d0">Future Techniques in Domain Specialization</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Emerging approaches such as hybrid methods, meta-learning, structured knowledge integration, human-in-the-loop, and active learning aimed at advancing domain-specific large language models.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Trust">
  <data key="d0">Trust</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">User confidence in the system's outputs, which depends on the model's transparency, reliability, and explainability.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Specialization">
  <data key="d0">Model Specialization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of tailoring a large language model to perform well within a specific domain by training or fine-tuning with domain-specific data.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain Evolution">
  <data key="d0">Domain Evolution</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The ongoing change and development of domain-specific knowledge, terminologies, and concepts over time, requiring models to adapt.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Data Scarcity">
  <data key="d0">Data Scarcity</data>
  <data key="d1">Variables</data>
  <data key="d2">Limited availability of domain-specific data necessary for training or fine-tuning models effectively.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Challenges">
  <data key="d0">Scaling Challenges</data>
  <data key="d1">Limitations</data>
  <data key="d2">The difficulties related to expanding domain-specific training or adaptation processes across multiple or large-scale domains, including computational and data constraints.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Black-box Methods">
  <data key="d0">Black-box Methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches to LLM development where internal workings are not transparent, posing challenges for understanding and domain-specific adaptation.&lt;SEP&gt;Techniques where the internal workings of the model are not interpretable, often used in initial or complex modeling scenarios.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Grey-box Methods">
  <data key="d0">Grey-box Methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches combining some interpretability with complexity, allowing partial understanding of model processes.&lt;SEP&gt;Hybrid approaches that combine elements of transparency and opacity in LLMs to facilitate domain adaptation while maintaining some interpretability.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="White-box Methods">
  <data key="d0">White-box Methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques that emphasize transparency and interpretability in LLMs, enabling detailed understanding and customization for specific domains.&lt;SEP&gt;Techniques with fully interpretable models, enabling transparency and detailed understanding of decision processes.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4&lt;SEP&gt;chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Graphs">
  <data key="d0">Knowledge Graphs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Structured representations of domain knowledge capturing entities, relationships, and hierarchies, used to augment model understanding.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Graph Neural Networks">
  <data key="d0">Graph Neural Networks</data>
  <data key="d1">Tools</data>
  <data key="d2">Neural network architectures designed to operate on graph-structured data, facilitating integration of structured knowledge into models.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Meta-Learning">
  <data key="d0">Meta-Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques enabling models to learn how to learn, optimizing the selection of training strategies or parameters based on prior experience.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AutoML">
  <data key="d0">AutoML</data>
  <data key="d1">Tools</data>
  <data key="d2">Automated machine learning systems that optimize model selection, hyperparameter tuning, and training processes to improve efficiency.</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Medical Ontology">
  <data key="d0">Medical Ontology</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A structured framework that organizes medical knowledge, enabling effective information retrieval, integration, and reasoning within healthcare and biomedical domains.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Domain-Specific Techniques">
  <data key="d0">Domain-Specific Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques designed to tailor large language models (LLMs) for specific fields, addressing challenges like limited expertise, knowledge elicitation, and model complexity.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Elicitation">
  <data key="d0">Knowledge Elicitation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Procedures and processes used to extract, gather, and incorporate domain-specific knowledge into LLMs to enhance their performance and accuracy.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Challenges and Limitations">
  <data key="d0">Challenges and Limitations</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current obstacles faced in domain-specific LLM deployment, including issues like knowledge gaps, model interpretability, and resource constraints.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Elicitation Techniques">
  <data key="d0">Knowledge Elicitation Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Procedures for extracting and integrating domain-specific knowledge into LLMs to improve their relevance and performance.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Interpretability">
  <data key="d0">Model Interpretability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model interpretability involves understanding how models produce their outputs, which is enhanced in RAG due to explicit retrieval mechanisms.&lt;SEP&gt;The degree to which the inner workings of a model are understandable to humans, impacting trust and usability in domain-specific contexts.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Challenges in Domain Adaptation">
  <data key="d0">Challenges in Domain Adaptation</data>
  <data key="d1">Limitations</data>
  <data key="d2">Obstacles like knowledge scarcity, model opacity, and resource constraints that limit effective domain-specific deployment of LLMs.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Transfer">
  <data key="d0">Knowledge Transfer</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Processes for applying domain knowledge to train or adapt LLMs, enhancing their accuracy and relevance in specific fields.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Interdisciplinary Collaboration">
  <data key="d0">Interdisciplinary Collaboration</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Collaborative efforts across fields like AI, domain sciences, and engineering to develop effective domain-specific LLMs.</data>
  <data key="d3">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-Head Adapter Routing">
  <data key="d0">Multi-Head Adapter Routing</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A method for data-efficient fine-tuning involving multiple adapter heads to improve model adaptability and performance.&lt;SEP&gt;A method for data-efficient fine-tuning of models using multiple adapter heads to improve performance and flexibility.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Data-Efficient Fine-Tuning">
  <data key="d0">Data-Efficient Fine-Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach aimed at optimizing model performance with less data, often involving specialized techniques like adapter routing.&lt;SEP&gt;Techniques and approaches aimed at training models effectively with less data, often utilizing adapter routing mechanisms.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2211.03831">
  <data key="d0">arXiv preprint arXiv:2211.03831</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A preprint publication presenting research on multi-head adapter routing for data-efficient fine-tuning.&lt;SEP&gt;A research paper presenting the design and evaluation of multi-head adapter routing for efficient fine-tuning.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yihan Cao">
  <data key="d0">Yihan Cao</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author of a comprehensive survey on AI-generated content, covering history and development of generative AI technologies.&lt;SEP&gt;Author of a comprehensive survey on AI-generated content, covering the evolution from GANs to ChatGPT.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AI-generated Content (AIGC)">
  <data key="d0">AI-generated Content (AIGC)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The field focused on automatically generating multimedia content using AI models, including text, images, and videos.&lt;SEP&gt;The field of artificial intelligence focused on creating content such as text, images, and videos through generative models like GANs and ChatGPT.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Generative AI">
  <data key="d0">Generative AI</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A class of models designed to generate new data resembling training data, including GANs, transformers, and diffusion models.&lt;SEP&gt;Models that generate new data resembling training data, such as GANs, transformers, and diffusion models.&lt;SEP&gt;Generative AI encompasses AI systems capable of producing code and content, with potential to impact software development, maintenance, and education in HPC.&lt;SEP&gt;Generative AI refers to AI systems capable of producing code and other content, with potential benefits for HPC software development, education, and modernization.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2303.04226">
  <data key="d0">arXiv preprint arXiv:2303.04226</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A publication reviewing the evolution and history of generative AI, including models from GANs to ChatGPT.&lt;SEP&gt;A survey documenting the history and development of generative AI technologies from early models to ChatGPT.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Derivative-free Prompt Learning">
  <data key="d0">Derivative-free Prompt Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for training models by optimizing prompts without gradient information, often involving reward signals.&lt;SEP&gt;An approach to training language models by optimizing prompts directly through reward signals without gradient information.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference where research such as clip-tuning is presented, focusing on empirical NLP methods.&lt;SEP&gt;A key conference where NLP methods like RLPrompt were presented, focusing on empirical evaluation of language model techniques.&lt;SEP&gt;A key conference where NLP prompt optimization methods like RLPrompt were introduced and evaluated.&lt;SEP&gt;An academic conference where research such as clip-tuning is presented, focusing on empirical NLP methodologies.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LEGAL-BERT">
  <data key="d0">LEGAL-BERT</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A BERT-based language model trained on legal texts to enhance NLP tasks in the legal domain.&lt;SEP&gt;A BERT-based language model trained on legal texts to improve performance on legal NLP tasks.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Law School">
  <data key="d0">Law School</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An educational institution providing legal training, from which legal language data for training LEGAL-BERT was sourced.&lt;SEP&gt;An institution providing legal education, from which the legal language data for LEGAL-BERT was derived.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal NLP">
  <data key="d0">Legal NLP</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The application of NLP techniques to legal texts for tasks like information extraction, classification, and legal reasoning.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Recall and learn">
  <data key="d0">Recall and learn</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates methods for fine-tuning deep pretrained language models with less forgetting, aiming to improve learning efficiency.&lt;SEP&gt;Investigates methods to fine-tune deep pretrained language models with minimal forgetting, aiming to improve continual learning.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuning Deep Pretrained Language Models">
  <data key="d0">Fine-tuning Deep Pretrained Language Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for adapting large language models to specific tasks while preserving prior knowledge.&lt;SEP&gt;Techniques to adapt large language models to specific tasks while minimizing catastrophic forgetting.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Program of Thoughts Prompting">
  <data key="d0">Program of Thoughts Prompting</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A prompting technique designed to disentangle computation from reasoning, thereby improving numerical reasoning in language models.&lt;SEP&gt;A prompting technique that disentangles computation from reasoning to enhance numerical reasoning tasks in language models.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Numerical Reasoning Tasks">
  <data key="d0">Numerical Reasoning Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks involving mathematical and logical reasoning that require models to perform accurate calculations and inferences.&lt;SEP&gt;Tasks requiring models to perform mathematical and logical reasoning accurately.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2211.12588">
  <data key="d0">arXiv preprint arXiv:2211.12588</data>
  <data key="d1">Study Design</data>
  <data key="d2">A publicly available preprint publication detailing research on prompting techniques for numerical reasoning.&lt;SEP&gt;A research paper proposing a prompting method to enhance reasoning capabilities in language models.&lt;SEP&gt;A research paper proposing a prompting method to improve reasoning in language models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Relation Extraction">
  <data key="d0">Relation Extraction</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A natural language processing task focused on identifying and classifying semantic relationships between entities in text.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Graphs for Healthcare">
  <data key="d0">Knowledge Graphs for Healthcare</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Structured data representations capturing medical and health-related information to support healthcare applications.&lt;SEP&gt;Structured representations of medical and health-related information used to enhance healthcare applications.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Healthcare">
  <data key="d0">Healthcare</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A field involving medical sciences and health informatics, utilizing knowledge graphs for resource integration and application.&lt;SEP&gt;A field involving medical sciences and health informatics, utilizing knowledge graphs for resource management and decision support.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Binding Language Models in Symbolic Languages">
  <data key="d0">Binding Language Models in Symbolic Languages</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Integrating language models with symbolic languages to improve reasoning, interpretability, and structured knowledge representation.&lt;SEP&gt;Integrating neural language models with symbolic representations to enhance reasoning, interpretability, and structured knowledge processing.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Language Modeling with Pathways">
  <data key="d0">Scaling Language Modeling with Pathways</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework designed to scale language models efficiently by routing computations through multiple pathways, enabling handling of large-scale data.&lt;SEP&gt;A framework for scaling language models efficiently by routing computations through pathways to handle large-scale data.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2204.02311">
  <data key="d0">arXiv preprint arXiv:2204.02311</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint detailing the pathway scaling approach for language models.&lt;SEP&gt;A publication describing the Pathways architecture for scalable language modeling.&lt;SEP&gt;A publication describing the Pathways architecture for scalable, efficient language model training.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Deep Reinforcement Learning from Human Preferences">
  <data key="d0">Deep Reinforcement Learning from Human Preferences</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research exploring how human feedback can guide reinforcement learning agents to align with human values and preferences.&lt;SEP&gt;Research exploring how human preferences can guide the training of reinforcement learning agents to align behaviors with human values.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Human Preferences">
  <data key="d0">Human Preferences</data>
  <data key="d1">Variables</data>
  <data key="d2">Preferences expressed by humans that influence the training and alignment of reinforcement learning models.&lt;SEP&gt;Preferences expressed by humans that influence the training and behavior of reinforcement learning models.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="AdapterSoup">
  <data key="d0">AdapterSoup</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A method involving the averaging of multiple pretrained adapters' weights to improve model generalization and robustness.&lt;SEP&gt;A method involving weight averaging of multiple pretrained model adapters to improve generalization and robustness.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Weight Averaging">
  <data key="d0">Weight Averaging</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">A technique for combining multiple model weights to produce a more stable and generalizable model.&lt;SEP&gt;A technique used to combine multiple model weights to enhance model performance and stability.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pretrained Language Models">
  <data key="d0">Pretrained Language Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large language models trained on vast corpora, serving as the foundation for various NLP tasks.&lt;SEP&gt;Large-scale neural models trained on extensive corpora, serving as foundations for numerous NLP tasks.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Instruction-Finetuned Language Models">
  <data key="d0">Scaling Instruction-Finetuned Language Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches to enhance language models by fine-tuning with instructions to improve task adaptability.&lt;SEP&gt;Techniques for enhancing language models by fine-tuning with task instructions to improve adaptability and performance.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2210.11416">
  <data key="d0">arXiv preprint arXiv:2210.11416</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A research publication on methods for scaling instruction-finetuned models.&lt;SEP&gt;A study investigating methods for scaling instruction-finetuning of language models.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Neurons in Transformers">
  <data key="d0">Knowledge Neurons in Transformers</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Identification of specific neurons within pretrained transformers that encode factual knowledge.&lt;SEP&gt;Identification of specific neurons within transformer models that encode factual knowledge, enabling interpretability.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge in Transformers">
  <data key="d0">Knowledge in Transformers</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Factual information stored within neural network models used for various NLP tasks.&lt;SEP&gt;Factual information stored within neural network neurons used for reasoning and knowledge retrieval.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2104.08696">
  <data key="d0">arXiv preprint arXiv:2104.08696</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A research analyzing how factual knowledge is represented and stored in transformer models.&lt;SEP&gt;A study analyzing how knowledge is represented and stored in transformer models.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Promptagator">
  <data key="d0">Promptagator</data>
  <data key="d1">Tools</data>
  <data key="d2">A dense retrieval system that uses few-shot learning with only 8 examples to improve information retrieval performance.&lt;SEP&gt;A few-shot retrieval system designed to perform dense information retrieval using only a small number of examples, such as 8.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Few-shot Dense Retrieval">
  <data key="d0">Few-shot Dense Retrieval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A retrieval approach that leverages minimal annotated examples to perform effective document or data retrieval.&lt;SEP&gt;A retrieval technique that leverages minimal examples to perform effective information search in large datasets.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2209.11755">
  <data key="d0">arXiv preprint arXiv:2209.11755</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A publication detailing the Promptagator system for few-shot dense retrieval tasks.&lt;SEP&gt;A publication presenting the Promptagator system for few-shot retrieval.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Collaborating with Language Models for Embodied Reasoning">
  <data key="d0">Collaborating with Language Models for Embodied Reasoning</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Explores how language models can be integrated with embodied agents to perform reasoning in physical or simulated environments.&lt;SEP&gt;Investigates how language models can be integrated with embodied agents to perform reasoning tasks involving physical or simulated environments.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Embodied Reasoning">
  <data key="d0">Embodied Reasoning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of reasoning that involves interaction with physical or simulated environments, often using language models.&lt;SEP&gt;The process of reasoning that involves interaction with physical or virtual environments, often requiring language models to interpret and act.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Second Workshop on Language and Reinforcement Learning">
  <data key="d0">Second Workshop on Language and Reinforcement Learning</data>
  <data key="d1">Study Design</data>
  <data key="d2">A workshop where research on combining language models and reinforcement learning, including embodied reasoning, is presented.&lt;SEP&gt;A workshop where research on language models and reinforcement learning, including embodied reasoning, is presented.&lt;SEP&gt;An academic workshop dedicated to exploring the integration of language models with reinforcement learning techniques.&lt;SEP&gt;An academic workshop focused on exploring the integration of language models with reinforcement learning techniques.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71&lt;SEP&gt;chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Editing Factual Knowledge in Language Models">
  <data key="d0">Editing Factual Knowledge in Language Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques aimed at modifying or correcting factual information stored within language models.&lt;SEP&gt;Techniques for modifying or updating the factual knowledge stored within large language models to correct errors or incorporate new information.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Factual Knowledge">
  <data key="d0">Factual Knowledge</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Factual knowledge refers to accurate, real-world information that models like RAG aim to incorporate to reduce hallucinations and improve trustworthiness.&lt;SEP&gt;Factual knowledge refers to verified information that language models aim to learn, store, and retrieve.&lt;SEP&gt;Factual knowledge refers to verified information that models aim to learn, store, and accurately retrieve.&lt;SEP&gt;Information encoded within models about real-world facts, which can be edited or updated to improve accuracy.&lt;SEP&gt;The information that models encode about the world, which can be edited or updated.</data>
  <data key="d3">chunk-d8b0b9da667ca7eef007d5dabeba89a9&lt;SEP&gt;chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Gupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus">
  <data key="d0">Gupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus</data>
  <data key="d1">Researcher Group</data>
  <data key="d2">A collective of researchers working on language models, embodied reasoning, and related NLP topics, publishing in workshops and conferences.&lt;SEP&gt;A group of researchers involved in studies related to language models and embodied reasoning, publishing in workshops and conferences in the field of natural language processing and machine learning.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Nicola De Cao, Wilker Aziz, Ivan Titov">
  <data key="d0">Nicola De Cao, Wilker Aziz, Ivan Titov</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who conducted studies on editing factual knowledge in language models, aiming to improve factual accuracy and consistency.&lt;SEP&gt;Researchers who conducted studies on editing factual knowledge in language models, contributing to understanding and improving model accuracy.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="2021 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">2021 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A major NLP conference where empirical research on language model editing and knowledge manipulation was presented.&lt;SEP&gt;A major conference where research on empirical methods in NLP, including editing factual knowledge, was presented.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, Zhiting Hu">
  <data key="d0">Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, Zhiting Hu</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who developed RLPrompt, a reinforcement learning-based method for optimizing discrete text prompts, advancing prompt engineering.&lt;SEP&gt;Researchers who developed RLPrompt, an approach for optimizing discrete text prompts using reinforcement learning, advancing prompt engineering techniques.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova">
  <data key="d0">Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who created BERT, a deep bidirectional transformer model pre-trained for language understanding, foundational for many NLP applications.&lt;SEP&gt;Researchers who created BERT, a deep bidirectional transformer pre-training model, foundational to many NLP applications.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:1810.04805">
  <data key="d0">arXiv preprint arXiv:1810.04805</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">A preprint publication detailing the BERT model, serving as a primary source for foundational NLP methodology.&lt;SEP&gt;Preprint publication detailing BERT, serving as a primary source for transformer-based language understanding models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al">
  <data key="d0">Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers studying delta tuning, a parameter-efficient method for adapting pre-trained language models, contributing to efficient NLP model fine-tuning.&lt;SEP&gt;Researchers who studied delta tuning, a parameter-efficient method for adapting pre-trained language models to new tasks with minimal additional parameters.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv">
  <data key="d0">arXiv</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">A platform for sharing preprints of scientific papers, including research on language models and AI methodologies.&lt;SEP&gt;A preprint repository for early dissemination of scientific research, hosting papers on language models, retrieval, and continual learning.&lt;SEP&gt;A preprint repository hosting surveys and research on domain adaptation and generalization in NLP models.&lt;SEP&gt;A repository for preprints where delta tuning research was published, providing accessible scientific evidence.&lt;SEP&gt;Repository hosting preprints on NLP model adaptation, domain generalization, and prompt tuning.&lt;SEP&gt;Repository hosting preprints on delta tuning and related parameter-efficient methods for NLP models.&lt;SEP&gt;arXiv is an open-access preprint repository for scientific papers, used here to publish research on language models.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3&lt;SEP&gt;chunk-00db2a825cc87520492b47392ed31c71&lt;SEP&gt;chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou">
  <data key="d0">Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers working on compositional semantic parsing with large language models, advancing understanding of model reasoning capabilities.&lt;SEP&gt;Researchers working on compositional semantic parsing with large language models, focusing on improving model reasoning and compositional understanding.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="The Eleventh International Conference on Learning Representations">
  <data key="d0">The Eleventh International Conference on Learning Representations</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference focusing on advances in representation learning, including continual learning and prompting techniques for language models.&lt;SEP&gt;A leading conference where research on semantic parsing and reasoning with large language models was presented.&lt;SEP&gt;A prominent conference where research on semantic parsing using large language models was presented.&lt;SEP&gt;An academic conference focusing on advancements in representation learning, including continual learning for language models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71&lt;SEP&gt;chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner">
  <data key="d0">Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers exploring successive prompting techniques for decomposing complex questions in NLP, improving question-answering systems.&lt;SEP&gt;Researchers exploring successive prompting techniques for decomposing complex questions, enhancing multi-step reasoning in NLP systems.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceedings publication where research on prompt techniques for complex question decomposition was shared.&lt;SEP&gt;Conference proceedings documenting research on prompt-based question decomposition and complex reasoning.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, Mehdi Rezagholizadeh">
  <data key="d0">Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, Mehdi Rezagholizadeh</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who developed KronA, a parameter-efficient tuning approach utilizing Kronecker adapters for scalable NLP model adaptation.&lt;SEP&gt;Researchers who developed KronA, a parameter-efficient tuning method using Kronecker adapters for NLP models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2212.10650">
  <data key="d0">arXiv preprint arXiv:2212.10650</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint detailing KronA, providing evidence for parameter-efficient tuning techniques.&lt;SEP&gt;Preprint detailing KronA, supporting efficient parameter adaptation techniques for large-scale language models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette">
  <data key="d0">Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who proposed P ODA, a prompt-driven zero-shot domain adaptation method to improve model generalization across diverse data distributions.&lt;SEP&gt;Researchers working on P ODA, a prompt-driven zero-shot domain adaptation method, enhancing model generalization across domains.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2212.03241">
  <data key="d0">arXiv preprint arXiv:2212.03241</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint presenting P ODA, serving as evidence for domain adaptation techniques in NLP.&lt;SEP&gt;Preprint presenting P ODA, providing evidence for prompt-based domain adaptation strategies in NLP.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel">
  <data key="d0">Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers focused on memory-augmented neural machine translation, integrating memory modules to improve translation quality.&lt;SEP&gt;Researchers who developed memory-augmented neural machine translation, integrating memory modules to enhance translation performance.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference where research on memory-augmented neural translation was presented, advancing neural translation methods.&lt;SEP&gt;Conference where research on memory-augmented neural translation was published, advancing neural translation techniques.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig">
  <data key="d0">Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who developed PAL, a program-aided language model framework that incorporates programmatic reasoning to enhance language understanding.&lt;SEP&gt;Researchers who developed PAL, a program-aided language model framework that incorporates programmatic reasoning to improve language understanding and reasoning.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2211.10435">
  <data key="d0">arXiv preprint arXiv:2211.10435</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint documenting PAL, providing evidence for program-assisted language modeling.&lt;SEP&gt;Preprint describing PAL, supporting the integration of programming and language models for enhanced reasoning capabilities.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, Gao Huang">
  <data key="d0">Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, Gao Huang</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers working on domain adaptation via prompt learning, aiming to improve model performance across different data domains and tasks.&lt;SEP&gt;Researchers working on domain adaptation via prompt learning, aiming to improve model performance across different data domains.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2202.06687">
  <data key="d0">arXiv preprint arXiv:2202.06687</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint illustrating domain adaptation through prompt learning, supporting methods for domain generalization.&lt;SEP&gt;Preprint demonstrating methods for domain adaptation through prompt learning techniques.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Edouard Grave, Armand Joulin, Nicolas Usunier">
  <data key="d0">Edouard Grave, Armand Joulin, Nicolas Usunier</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who enhanced neural language models with a continuous cache, improving language modeling by leveraging cached data.&lt;SEP&gt;Researchers who improved neural language models with a continuous cache, enhancing language modeling by leveraging recent context and cached data.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="International Conference on Learning Representations">
  <data key="d0">International Conference on Learning Representations</data>
  <data key="d1">Conference</data>
  <data key="d2">A conference where work on continuous cache methods for neural language models was presented.&lt;SEP&gt;Conference where work on cache-based neural language models was presented, showing improvements in language modeling efficiency.&lt;SEP&gt;A major conference where research on neural text degeneration was presented.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang">
  <data key="d0">Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who developed PPT, a pre-trained prompt tuning method for few-shot learning, aiming to improve sample efficiency in NLP tasks.&lt;SEP&gt;Researchers who developed PPT, a pre-trained prompt tuning method for few-shot learning, improving data efficiency in NLP tasks.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Proceedings of the 60th Annual Meeting of the ACL">
  <data key="d0">Proceedings of the 60th Annual Meeting of the ACL</data>
  <data key="d1">Study Design</data>
  <data key="d2">A major NLP conference where PPT was presented, emphasizing few-shot learning techniques.&lt;SEP&gt;Major NLP conference where PPT was presented, emphasizing few-shot learning and prompt tuning methods.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xu Guo, Boyang Li, Han Yu">
  <data key="d0">Xu Guo, Boyang Li, Han Yu</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers studying sample efficiency and domain adaptation for prompt tuning, enhancing model training with less data and better generalization.&lt;SEP&gt;Researchers studying sample-efficient prompt tuning and domain adaptation, aiming to enhance generalization with less data.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Findings of the Association for Computational Linguistics: EMNLP 2022">
  <data key="d0">Findings of the Association for Computational Linguistics: EMNLP 2022</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceedings publication where domain adaptation in prompt tuning was discussed.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xu Guo and Han Yu">
  <data key="d0">Xu Guo and Han Yu</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers conducting surveys on domain adaptation and generalization of pretrained language models, summarizing current challenges and methods.&lt;SEP&gt;Researchers conducting surveys on domain adaptation and generalization of pretrained language models, summarizing current challenges and techniques.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Karen Hambardzumyan, Hrant Khachatrian, Jonathan May">
  <data key="d0">Karen Hambardzumyan, Hrant Khachatrian, Jonathan May</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who proposed WARP, an adversarial reprogramming method at the word level to reprogram NLP models for new tasks.&lt;SEP&gt;Researchers who proposed WARP, an adversarial reprogramming technique at the word level, for NLP model reprogramming.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Annual Meeting of the Association for Computational Linguistics">
  <data key="d0">Annual Meeting of the Association for Computational Linguistics</data>
  <data key="d1">Study Design</data>
  <data key="d2">A major conference where adversarial reprogramming techniques like WARP were presented.&lt;SEP&gt;Major conference where adversarial reprogramming approaches like WARP were introduced and discussed.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hangfeng He, Hongming Zhang, Dan Roth">
  <data key="d0">Hangfeng He, Hongming Zhang, Dan Roth</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers rethinking large language model inference strategies, incorporating retrieval techniques to improve inference faithfulness and accuracy.&lt;SEP&gt;Researchers rethinking large language model inference using retrieval techniques to improve faithfulness and accuracy.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2301.00303">
  <data key="d0">arXiv preprint arXiv:2301.00303</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint demonstrating retrieval-based inference methods for large language models, providing evidence for improved inference strategies.&lt;SEP&gt;Preprint demonstrating retrieval-based inference methods for large language models, supporting improved inference fidelity.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig">
  <data key="d0">Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers who proposed a unified framework for transfer learning focusing on parameter-efficient adaptation methods.&lt;SEP&gt;Researchers who unified transfer learning approaches focusing on parameter-efficient methods, contributing to efficient model adaptation.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2110.04366">
  <data key="d0">arXiv preprint arXiv:2110.04366</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint detailing transfer learning strategies, serving as evidence for parameter-efficient adaptation techniques.&lt;SEP&gt;Preprint detailing transfer learning strategies for efficient model adaptation, supporting parameter-efficient transfer learning.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shwai He, Liang Ding, Daize Dong, Miao Zhang, Dacheng Tao">
  <data key="d0">Shwai He, Liang Ding, Daize Dong, Miao Zhang, Dacheng Tao</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers proposing Sparseadapter, a method to improve the parameter efficiency of adapters in language models.&lt;SEP&gt;Researchers who developed Sparseadapter, an approach to improve the parameter efficiency of adapters in large language models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2210">
  <data key="d0">arXiv preprint arXiv:2210</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">An arXiv preprint presenting Sparseadapter, supporting methods to enhance parameter efficiency of adapters.&lt;SEP&gt;Preprint presenting Sparseadapter, supporting methods to improve parameter efficiency of adapters in NLP models.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Findings of the ACL: EMNLP 2022">
  <data key="d0">Findings of the ACL: EMNLP 2022</data>
  <data key="d1">Study Design</data>
  <data key="d2">Conference proceedings discussing advances in prompt tuning, domain adaptation, and generalization in NLP.</data>
  <data key="d3">chunk-00db2a825cc87520492b47392ed31c71</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ma">
  <data key="d0">Ma</data>
  <data key="d1">Researcher</data>
  <data key="d2">Ma is an author contributing to studies on transfer learning and natural language processing.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Taylor Berg-Kirkpatrick">
  <data key="d0">Taylor Berg-Kirkpatrick</data>
  <data key="d1">Researcher</data>
  <data key="d2">Taylor Berg-Kirkpatrick is an author involved in research on parameter-efficient transfer learning.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Graham Neubig">
  <data key="d0">Graham Neubig</data>
  <data key="d1">Researcher</data>
  <data key="d2">Graham Neubig is a researcher contributing to the development of transfer learning models in NLP.&lt;SEP&gt;Graham Neubig is a researcher contributing to the study of transfer learning models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shwai He">
  <data key="d0">Shwai He</data>
  <data key="d1">Researcher</data>
  <data key="d2">Shwai He is an author of work on improving parameter-efficiency of adapters in NLP models.&lt;SEP&gt;Shwai He is an author of work on improving parameter-efficiency of adapters in NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Liang Ding">
  <data key="d0">Liang Ding</data>
  <data key="d1">Researcher</data>
  <data key="d2">Liang Ding is a researcher involved in developing sparse adapters for NLP models.&lt;SEP&gt;Liang Ding is a researcher involved in developing sparse adapters for NLP transfer learning.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Daize Dong">
  <data key="d0">Daize Dong</data>
  <data key="d1">Researcher</data>
  <data key="d2">Daize Dong is an author contributing to research on parameter-efficient NLP techniques.&lt;SEP&gt;Daize Dong is an author contributing to research on parameter-efficient transfer learning methods.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Miao Zhang">
  <data key="d0">Miao Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Miao Zhang is a researcher involved in studies on efficient transfer learning methods.&lt;SEP&gt;Miao Zhang is a researcher involved in studies on efficient transfer learning techniques in NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dacheng Tao">
  <data key="d0">Dacheng Tao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Dacheng Tao is an author of studies on neural activation functions, including Gaussian error linear units (GELUs).&lt;SEP&gt;Dacheng Tao is an author of studies on transfer learning and model efficiency.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dan Hendrycks">
  <data key="d0">Dan Hendrycks</data>
  <data key="d1">Researcher</data>
  <data key="d2">Dan Hendrycks contributed to the development of Gaussian error linear units (GELUs) for neural networks.&lt;SEP&gt;Dan Hendrycks is an author of work on Gaussian error linear units (GELUs) for neural networks.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kevin Gimpel">
  <data key="d0">Kevin Gimpel</data>
  <data key="d1">Researcher</data>
  <data key="d2">Kevin Gimpel contributed to the development of GELUs and neural activation functions.&lt;SEP&gt;Kevin Gimpel is an author of work on neural activation functions like GELUs.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Evan Hernandez">
  <data key="d0">Evan Hernandez</data>
  <data key="d1">Researcher</data>
  <data key="d2">Evan Hernandez researches knowledge representations and their manipulation in language models.&lt;SEP&gt;Evan Hernandez researches measuring and manipulating knowledge representations in large language models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Belinda Z Li">
  <data key="d0">Belinda Z Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Belinda Z Li is involved in studies measuring and manipulating knowledge in NLP models.&lt;SEP&gt;Belinda Z Li is involved in studies on knowledge representations and their manipulation in NLP models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacob Andreas">
  <data key="d0">Jacob Andreas</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jacob Andreas is a researcher studying knowledge representation and manipulation in language models.&lt;SEP&gt;Jacob Andreas is a researcher studying knowledge representations in language models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neil Houlsby">
  <data key="d0">Neil Houlsby</data>
  <data key="d1">Researcher</data>
  <data key="d2">Neil Houlsby is an author of work on parameter-efficient transfer learning for NLP applications.&lt;SEP&gt;Neil Houlsby is an author of work on parameter-efficient transfer learning for NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Andrei Giurgiu">
  <data key="d0">Andrei Giurgiu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Andrei Giurgiu contributed to research on transfer learning techniques and model efficiency in NLP.&lt;SEP&gt;Andrei Giurgiu contributed to research on transfer learning techniques in NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Stanislaw Jastrzebski">
  <data key="d0">Stanislaw Jastrzebski</data>
  <data key="d1">Researcher</data>
  <data key="d2">Stanislaw Jastrzebski is involved in studies on efficient transfer learning methods in NLP.&lt;SEP&gt;Stanislaw Jastrzebski is involved in studies on efficient transfer learning methods.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bruna Morrone">
  <data key="d0">Bruna Morrone</data>
  <data key="d1">Researcher</data>
  <data key="d2">Bruna Morrone contributes to research on parameter-efficient NLP models.&lt;SEP&gt;Bruna Morrone researches parameter-efficient transfer learning approaches in NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Quentin De Laroussilhe">
  <data key="d0">Quentin De Laroussilhe</data>
  <data key="d1">Researcher</data>
  <data key="d2">Quentin De Laroussilhe works on transfer learning and model optimization in NLP.&lt;SEP&gt;Quentin De Laroussilhe works on transfer learning and model optimization.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Andrea Gesmundo">
  <data key="d0">Andrea Gesmundo</data>
  <data key="d1">Researcher</data>
  <data key="d2">Andrea Gesmundo contributes to NLP transfer learning methodologies.&lt;SEP&gt;Andrea Gesmundo researches NLP transfer learning approaches.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mona Attariyan">
  <data key="d0">Mona Attariyan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Mona Attariyan is involved in NLP model efficiency studies.&lt;SEP&gt;Mona Attariyan is involved in research on NLP transfer learning and model efficiency.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sylvain Gelly">
  <data key="d0">Sylvain Gelly</data>
  <data key="d1">Researcher</data>
  <data key="d2">Sylvain Gelly contributes to transfer learning and NLP model research.&lt;SEP&gt;Sylvain Gelly researches NLP transfer learning and neural network optimization.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jeremy Howard">
  <data key="d0">Jeremy Howard</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jeremy Howard is an author on universal language model fine-tuning for text classification tasks.&lt;SEP&gt;Jeremy Howard is an author on universal language model fine-tuning for text classification.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sebastian Ruder">
  <data key="d0">Sebastian Ruder</data>
  <data key="d1">Researcher</data>
  <data key="d2">Sebastian Ruder co-authored work on language model fine-tuning for NLP tasks.&lt;SEP&gt;Sebastian Ruder is a researcher who co-authored work on language model fine-tuning for NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Cheng-Yu Hsieh">
  <data key="d0">Cheng-Yu Hsieh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Cheng-Yu Hsieh researches data-efficient training of language models and prompt-tuning techniques.&lt;SEP&gt;Cheng-Yu Hsieh researches data-efficient training of language models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yasuhisa Fujii">
  <data key="d0">Yasuhisa Fujii</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yasuhisa Fujii is involved in NLP research, including model distillation and training efficiency.&lt;SEP&gt;Yasuhisa Fujii is involved in NLP research, including model distillation.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alexander Ratner">
  <data key="d0">Alexander Ratner</data>
  <data key="d1">Researcher</data>
  <data key="d2">Alexander Ratner contributes to NLP methodologies and model training techniques.&lt;SEP&gt;Alexander Ratner contributes to NLP methodologies, especially in model training and knowledge manipulation.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tomas Pfister">
  <data key="d0">Tomas Pfister</data>
  <data key="d1">Researcher</data>
  <data key="d2">Tomas Pfister works on NLP and machine learning model improvements.&lt;SEP&gt;Tomas Pfister works on NLP model development and training techniques.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Edward J Hu">
  <data key="d0">Edward J Hu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Edward J Hu researches large language models and their adaptation techniques, including low-rank adaptation methods.&lt;SEP&gt;Edward J Hu researches large language models and their adaptation techniques.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yelong Shen">
  <data key="d0">Yelong Shen</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on chain-of-thought demonstrations, published in 2023.&lt;SEP&gt;A researcher working on chain-of-thought demonstrations, published in 2023.&lt;SEP&gt;Yelong Shen contributes to research on model adaptation and fine-tuning of large language models.&lt;SEP&gt;Yelong Shen is involved in studies on large language model adaptation and efficiency.&lt;SEP&gt;Yelong Shen is involved in studies on large language models, retrieval augmentation, and knowledge representation.&lt;SEP&gt;Yelong Shen works on model adaptation, fine-tuning, and improving large language models using techniques like LoRA.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Phillip Wallis">
  <data key="d0">Phillip Wallis</data>
  <data key="d1">Researcher</data>
  <data key="d2">Phillip Wallis contributes to NLP research on knowledge and model manipulation.&lt;SEP&gt;Phillip Wallis researches NLP, focusing on knowledge incorporation and model optimization.&lt;SEP&gt;Phillip Wallis contributes to research on model training, adaptation, and efficiency improvements in NLP models.&lt;SEP&gt;Phillip Wallis works on model training and adaptation techniques for NLP models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zeyuan Allen-Zhu">
  <data key="d0">Zeyuan Allen-Zhu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Zeyuan Allen-Zhu contributes to theoretical research on large language model training and adaptation.&lt;SEP&gt;Zeyuan Allen-Zhu researches optimization and adaptation in large language models.&lt;SEP&gt;Zeyuan Allen-Zhu develops theoretical foundations for low-rank adaptation methods like LoRA in large language models.&lt;SEP&gt;Zeyuan Allen-Zhu researches theoretical aspects of model adaptation and low-rank techniques.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yuanzhi Li">
  <data key="d0">Yuanzhi Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yuanzhi Li contributes to the theoretical foundation of model adaptation methods.&lt;SEP&gt;Yuanzhi Li is involved in theoretical aspects of language model training.&lt;SEP&gt;Yuanzhi Li researches theoretical aspects of large language model training and optimization.&lt;SEP&gt;Yuanzhi Li researches mathematical and theoretical aspects underpinning model adaptation techniques such as LoRA.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shean Wang">
  <data key="d0">Shean Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Shean Wang researches NLP model training and knowledge representation.&lt;SEP&gt;Shean Wang works on large language models, knowledge representation, and model adaptation.&lt;SEP&gt;Shean Wang researches large language model training and adaptation techniques.&lt;SEP&gt;Shean Wang works on training, fine-tuning, and adaptation of large language models, including applications in domain-specific tasks.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Lu Wang">
  <data key="d0">Lu Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Lu Wang contributes to NLP research, especially in model fine-tuning.&lt;SEP&gt;Lu Wang contributes to research on large language models and their fine-tuning.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Weizhu Chen">
  <data key="d0">Weizhu Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on chain-of-thought demonstration techniques, published in 2023.&lt;SEP&gt;A researcher working on synthetic prompting, published in 2023.&lt;SEP&gt;Weizhu Chen is involved in large language model research and optimization.&lt;SEP&gt;Weizhu Chen researches large language models, including optimization and adaptation techniques.&lt;SEP&gt;Weizhu Chen researches optimization and technical methods for adapting large language models effectively.&lt;SEP&gt;Weizhu Chen works on optimization and adaptation methods for NLP models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Shengding Hu">
  <data key="d0">Shengding Hu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Shengding Hu researches prompt-tuning and knowledge incorporation in NLP.&lt;SEP&gt;Shengding Hu researches prompt-tuning, knowledge integration, and model training in NLP.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ning Ding">
  <data key="d0">Ning Ding</data>
  <data key="d1">Researcher</data>
  <data key="d2">Ning Ding is involved in knowledge-enhanced NLP methodologies.&lt;SEP&gt;Ning Ding works on knowledge-enhanced NLP methodologies.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Huadong Wang">
  <data key="d0">Huadong Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.&lt;SEP&gt;Huadong Wang contributes to NLP model training and transfer learning.&lt;SEP&gt;Huadong Wang contributes to NLP transfer learning and knowledge incorporation.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhiyuan Liu">
  <data key="d0">Zhiyuan Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.&lt;SEP&gt;Zhiyuan Liu researches knowledge integration and prompt-tuning in language models.&lt;SEP&gt;Zhiyuan Liu researches knowledge integration in language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jingang Wang">
  <data key="d0">Jingang Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jingang Wang is involved in NLP model development.&lt;SEP&gt;Jingang Wang works on knowledge-aware NLP models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Juanzi Li">
  <data key="d0">Juanzi Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.&lt;SEP&gt;Juanzi Li researches knowledge-aware NLP and model training.&lt;SEP&gt;Juanzi Li works on knowledge-aware NLP models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wei Wu">
  <data key="d0">Wei Wu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Wei Wu contributes to NLP transfer learning and model efficiency.&lt;SEP&gt;Wei Wu contributes to NLP transfer learning, knowledge incorporation, and model efficiency.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Maosong Sun">
  <data key="d0">Maosong Sun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Maosong Sun researches knowledge incorporation and transfer learning in NLP.&lt;SEP&gt;Maosong Sun researches knowledge incorporation into language models.</data>
  <data key="d3">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt-based Distribution Alignment for Domain Generalization in Text Classification">
  <data key="d0">Prompt-based Distribution Alignment for Domain Generalization in Text Classification</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A method aimed at improving domain generalization in text classification by aligning distributions based on prompts, discussed in the context of natural language processing research.&lt;SEP&gt;A method designed to improve the ability of models to generalize across different domains in text classification tasks by aligning data distributions using prompts.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Conference on Empirical Methods in Natural Language Processing 2022">
  <data key="d0">Conference on Empirical Methods in Natural Language Processing 2022</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference where research papers related to empirical methods in NLP are presented, including the one on distribution alignment.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Manuscript submitted to ACM">
  <data key="d0">Manuscript submitted to ACM</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A research manuscript related to NLP domain generalization techniques submitted to the Association for Computing Machinery.&lt;SEP&gt;A research manuscript submitted to the Association for Computing Machinery, related to NLP methodologies.&lt;SEP&gt;Indicates the manuscript's submission to the Association for Computing Machinery, highlighting its academic context.&lt;SEP&gt;Indicates the research manuscript on GingGPT has been submitted to the ACM for peer review and publication.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Theorem Provers">
  <data key="d0">Theorem Provers</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Automated systems that verify mathematical proofs, guided by informal proofs as per recent research.&lt;SEP&gt;Automated systems that verify mathematical proofs, guided by informal proofs to improve their effectiveness.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Informal Proofs">
  <data key="d0">Informal Proofs</data>
  <data key="d1">Tools</data>
  <data key="d2">Unstructured or non-formal proofs used to guide formal theorem provers, facilitating their efficiency.&lt;SEP&gt;Unstructured proofs used to guide formal theorem proving, aiding in automating complex logical reasoning.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Learning">
  <data key="d0">Prompt Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique where prompts are used to steer language models toward desired tasks, including instance-aware prompt approaches for better task adaptation.&lt;SEP&gt;Technique involving crafting prompts to steer language models for specific language understanding and generation tasks, including instance-aware approaches.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Biomedical Information Access">
  <data key="d0">Biomedical Information Access</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhancing access to biomedical data by augmenting large language models with domain-specific tools, improving information retrieval.&lt;SEP&gt;Enhancing the ability of language models to access and retrieve biomedical data by integrating domain-specific tools, improving information retrieval in healthcare.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Laws for Neural Language Models">
  <data key="d0">Scaling Laws for Neural Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Frameworks describing how the performance of neural language models scales with model size, data, and compute.&lt;SEP&gt;Frameworks that describe how the performance of neural language models scales with increases in model size, data, and compute resources.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hypercomplex Adapter Layers">
  <data key="d0">Hypercomplex Adapter Layers</data>
  <data key="d1">Tools</data>
  <data key="d2">Efficient neural network components enabling low-rank adaptations in models, improving parameter efficiency.&lt;SEP&gt;Neural network components that enable efficient low-rank adaptation of large models, reducing parameter overhead while maintaining performance.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Decomposed Prompting">
  <data key="d0">Decomposed Prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A modular prompting approach that decomposes complex tasks into smaller, manageable sub-prompts to improve problem-solving capabilities.&lt;SEP&gt;A modular prompting approach to tackle complex tasks by decomposing problems into manageable parts.&lt;SEP&gt;A modular prompting technique that decomposes complex tasks into simpler components to improve reasoning and problem-solving in language models.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zero-Shot Reasoning">
  <data key="d0">Zero-Shot Reasoning</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigating the capacity of large language models to perform reasoning tasks without explicit training on those specific tasks.&lt;SEP&gt;Investigating whether large language models can perform reasoning tasks without explicit task-specific training, assessing their generalization abilities.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Internet-Augmented Dialogue Generation">
  <data key="d0">Internet-Augmented Dialogue Generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhancing dialogue systems by integrating internet access to improve responses and knowledge retrieval.&lt;SEP&gt;Using internet access to augment dialogue systems, enabling more accurate and knowledge-rich responses.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Clinical Language Models">
  <data key="d0">Clinical Language Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Language models designed for clinical settings, with ongoing debates about their continued necessity and utility.&lt;SEP&gt;Language models specifically designed or adapted for clinical and healthcare-related NLP tasks, with ongoing debates about their necessity versus general models.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Financial Sentiment Analysis">
  <data key="d0">Financial Sentiment Analysis</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Applying large language models like GPT-3 to analyze and influence financial sentiment, demonstrating their practical use in finance.&lt;SEP&gt;Using GPT-3 to analyze and attack financial sentiment, indicating applications in financial data interpretation.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter-Efficient Prompt Tuning">
  <data key="d0">Parameter-Efficient Prompt Tuning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques that optimize large language models with minimal additional parameters, enhancing scalability and resource efficiency.&lt;SEP&gt;Techniques to optimize large models with minimal additional parameters, improving efficiency and scalability.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Retrieval-Augmented Generation">
  <data key="d0">Retrieval-Augmented Generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Combining retrieval mechanisms with generation models to handle knowledge-intensive NLP tasks effectively.&lt;SEP&gt;Retrieval-augmented generation (RAG) is a method that combines pre-trained parametric models with non-parametric memory to enhance language generation, particularly in knowledge-intensive NLP tasks.&lt;SEP&gt;Retrieval-augmented generation (RAG) is a method that combines pre-trained parametric models with non-parametric memory to improve language generation, especially for knowledge-intensive NLP tasks.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb&lt;SEP&gt;chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Controllable Working Memory">
  <data key="d0">Controllable Working Memory</data>
  <data key="d1">Variables</data>
  <data key="d2">A feature in large language models that allows controlling the amount of information retained during processing, impacting reasoning and performance.&lt;SEP&gt;A feature in large language models that allows managing and controlling the amount of information retained during processing.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Communicative Agents">
  <data key="d0">Communicative Agents</data>
  <data key="d1">Tools</data>
  <data key="d2">AI agents designed for social and informational interaction, exploring societal implications of large-scale language models.&lt;SEP&gt;AI agents designed for social interaction and information exchange, exploring societal implications of large language models.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="2022 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">2022 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A major conference where empirical NLP research, including this distribution alignment method, was presented.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Retrieval-augmented Generation">
  <data key="d0">Retrieval-augmented Generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A hybrid approach combining retrieval mechanisms with generative models to improve performance on knowledge-intensive NLP tasks.</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Guohao Li">
  <data key="d0">Guohao Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Guohao Li is an author involved in research on large-scale language models and their societal implications.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hasan Abed Al Kader Hammoud">
  <data key="d0">Hasan Abed Al Kader Hammoud</data>
  <data key="d1">Researcher</data>
  <data key="d2">Hassan Abed Al Kader Hammoud is an author contributing to studies on communicative agents and language models.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hani Itani">
  <data key="d0">Hani Itani</data>
  <data key="d1">Researcher</data>
  <data key="d2">Hani Itani is a researcher involved in studies related to language models and AI communication.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dmitrii Khizbullin">
  <data key="d0">Dmitrii Khizbullin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Dmitrii Khizbullin is an author working on large language models and their societal impacts.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bernard Ghanem">
  <data key="d0">Bernard Ghanem</data>
  <data key="d1">Researcher</data>
  <data key="d2">Bernard Ghanem is a researcher contributing to AI and language model research.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Large Scale Language Model Society">
  <data key="d0">Large Scale Language Model Society</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The society of large-scale language models refers to the community and ecosystem of research, development, and societal impact surrounding large language models.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:2303.17760">
  <data key="d0">arXiv preprint arXiv:2303.17760</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint publication presenting research findings on communicative agents for understanding large-scale language models and their societal impact.&lt;SEP&gt;A preprint publication presenting research findings on communicative agents for understanding large-scale language models.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jinyang Li">
  <data key="d0">Jinyang Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jinyang Li is an author exploring the capabilities of large language models as database interfaces, especially for text-to-SQL tasks.&lt;SEP&gt;Jinyang Li is an author exploring the capabilities of large language models as database interfaces, focusing on text-to-SQL tasks.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Can LLM Already Serve as A Database Interface?">
  <data key="d0">Can LLM Already Serve as A Database Interface?</data>
  <data key="d1">Study Title</data>
  <data key="d2">A research study evaluating whether large language models can effectively function as database interfaces for querying and data retrieval.&lt;SEP&gt;A research study evaluating whether large language models can effectively serve as interfaces for databases, enabling natural language querying.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiang Lisa Li">
  <data key="d0">Xiang Lisa Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xiang Lisa Li is an author working on prompt optimization techniques for language generation, including prefix-tuning.&lt;SEP&gt;Xiang Lisa Li is an author working on prompt optimization techniques for language generation.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prefix-Tuning">
  <data key="d0">Prefix-Tuning</data>
  <data key="d1">Methodology</data>
  <data key="d2">Prefix-Tuning is a technique that involves optimizing continuous prompts at the beginning of the input sequence to improve generation performance.&lt;SEP&gt;Prefix-Tuning is a technique that involves optimizing continuous prompts to improve language model generation performance.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacky Liang">
  <data key="d0">Jacky Liang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jacky Liang is involved in research on language models applied to embodied control and robotics, focusing on control policies derived from language models.&lt;SEP&gt;Jacky Liang is involved in research on language models applied to embodied control and robotics.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Code as Policies">
  <data key="d0">Code as Policies</data>
  <data key="d1">Methodology</data>
  <data key="d2">Code as Policies refers to using language model programs as control policies for embodied agents and robotics.&lt;SEP&gt;Code as Policies refers to using language model-generated code to serve as control policies for embodied agents and robotic systems.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yaobo Liang">
  <data key="d0">Yaobo Liang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yaobo Liang is an author working on task completion by connecting foundation models with APIs, enabling automation of complex tasks.&lt;SEP&gt;Yaobo Liang is an author working on task completion by connecting foundation models with APIs.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="TaskMatrix AI">
  <data key="d0">TaskMatrix AI</data>
  <data key="d1">Application/Implication</data>
  <data key="d2">TaskMatrix AI is a system that connects foundation models with a large number of APIs to facilitate task execution and automation.&lt;SEP&gt;TaskMatrix AI is an application that connects foundation models with numerous APIs to enable task completion.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hongzhan Lin">
  <data key="d0">Hongzhan Lin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Hongzhan Lin is involved in research on zero-shot rumor detection using prompt learning.&lt;SEP&gt;Hongzhan Lin is involved in research on zero-shot rumor detection using propagation structures and prompt learning techniques.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-Shot Rumor Detection">
  <data key="d0">Zero-Shot Rumor Detection</data>
  <data key="d1">Study Design</data>
  <data key="d2">A method for detecting rumors without prior training data, utilizing propagation structure and prompt learning techniques.&lt;SEP&gt;A method for detecting rumors without prior training data, utilizing propagation structure and prompt learning to identify false information.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Qi Liu">
  <data key="d0">Qi Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Qi Liu is an author studying relational memory-augmented language models for improved reasoning.&lt;SEP&gt;Qi Liu is an author studying relational memory-augmented language models to enhance reasoning and contextual understanding.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Relational Memory-Augmented Language Models">
  <data key="d0">Relational Memory-Augmented Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A model that incorporates relational memory to enhance reasoning and contextual understanding in language models.&lt;SEP&gt;Language models enhanced with relational memory components to improve reasoning, reasoning over structured data, and contextual understanding.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ruibo Liu">
  <data key="d0">Ruibo Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Ruibo Liu is involved in grounded language model reasoning through simulation, aiming to improve reasoning by grounding language in perceptual or environmental context.&lt;SEP&gt;Ruibo Liu is involved in grounded language model reasoning through simulation, enhancing language understanding with grounding techniques.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mind’s Eye">
  <data key="d0">Mind’s Eye</data>
  <data key="d1">Tools</data>
  <data key="d2">Mind’s Eye is a grounded reasoning framework that uses simulation techniques to enable language models to perform grounded reasoning tasks.&lt;SEP&gt;Mind’s Eye is a grounded reasoning technique that uses simulation to improve language model reasoning.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiangyang Liu">
  <data key="d0">Xiangyang Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xiangyang Liu is working on prompt tuning strategies, including late prompt tuning, to improve language model performance in generation tasks.&lt;SEP&gt;Xiangyang Liu is working on prompt tuning techniques, including late prompt tuning, to enhance language model performance.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Late Prompt Tuning">
  <data key="d0">Late Prompt Tuning</data>
  <data key="d1">Methodology</data>
  <data key="d2">Late Prompt Tuning involves applying prompts at later stages in the model to potentially improve generation quality.&lt;SEP&gt;Late Prompt Tuning involves applying prompts at later stages in the model's input process, potentially leading to better performance than early prompts.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yiheng Liu">
  <data key="d0">Yiheng Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yiheng Liu is an author summarizing recent research on ChatGPT, GPT-4, and the future of large language models, providing perspectives on their development and impact.&lt;SEP&gt;Yiheng Liu is an author summarizing research on ChatGPT, GPT-4, and future perspectives of large language models.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Summary of ChatGPT/GPT-4 Research">
  <data key="d0">Summary of ChatGPT/GPT-4 Research</data>
  <data key="d1">Study Summary</data>
  <data key="d2">A comprehensive overview of recent advances, research findings, and future perspectives related to ChatGPT, GPT-4, and large language models.&lt;SEP&gt;A comprehensive overview of recent research developments and future outlooks on large language models like ChatGPT and GPT-4.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alejandro Lopez-Lira">
  <data key="d0">Alejandro Lopez-Lira</data>
  <data key="d1">Researcher</data>
  <data key="d2">Alejandro Lopez-Lira is an author investigating the ability of large language models like ChatGPT to forecast stock price movements.&lt;SEP&gt;Alejandro Lopez-Lira is an author investigating the use of large language models for stock price forecasting.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Can ChatGPT Forecast Stock Price Movements?">
  <data key="d0">Can ChatGPT Forecast Stock Price Movements?</data>
  <data key="d1">Research Study</data>
  <data key="d2">A study assessing the predictive capabilities of ChatGPT for stock market movements.&lt;SEP&gt;A study examining whether ChatGPT can predict stock market movements and its potential for return predictability.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jiaying Lu">
  <data key="d0">Jiaying Lu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jiaying Lu is involved in biomedical NLP, focusing on few-shot biomedical knowledge fusion using hierarchy-oriented prompting techniques.&lt;SEP&gt;Jiaying Lu is involved in biomedical knowledge fusion using few-shot prompting techniques.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="HiPrompt">
  <data key="d0">HiPrompt</data>
  <data key="d1">Methodology</data>
  <data key="d2">HiPrompt is a hierarchy-oriented prompt technique designed to facilitate few-shot biomedical knowledge fusion.&lt;SEP&gt;HiPrompt is a hierarchy-oriented prompting method designed for few-shot biomedical knowledge fusion.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yao Lu">
  <data key="d0">Yao Lu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yao Lu is working on prompt order sensitivity and methods to overcome it in few-shot learning.&lt;SEP&gt;Yao Lu is working on prompt order sensitivity issues in few-shot learning and methods to overcome them.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fantastically Ordered Prompts">
  <data key="d0">Fantastically Ordered Prompts</data>
  <data key="d1">Methodology</data>
  <data key="d2">A prompt ordering strategy that improves few-shot learning robustness by arranging prompts optimally.&lt;SEP&gt;A prompting technique that arranges prompts in an order that maximizes learning efficiency and robustness.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Aman Madaan">
  <data key="d0">Aman Madaan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Aman Madaan is studying language models of code as few-shot learners for commonsense reasoning.&lt;SEP&gt;Aman Madaan studies language models of code as few-shot learners for commonsense reasoning tasks.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Language models of code">
  <data key="d0">Language models of code</data>
  <data key="d1">Tools</data>
  <data key="d2">Language models of code are used to perform few-shot learning tasks, including commonsense reasoning, by generating or understanding code snippets.&lt;SEP&gt;Language models of code are used to perform few-shot learning tasks, including commonsense reasoning.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Babak Mahjour">
  <data key="d0">Babak Mahjour</data>
  <data key="d1">Researcher</data>
  <data key="d2">Babak Mahjour is involved in designing chemical reaction arrays using AI tools like ChatGPT for chemical research and automation.&lt;SEP&gt;Babak Mahjour is involved in designing chemical reaction arrays using AI tools like ChatGPT.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chemical Reaction Arrays">
  <data key="d0">Chemical Reaction Arrays</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Chemical Reaction Arrays are structured sets of chemical reactions designed for systematic experimentation and analysis, utilizing tools like phactor and ChatGPT.&lt;SEP&gt;Chemical reaction arrays are structured sets of chemical reactions designed with AI assistance for research, testing, and synthesis.&lt;SEP&gt;Chemical reaction arrays are structured sets of reactions, designed with AI assistance for chemical research.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d&lt;SEP&gt;chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bhavitvya Malik">
  <data key="d0">Bhavitvya Malik</data>
  <data key="d1">Researcher</data>
  <data key="d2">Bhavitvya Malik is an author involved in research on domain adaptation using adapters, contributing to methodologies in machine learning.&lt;SEP&gt;Bhavitvya Malik is working on domain adaptation techniques for NLP models, specifically using adapters for efficient transfer learning.&lt;SEP&gt;Bhavitvya Malik is working on domain adaptation techniques using adapters for NLP models.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d&lt;SEP&gt;chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="UDApter">
  <data key="d0">UDApter</data>
  <data key="d1">Methodology</data>
  <data key="d2">UDApter is an approach employing adapters to enable efficient domain adaptation in NLP tasks.&lt;SEP&gt;UDApter is an approach for efficient domain adaptation in NLP using adapters.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="CAMEL">
  <data key="d0">CAMEL</data>
  <data key="d1">Methodology</data>
  <data key="d2">CAMEL (Communicative Agents for 'Mind' Exploration of Large Scale Language Model Society) is a proposed framework or approach for studying large language models' societal roles and interactions.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge-injected Prompt Tuning">
  <data key="d0">Knowledge-injected Prompt Tuning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A prompt tuning approach that incorporates external knowledge to improve the accuracy and robustness of event detection in NLP tasks.</data>
  <data key="d3">chunk-213967d818970d7799e7d9779765ed79</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="phactor">
  <data key="d0">phactor</data>
  <data key="d1">Tools</data>
  <data key="d2">phactor is a software tool used for designing and managing chemical reaction arrays.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ChatGPT">
  <data key="d0">ChatGPT</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An advanced AI language model developed by OpenAI, used for generating human-like text and assisting in various research activities.&lt;SEP&gt;ChatGPT is an AI language model used to assist in designing chemical reaction arrays and related activities.&lt;SEP&gt;ChatGPT is an advanced conversational AI model developed by OpenAI, serving as a foundation for various AI applications.&lt;SEP&gt;ChatGPT is an advanced conversational AI model developed by OpenAI, used as a foundation for various AI task solutions.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-0cdbfe705edb382a6ee1927e70804633&lt;SEP&gt;chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Abhinav Ramesh Kashyap">
  <data key="d0">Abhinav Ramesh Kashyap</data>
  <data key="d1">Researcher</data>
  <data key="d2">Abhinav Ramesh Kashyap is an author contributing to research on efficient domain adaptation techniques.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Min-Yen Kan">
  <data key="d0">Min-Yen Kan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Min-Yen Kan is an author involved in research on domain adaptation and language model tuning.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Soujanya Poria">
  <data key="d0">Soujanya Poria</data>
  <data key="d1">Researcher</data>
  <data key="d2">Soujanya Poria is an author contributing to research on domain adaptation and natural language processing.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yuning Mao">
  <data key="d0">Yuning Mao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yuning Mao is an author involved in research on language model frameworks and parameter-efficient tuning.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Lambert Mathias">
  <data key="d0">Lambert Mathias</data>
  <data key="d1">Researcher</data>
  <data key="d2">Lambert Mathias is an author contributing to studies on language model tuning and efficiency.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Rui Hou">
  <data key="d0">Rui Hou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Rui Hou is an author involved in research on language model frameworks.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Amjad Almahairi">
  <data key="d0">Amjad Almahairi</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in Llama 2 development, published in 2023.&lt;SEP&gt;Amjad Almahairi is an author contributing to research on language model tuning and psychophysical knowledge.&lt;SEP&gt;Researcher contributing to Llama 2, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b&lt;SEP&gt;chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Hao Ma">
  <data key="d0">Hao Ma</data>
  <data key="d1">Researcher</data>
  <data key="d2">Hao Ma is an author involved in research on language models and their applications.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jiawei Han">
  <data key="d0">Jiawei Han</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jiawei Han is an author contributing to research on language models and data analysis.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wen-tau Yih">
  <data key="d0">Wen-tau Yih</data>
  <data key="d1">Researcher</data>
  <data key="d2">Wen-tau Yih is an author involved in research on language models and perception.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Madian Khabsa">
  <data key="d0">Madian Khabsa</data>
  <data key="d1">Researcher</data>
  <data key="d2">Madian Khabsa is an author contributing to research on language models and psychophysical insights.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kevin Meng">
  <data key="d0">Kevin Meng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Kevin Meng is an author involved in research on locating, editing, and memory manipulation in language models.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="David Bau">
  <data key="d0">David Bau</data>
  <data key="d1">Researcher</data>
  <data key="d2">David Bau is an author contributing to research on factual associations and memory editing in transformers.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alex Andonian">
  <data key="d0">Alex Andonian</data>
  <data key="d1">Researcher</data>
  <data key="d2">Alex Andonian is an author involved in research on factual knowledge and model editing.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yonatan Belinkov">
  <data key="d0">Yonatan Belinkov</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yonatan Belinkov is an author contributing to research on language model editing and factual accuracy.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacob Menick">
  <data key="d0">Jacob Menick</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jacob Menick is an author involved in research on supporting answers with verified quotes in language models.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Maja Trebacz">
  <data key="d0">Maja Trebacz</data>
  <data key="d1">Researcher</data>
  <data key="d2">Maja Trebacz is an author contributing to research on language model verification and quote support.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Vladimir Mikulik">
  <data key="d0">Vladimir Mikulik</data>
  <data key="d1">Researcher</data>
  <data key="d2">Vladimir Mikulik is an author involved in research on model support and quote verification.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="John Aslanides">
  <data key="d0">John Aslanides</data>
  <data key="d1">Researcher</data>
  <data key="d2">John Aslanides is an author contributing to research on language model answer support.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Francis Song">
  <data key="d0">Francis Song</data>
  <data key="d1">Researcher</data>
  <data key="d2">Francis Song is an author involved in research on language model answer validation.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Martin Chadwick">
  <data key="d0">Martin Chadwick</data>
  <data key="d1">Researcher</data>
  <data key="d2">Martin Chadwick is an author contributing to research on language model answer support.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mia Glaese">
  <data key="d0">Mia Glaese</data>
  <data key="d1">Researcher</data>
  <data key="d2">Mia Glaese is an author involved in research on language model verification.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Susannah Young">
  <data key="d0">Susannah Young</data>
  <data key="d1">Researcher</data>
  <data key="d2">Susannah Young is an author contributing to research on language model answer validation.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Lucy Campbell-Gillingham">
  <data key="d0">Lucy Campbell-Gillingham</data>
  <data key="d1">Researcher</data>
  <data key="d2">Lucy Campbell-Gillingham is an author involved in research on supporting answers with verified quotes.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Geoffrey Irving">
  <data key="d0">Geoffrey Irving</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in AI research, contributing to AI safety and preferences.&lt;SEP&gt;Geoffrey Irving is an author contributing to research on language model answer verification.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633&lt;SEP&gt;chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pointer Sentinel Mixture Models">
  <data key="d0">Pointer Sentinel Mixture Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Pointer sentinel mixture models are a specific neural architecture designed to enhance language modeling capabilities.&lt;SEP&gt;Pointer sentinel mixture models are a type of neural network architecture designed to improve language modeling capabilities.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Augmented Language Models">
  <data key="d0">Augmented Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Augmented language models are models that incorporate additional modules or data to improve performance beyond standard architectures.&lt;SEP&gt;Augmented language models refer to enhanced models that incorporate additional modules or data to improve performance.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Large Pre-trained Language Models">
  <data key="d0">Large Pre-trained Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Large pre-trained language models are extensive neural networks trained on large datasets, forming the basis for many NLP tasks.&lt;SEP&gt;Large pre-trained language models are models trained on vast datasets, forming the basis for many NLP applications.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Editing">
  <data key="d0">Model Editing</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Model editing involves techniques to modify neural network models post-training, such as locating, editing, and updating parameters.&lt;SEP&gt;Model editing involves techniques to modify or update neural network models without retraining from scratch.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Memory-based Model Editing">
  <data key="d0">Memory-based Model Editing</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Memory-based model editing techniques use stored information to efficiently update or correct model knowledge.&lt;SEP&gt;Memory-based model editing uses stored information to update or correct models efficiently.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="In-Context Learning">
  <data key="d0">In-Context Learning</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">In-Context Learning explores how models learn from demonstrations within prompts to perform tasks.&lt;SEP&gt;In-Context Learning investigates how models learn from demonstrations within prompts to perform new tasks.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Factual Associations">
  <data key="d0">Factual Associations</data>
  <data key="d1">Variables</data>
  <data key="d2">Factual associations are the relationships between data points and facts stored within language models.&lt;SEP&gt;Factual associations are the relationships between stored data points and factual information within language models.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Question-Answering with Human Feedback">
  <data key="d0">Question-Answering with Human Feedback</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">WebGPT demonstrates how human feedback can improve question-answering systems.&lt;SEP&gt;WebGPT demonstrates how incorporating human feedback can improve the accuracy and reliability of question-answering systems.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Browser-Assisted Question-Answering">
  <data key="d0">Browser-Assisted Question-Answering</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">WebGPT employs browser assistance combined with human feedback to enhance answer correctness and user experience.&lt;SEP&gt;WebGPT utilizes browser assistance and human feedback to enhance answer accuracy.</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Reiichiro Nakano">
  <data key="d0">Reiichiro Nakano</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Reiichiro Nakano is an author involved in research related to human factors and computing systems, contributing to understanding user interactions with technology.&lt;SEP&gt;Reiichiro Nakano researches AI verification, problem-solving, and mathematical reasoning in language models.&lt;SEP&gt;Reiichiro Nakano works on AI models for mathematical reasoning and verification.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacob Hilton">
  <data key="d0">Jacob Hilton</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Jacob Hilton is an author contributing to research on human-computer interaction and system design.&lt;SEP&gt;Jacob Hilton is involved in research on language models and their capabilities in solving math problems.&lt;SEP&gt;Jacob Hilton works on AI models for solving math word problems and verification techniques.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Suchir Balaji">
  <data key="d0">Suchir Balaji</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code understanding and generation.&lt;SEP&gt;Study on neural models' capabilities for parallel code synthesis and correctness verification.&lt;SEP&gt;Suchir Balaji is involved in studies related to human factors in computing systems.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jeff Wu">
  <data key="d0">Jeff Wu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Jeff Wu is a researcher contributing to human-computer interaction research.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Long Ouyang">
  <data key="d0">Long Ouyang</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Long Ouyang is an author whose work focuses on training language models to follow instructions with human feedback, hypothesizing that human feedback improves model performance.&lt;SEP&gt;Long Ouyang is an author whose work focuses on training language models with human feedback, hypothesizing that human feedback improves model performance.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Christina Kim">
  <data key="d0">Christina Kim</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Christina Kim is involved in research related to human factors in computing.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Christopher Hesse">
  <data key="d0">Christopher Hesse</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Christopher Hesse contributes to AI research focused on problem-solving and model verification.&lt;SEP&gt;Christopher Hesse contributes to research on human-computer interaction and system usability.&lt;SEP&gt;Christopher Hesse contributes to training and verifying AI models for solving math word problems.&lt;SEP&gt;Research on neural models for code synthesis.&lt;SEP&gt;Research on neural models' capacity to generate correct parallel code and detect data races.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shantanu Jain">
  <data key="d0">Shantanu Jain</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research evaluating neural models' performance in parallel code understanding and data race detection.&lt;SEP&gt;Research on neural code models and their evaluation.&lt;SEP&gt;Shantanu Jain is engaged in studies related to human factors and computing systems.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Vineet Kosaraju">
  <data key="d0">Vineet Kosaraju</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Vineet Kosaraju is involved in research on AI verification and mathematical reasoning in language models.&lt;SEP&gt;Vineet Kosaraju is involved in research on human factors in computing environments.&lt;SEP&gt;Vineet Kosaraju researches AI verification, especially in relation to solving math problems and model reliability.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="William Saunders">
  <data key="d0">William Saunders</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code performance assessment.&lt;SEP&gt;Study assessing neural models' effectiveness in code generation for HPC environments.&lt;SEP&gt;William Saunders is a researcher contributing to the understanding of human-computer interaction.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="WebGPT">
  <data key="d0">WebGPT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">WebGPT is a browser-assisted question-answering system that incorporates human feedback to improve its responses.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ChatGPT plugins">
  <data key="d0">ChatGPT plugins</data>
  <data key="d1">Tools</data>
  <data key="d2">ChatGPT plugins are software extensions that enhance the functionality of ChatGPT by integrating external tools or services.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="GPT-4">
  <data key="d0">GPT-4</data>
  <data key="d1">GPT-4</data>
  <data key="d2">A large language model used alongside GPT-3.5 for evaluation and comparative analysis.&lt;SEP&gt;A large language model used for evaluation purposes in the research.&lt;SEP&gt;An advanced GPT model that performs well on many problem types but struggles with certain parallel models like Kokkos search problems.&lt;SEP&gt;GPT-4 is a large language model (LLM) used for evaluating natural language understanding and generation capabilities, especially in code generation tasks.&lt;SEP&gt;GPT-4 is a large language model developed by OpenAI, capable of natural language understanding and generation, with applications in code and text processing.&lt;SEP&gt;GPT-4 is a state-of-the-art language model developed by OpenAI, capable of understanding and generating human-like text.&lt;SEP&gt;GPT-4 is a state-of-the-art language model evaluated for efficiency, speedup, and scalability in parallel code generation and translation tasks, demonstrating superior performance.&lt;SEP&gt;GPT-4 is a state-of-the-art language model evaluated for efficiency, speedup, and scalability in parallel code generation and translation tasks.&lt;SEP&gt;GPT-4 is a state-of-the-art large language model developed by OpenAI, capable of natural language understanding, code generation, and assisting in software engineering tasks.&lt;SEP&gt;A state-of-the-art language model with 175 billion parameters, achieving high pass@1 scores in code generation benchmarks.&lt;SEP&gt;GPT-4 is OpenAI's latest state-of-the-art commercial large language model, accessible via API, used for high-performance language understanding and generation.&lt;SEP&gt;GPT-4 is a language model evaluated on problem types and parallel execution models, with pass@1 scores indicating its performance.&lt;SEP&gt;GPT-4 is a state-of-the-art language model with 175 billion parameters, demonstrating high accuracy in code generation tasks.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Training language models to follow instructions with human feedback">
  <data key="d0">Training language models to follow instructions with human feedback</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">This theory posits that incorporating human feedback during training improves the ability of language models to follow instructions accurately.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Generative Agents">
  <data key="d0">Generative Agents</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Generative Agents are interactive models simulating human behavior, used to study human-like interactions in AI systems.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Instruction tuning with GPT-4">
  <data key="d0">Instruction tuning with GPT-4</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Instruction tuning involves fine-tuning large language models like GPT-4 on specific tasks or instructions to improve performance and adaptability.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapterhub">
  <data key="d0">Adapterhub</data>
  <data key="d1">Tools</data>
  <data key="d2">Adapterhub is a framework that facilitates the adaptation of transformer models for various tasks through modular adapters.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mad-x">
  <data key="d0">Mad-x</data>
  <data key="d1">Tools</data>
  <data key="d2">Mad-x is a framework for multi-task cross-lingual transfer learning using adapter-based approaches.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Decomposed In-Context Learning of Text-to-SQL with Self-Correction">
  <data key="d0">Decomposed In-Context Learning of Text-to-SQL with Self-Correction</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The approach improves accuracy and efficiency in Text-to-SQL tasks by decomposing learning and applying self-correction.&lt;SEP&gt;This methodology involves decomposing the process of in-context learning for Text-to-SQL tasks and applying self-correction to improve accuracy.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Gradient-free, Edit-based Instruction Search (GrIPS)">
  <data key="d0">Gradient-free, Edit-based Instruction Search (GrIPS)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">GrIPS is a technique for optimizing prompts for large language models without gradient-based methods, using edit-based search.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hierarchical Domain-specific Language Models">
  <data key="d0">Hierarchical Domain-specific Language Models</data>
  <data key="d1">Variables</data>
  <data key="d2">Hierarchical domain-specific language models are tailored models designed to improve performance in specific fields, such as legal decision classification.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Effect of Hierarchical Domain-specific Language Models and Attention in the Classification of Decisions for Legal Cases">
  <data key="d0">Effect of Hierarchical Domain-specific Language Models and Attention in the Classification of Decisions for Legal Cases</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This study hypothesizes that hierarchical domain-specific language models combined with attention mechanisms enhance legal decision classification accuracy.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Tuning of T5">
  <data key="d0">Prompt Tuning of T5</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Prompt tuning involves adjusting prompts to adapt large models like T5 for lifelong few-shot learning tasks.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tool Learning with Foundation Models">
  <data key="d0">Tool Learning with Foundation Models</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">This research explores how foundation models can be used for tool learning, impacting automation and AI utility.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pre-trained models for natural language processing">
  <data key="d0">Pre-trained models for natural language processing</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Pre-trained models serve as foundational architectures that capture language understanding, enabling various NLP tasks.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Improving language understanding by generative pre-training">
  <data key="d0">Improving language understanding by generative pre-training</data>
  <data key="d1">Results</data>
  <data key="d2">This study shows that generative pre-training significantly enhances language understanding capabilities.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Exploring the limits of transfer learning with a unified text-to-text transformer">
  <data key="d0">Exploring the limits of transfer learning with a unified text-to-text transformer</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This research investigates how transfer learning can be maximized using a unified transformer architecture.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Combining modular skills in multitask learning">
  <data key="d0">Combining modular skills in multitask learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">This methodology involves integrating multiple skills into a single model to improve multitask learning performance.</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu">
  <data key="d0">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu</data>
  <data key="d1">Research Authors</data>
  <data key="d2">The authors are researchers contributing to studies on transfer learning and language models, publishing in scientific journals and preprints.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="The Journal of Machine Learning Research">
  <data key="d0">The Journal of Machine Learning Research</data>
  <data key="d1">Study Venue</data>
  <data key="d2">An academic journal publishing peer-reviewed research on machine learning, including studies on transfer learning and transformer models.&lt;SEP&gt;An academic journal publishing research articles on machine learning topics, including transfer learning and transformer models.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="medRxiv">
  <data key="d0">medRxiv</data>
  <data key="d1">Preprint Repository</data>
  <data key="d2">A preprint server for health sciences research, hosting studies evaluating AI tools like ChatGPT in medical contexts.&lt;SEP&gt;A preprint server for health sciences research, including evaluations of ChatGPT in clinical decision-making.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Advances in neural information processing systems">
  <data key="d0">Advances in neural information processing systems</data>
  <data key="d1">Conference Proceedings</data>
  <data key="d2">A leading conference publishing research on neural networks, multi-domain learning, and continual learning methods.&lt;SEP&gt;A publication venue for neural network and machine learning research, including studies on learning multiple visual domains and continual learning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Nature Machine Intelligence">
  <data key="d0">Nature Machine Intelligence</data>
  <data key="d1">Journal</data>
  <data key="d2">A journal publishing research on AI applications, including chemical language representations and molecular modeling.&lt;SEP&gt;A scientific journal publishing research on AI, including molecular structure representation and language models in chemistry.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ICLR (International Conference on Learning Representations)">
  <data key="d0">ICLR (International Conference on Learning Representations)</data>
  <data key="d1">Conference</data>
  <data key="d2">A premier conference for machine learning research, including zero-shot generalization and multitask prompting in language models.&lt;SEP&gt;A prominent conference for machine learning research, including multitask prompting and zero-shot generalization in language models.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="IEEE International Conference on Big Data">
  <data key="d0">IEEE International Conference on Big Data</data>
  <data key="d1">Conference</data>
  <data key="d2">A conference focused on big data and AI applications, including legal NLP and healthcare AI.&lt;SEP&gt;A conference focusing on big data analytics and AI applications, including customized legal document review using language models.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="HuggingFace">
  <data key="d0">HuggingFace</data>
  <data key="d1">Tools</data>
  <data key="d2">A community and platform for sharing NLP models and tools, including HuggingGPT for AI task solving.&lt;SEP&gt;A platform and community for sharing NLP models, including tools like HuggingGPT for multi-task AI solutions.&lt;SEP&gt;HuggingFace is a platform providing libraries and tools for deploying and developing machine learning models, including language models like GingGPT.&lt;SEP&gt;HuggingFace is a platform providing tools and libraries for deploying and developing machine learning models, including language models like GingGPT.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Memory Augmented Large Language Models">
  <data key="d0">Memory Augmented Large Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A class of language models enhanced with external memory to achieve computational universality, enabling advanced reasoning and continual learning.&lt;SEP&gt;Models that incorporate external memory to achieve computational universality, enabling advanced reasoning and continual learning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Toolformer">
  <data key="d0">Toolformer</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A language model that can teach itself to use external tools, enhancing task performance and self-sufficiency.&lt;SEP&gt;A model that can teach itself to use external tools, enhancing its capabilities for various tasks through self-supervised tool learning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Progressive Prompts">
  <data key="d0">Progressive Prompts</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A continual learning approach using progressive prompts to improve language model performance over time.&lt;SEP&gt;A continual learning methodology that employs progressive prompting to enable models to learn over time without catastrophic forgetting.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="In-Context Retrieval-Augmented Language Models">
  <data key="d0">In-Context Retrieval-Augmented Language Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Language models that incorporate retrieval mechanisms to enhance context understanding and knowledge access during inference.&lt;SEP&gt;Models that integrate retrieval mechanisms to access external knowledge during inference, improving accuracy and context understanding.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multitask Prompted Training">
  <data key="d0">Multitask Prompted Training</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training strategy that enables language models to generalize across multiple tasks without explicit task-specific training, facilitating zero-shot capabilities.&lt;SEP&gt;Training approach that enables models to perform multiple tasks via prompting, leading to zero-shot generalization.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuned Language Models">
  <data key="d0">Fine-tuned Language Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Language models that are further trained on specific datasets to improve performance on targeted tasks, including continual learning scenarios.&lt;SEP&gt;Models that are further trained on specific datasets to improve performance on targeted applications.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-Shot Task Generalization">
  <data key="d0">Zero-Shot Task Generalization</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The hypothesis that large language models can generalize to new tasks without additional training by leveraging prompts and prior knowledge.&lt;SEP&gt;The hypothesis that large language models can perform new tasks without explicit retraining by leveraging prior knowledge and prompting techniques.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chemical Language Representations">
  <data key="d0">Chemical Language Representations</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Language-based models encoding molecular structures and properties to facilitate machine learning in chemistry.&lt;SEP&gt;Models that encode molecular structures and properties into language-like formats, capturing chemical information for machine learning applications.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Radiologic Decision-Making">
  <data key="d0">Radiologic Decision-Making</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Application of AI models like ChatGPT to assist clinicians in radiology diagnoses and decision processes.&lt;SEP&gt;The use of AI models like ChatGPT as decision support tools in radiology to assist clinicians in diagnosis and treatment planning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal Document Reviews">
  <data key="d0">Legal Document Reviews</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Customization of language models to review and analyze legal texts, improving efficiency and accuracy in legal workflows.&lt;SEP&gt;Use of customized language models to review and analyze legal documents, improving legal workflows.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Healthcare Education, Research, and Practice">
  <data key="d0">Healthcare Education, Research, and Practice</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Utilizing large language models like ChatGPT to enhance medical education, facilitate research, and support clinical practice.&lt;SEP&gt;Utilizing large language models like ChatGPT to enhance medical training, research, and clinical practice.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Continual Learning">
  <data key="d0">Continual Learning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A learning paradigm where models acquire knowledge incrementally over time, adapting to new data without forgetting previous information.&lt;SEP&gt;A learning paradigm where models incrementally acquire knowledge over time, maintaining performance across tasks.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Packing">
  <data key="d0">Knowledge Packing</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The concept of embedding extensive knowledge into model parameters to maximize informational capacity.&lt;SEP&gt;The concept of embedding extensive knowledge into the parameters of a language model to maximize its informational capacity.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter Efficiency">
  <data key="d0">Parameter Efficiency</data>
  <data key="d1">Variables</data>
  <data key="d2">A measure of how effectively a language model uses its parameters to encode knowledge and perform tasks, impacting model size and performance.&lt;SEP&gt;A measure of how effectively a model uses its parameters to encode knowledge and perform tasks, impacting size and performance.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Colin Raffel">
  <data key="d0">Colin Raffel</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher involved in developing transfer learning techniques and transformer models, author of studies on language model limits.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Noam Shazeer">
  <data key="d0">Noam Shazeer</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.&lt;SEP&gt;Researcher contributing to transformer architectures and language models, co-author of transfer learning research.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df&lt;SEP&gt;chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adam Roberts">
  <data key="d0">Adam Roberts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher exploring knowledge capacity and parameter efficiency in large language models.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Katherine Lee">
  <data key="d0">Katherine Lee</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher involved in studies on transfer learning and model evaluation in NLP.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sharan Narang">
  <data key="d0">Sharan Narang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher working on transfer learning and model generalization in NLP contexts.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Michael Matena">
  <data key="d0">Michael Matena</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher contributing to transformer-based NLP models and transfer learning studies.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yanqi Zhou">
  <data key="d0">Yanqi Zhou</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher focusing on transformer models and their applications in NLP.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wei Li">
  <data key="d0">Wei Li</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher involved in language model research, particularly in transfer learning and model scaling.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Peter J Liu">
  <data key="d0">Peter J Liu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researcher contributing to the development of text-to-text transformer models and transfer learning.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="GingGPT">
  <data key="d0">GingGPT</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">GingGPT is a language model designed for solving AI tasks, utilizing ChatGPT and related models within the HuggingFace ecosystem.&lt;SEP&gt;GingGPT is a language model focused on solving AI tasks using ChatGPT and related models within the HuggingFace ecosystem.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv:2303.17580">
  <data key="d0">arXiv:2303.17580</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint identifier for the manuscript discussing GingGPT and its approach to AI tasks.&lt;SEP&gt;A preprint identifier for the research manuscript discussing AI task solutions with ChatGPT and friends.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim">
  <data key="d0">Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2017 paper on continual learning with deep generative replay, contributing to the field of neural information processing.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan">
  <data key="d0">Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 preprint on distilling reasoning capabilities of large language models into smaller models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, Dani Yogatama">
  <data key="d0">Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, Dani Yogatama</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2021 paper on end-to-end training of multi-document readers and retrievers for open-domain question answering.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg">
  <data key="d0">Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a workshop paper on generating situated robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al.">
  <data key="d0">Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 paper on transferability of prompt tuning in NLP, exploring model adaptation techniques.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu">
  <data key="d0">Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 paper on black-box tuning for language-model-as-a-service, focusing on model tuning methods.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yi-Lin Sung, Jaemin Cho, Mohit Bansal">
  <data key="d0">Yi-Lin Sung, Jaemin Cho, Mohit Bansal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 preprint on ladder side-tuning for efficient transfer learning in language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dídac Surís, Sachit Menon, Carl Vondrick">
  <data key="d0">Dídac Surís, Sachit Menon, Carl Vondrick</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2023 preprint on visual inference via Python execution for reasoning with language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.">
  <data key="d0">Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi">
  <data key="d0">Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 preprint on DyLoRA, a parameter-efficient tuning method for pre-trained models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Josef Valvoda, Ryan Cotterell, Simone Teufel">
  <data key="d0">Josef Valvoda, Ryan Cotterell, Simone Teufel</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2023 publication on negative precedent in legal outcome prediction.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin">
  <data key="d0">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of the influential 2017 paper 'Attention is all you need,' foundational to transformer models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, Daniel Matthew Cer">
  <data key="d0">Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, Daniel Matthew Cer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2021 paper on SPoT, a method for better frozen model adaptation through soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, Warren J Gross">
  <data key="d0">Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, Warren J Gross</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 preprint on efficient fine-tuning of compressed language models with learners.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu">
  <data key="d0">Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2022 paper on G-MAP, a general memory-augmented pre-trained language model for domain-specific tasks.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu">
  <data key="d0">Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a 2023 preprint on document-level machine translation using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, Tarek Abdelzaher">
  <data key="d0">Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, Tarek Abdelzaher</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a study on learning to sample and aggregate for few-shot reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ling, et al.">
  <data key="d0">Ling, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of the manuscript, contributing to the research context of GingGPT.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hanul Shin">
  <data key="d0">Hanul Shin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2017 paper on continual learning with deep generative replay.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jung Kwon Lee">
  <data key="d0">Jung Kwon Lee</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2017 paper on continual learning with deep generative replay.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jaehong Kim">
  <data key="d0">Jaehong Kim</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2017 paper on continual learning with deep generative replay.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jiwon Kim">
  <data key="d0">Jiwon Kim</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2017 paper on continual learning with deep generative replay.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kumar Shridhar">
  <data key="d0">Kumar Shridhar</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on distilling reasoning capabilities into smaller models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alessandro Stolfo">
  <data key="d0">Alessandro Stolfo</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on distilling reasoning capabilities into smaller models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mrinmaya Sachan">
  <data key="d0">Mrinmaya Sachan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on distilling reasoning capabilities into smaller models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Devendra Singh">
  <data key="d0">Devendra Singh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on training multi-document readers and retrievers for open-domain QA.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Siva Reddy">
  <data key="d0">Siva Reddy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on training multi-document readers and retrievers.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Will Hamilton">
  <data key="d0">Will Hamilton</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on training multi-document readers and retrievers.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chris Dyer">
  <data key="d0">Chris Dyer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on training multi-document readers and retrievers.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dani Yogatama">
  <data key="d0">Dani Yogatama</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on training multi-document readers and retrievers.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ishika Singh">
  <data key="d0">Ishika Singh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Valts Blukis">
  <data key="d0">Valts Blukis</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Arsalan Mousavian">
  <data key="d0">Arsalan Mousavian</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ankit Goyal">
  <data key="d0">Ankit Goyal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Danfei Xu">
  <data key="d0">Danfei Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jonathan Tremblay">
  <data key="d0">Jonathan Tremblay</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dieter Fox">
  <data key="d0">Dieter Fox</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop paper on generating robot task plans using large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jesse Thomason">
  <data key="d0">Jesse Thomason</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop on robot task planning with language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Animesh Garg">
  <data key="d0">Animesh Garg</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a workshop on robot task planning with language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yusheng Su">
  <data key="d0">Yusheng Su</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiaozhi Wang">
  <data key="d0">Xiaozhi Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yujia Qin">
  <data key="d0">Yujia Qin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chi-Min Chan">
  <data key="d0">Chi-Min Chan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yankai Lin">
  <data key="d0">Yankai Lin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kaiyue Wen">
  <data key="d0">Kaiyue Wen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Peng Li">
  <data key="d0">Peng Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on transferability of prompt tuning in NLP.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tianxiang Sun">
  <data key="d0">Tianxiang Sun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on black-box tuning for language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yunfan Shao">
  <data key="d0">Yunfan Shao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on black-box tuning for language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hong Qian">
  <data key="d0">Hong Qian</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on black-box tuning for language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xuanjing Huang">
  <data key="d0">Xuanjing Huang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on black-box tuning for language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xipeng Qiu">
  <data key="d0">Xipeng Qiu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on black-box tuning for language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yi-Lin Sung">
  <data key="d0">Yi-Lin Sung</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on ladder side-tuning for parameter and memory efficient transfer learning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jaemin Cho">
  <data key="d0">Jaemin Cho</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on ladder side-tuning for transfer learning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mohit Bansal">
  <data key="d0">Mohit Bansal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on ladder side-tuning for transfer learning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dídac Surís">
  <data key="d0">Dídac Surís</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sachit Menon">
  <data key="d0">Sachit Menon</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Carl Vondrick">
  <data key="d0">Carl Vondrick</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mohammadreza Tayaranian">
  <data key="d0">Mohammadreza Tayaranian</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.&lt;SEP&gt;Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Maryam Ziaeefard">
  <data key="d0">Maryam Ziaeefard</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.&lt;SEP&gt;Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="James J Clark">
  <data key="d0">James J Clark</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.&lt;SEP&gt;Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Brett H Meyer">
  <data key="d0">Brett H Meyer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.&lt;SEP&gt;Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Warren J Gross">
  <data key="d0">Warren J Gross</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.&lt;SEP&gt;Author of a 2023 preprint on visual inference via Python execution for reasoning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hugo Touvron">
  <data key="d0">Hugo Touvron</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on Llama 2, open foundation and fine-tuned chat models, published in 2023.&lt;SEP&gt;Author of Llama 2, open foundation and fine-tuned chat models, published in 2023.&lt;SEP&gt;Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Thibaut Lavril">
  <data key="d0">Thibaut Lavril</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Gautier Izacard">
  <data key="d0">Gautier Izacard</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xavier Martinet">
  <data key="d0">Xavier Martinet</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Marie-Anne Lachaux">
  <data key="d0">Marie-Anne Lachaux</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Timothée Lacroix">
  <data key="d0">Timothée Lacroix</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Baptiste Rozière">
  <data key="d0">Baptiste Rozière</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Naman Goyal">
  <data key="d0">Naman Goyal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Eric Hambro">
  <data key="d0">Eric Hambro</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Faisal Azhar">
  <data key="d0">Faisal Azhar</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on Llama, an open and efficient foundation language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ivan Kobyzev">
  <data key="d0">Ivan Kobyzev</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on DyLoRA, parameter-efficient tuning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ali Ghodsi">
  <data key="d0">Ali Ghodsi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on DyLoRA, parameter-efficient tuning.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Josef Valvoda">
  <data key="d0">Josef Valvoda</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 publication on negative precedent in legal outcome prediction.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ryan Cotterell">
  <data key="d0">Ryan Cotterell</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 publication on negative precedent in legal outcome prediction.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Simone Teufel">
  <data key="d0">Simone Teufel</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 publication on negative precedent in legal outcome prediction.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ashish Vaswani">
  <data key="d0">Ashish Vaswani</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Niki Parmar">
  <data key="d0">Niki Parmar</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jakob Uszkoreit">
  <data key="d0">Jakob Uszkoreit</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on models and techniques for question answering and language modeling.&lt;SEP&gt;A researcher working on models for natural language understanding.&lt;SEP&gt;Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.&lt;SEP&gt;Jakob Uszkoreit contributes to transformer architectures and deep learning methodologies.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Llion Jones">
  <data key="d0">Llion Jones</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.&lt;SEP&gt;Llion Jones contributes to research on neural network models and language representation.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Aidan N Gomez">
  <data key="d0">Aidan N Gomez</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Łukasz Kaiser">
  <data key="d0">Łukasz Kaiser</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Illia Polosukhin">
  <data key="d0">Illia Polosukhin</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focused on neural network models and question answering systems.&lt;SEP&gt;A researcher involved in developing question answering methodologies.&lt;SEP&gt;Author of the 2017 paper 'Attention is all you need' introducing the transformer architecture.&lt;SEP&gt;Illia Polosukhin is a researcher contributing to advancements in neural network architectures and question answering systems.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tu Vu">
  <data key="d0">Tu Vu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on SPoT, improving frozen model adaptation via soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Brian Lester">
  <data key="d0">Brian Lester</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on SPoT, improving frozen model adaptation via soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Noah Constant">
  <data key="d0">Noah Constant</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on SPoT, improving frozen model adaptation via soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Rami Al-Rfou">
  <data key="d0">Rami Al-Rfou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on SPoT, improving frozen model adaptation via soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Daniel Matthew Cer">
  <data key="d0">Daniel Matthew Cer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2021 paper on SPoT, improving frozen model adaptation via soft prompt transfer.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Danilo Vucetic">
  <data key="d0">Danilo Vucetic</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 preprint on efficient fine-tuning of compressed language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhongwei Wan">
  <data key="d0">Zhongwei Wan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yichun Yin">
  <data key="d0">Yichun Yin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wei Zhang">
  <data key="d0">Wei Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.&lt;SEP&gt;Researcher involved in evidence aggregation and NLP model development for answer re-ranking.&lt;SEP&gt;Researcher involved in evidence aggregation and NLP model development for question answering.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jiaxin Shi">
  <data key="d0">Jiaxin Shi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Lifeng Shang">
  <data key="d0">Lifeng Shang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Guangyong Chen">
  <data key="d0">Guangyong Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xin Jiang">
  <data key="d0">Xin Jiang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Qun Liu">
  <data key="d0">Qun Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2022 paper on G-MAP, a memory-augmented pre-trained language model.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Longyue Wang">
  <data key="d0">Longyue Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chenyang Lyu">
  <data key="d0">Chenyang Lyu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tianbo Ji">
  <data key="d0">Tianbo Ji</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhirui Zhang">
  <data key="d0">Zhirui Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dian Yu">
  <data key="d0">Dian Yu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shuming Shi">
  <data key="d0">Shuming Shi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhaopeng Tu">
  <data key="d0">Zhaopeng Tu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a 2023 preprint on document-level machine translation with large language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ruijie Wang">
  <data key="d0">Ruijie Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zheng Li">
  <data key="d0">Zheng Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dachun Sun">
  <data key="d0">Dachun Sun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shengzhong Liu">
  <data key="d0">Shengzhong Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jinning Li">
  <data key="d0">Jinning Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bing Yin">
  <data key="d0">Bing Yin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.&lt;SEP&gt;Participates in studies on semantic parsing and language models.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053&lt;SEP&gt;chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tarek Abdelzaher">
  <data key="d0">Tarek Abdelzaher</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a study on learning to sample and aggregate for reasoning over temporal knowledge graphs.</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Preprint">
  <data key="d0">Preprint</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A preliminary research publication that shares findings prior to peer review, used here to disseminate new methodologies and models.&lt;SEP&gt;A preliminary version of a scientific or technical document that presents initial findings and methods before formal peer review.&lt;SEP&gt;A preliminary version of a scientific or technical paper that presents initial findings and ideas before formal peer review.&lt;SEP&gt;A preprint indicates a preliminary version of a research paper or report shared before peer review, providing context for the research process.&lt;SEP&gt;A preprint is a preliminary version of a research paper shared publicly before peer review, providing context for ongoing research or methodology demonstration.&lt;SEP&gt;Preprint indicates a preliminary version of a research paper shared before formal peer review, often used to disseminate early findings.&lt;SEP&gt;Preprint indicates a preliminary, non-peer-reviewed version of research or findings shared publicly to disseminate early results and gather feedback.&lt;SEP&gt;Preprints are early versions of research papers shared publicly before peer review, providing preliminary evidence and insights.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3&lt;SEP&gt;chunk-d47a0e3d5d118be72a9f7b527b440956&lt;SEP&gt;chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-66e5a3f496b5e1530da70355696b5224&lt;SEP&gt;chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Learning to Sample and Aggregate">
  <data key="d0">Learning to Sample and Aggregate</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework for few-shot reasoning over temporal knowledge graphs, involving sampling and aggregation techniques to improve reasoning capabilities.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Few-Shot Reasoning">
  <data key="d0">Few-Shot Reasoning</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates how models can perform reasoning tasks with limited training examples, especially over temporal data.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Temporal Knowledge Graphs">
  <data key="d0">Temporal Knowledge Graphs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Structured representations of entities and their relationships over time, used to evaluate reasoning models.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="K-Adapter">
  <data key="d0">K-Adapter</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for infusing knowledge into pre-trained language models using adapter modules, enabling efficient knowledge integration.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Infusion">
  <data key="d0">Knowledge Infusion</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Methods designed to incorporate external knowledge into pre-trained models to enhance their understanding and performance.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Rationale-Augmented Ensembles">
  <data key="d0">Rationale-Augmented Ensembles</data>
  <data key="d1">Tools</data>
  <data key="d2">Ensemble methods that incorporate rationales or explanations to improve language model reasoning and interpretability.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-Consistency">
  <data key="d0">Self-Consistency</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that encourages models to generate consistent reasoning chains, thereby improving chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chain of Thought Reasoning">
  <data key="d0">Chain of Thought Reasoning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A prompting technique that guides language models to produce intermediate reasoning steps, enhancing complex problem-solving.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-Instruct">
  <data key="d0">Self-Instruct</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A method for aligning language models with self-generated instructions to improve instruction following and task performance.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adamix">
  <data key="d0">Adamix</data>
  <data key="d1">Tools</data>
  <data key="d2">A parameter-efficient tuning method using a mixture of adapters to adapt large language models to new tasks with minimal additional parameters.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Preserving In-Context Learning">
  <data key="d0">Preserving In-Context Learning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques aimed at maintaining a language model's ability to learn from context during fine-tuning processes.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Interactive Planning">
  <data key="d0">Interactive Planning</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Using large language models to enable open-world multi-task agents through interactive planning, supporting complex decision-making.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Describe, Explain, Plan and Select">
  <data key="d0">Describe, Explain, Plan and Select</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework for interactive planning in large language models to facilitate multi-task agent capabilities.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wolfram Superpowers">
  <data key="d0">Wolfram Superpowers</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhancement of ChatGPT with Wolfram Alpha functionalities, adding computational and knowledge capabilities.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bloomberggpt">
  <data key="d0">Bloomberggpt</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large language model specialized for finance, designed to generate financial insights and analysis.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Raise a Child in Large Language Model">
  <data key="d0">Raise a Child in Large Language Model</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A fine-tuning approach aimed at making large language models more effective and generalizable.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="SEQZERO">
  <data key="d0">SEQZERO</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A few-shot compositional semantic parsing approach using sequential prompts and zero-shot models to enhance semantic understanding.</data>
  <data key="d3">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="2109.05687">
  <data key="d0">2109.05687</data>
  <data key="d1">Object of Study</data>
  <data key="d2">A preprint identifier and publication year indicating a specific research document on arXiv.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jingfeng Yang">
  <data key="d0">Jingfeng Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in multiple studies focusing on semantic parsing, language models, and prompt tuning.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Haoming Jiang">
  <data key="d0">Haoming Jiang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in studies related to semantic parsing and language model applications.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Qingyu Yin">
  <data key="d0">Qingyu Yin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to research on semantic parsing and language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Danqing Zhang">
  <data key="d0">Danqing Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in semantic parsing research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Diyi Yang">
  <data key="d0">Diyi Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of studies on semantic parsing and language model applications.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hongye Jin">
  <data key="d0">Hongye Jin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ruixiang Tang">
  <data key="d0">Ruixiang Tang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiaotian Han">
  <data key="d0">Xiaotian Han</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in semantic parsing research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Qizhang Feng">
  <data key="d0">Qizhang Feng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to language model and prompt tuning studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xia Hu">
  <data key="d0">Xia Hu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kai-Cheng Yang">
  <data key="d0">Kai-Cheng Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of work on large language models and credibility rating.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Filippo Menczer">
  <data key="d0">Filippo Menczer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to credibility assessment using large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xianjun Yang">
  <data key="d0">Xianjun Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in dynamic prompting and prompt tuning frameworks.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wei Cheng">
  <data key="d0">Wei Cheng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author to prompt tuning research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xujiang Zhao">
  <data key="d0">Xujiang Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Linda Petzold">
  <data key="d0">Linda Petzold</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to studies on dynamic prompting and optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Haifeng Chen">
  <data key="d0">Haifeng Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in ontology-enhanced prompt tuning research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shumin Deng">
  <data key="d0">Shumin Deng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to language generation and prompt optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hui Chen">
  <data key="d0">Hui Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Feiyu Xiong">
  <data key="d0">Feiyu Xiong</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in ontology and prompt-tuning studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xi Chen">
  <data key="d0">Xi Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in language model optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Huajun Chen">
  <data key="d0">Huajun Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in language generation research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Elad Ben Zaken">
  <data key="d0">Elad Ben Zaken</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of Bitfit, a parameter-efficient fine-tuning method for transformers.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shauli Ravfogel">
  <data key="d0">Shauli Ravfogel</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in fine-tuning language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yoav Goldberg">
  <data key="d0">Yoav Goldberg</data>
  <data key="d1">Researcher</data>
  <data key="d2">Expert in language model fine-tuning and NLP techniques.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Aston Zhang">
  <data key="d0">Aston Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to hypercomplex parameterization and model efficiency studies.&lt;SEP&gt;Participates in AI model prompting research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yi Tay">
  <data key="d0">Yi Tay</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to recitation-augmented models, published in 2022.&lt;SEP&gt;A researcher working on recitation-augmented models, published in 2022.&lt;SEP&gt;Involved in research on model parameterization and efficiency.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Shuai Zhang">
  <data key="d0">Shuai Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in transformer and AI model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alvin Chan">
  <data key="d0">Alvin Chan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in AI model studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Siu Cheung Hui">
  <data key="d0">Siu Cheung Hui</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model parameterization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jie Fu">
  <data key="d0">Jie Fu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to AI model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chaoning Zhang">
  <data key="d0">Chaoning Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of comprehensive surveys on generative AI.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chenshuang Zhang">
  <data key="d0">Chenshuang Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to AI model survey studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sheng Zheng">
  <data key="d0">Sheng Zheng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in generative AI research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yu Qiao">
  <data key="d0">Yu Qiao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI and domain adaptation research.&lt;SEP&gt;See above—also involved in multiple studies related to domain adaptation and language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chenghao Li">
  <data key="d0">Chenghao Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in AI model optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mengchun Zhang">
  <data key="d0">Mengchun Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in AI research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sumit Kumar Dam">
  <data key="d0">Sumit Kumar Dam</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model survey and optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Chu Myaet Thwal">
  <data key="d0">Chu Myaet Thwal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in AI research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ye Lin Tun">
  <data key="d0">Ye Lin Tun</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in AI model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Le Luang Huy">
  <data key="d0">Le Luang Huy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhi Jin">
  <data key="d0">Zhi Jin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in AI fine-tuning research.&lt;SEP&gt;Contributing author in AI fine-tuning.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jia Li">
  <data key="d0">Jia Li</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Jia Li is a researcher contributing to source code datasets, software analysis, and machine learning for code understanding.&lt;SEP&gt;Jia Li is a researcher working on software datasets and source code analysis.&lt;SEP&gt;Participates in language model fine-tuning studies.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ge Li">
  <data key="d0">Ge Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to AI fine-tuning methods.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhongjin Zhang">
  <data key="d0">Zhongjin Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yuqi Zhu">
  <data key="d0">Yuqi Zhu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in AI research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zhang">
  <data key="d0">Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Common surname among multiple authors, involved in AI research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Renrui Zhang">
  <data key="d0">Renrui Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of Llama-adapter, focused on efficient language model fine-tuning.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jiaming Han">
  <data key="d0">Jiaming Han</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on domain adaptation and model efficiency.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Aojun Zhou">
  <data key="d0">Aojun Zhou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in language model fine-tuning research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiangfei Hu">
  <data key="d0">Xiangfei Hu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in domain adaptation studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shilin Yan">
  <data key="d0">Shilin Yan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pan Lu">
  <data key="d0">Pan Lu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in AI model optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hongsheng Li">
  <data key="d0">Hongsheng Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model adaptation research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Peng Gao">
  <data key="d0">Peng Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author in language model fine-tuning.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yinhe Zheng">
  <data key="d0">Yinhe Zheng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiaoxi Mao">
  <data key="d0">Xiaoxi Mao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI fine-tuning studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Minlie Huang">
  <data key="d0">Minlie Huang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in generating chain-of-thought demonstrations, published in 2023.&lt;SEP&gt;A researcher contributing to synthetic prompting techniques, published in 2023.&lt;SEP&gt;Contributing author on unsupervised domain adaptation.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhuosheng Zhang">
  <data key="d0">Zhuosheng Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on automating chain of thought prompting in large language models for better reasoning performance.&lt;SEP&gt;An author working on automating chain of thought prompting in large language models.&lt;SEP&gt;Author of work on chain-of-thought prompting in large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mu Li">
  <data key="d0">Mu Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model efficiency studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alex Smola">
  <data key="d0">Alex Smola</data>
  <data key="d1">Researcher</data>
  <data key="d2">Expert in machine learning and AI model optimization.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hongyu Zhao">
  <data key="d0">Hongyu Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of Tiny-Attention Adapter studies.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hao Tan">
  <data key="d0">Hao Tan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on context importance in attention mechanisms.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hongyuan Mei">
  <data key="d0">Hongyuan Mei</data>
  <data key="d1">Researcher</data>
  <data key="d2">Works on attention mechanisms and model efficiency.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Wayne Xin Zhao">
  <data key="d0">Wayne Xin Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of surveys on large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kun Zhou">
  <data key="d0">Kun Zhou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in large language model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Junjie Li">
  <data key="d0">Junjie Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in AI model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tianyi Tang">
  <data key="d0">Tianyi Tang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on large language model surveys.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xiaolei Wang">
  <data key="d0">Xiaolei Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in AI model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yupeng Hou">
  <data key="d0">Yupeng Hou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yingqian Min">
  <data key="d0">Yingqian Min</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on model surveys.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Beichen Zhang">
  <data key="d0">Beichen Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in research on large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Junjie Zhang">
  <data key="d0">Junjie Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in large model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zican Dong">
  <data key="d0">Zican Dong</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Denny Zhou">
  <data key="d0">Denny Zhou</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in recitation-augmented models, published in 2022.&lt;SEP&gt;A researcher involved in recitation-augmented models, published in 2022.&lt;SEP&gt;An author contributing to research on ensemble methods and reasoning techniques in large language models.&lt;SEP&gt;Author of work on prompting techniques for reasoning in large language models.&lt;SEP&gt;Denny Zhou is an author contributing to research on language model reasoning and ensemble methods.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-2a8c8185b8c21a596df9bcd098f56ca4&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nathanael Schärli">
  <data key="d0">Nathanael Schärli</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributes to prompting and reasoning research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jason Wei">
  <data key="d0">Jason Wei</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author whose work demonstrates that chain of thought prompting elicits reasoning in large language models.&lt;SEP&gt;Involved in human preferences and fine-tuning of language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nathan Scales">
  <data key="d0">Nathan Scales</data>
  <data key="d1">Researcher</data>
  <data key="d2">Participates in model training research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Xuezhi Wang">
  <data key="d0">Xuezhi Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in recitation-augmented language models, published in 2022.&lt;SEP&gt;A researcher involved in recitation-augmented language models, published in 2022.&lt;SEP&gt;Contributing author on large language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Dale Schuurmans">
  <data key="d0">Dale Schuurmans</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in prompting and reasoning techniques.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Claire Cui">
  <data key="d0">Claire Cui</data>
  <data key="d1">Researcher</data>
  <data key="d2">Works on model prompting and inference.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Olivier Bousquet">
  <data key="d0">Olivier Bousquet</data>
  <data key="d1">Researcher</data>
  <data key="d2">Involved in large-scale model research.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Quoc V Le">
  <data key="d0">Quoc V Le</data>
  <data key="d1">Researcher</data>
  <data key="d2">Expert in language model training and prompting.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ed H. Chi">
  <data key="d0">Ed H. Chi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributing author on reasoning in language models.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv:2109.05687">
  <data key="d0">arXiv:2109.05687</data>
  <data key="d1">Object of Study</data>
  <data key="d2">A preprint identifier and publication year indicating a specific research document on arXiv.</data>
  <data key="d3">chunk-fbf68cc2a5698b4c63c49f1529f3175c</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ChatGPT and environmental research">
  <data key="d0">ChatGPT and environmental research</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">This refers to the application of ChatGPT, an AI language model, within the context of environmental research, exploring its potential and implications.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Environmental Science &amp; Technology">
  <data key="d0">Environmental Science &amp; Technology</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A scientific journal focusing on research in environmental science, technology, and related fields.&lt;SEP&gt;A scientific journal publishing research related to environmental science and technology.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving">
  <data key="d0">Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the study on fine-tuning language models from human preferences, contributing to AI methodology and understanding.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuning language models from human preferences">
  <data key="d0">Fine-tuning language models from human preferences</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology for improving AI language models by adjusting them based on human feedback to better align with human values.&lt;SEP&gt;A research approach for improving language models by adjusting them based on human feedback to better align with human values and preferences.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="arXiv preprint arXiv:1909.08593">
  <data key="d0">arXiv preprint arXiv:1909.08593</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint publication presenting research findings on language model fine-tuning, serving as a scientific report and preliminary dissemination of results.&lt;SEP&gt;A preprint publication presenting research on the process and results of fine-tuning language models with human preferences.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Environmental research">
  <data key="d0">Environmental research</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The scientific investigation of environmental issues, including climate change, pollution, and sustainability, often utilizing AI tools like ChatGPT.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="2023">
  <data key="d0">2023</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The year when the referenced research was published, indicating the temporal context of the study.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Daniel M Ziegler">
  <data key="d0">Daniel M Ziegler</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author involved in AI research, particularly on language model fine-tuning.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Nisan Stiennon">
  <data key="d0">Nisan Stiennon</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author involved in AI research, contributing to language model development.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jeffrey Wu">
  <data key="d0">Jeffrey Wu</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author involved in AI research, focusing on language model training and preferences.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Tom B Brown">
  <data key="d0">Tom B Brown</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author involved in AI research, known for work on language models like GPT.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Alec Radford">
  <data key="d0">Alec Radford</data>
  <data key="d1">Researcher</data>
  <data key="d2">Alec Radford is a key figure in developing large language models like GPT, and his work includes training methodologies and evaluation.&lt;SEP&gt;Alec Radford is known for pioneering work on large language models, including GPT series, and their training methodologies.&lt;SEP&gt;Author involved in AI research, significant for contributions to language model architectures.&lt;SEP&gt;Research on neural language models for code tasks.&lt;SEP&gt;Research on neural language models' capacity for parallel code generation and correctness improvement.&lt;SEP&gt;An author known for improving language understanding through generative pre-training, published in 2018.&lt;SEP&gt;An author of work on improving language understanding through generative pre-training, published in 2018.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb&lt;SEP&gt;chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-0cdbfe705edb382a6ee1927e70804633&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Dario Amodei">
  <data key="d0">Dario Amodei</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Author involved in AI research, specializing in model training and safety.&lt;SEP&gt;Dario Amodei is a prominent AI researcher known for work on model safety, evaluation, and large-scale AI systems.&lt;SEP&gt;Dario Amodei is a prominent researcher in AI safety and large language models, contributing to their evaluation and societal impact assessment.&lt;SEP&gt;Research on safety, evaluation, and capabilities of large language models.&lt;SEP&gt;Study on safety, robustness, and evaluation of neural models for parallel code synthesis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-0cdbfe705edb382a6ee1927e70804633&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Paul Christiano">
  <data key="d0">Paul Christiano</data>
  <data key="d1">Researchers</data>
  <data key="d2">Author involved in AI research, focusing on alignment and preferences in language models.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Human preferences">
  <data key="d0">Human preferences</data>
  <data key="d1">Variables</data>
  <data key="d2">The preferences and feedback provided by humans used to guide the fine-tuning of language models.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Language models">
  <data key="d0">Language models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">AI systems like GPT that generate human-like text, subject to fine-tuning and improvements based on research.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model alignment">
  <data key="d0">Model alignment</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The goal of aligning AI language models with human values and preferences through training methodologies.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model safety">
  <data key="d0">Model safety</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges associated with ensuring AI language models behave safely and as intended in various applications.</data>
  <data key="d3">chunk-0cdbfe705edb382a6ee1927e70804633</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Systematic Taxonomy">
  <data key="d0">Systematic Taxonomy</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d2">The taxonomy helps identify and categorize open challenges in domain specialization, guiding future research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Taxonomy of Domain Specialization Techniques">
  <data key="d0">Taxonomy of Domain Specialization Techniques</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d2">The taxonomy categorizes various techniques based on their applicability to different domains and their accessibility to LLMs."|&gt;"classification, domain applicability</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Open Challenges in Domain Specialization">
  <data key="d0">Open Challenges in Domain Specialization</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d2">Different application domains face unique challenges that require tailored solutions for effective LLM adaptation."|&gt;"domain-specific challenges</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Future Research Trends">
  <data key="d0">Future Research Trends</data>
  <data key="d3">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d2">Emerging research directions aim to address current limitations and expand the capabilities of domain-specific LLMs."|&gt;"research development, innovation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="External repositories">
  <data key="d0">External repositories</data>
  <data key="d3">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d2">External repositories like Wikipedia or Wikidata serve as sources of domain-specific information for retrieval."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-Tool Coordination">
  <data key="d0">Multi-Tool Coordination</data>
  <data key="d3">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d2">Models coordinate multiple domain tools and decompose complex tasks into subtasks, guiding the overall process.&lt;SEP&gt;Models coordinate multiple tools and decompose complex tasks into subtasks, guiding the overall process.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LLM">
  <data key="d0">LLM</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d2">Zero-shot prompts are used to test the LLM's ability to perform tasks without prior examples, relying solely on instructions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Zero-shot-CoT">
  <data key="d0">Zero-shot-CoT</data>
  <data key="d3">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d2">Zero-shot-CoT prompts enhance the reasoning ability of LLMs by prompting them to think step-by-step, leading to improved performance on reasoning tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Attention Blocks">
  <data key="d0">Attention Blocks</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d2">Prefix-tuning prepends prompts to sentence embeddings and attention activations, leveraging autoregressive model properties to influence subsequent words.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Event Detection">
  <data key="d0">Event Detection</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d2">KiPT identifies trigger words based on semantic similarity and reformulates sequence tagging into generative event record outputs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt Structure Optimization">
  <data key="d0">Prompt Structure Optimization</data>
  <data key="d3">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d2">Dynamic prompting learns to define prompt position, length, and content dynamically for each input, improving flexibility and effectiveness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Unsupervised Domain Adaptation">
  <data key="d0">Unsupervised Domain Adaptation</data>
  <data key="d3">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d2">Unsupervised domain adaptation utilizes adapters to improve model performance across domains without labeled data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="LoRA">
  <data key="d0">LoRA</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">LoRA implants low-rank modules in parallel to frozen pre-trained weights, enabling efficient fine-tuning without latency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Pre-trained Weights">
  <data key="d0">Pre-trained Weights</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">LoRA implants low-rank modules in parallel to frozen pre-trained weights, enabling efficient fine-tuning without latency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="KronA">
  <data key="d0">KronA</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">Kronecker adapter replaces SVD modules in LoRA with Kronecker product modules to enhance expressivity while maintaining parameter efficiency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multiple Adapters">
  <data key="d0">Multiple Adapters</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">AdapterFusion combines embeddings from multiple adapters trained on different tasks to boost overall performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapter Methods">
  <data key="d0">Adapter Methods</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">UniPELT activates different adapter combinations via gating to adapt to specific data or task setups.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adapters and Routing">
  <data key="d0">Adapters and Routing</data>
  <data key="d3">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d2">Polytropon jointly learns adapters and routing functions to improve multi-task learning across diverse tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Methods and tools">
  <data key="d0">Methods and tools</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">Earth science employs methods like Earth observation, spatial analysis, and simulation modeling, with LLMs aiding in knowledge dissemination and data processing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Fine-tuning, few-shot, zero-shot learning">
  <data key="d0">Fine-tuning, few-shot, zero-shot learning</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">LLMs</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Human Computer Interaction and Software Engineering">
  <data key="d0">Human Computer Interaction and Software Engineering</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">LLMs are tailored to understand user inputs and assist in coding, bug detection, and interface design.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Satellite and aerial data">
  <data key="d0">Satellite and aerial data</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">Satellite and aerial data are used in Earth science to monitor environmental changes, land-use, and climate phenomena.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Climate change, land-use change, natural disasters, environmental development, urbanization">
  <data key="d0">Climate change, land-use change, natural disasters, environmental development, urbanization</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">These core concepts are studied through data, modeling, and analysis to understand and address environmental challenges.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="HCI and Software Engineering">
  <data key="d0">HCI and Software Engineering</data>
  <data key="d3">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d2">LLMs are tailored to understand user inputs, improve interface design, and assist coding tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Adaptation">
  <data key="d0">Model Adaptation</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d2">Evolving domain knowledge necessitates continuous model updates to maintain relevance and accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Understanding">
  <data key="d0">Model Understanding</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d2">Incorporating knowledge graphs into models enhances their ability to understand complex relationships within domain data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Optimization">
  <data key="d0">Model Optimization</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d2">Meta-learning strategies optimize the process of selecting training data, hyperparameters, and adaptation techniques for domain-specific models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model Refinement">
  <data key="d0">Model Refinement</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d2">Human feedback directly influences model updates, ensuring the model remains aligned with domain expertise.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge Acquisition">
  <data key="d0">Knowledge Acquisition</data>
  <data key="d3">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d2">Models actively seek out information on uncertain concepts, improving their understanding of specialized topics.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt-based Distribution Alignment">
  <data key="d0">Prompt-based Distribution Alignment</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">The distribution alignment method is applied within NLP to improve domain generalization in text classification.&lt;SEP&gt;The method is applied to NLP tasks to improve domain generalization in text classification.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Guiding Formal Theorem Provers">
  <data key="d0">Guiding Formal Theorem Provers</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Informal proofs are used to guide the operation of formal theorem provers, facilitating proof generation and validation.&lt;SEP&gt;Informal proofs serve as guidance to formal theorem provers, helping automate the proof process and improve their effectiveness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Robustness and Efficiency">
  <data key="d0">Robustness and Efficiency</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Fine-tuning improves the robustness and efficiency of pre-trained models through regularized optimization techniques.&lt;SEP&gt;Fine-tuning through regularized optimization improves the robustness and efficiency of pre-trained models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Language Understanding and Generation">
  <data key="d0">Language Understanding and Generation</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Prompt learning techniques are employed to improve language understanding and generation tasks, including instance-aware prompt methods.&lt;SEP&gt;Prompt learning techniques are used to enhance models' capabilities in understanding and generating language, including approaches like instance-aware prompting.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neural Language Models">
  <data key="d0">Neural Language Models</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Scaling laws describe how increasing model size, data, and compute impacts the performance of neural language models.&lt;SEP&gt;Scaling laws describe how the performance of neural language models improves with increased size and data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Scaling Laws">
  <data key="d0">Scaling Laws</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Scaling laws describe how increasing model size, data, and compute impacts the performance of neural language models.&lt;SEP&gt;Scaling laws describe how the performance of neural language models improves with increased size and data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Neural Network Efficiency">
  <data key="d0">Neural Network Efficiency</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">These adapter layers enable efficient low-rank adaptation, reducing parameters while maintaining or improving model performance.&lt;SEP&gt;These adapter layers enable efficient low-rank adaptations, reducing the parameter footprint while maintaining performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Dialogue Systems">
  <data key="d0">Dialogue Systems</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Integrating internet access into dialogue systems enhances response quality and knowledge retrieval.&lt;SEP&gt;Integrating internet access into dialogue systems enhances their ability to provide accurate, up-to-date responses and access external knowledge.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Utility">
  <data key="d0">Utility</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">Debates concern whether specialized clinical language models are still necessary given the capabilities of general models, impacting healthcare NLP applications.&lt;SEP&gt;Debates focus on whether specialized clinical language models are still necessary given advances in general models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Financial Sentiment">
  <data key="d0">Financial Sentiment</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d2">GPT-3 is used to analyze and manipulate financial sentiment, demonstrating its application in financial NLP.&lt;SEP&gt;GPT-3 is used to analyze, generate, and manipulate financial sentiment, demonstrating its application in financial NLP tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge-Intensive Tasks">
  <data key="d0">Knowledge-Intensive Tasks</data>
  <data key="d3">chunk-50aebbb39645b3b6929fbd72650593cd&lt;SEP&gt;chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d2">Can combining parametric and non-parametric memory improve model performance on tasks requiring external knowledge?&lt;SEP&gt;Enhancing NLP systems' ability to perform tasks that require external factual knowledge, such as question answering and fact verification.&lt;SEP&gt;Tasks that require models to access and utilize external knowledge sources to perform well, such as open question answering, fact verification, and question generation.&lt;SEP&gt;The study investigates whether combining parametric and non-parametric memory improves performance on knowledge-intensive NLP tasks such as open question answering and fact verification.&lt;SEP&gt;This approach combines retrieval with generation to improve performance on knowledge-intensive NLP tasks.&lt;SEP&gt;This approach enhances performance on knowledge-intensive NLP tasks by combining retrieval mechanisms with generative models.</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Designing">
  <data key="d0">Designing</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">Designing chemical reaction arrays involves using tools like phactor and ChatGPT to systematically plan and execute experiments.&lt;SEP&gt;Designing chemical reaction arrays involves using tools like phactor and ChatGPT to systematically plan experiments.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen Kan, Soujanya Poria">
  <data key="d0">Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen Kan, Soujanya Poria</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These researchers contribute to methodologies for efficient domain adaptation in machine learning, especially using adapters."|&lt;SEP&gt;These researchers contribute to methodologies for efficient domain adaptation using adapters in machine learning."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Domain Adaptation">
  <data key="d0">Research on Domain Adaptation</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These researchers contribute to methodologies for efficient domain adaptation in machine learning, especially using adapters."|&lt;SEP&gt;These researchers contribute to methodologies for efficient domain adaptation using adapters in machine learning."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Language Model Tuning">
  <data key="d0">Research on Language Model Tuning</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These authors explore frameworks for parameter-efficient language model tuning and unified approaches."|&lt;SEP&gt;These authors explore frameworks for parameter-efficient language model tuning."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa">
  <data key="d0">Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These authors explore frameworks for parameter-efficient language model tuning and unified approaches."|&lt;SEP&gt;These authors explore frameworks for parameter-efficient language model tuning."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov">
  <data key="d0">Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These researchers investigate locating, editing, and supporting factual knowledge in language models."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Model Editing">
  <data key="d0">Research on Model Editing</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These researchers investigate locating, editing, and supporting factual knowledge in language models."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving">
  <data key="d0">Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These authors focus on teaching models to support answers with verified quotes."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Verified Quotes in Language Models">
  <data key="d0">Research on Verified Quotes in Language Models</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These authors focus on teaching models to support answers with verified quotes."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Locating and Editing Factual Associations">
  <data key="d0">Research on Locating and Editing Factual Associations</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These researchers focus on methods to locate, edit, and support factual associations and memory in language models."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Research on Supporting Answers with Verified Quotes">
  <data key="d0">Research on Supporting Answers with Verified Quotes</data>
  <data key="d3">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d2">These authors develop techniques for teaching language models to provide answers supported by verified quotes."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Multi-task transfer">
  <data key="d0">Multi-task transfer</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d2">Mad-x enables multi-task cross-lingual transfer learning using adapters, enhancing multilingual NLP capabilities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Decomposed In-Context Learning">
  <data key="d0">Decomposed In-Context Learning</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d2">This methodology improves Text-to-SQL accuracy by decomposing the learning process and applying self-correction techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Self-Correction">
  <data key="d0">Self-Correction</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d2">This methodology improves Text-to-SQL accuracy by decomposing the learning process and applying self-correction techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Lifelong Few-Shot Learning">
  <data key="d0">Lifelong Few-Shot Learning</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d2">Prompt tuning of T5 facilitates lifelong few-shot learning by optimizing prompts for specific tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal Decision Classification">
  <data key="d0">Legal Decision Classification</data>
  <data key="d3">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d2">Hierarchical models with attention improve the classification of legal decisions, demonstrating domain-specific effectiveness.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Colin Raffel et al.">
  <data key="d0">Colin Raffel et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Published research exploring transfer learning limits with a unified text-to-text transformer.&lt;SEP&gt;Published research on transfer learning limits with transformer models, contributing to foundational knowledge in NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Ori Ram et al.">
  <data key="d0">Ori Ram et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Preprint discussing in-context retrieval-augmented language models, contributing to advancements in model retrieval capabilities.&lt;SEP&gt;Preprint discussing retrieval-augmented language models, advancing retrieval techniques in NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Arya Rao et al.">
  <data key="d0">Arya Rao et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Study evaluating ChatGPT as an adjunct in radiologic decision-making, assessing practical healthcare applications.&lt;SEP&gt;Study evaluating ChatGPT as an adjunct in radiology, exploring practical healthcare applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Anastasia Razdaibiedina et al.">
  <data key="d0">Anastasia Razdaibiedina et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on progressive prompts for continual learning in language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Sylvestre-Alvise Rebuffi et al.">
  <data key="d0">Sylvestre-Alvise Rebuffi et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on learning multiple visual domains with residual adapters, relevant for multi-domain models.&lt;SEP&gt;Research on learning multiple visual domains with residual adapters, relevant to multi-domain learning models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Adam Roberts et al.">
  <data key="d0">Adam Roberts et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on knowledge capacity of language models, focusing on parameter packing and information density.&lt;SEP&gt;Research on the capacity of language models to store knowledge, focusing on parameter utilization.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Joshua Robinson et al.">
  <data key="d0">Joshua Robinson et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research demonstrating the use of large language models for multiple choice question answering.&lt;SEP&gt;Study leveraging large language models for multiple choice question answering, demonstrating model capabilities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Jerret Ross et al.">
  <data key="d0">Jerret Ross et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on chemical language representations capturing molecular structures and properties for ML applications in chemistry.&lt;SEP&gt;Research on chemical language representations capturing molecular structures and properties, enabling ML in chemistry.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Malik Sallam">
  <data key="d0">Malik Sallam</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Systematic review on ChatGPT's utility in healthcare education, research, and practice, discussing future perspectives and limitations.&lt;SEP&gt;Systematic review on ChatGPT's utility in healthcare education, research, and practice, highlighting future perspectives and limitations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="ICLR">
  <data key="d0">ICLR</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research showing multitask prompted training enables zero-shot task generalization in language models.&lt;SEP&gt;Research showing that multitask prompted training enables zero-shot task generalization in language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Victor Sanh et al.">
  <data key="d0">Victor Sanh et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research showing multitask prompted training enables zero-shot task generalization in language models.&lt;SEP&gt;Research showing that multitask prompted training enables zero-shot task generalization in language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Timo Schick et al.">
  <data key="d0">Timo Schick et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on Toolformer, a model that can teach itself to use external tools for improved performance.&lt;SEP&gt;Research on Toolformer, a model that can teach itself to use external tools, enhancing self-sufficiency and task performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Timo Schick and Hinrich Schütze">
  <data key="d0">Timo Schick and Hinrich Schütze</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research exploiting cloze questions for few-shot classification and inference, improving few-shot learning techniques.&lt;SEP&gt;Research exploiting cloze questions for few-shot classification and inference.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Proceedings of EMNLP">
  <data key="d0">Proceedings of EMNLP</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on fine-tuned language models as continual learners, emphasizing adaptability across tasks.&lt;SEP&gt;Research on fine-tuned language models as continual learners, emphasizing adaptability.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Thomas Scialom et al.">
  <data key="d0">Thomas Scialom et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on fine-tuned language models as continual learners, emphasizing adaptability across tasks.&lt;SEP&gt;Research on fine-tuned language models as continual learners, emphasizing adaptability.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="IEEE Big Data Conference">
  <data key="d0">IEEE Big Data Conference</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on customizing contextualized language models for legal document review, enhancing legal NLP applications.&lt;SEP&gt;Research on customizing contextualized language models for legal document review, enhancing legal tech applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Shohreh Shaghaghian et al.">
  <data key="d0">Shohreh Shaghaghian et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on customizing contextualized language models for legal document review, enhancing legal NLP applications.&lt;SEP&gt;Research on customizing contextualized language models for legal document review, enhancing legal tech applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yongliang Shen et al.">
  <data key="d0">Yongliang Shen et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Introduction of HuggingGPT, combining ChatGPT with HuggingFace tools for multi-task AI solutions.&lt;SEP&gt;Introduction of HuggingGPT, integrating ChatGPT with HuggingFace tools for multi-task AI solutions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Hanul Shin et al.">
  <data key="d0">Hanul Shin et al.</data>
  <data key="d3">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d2">Research on continual learning with deep generative replay to address catastrophic forgetting.&lt;SEP&gt;Research on continual learning with deep generative replay, addressing catastrophic forgetting.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Deep generative replay">
  <data key="d0">Deep generative replay</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2017 paper contributes to continual learning methods in neural information processing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model distillation">
  <data key="d0">Model distillation</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 preprint focuses on transferring reasoning capabilities into smaller models via semantic decompositions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="End-to-end training">
  <data key="d0">End-to-end training</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2021 work involves training multi-document readers and retrievers for open-domain QA.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Task planning using LLMs">
  <data key="d0">Task planning using LLMs</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their workshop paper discusses generating robot task plans with language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Prompt transferability">
  <data key="d0">Prompt transferability</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 paper explores the transferability of prompt tuning techniques in NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al">
  <data key="d0">Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 paper explores the transferability of prompt tuning techniques in NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Black-box tuning">
  <data key="d0">Black-box tuning</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 work addresses tuning language models as a service without internal access.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter-efficient transfer">
  <data key="d0">Parameter-efficient transfer</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 preprint proposes ladder side-tuning for efficient transfer learning.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Visual inference">
  <data key="d0">Visual inference</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2023 preprint introduces reasoning via Python execution for visual inference tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Foundation models">
  <data key="d0">Foundation models</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2023 preprint presents Llama, an open and efficient foundation language model.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parameter search-free adaptation">
  <data key="d0">Parameter search-free adaptation</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 preprint discusses DyLoRA, a method for parameter-efficient tuning using dynamic low-rank adaptation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Legal outcome prediction">
  <data key="d0">Legal outcome prediction</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2023 publication examines the role of negative precedent in legal outcome predictions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Transformer architecture">
  <data key="d0">Transformer architecture</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2017 paper 'Attention is all you need' introduces the transformer model, foundational to modern NLP.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Model adaptation">
  <data key="d0">Model adaptation</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2021 paper on SPoT discusses better frozen model adaptation through soft prompt transfer.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Memory-augmented models">
  <data key="d0">Memory-augmented models</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 paper introduces G-MAP, a memory-augmented pre-trained language model for domain-specific tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Document-level translation">
  <data key="d0">Document-level translation</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2023 preprint discusses translation at the document level using large language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Knowledge graph reasoning">
  <data key="d0">Knowledge graph reasoning</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their study addresses few-shot reasoning over temporal knowledge graphs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Mojtaba Valipour">
  <data key="d0">Mojtaba Valipour</data>
  <data key="d3">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d2">Their 2022 preprint discusses DyLoRA, a method for efficient parameter tuning of pre-trained models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</node>
<node id="Parallel Code Generation">
  <data key="d0">Parallel Code Generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallel code generation refers to the automated creation of parallel programs using language models, evaluated for correctness and efficiency.&lt;SEP&gt;The study investigates the capability of state-of-the-art large language models to generate parallel code across various computational problem types and programming models, assessing their effectiveness and limitations.&lt;SEP&gt;The study investigates whether state-of-the-art large language models can effectively generate parallel code across various computational problems and programming models, assessing their capabilities, limitations, and potential for aiding software development.&lt;SEP&gt;The process and output of producing code that can run concurrently on multiple processors or cores, a key focus of HPC model evaluation.&lt;SEP&gt;The process of automatically generating code that can execute in parallel, leveraging specific models and datasets to improve efficiency and accuracy.&lt;SEP&gt;The process of producing code capable of executing concurrently across multiple processors or cores, a primary focus for evaluating HPC LLMs.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="ParEval">
  <data key="d0">ParEval</data>
  <data key="d1">Tools</data>
  <data key="d2">A benchmark created to evaluate large language models' performance on 420 diverse coding tasks related to scientific and parallel computing, utilizing novel metrics to assess correctness, efficiency, and suitability of generated code.&lt;SEP&gt;A benchmarking framework designed to evaluate the ability of large language models to generate parallel code across various problem types and execution models.&lt;SEP&gt;A comprehensive benchmarking framework designed to evaluate the ability of large language models to generate parallel code across multiple problem types and execution models.&lt;SEP&gt;ParEval is a benchmark created to evaluate the performance of language models on 420 coding tasks related to scientific and parallel computing, utilizing novel metrics to assess code quality and effectiveness.&lt;SEP&gt;ParEval is a methodology for evaluating language models on their ability to generate and complete code prompts, including execution and translation tasks.&lt;SEP&gt;ParEval is a methodology for systematically evaluating LLMs' ability to generate, complete, and translate code across different execution models, using metrics derived from execution and correctness.&lt;SEP&gt;A benchmark for evaluating parallel code generation capabilities of models, used to compare the performance of fine-tuned models.&lt;SEP&gt;ParEval is a benchmark comprising 420 coding problems across various problem types and execution models used to evaluate the correctness and efficiency of code generated by language models.&lt;SEP&gt;ParEval is a benchmarking suite containing 420 coding problems across various problem types and execution models, used to evaluate models' correctness and diversity in code generation.&lt;SEP&gt;ParEval is a benchmarking tool used to assess the correctness of code generated by models across multiple problems and models, providing pass@1 metrics.&lt;SEP&gt;ParEval is a benchmarking tool used to assess the correctness of generated code across multiple models and problem types, providing pass@1 metrics.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance Evaluation Metrics">
  <data key="d0">Performance Evaluation Metrics</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Novel metrics are introduced to assess how well generated code performs in terms of correctness, efficiency, and suitability for parallel computing tasks.&lt;SEP&gt;Novel metrics introduced to measure the quality, correctness, and performance of generated code, especially in the context of parallel programming tasks.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Scientific and Parallel Computing">
  <data key="d0">Scientific and Parallel Computing</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Domains involving complex algorithms and systems that benefit from parallelization to improve computational performance on multi-core, GPU, and distributed systems.&lt;SEP&gt;The domain of scientific and parallel computing involves complex algorithms and systems that benefit from parallel code to improve performance on multi-core and distributed systems.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Coding Tasks">
  <data key="d0">Coding Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The 420 coding tasks in ParEval represent diverse scientific and parallel computing problems used to evaluate language models' capabilities.&lt;SEP&gt;The 420 specific programming problems in ParEval representing a wide range of scientific and parallel computing challenges used to evaluate model performance.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Evaluation of Language Models">
  <data key="d0">Evaluation of Language Models</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A systematic assessment of multiple large language models' ability to generate correct and efficient parallel code across diverse tasks and programming models, using the ParEval benchmark.&lt;SEP&gt;The study uses a systematic evaluation of multiple language models on the ParEval benchmark, analyzing their ability to generate correct and efficient parallel code across various problem types and models.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code Performance">
  <data key="d0">Code Performance</data>
  <data key="d1">Results</data>
  <data key="d2">The effectiveness of generated code is measured through new metrics, revealing strengths and weaknesses of models in handling complex parallel programming tasks.&lt;SEP&gt;The effectiveness of generated parallel code is measured through novel metrics, revealing strengths and limitations of current language models in handling complex parallel programming tasks.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Complexity of Parallel Code">
  <data key="d0">Complexity of Parallel Code</data>
  <data key="d1">Results</data>
  <data key="d2">Models show limitations in generating highly complex parallel algorithms that require reasoning about data distribution, synchronization, and performance optimization.&lt;SEP&gt;The study finds that language models struggle with generating highly complex parallel algorithms, especially those requiring reasoning about data distribution and synchronization.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Limitations of Current Models">
  <data key="d0">Limitations of Current Models</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current large language models have limitations in generating correct, optimized, and reliable parallel code for complex scientific computing tasks.&lt;SEP&gt;Existing large language models struggle with reliably producing correct, optimized, and robust parallel code for complex scientific problems, indicating areas for future research and development.</data>
  <data key="d3">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Implications for Software Development">
  <data key="d0">Implications for Software Development</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Enhanced tools and methodologies for developing high-performance parallel software, leading to more efficient computation workflows.&lt;SEP&gt;Findings suggest that while LLMs can assist with certain aspects of parallel code generation, they are not yet capable of replacing expert programmers for complex parallel programming tasks, highlighting the need for further improvements.&lt;SEP&gt;Findings suggest that while LLMs can assist with some aspects of parallel code generation, further improvements are needed before they can fully replace expert programmers in complex parallel computing tasks.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="benchmarks">
  <data key="d0">benchmarks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Benchmarks are standardized sets of tasks and problems designed to evaluate the capabilities of large language models (LLMs) in generating parallel code across different problem types and execution models.&lt;SEP&gt;Benchmarks are standardized tests or sets of tasks designed to evaluate the capabilities of LLMs in generating parallel code, covering various problem types and execution models.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="metrics">
  <data key="d0">metrics</data>
  <data key="d1">Variables</data>
  <data key="d2">Metrics are quantitative measures used to assess the correctness, performance, scalability, and usefulness of generated parallel code, including novel metrics such as speedup n@k and efficiency n@k.&lt;SEP&gt;Metrics are quantitative measures used to evaluate the usefulness, correctness, performance, and scalability of generated parallel code, including novel metrics like speedup n@k and efficiency n@k.&lt;SEP&gt;Two novel metrics introduced to evaluate runtime performance and scaling behavior of generated parallel code.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel Code Generation Evaluation (ParEval)">
  <data key="d0">Parallel Code Generation Evaluation (ParEval)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">ParEval is a benchmark designed to assess how well LLMs generate and translate parallel code across different computational problem types and execution models.&lt;SEP&gt;ParEval is a benchmark designed to evaluate the ability of large language models (LLMs) to generate parallel code, incorporating metrics for runtime performance and scalability.&lt;SEP&gt;ParEval is a comprehensive benchmark comprising multiple prompts and problem types to evaluate how well LLMs generate and translate parallel code across various computational problem types and parallel execution models.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="computational problem types">
  <data key="d0">computational problem types</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Different classes of computational problems such as sparse, unstructured, and other problem types used to test LLMs' ability to generate appropriate parallel code.&lt;SEP&gt;Different types of computational problems (e.g., sparse, unstructured problems) used to evaluate LLMs' capabilities in parallel code generation.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="execution models">
  <data key="d0">execution models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Execution models such as MPI, OpenMP, and Kokkos define different paradigms for parallel programming and are analyzed for their compatibility with code translation and performance.&lt;SEP&gt;Execution models such as MPI, OpenMP, and Kokkos define different paradigms for parallel programming and are used as contexts for code translation and performance assessment.&lt;SEP&gt;Various parallel execution models including serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP, used in benchmarking LLMs' code generation capabilities.&lt;SEP&gt;Various parallel execution models such as serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP, used in benchmarking LLMs' performance.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-ecf1af014959d2db25d1a41ffd04a15c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="state-of-the-art LLMs">
  <data key="d0">state-of-the-art LLMs</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Advanced large language models like GPT-3.5 evaluated for their ability to generate parallel code, with performance metrics indicating current limitations and areas for improvement.&lt;SEP&gt;Advanced large language models like GPT-3.5, evaluated for their ability to generate correct, performant, and scalable parallel code, with performance metrics indicating current limitations and areas for improvement.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="correctness">
  <data key="d0">correctness</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of whether generated code meets functional requirements and accurately implements intended algorithms across different models and problem types.&lt;SEP&gt;Assessment of whether the generated code correctly implements the intended algorithms and problem solutions across different models and problem types.&lt;SEP&gt;Correctness indicates whether the generated code produces the expected results or outputs without errors, often assessed through pass@1 scores or validation checks.&lt;SEP&gt;The degree to which generated code accurately implements the intended parallel algorithms, measured by pass@1.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="scalability">
  <data key="d0">scalability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Evaluation of how well the generated parallel code scales with increased computational resources, often revealing poor scalability in current models.&lt;SEP&gt;The ability of generated parallel code to scale efficiently with increased computational resources, often found to be poor in current LLM outputs.&lt;SEP&gt;The ability of parallel code to maintain performance as problem size or computational resources increase.&lt;SEP&gt;Scalability concerns how well a system or algorithm performs as the size or resources increase, crucial for large applications.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-ecf1af014959d2db25d1a41ffd04a15c&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="translation between execution models">
  <data key="d0">translation between execution models</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates how well LLMs can translate code between different parallel execution models and how this affects correctness and performance.&lt;SEP&gt;Investigation into how well LLMs can translate code between different parallel execution models and how this impacts correctness and performance.</data>
  <data key="d3">chunk-ecf1af014959d2db25d1a41ffd04a15c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Token in a sequence">
  <data key="d0">Token in a sequence</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A fundamental unit in text processing representing individual elements within a sequence, used in models like transformers for NLP tasks.&lt;SEP&gt;A fundamental unit representing individual elements within a sequence used in language models like transformers for processing text data.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Transformer-based models">
  <data key="d0">Transformer-based models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Machine learning architectures that utilize self-attention mechanisms to effectively model sequential data, foundational in NLP and language modeling.&lt;SEP&gt;Machine learning models that utilize self-attention mechanisms to process sequential data effectively, widely used in NLP.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Large Language Models for Code">
  <data key="d0">Large Language Models for Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">AI models trained on extensive code repositories and natural language corpora to generate, understand, and manipulate programming code across multiple languages.&lt;SEP&gt;AI models trained on large corpora of code and natural language to generate or understand programming code.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pre-training corpus">
  <data key="d0">Pre-training corpus</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large datasets like The Stack and The Pile used to train large language models, covering diverse code and natural language data.&lt;SEP&gt;Large datasets such as The Stack and The Pile, comprising diverse code and natural language data, used to train large language models for code and NLP tasks.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CodeLlama">
  <data key="d0">CodeLlama</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A family of large language models that demonstrate varying performance levels on parallel programming tasks, especially with complex models like CodeLlama-34B.&lt;SEP&gt;A family of models based on Llama 2, fine-tuned specifically for code generation, supporting multiple sizes, infilling, and extended context lengths.&lt;SEP&gt;A specific natural language model fine-tuned on a corpus of code to enhance code generation capabilities.&lt;SEP&gt;A specific natural language model fine-tuned on a large corpus of code to improve performance in code generation and understanding.&lt;SEP&gt;CodeLlama is a family of open-source models based on Llama 2, fine-tuned for code generation, supporting various sizes, infilling, and extended context lengths, designed for practical code synthesis.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-107fb981331e6facca90c9849f179ee2&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Token selection strategy">
  <data key="d0">Token selection strategy</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Methods for choosing the next token during text generation to optimize output quality, including techniques like nucleus sampling and temperature scaling.&lt;SEP&gt;Methods used to choose the next token during text generation to improve output quality.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model Temperature">
  <data key="d0">Model Temperature</data>
  <data key="d1">Variables</data>
  <data key="d2">A parameter that scales logits before softmax, influencing the randomness and creativity of generated outputs; lower values produce more conservative results, higher values more diverse outputs.&lt;SEP&gt;A parameter that scales the logits before softmax, controlling the randomness of generated outputs; lower values produce more conservative outputs, higher values increase diversity.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code generation tasks">
  <data key="d0">Code generation tasks</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigations into how effectively language models can generate syntactically correct and semantically meaningful code snippets across various programming languages.&lt;SEP&gt;Questions about how effectively language models can generate syntactically correct, semantically meaningful code snippets across different programming languages.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Applying LLMs to parallel and HPC code">
  <data key="d0">Applying LLMs to parallel and HPC code</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigations into how large language models can be adapted or fine-tuned to generate, analyze, or optimize high-performance computing code, including tasks like labeling pragmas and predicting performance.&lt;SEP&gt;Research exploring the adaptation, fine-tuning, and application of large language models to generate, analyze, or optimize high-performance computing (HPC) code, including tasks like labeling OpenMP pragmas and performance prediction.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Benchmarking LLMs for code-related tasks">
  <data key="d0">Benchmarking LLMs for code-related tasks</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Evaluation frameworks and datasets such as HumanEval, MBPP, DS-1000, GSM8K, and CoderEval used to assess the performance, accuracy, and capabilities of language models in code generation.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="HPC-specific models">
  <data key="d0">HPC-specific models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Models like HPCCoder and other fine-tuned LLMs designed specifically for HPC code, capable of generating parallel code, labeling pragmas, and predicting performance metrics.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="HPC tasks">
  <data key="d0">HPC tasks</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Specific tasks such as code generation, pragma labeling, and performance prediction in high-performance computing environments to evaluate the effectiveness of LLMs in HPC contexts.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Performance prediction">
  <data key="d0">Performance prediction</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The task of estimating execution metrics like runtime or efficiency in HPC code, relevant for evaluating LLM capabilities.</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="HPCCoder">
  <data key="d0">HPCCoder</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A model fine-tuned on HPC code designed to generate HPC programs, label OpenMP pragmas, and predict performance metrics.&lt;SEP&gt;A model fine-tuned specifically on HPC code to generate, label, and predict performance of HPC programs.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="TOKOMPILER">
  <data key="d0">TOKOMPILER</data>
  <data key="d1">Tools</data>
  <data key="d2">An HPC-specific tokenizer designed for large language models to improve understanding and processing of HPC code.&lt;SEP&gt;An HPC-specific tokenizer developed for large language models to improve processing and understanding of HPC code.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="COMPCODER">
  <data key="d0">COMPCODER</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A model trained on C, C++, and Fortran code to assist in HPC code generation and analysis.&lt;SEP&gt;A trained model on C, C++, and Fortran code used for HPC code generation and analysis.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Munley et al.">
  <data key="d0">Munley et al.</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Evaluate the ability of large language models to generate compiler verification tests for parallel OpenACC code.&lt;SEP&gt;Evaluate whether large language models can generate compiler verification tests for parallel OpenACC code, assessing their utility in compiler validation processes.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Chen et al.">
  <data key="d0">Chen et al.</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigate if large language models can identify data races in parallel code and contribute to datasets for data race detection.&lt;SEP&gt;Use large language models to identify data races in parallel code and develop datasets for this purpose.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="DRB-ML dataset">
  <data key="d0">DRB-ML dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A dataset created to facilitate training and evaluation of models for data race detection in HPC code, integrated into the LM4HPC framework.&lt;SEP&gt;A dataset integrated into the LM4HPC framework to facilitate training and evaluation of models on data race detection.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Godoy et al.">
  <data key="d0">Godoy et al.</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Assess the capabilities of large language models in generating HPC kernels across a limited set of problems, focusing on their performance and generalization.&lt;SEP&gt;Assess the capabilities of large language models in generating HPC kernels across limited problem sets.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Valero-Lara et al.">
  <data key="d0">Valero-Lara et al.</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Evaluate the effectiveness of LLMs in generating HPC kernels using standard evaluation practices across diverse problem sets.&lt;SEP&gt;Evaluate the performance of LLMs in generating HPC kernels using standard evaluation practices.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Prompts for Parallel Code Generation">
  <data key="d0">Prompts for Parallel Code Generation</data>
  <data key="d1">Tools</data>
  <data key="d2">Concrete prompts created to test LLMs' ability to generate parallel code for different computational problems and models.&lt;SEP&gt;Concrete prompts created to test LLMs' ability to produce parallel code solutions for various computational problems and models.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Problem Types">
  <data key="d0">Problem Types</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Categories of computational problems such as sorting, scanning, linear algebra, search, reduction, histogram, stencil, graph algorithms, geometry, Fourier transforms, and mapping, each with specific characteristics and challenges.&lt;SEP&gt;Categories of computational problems such as sorting, scanning, linear algebra, search, reduction, histogram, stencil, graph algorithms, geometry, Fourier transforms, and mapping, each with specific characteristics.&lt;SEP&gt;Different problem types are used to evaluate the models' versatility and performance across various computational challenges, such as serial, parallel, and hybrid approaches.&lt;SEP&gt;Problem types refer to different types of computational problems or tasks that models are tested on, such as serial processing, parallel processing, and specific algorithms.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Execution Models">
  <data key="d0">Execution Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Different paradigms for parallel execution such as serial, OpenMP, CUDA/HIP, MPI, and Kokkos, influencing model success rates.&lt;SEP&gt;Different programming paradigms or APIs (seven in total) used to implement parallel code in prompts, including sequential, OpenMP, MPI, etc.&lt;SEP&gt;Different programming paradigms or APIs (seven in total) used to implement parallel code in prompts, such as sequential, OpenMP, MPI, etc.&lt;SEP&gt;Execution models like OpenMP, MPI, CUDA, Kokkos are used as the environments for code translation and generation tasks to evaluate model adaptability and accuracy.&lt;SEP&gt;Execution models refer to programming paradigms such as serial, OpenMP, MPI, CUDA, HIP, and Kokkos, representing different approaches to parallel and distributed computing.&lt;SEP&gt;Execution models such as serial, OpenMP, CUDA/HIP, MPI, and Kokkos define the parallelization approach and influence model success rates.&lt;SEP&gt;Execution models such as serial, OpenMP, MPI, CUDA, HIP, and Kokkos define paradigms for parallel and distributed computing, influencing code structure and performance.&lt;SEP&gt;Execution models describe the computational approaches used to run problems, including serial, OpenMP (omp), Kokkos, CUDA, HIP, MPI, and MPI+OpenMP, affecting performance outcomes.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Evaluation Techniques">
  <data key="d0">Evaluation Techniques</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Standardized methods for automatically assessing correctness and performance of generated code across various prompts and models.&lt;SEP&gt;Standardized methods to automatically assess the correctness and performance of generated code across various prompts and models.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Design of ParEval">
  <data key="d0">Design of ParEval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A comprehensive benchmark comprising 420 prompts, covering 12 problem types and 7 execution models, aimed at systematically evaluating LLM capabilities in parallel code generation.&lt;SEP&gt;A systematic benchmark comprising 420 prompts, covering 12 problem types and 7 execution models, aimed at evaluating LLMs' capabilities in parallel code generation.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Problem Sets">
  <data key="d0">Problem Sets</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Sets of problems designed to test the core functionalities within each problem type, with variations to prevent copying from training data.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Evaluation Benchmarks">
  <data key="d0">Evaluation Benchmarks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Standardized sets of prompts and evaluation procedures used to compare LLM performance across different problem types and models.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Assessment Criteria">
  <data key="d0">Assessment Criteria</data>
  <data key="d1">Results</data>
  <data key="d2">Criteria such as correctness, efficiency, and compliance with parallel programming standards used to evaluate generated code.</data>
  <data key="d3">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="component counting">
  <data key="d0">component counting</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Component counting involves quantifying individual parts within a system or structure, serving as a fundamental aspect of analysis in various fields.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Geometry">
  <data key="d0">Geometry</data>
  <data key="d1">Methods/Techniques</data>
  <data key="d2">Geometry involves analyzing shapes, sizes, and properties of figures, including computing geometric attributes such as convex hulls.&lt;SEP&gt;Geometry is the branch of mathematics concerned with the properties and relations of points, lines, surfaces, and solids, including computational methods like convex hulls.&lt;SEP&gt;Problems involving geometric computations, known to be difficult for LLMs.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Convex Hull">
  <data key="d0">Convex Hull</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The convex hull is the smallest convex polygon or polyhedron that contains all points in a given set, used in computational geometry for shape analysis.&lt;SEP&gt;The convex hull is the smallest convex shape that encloses a set of points, used in computational geometry to analyze spatial data.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Fourier Transform">
  <data key="d0">Fourier Transform</data>
  <data key="d1">Tools</data>
  <data key="d2">Fourier Transform is a mathematical technique to decompose functions or signals into frequencies, with standard and inverse forms used in signal processing.&lt;SEP&gt;The Fourier Transform is a mathematical technique that decomposes functions or signals into their constituent frequencies, with standard and inverse forms used in signal processing.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Transform">
  <data key="d0">Transform</data>
  <data key="d1">Methods/Techniques</data>
  <data key="d2">Mapping a constant function to each element of an array involves applying a transformation to data structures, facilitating operations like normalization or data processing.&lt;SEP&gt;Mapping a constant function to each element of an array using transform techniques to manipulate data in computational tasks.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="C++&quot;,">
  <data key="d0">C++",</data>
  <data key="d1">Tools</data>
  <data key="d2">C++ is a programming language used in the prompts for code generation tasks, supporting various parallel programming models.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CUDA">
  <data key="d0">CUDA</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A parallel computing platform and API developed by NVIDIA for GPU acceleration, allowing high-performance computations.&lt;SEP&gt;A parallel computing platform and API model created by NVIDIA for leveraging GPU acceleration.&lt;SEP&gt;CUDA is a GPU programming platform by Nvidia, facilitating high-performance computing, and is compared with HIP regarding LLM efficiency.&lt;SEP&gt;CUDA is a parallel computing platform and API model created by NVIDIA, used for GPU-accelerated code execution in the evaluation.&lt;SEP&gt;CUDA is a parallel computing platform and API model created by Nvidia, used to develop applications that run on Nvidia GPUs, and is compared to HIP in the study.&lt;SEP&gt;CUDA is a parallel computing platform and API model developed by NVIDIA for GPU programming, enabling high-performance computations.&lt;SEP&gt;CUDA is a parallel computing platform and programming model for NVIDIA GPUs, used in code prompts to generate GPU-accelerated code.&lt;SEP&gt;Compute Unified Device Architecture (CUDA) is a parallel computing platform and API model created by Nvidia, used as an input code source in the study.&lt;SEP&gt;CUDA is a parallel computing platform and programming model developed by NVIDIA for leveraging GPU acceleration in computations.&lt;SEP&gt;CUDA is a parallel computing platform and API developed by NVIDIA, allowing direct programming of GPUs for offloading and accelerating computations.&lt;SEP&gt;CUDA is a parallel computing platform and API developed by NVIDIA, used for executing computations on GPUs to accelerate processing tasks.&lt;SEP&gt;CUDA is a parallel computing platform and API model created by NVIDIA, allowing direct programming of GPUs for offloading computations.&lt;SEP&gt;CUDA is a parallel computing platform and API model created by NVIDIA, used for leveraging GPU hardware to accelerate computations.&lt;SEP&gt;CUDA is a parallel computing platform and API model supporting GPU programming, enabling data transfer, kernel execution, and synchronization on NVIDIA GPUs.&lt;SEP&gt;CUDA is a parallel computing platform and API model created by NVIDIA, enabling GPU acceleration for computational tasks.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-b943bff32a36cd0ce9ab43893da92b5b&lt;SEP&gt;chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="HIP">
  <data key="d0">HIP</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A C++ runtime API and kernel language that allows developers to create portable applications across AMD and NVIDIA GPUs.&lt;SEP&gt;A C++ runtime API and kernel language that offers portable GPU programming across AMD and NVIDIA hardware.&lt;SEP&gt;HIP (Heterogeneous-compute Interface for Portability) is a C++ runtime API that allows code to run on AMD and Nvidia GPUs, similar to CUDA, and is analyzed for its efficiency with LLMs.&lt;SEP&gt;HIP is a GPU programming platform similar to CUDA, supporting AMD GPUs, used in code prompts for parallel programming.&lt;SEP&gt;HIP is a GPU programming platform supporting AMD hardware, allowing code portability across different GPU architectures.&lt;SEP&gt;HIP is an API for GPU programming that supports AMD and Nvidia hardware, similar to CUDA, and is evaluated for LLM performance.&lt;SEP&gt;HIP (Heterogeneous-compute Interface for Portability) is an API that allows code portability between AMD and NVIDIA GPUs, facilitating cross-platform GPU programming.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Kokkos">
  <data key="d0">Kokkos</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A C++ library aimed at performance portability across different hardware architectures.&lt;SEP&gt;A C++ library that provides performance portability across diverse hardware architectures, enabling scalable parallel programming.&lt;SEP&gt;A high-level parallel programming library providing abstractions for writing portable parallel code across different hardware architectures.&lt;SEP&gt;Kokkos is a C++ library designed for performance portability, allowing code to run efficiently across diverse architectures including CPUs and GPUs.&lt;SEP&gt;Kokkos is a C++ library for performance portability in parallel programming, enabling code to run efficiently across different architectures.&lt;SEP&gt;Kokkos is a high-level parallel programming model providing abstractions to write portable code for different parallel architectures.&lt;SEP&gt;Kokkos is a parallel programming framework aimed at performance portability across hardware architectures, used as a benchmark for LLM code generation capabilities.&lt;SEP&gt;Kokkos is a parallel programming model designed to enable performance portability across different hardware architectures, such as CUDA and HIP.&lt;SEP&gt;Kokkos is a performance portability library for parallel programming, used as an execution model in the evaluation.&lt;SEP&gt;Kokkos is a programming model in C++ for writing performance-portable applications targeting diverse hardware architectures, used as an execution model.&lt;SEP&gt;A software library enabling performance portability across manycore architectures via polymorphic memory access patterns.&lt;SEP&gt;Kokkos is a high-level programming model aimed at performance portability across diverse hardware architectures in HPC.&lt;SEP&gt;Kokkos is a performance portability library for writing scalable parallel applications across diverse hardware architectures.&lt;SEP&gt;A C++ library for performance portability across different hardware architectures, focusing on node-level parallelism.&lt;SEP&gt;A performance portability library for HPC applications, supporting execution across different hardware architectures at the node level.&lt;SEP&gt;Kokkos is a programming model and library designed for performance portability across different hardware architectures, enabling efficient parallel computations.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PartialMinimums">
  <data key="d0">PartialMinimums</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">PartialMinimums is a function that computes the minimum value in an array up to each index, exemplifying a parallel computation task.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Example Inputs and Outputs">
  <data key="d0">Example Inputs and Outputs</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Examples of input/output pairs demonstrating the behavior of the PartialMinimums function, used to clarify problem requirements.&lt;SEP&gt;Sample input/output pairs demonstrate how the PartialMinimums function operates, clarifying problem requirements and expected behavior.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Component Counting">
  <data key="d0">Component Counting</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Component counting involves quantifying individual parts within a system or structure, fundamental to analysis in various scientific and engineering disciplines.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Problem Variations">
  <data key="d0">Problem Variations</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Variations of problems, such as reverse prefix sums, are used to evaluate the adaptability and generalization of computational models.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="C++">
  <data key="d0">C++</data>
  <data key="d1">The C++ programming language</data>
  <data key="d2">A widely used programming language known for its performance and system-level capabilities.&lt;SEP&gt;A widely used programming language known for performance, system-level programming, and object-oriented features.&lt;SEP&gt;C++ is a programming language used in prompts to generate code for various parallel programming models, supporting object-oriented and generic programming.&lt;SEP&gt;C++ is a widely used programming language known for its performance and system-level capabilities, detailed in Bjarne Stroustrup’s book.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PartialMinimums Function">
  <data key="d0">PartialMinimums Function</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The PartialMinimums function computes, for each array element, the minimum value from the start up to that position, exemplifying parallel prefix computations.</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="MBPP">
  <data key="d0">MBPP</data>
  <data key="d1">Results</data>
  <data key="d2">MBPP results refer to performance metrics obtained from a specific benchmark dataset used to evaluate code generation quality.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Section 7">
  <data key="d0">Section 7</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Section 7 describes the metrics and evaluation procedures used to assess the models' code generation performance.&lt;SEP&gt;Section 7 details the evaluation procedures, metrics, and analysis methods used to assess the performance of LLMs in code generation and translation tasks.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="OpenMP, MPI, MPI+OpenMP, Kokkos, CUDA, HIP">
  <data key="d0">OpenMP, MPI, MPI+OpenMP, Kokkos, CUDA, HIP</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">These are different parallel execution models and programming frameworks used to evaluate the models' ability to generate code for various hardware and software environments.&lt;SEP&gt;These are parallel programming frameworks and execution models used as targets and sources for code translation tasks to evaluate LLMs' translation accuracy and effectiveness.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Serial, OpenMP, MPI, CUDA, Kokkos">
  <data key="d0">Serial, OpenMP, MPI, CUDA, Kokkos</data>
  <data key="d1">Variables</data>
  <data key="d2">These are the different programming paradigms and execution models involved in the translation experiments, representing source and target environments for code transformation.&lt;SEP&gt;These represent different programming paradigms or execution models, with the model translating code between these models to assess performance and correctness.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="sumOfMinimumElements">
  <data key="d0">sumOfMinimumElements</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A function that computes the sum of the minimum elements between two vectors, used as an example task for code translation evaluation.&lt;SEP&gt;A specific code function used as a benchmark task to evaluate LLMs' ability to generate correct code for different execution models.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Llama 2">
  <data key="d0">Llama 2</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An open foundation and fine-tuned chat model designed for natural language processing tasks, representing an advancement in large language models.&lt;SEP&gt;Llama 2 is a foundational language model architecture that serves as the base for models like CodeLlama, supporting long contexts and fine-tuning for code tasks.&lt;SEP&gt;Llama 2 is a foundational language model architecture, extended and fine-tuned for code generation in models like CodeLlama, supporting long contexts and infilling.&lt;SEP&gt;Llama 2 is an open foundation and fine-tuned chat model developed for natural language understanding and generation tasks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="StarCoderBase">
  <data key="d0">StarCoderBase</data>
  <data key="d1">Tools</data>
  <data key="d2">StarCoderBase is a 15.5 billion parameter code-focused language model trained on extensive datasets, supporting multiple programming languages, infilling, and context lengths up to 8K tokens.&lt;SEP&gt;StarCoderBase is a 15.5-billion-parameter code-focused language model trained on a large dataset, supporting infilling and multiple programming languages.&lt;SEP&gt;StarCoderBase is a foundational language model applied in code generation tasks, including parallel code translation and efficiency analysis, serving as a baseline for performance comparison.&lt;SEP&gt;StarCoderBase is a foundational language model applied in code generation tasks, including parallel code translation and efficiency analysis.&lt;SEP&gt;StarCoderBase is a language model supporting code infilling and custom tokens, based on the SantaCoder architecture, with a context length of 8K tokens, used for code generation and comparison in research.&lt;SEP&gt;StarCoderBase is an open-source language model supporting code infilling, custom tokens, and trained based on SantaCoder architecture, with a context length of 8K tokens, used for code generation and benchmarking.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Translation Tasks">
  <data key="d0">Translation Tasks</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The study hypothesizes that LLMs can effectively translate code between different execution models, and the evaluation measures this capability.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel Code Translation">
  <data key="d0">Parallel Code Translation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A specific experiment focused on translating code from one parallel execution model to another, such as serial to OpenMP or CUDA to Kokkos.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Natural Language Descriptions">
  <data key="d0">Natural Language Descriptions</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Natural language prompts used to instruct LLMs for code generation and translation tasks.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Problem Type">
  <data key="d0">Problem Type</data>
  <data key="d1">Variables</data>
  <data key="d2">Different problem types or code tasks used in evaluation, such as sum of minimum elements, serve as benchmarks for model performance.</data>
  <data key="d3">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="The Stack">
  <data key="d0">The Stack</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The Stack is a comprehensive dataset containing 1 trillion tokens, including code from over 80 programming languages, natural language in git commits, and Jupyter notebooks, used to train and evaluate large language models.&lt;SEP&gt;The Stack is a large dataset comprising 1 trillion tokens, including code from over 80 programming languages, natural language in git commits, and Jupyter notebooks, used for training and evaluating language models.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Phind-CodeLlama-V2">
  <data key="d0">Phind-CodeLlama-V2</data>
  <data key="d1">Tools</data>
  <data key="d2">Phind-CodeLlama-V2 is a fine-tuned CodeLlama-34B model trained on over 1.5 billion tokens of code data, ranked highly on the BigCode Models Leaderboard, used for code generation and evaluation.&lt;SEP&gt;Phind-CodeLlama-V2 is a fine-tuned CodeLlama-34B model trained on over 1.5 billion tokens of code data, ranked highly on the BigCode leaderboard, used for code generation and comparison.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="GPT-3.5 and GPT-4">
  <data key="d0">GPT-3.5 and GPT-4</data>
  <data key="d1">Tools</data>
  <data key="d2">GPT-3.5 and GPT-4 are closed-source language models from OpenAI, instruction-tuned and aligned to human preferences, used for inference via API in code-related tasks.&lt;SEP&gt;GPT-3.5 and GPT-4 are proprietary, instruction-tuned language models from OpenAI, accessed via API, used for inference and code generation tasks.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="speedup n@k">
  <data key="d0">speedup n@k</data>
  <data key="d1">Variables</data>
  <data key="d2">speedup n@k measures the expected performance speedup of generated code relative to a sequential baseline when using n processes or threads over k attempts, assessing code efficiency.&lt;SEP&gt;speedup n@k measures the expected performance speedup of generated code relative to a sequential baseline when using n processes or threads over k attempts, assessing efficiency.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="efficiency n@k">
  <data key="d0">efficiency n@k</data>
  <data key="d1">Variables</data>
  <data key="d2">efficiency n@k evaluates the runtime efficiency of code generated by language models, comparing execution times relative to a baseline.&lt;SEP&gt;efficiency n@k is a performance metric assessing the efficiency of code generated by language models, considering runtime improvements relative to a baseline.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="benchmarking">
  <data key="d0">benchmarking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Benchmarking involves evaluating language models using standardized datasets and metrics like pass@k to compare their performance in code generation tasks.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="training datasets">
  <data key="d0">training datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Training datasets such as The Stack and code corpora are used to train language models, providing extensive code and natural language data.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="model comparison">
  <data key="d0">model comparison</data>
  <data key="d1">Results</data>
  <data key="d2">Comparison between open-source and closed-source LLMs, with findings that closed-source models outperform open-source ones in this domain.&lt;SEP&gt;Model comparison involves evaluating different language models' performance using metrics like pass@k, speedup, and efficiency on benchmark datasets.&lt;SEP&gt;The study compares different LLMs (GPT-3.5, GPT-4, open-source models) to determine which are most effective at generating parallel code.&lt;SEP&gt;Models such as StarCoder2, Magicoder, and Phind-V2 are compared based on size, training data, pass@k scores, throughput, and memory usage to analyze their relative performance.&lt;SEP&gt;Models such as StarCoder2, Magicoder, and Phind-V2 are compared based on their size, training data, and performance metrics like pass@k, throughput, and memory requirements.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="model architecture">
  <data key="d0">model architecture</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Model architecture refers to the structural design of language models like SantaCoder and CodeLlama, influencing their capabilities in code generation.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="benchmark datasets">
  <data key="d0">benchmark datasets</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Benchmark datasets like HumanEval and BigCode are used to evaluate and compare the performance of different language models.&lt;SEP&gt;Datasets like HumanEval, MBPP, and DS-1000 serve as benchmarks to evaluate the performance and correctness of generated code.</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sequential Baseline">
  <data key="d0">Sequential Baseline</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A performance measure representing the standard or initial benchmark against which improvements or comparisons are made in code generation performance studies.&lt;SEP&gt;A reference point representing the standard or initial performance measure used to compare the performance of models or algorithms in generating code.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Equation (2)">
  <data key="d0">Equation (2)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A mathematical formula designed to compute the expected maximum speedup of code generated by a model over multiple samples and resource configurations, considering the distribution of runtimes.&lt;SEP&gt;A mathematical formula used to compute the expected maximum speedup of code generated by a model over multiple samples and resources.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Equation (3)">
  <data key="d0">Equation (3)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A formula that calculates the average speedup across prompts, providing an overall performance metric for models generating code, normalized by the number of prompts.&lt;SEP&gt;A formula that defines the average speedup metric across prompts, measuring the performance of generated code relative to the baseline.&lt;SEP&gt;Equation (3) defines how to calculate efficiency𝑛@𝑘 by considering the performance across multiple attempts and processes, providing a mathematical basis for performance measurement.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Equation (4)">
  <data key="d0">Equation (4)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A formula estimating the maximum speedup achievable over all resource counts, serving as an upper bound on model performance across different hardware configurations.&lt;SEP&gt;A formula estimating the maximum speedup over all resource counts, providing an upper bound on performance gains.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Speedupmax@k">
  <data key="d0">Speedupmax@k</data>
  <data key="d1">Results</data>
  <data key="d2">A variant of the speedup metric that estimates the maximum possible speedup across different resource configurations, indicating peak performance.&lt;SEP&gt;A variant of the speedup metric that estimates the maximum speedup across all resource configurations, indicating the peak performance potential of generated code.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Efficiency n@k">
  <data key="d0">Efficiency n@k</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A metric that measures the performance efficiency of generated code, defined as speedup per process or thread, with values between 0 and 1.&lt;SEP&gt;A performance metric that measures the efficiency of generated code in utilizing hardware resources such as processes or threads, expressed as speedup per resource, ranging from 0 to 1.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel Code">
  <data key="d0">Parallel Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code designed for concurrent execution on HPC systems, the primary focus of the dataset and model training in this study.&lt;SEP&gt;Code designed for parallel execution on high-performance computing systems, the focus of the fine-tuning and evaluation in the study.&lt;SEP&gt;Code designed to execute multiple computations simultaneously, leveraging hardware parallelism for performance improvements.&lt;SEP&gt;Code designed to execute multiple computations simultaneously, leveraging hardware resources for performance gains.&lt;SEP&gt;Code designed to execute multiple tasks simultaneously, crucial for high-performance computing applications.&lt;SEP&gt;Parallel code is code designed to execute multiple computations simultaneously, essential for high-performance computing tasks.&lt;SEP&gt;Parallel code is computer code designed to execute multiple computations simultaneously, crucial for leveraging HPC systems for performance gains.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-aebb652b359c14534612e14559754bbc&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Resources">
  <data key="d0">Resources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Hardware resources like processes or threads used to execute parallel code and evaluate performance metrics.&lt;SEP&gt;Hardware resources such as processes or threads used to execute parallel code and measure performance metrics.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Hardware">
  <data key="d0">Hardware</data>
  <data key="d1">Disciplines</data>
  <data key="d2">The physical computing infrastructure on which performance metrics like speedup and efficiency are measured and analyzed.&lt;SEP&gt;The physical computing infrastructure, including cores, processors, and threads, on which performance metrics are measured and analyzed.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Runtime of Sample j">
  <data key="d0">Runtime of Sample j</data>
  <data key="d1">Variables</data>
  <data key="d2">The execution time of the j-th sample of prompt p when run on n resources, used to analyze and compute speedup metrics.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Runtime of Sequential Baseline for prompt p">
  <data key="d0">Runtime of Sequential Baseline for prompt p</data>
  <data key="d1">Variables</data>
  <data key="d2">The execution time of the baseline sequential process for prompt p, serving as the reference point for speedup calculations.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Number of Prompts | P">
  <data key="d0">Number of Prompts | P</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The total set of prompts used in the study, over which average performance metrics are computed.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Number of Samples N">
  <data key="d0">Number of Samples N</data>
  <data key="d1">Variables</data>
  <data key="d2">The total number of generated samples per prompt, used to evaluate the distribution of runtimes and compute expected speedups.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Number of Resources n">
  <data key="d0">Number of Resources n</data>
  <data key="d1">Variables</data>
  <data key="d2">The number of hardware processes or threads used during evaluation, affecting parallel performance metrics.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Set of Resource Counts procs">
  <data key="d0">Set of Resource Counts procs</data>
  <data key="d1">Variables</data>
  <data key="d2">A collection of different resource quantities (e.g., 1,2,4,8,16,32,64,128) over which maximum speedup is evaluated, indicating scalability.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Performance Efficiency">
  <data key="d0">Performance Efficiency</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A metric that measures the expected best performance efficiency of generated code, defined as speedup divided by the number of resources, ranging from 0 to 1.</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="performance of the generated code">
  <data key="d0">performance of the generated code</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The performance of generated code is assessed using metrics like efficiency𝑛@𝑘 and efficiencymax@𝑘, which evaluate how effectively the code utilizes parallel resources and scales with attempts.&lt;SEP&gt;The performance of generated code is evaluated using efficiency𝑛@𝑘 and efficiencymax@𝑘 metrics, which measure how well the code utilizes parallel resources and scales with attempts.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="efficiency𝑛@𝑘">
  <data key="d0">efficiency𝑛@𝑘</data>
  <data key="d1">Variables</data>
  <data key="d2">This metric measures the expected best performance efficiency (speedup per process or thread) when given 𝑘 attempts to generate code, normalized by dividing by 𝑛, with values ranging between 0 and 1.0, indicating perfect scaling.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="efficiencymax@𝑘">
  <data key="d0">efficiencymax@𝑘</data>
  <data key="d1">Variables</data>
  <data key="d2">This metric is similar to efficiency𝑛@𝑘 but represents the maximum achievable efficiency at attempt 𝑘, used to understand the best possible performance scaling of generated code.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parallel resources">
  <data key="d0">parallel resources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Parallel resources refer to computational resources such as processes or threads that the generated code can utilize, and understanding their usage helps evaluate code efficiency.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Equation (5)">
  <data key="d0">Equation (5)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Equation (5) shows the formula for calculating efficiency𝑛@𝑘, normalizing the speedup by the number of attempts, used to quantify parallel efficiency.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Benchmarks (HumanEval, MBPP, DS-1000)">
  <data key="d0">Benchmarks (HumanEval, MBPP, DS-1000)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Benchmarks are standardized test sets used to evaluate the performance of generated code, especially in sequential code generation scenarios.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Speedup1@𝑘">
  <data key="d0">Speedup1@𝑘</data>
  <data key="d1">Results</data>
  <data key="d2">Speedup1@𝑘 measures how much faster the generated code runs compared to a human baseline, providing insight into the efficiency of code generation for specific benchmarks.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="ParEval test harness">
  <data key="d0">ParEval test harness</data>
  <data key="d1">Tools</data>
  <data key="d2">The ParEval test harness compiles and runs generated code, recording correctness, execution time, and compile status, to evaluate the quality of code outputs.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="GCC (GNU Compiler Collection) 9.4.0">
  <data key="d0">GCC (GNU Compiler Collection) 9.4.0</data>
  <data key="d1">Tools</data>
  <data key="d2">GCC is used for compiling generated code with specific flags to ensure correctness and performance, supporting multiple programming models such as serial, OpenMP, and Kokkos.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="NVIDIA A100 80GB GPU">
  <data key="d0">NVIDIA A100 80GB GPU</data>
  <data key="d1">Tools</data>
  <data key="d2">A high-performance GPU used for executing kernels during the evaluation.&lt;SEP&gt;The GPU hardware used for inference when generating code with large models, enabling high-performance code generation.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Prompt workloads">
  <data key="d0">Prompt workloads</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Prompt workloads refer to the set of input prompts used to generate code, which are structured to be regular for efficient inference.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nucleus sampling p=0.95">
  <data key="d0">Nucleus sampling p=0.95</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A sampling technique used during code generation to produce diverse outputs, with a probability threshold p=0.95 to control randomness.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Temperature (0.2 and 0.8)">
  <data key="d0">Temperature (0.2 and 0.8)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Parameters controlling the randomness of output generation, with lower temperature favoring more deterministic outputs and higher temperature promoting diversity.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code evaluation metrics (pass@1, pass@k)">
  <data key="d0">Code evaluation metrics (pass@1, pass@k)</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics used to assess the correctness and performance of generated code, with pass@1 indicating top-1 success rate, and pass@k evaluating success within top k outputs.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sequential code generation">
  <data key="d0">Sequential code generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Refers to generating code that runs in sequence, as opposed to parallel or distributed models, used as a baseline for performance comparison.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="computational complexity">
  <data key="d0">computational complexity</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The study considers how performance metrics can be adapted to analyze the complexity of generated code relative to problem size.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="inference">
  <data key="d0">inference</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Inference involves generating code outputs from large language models using specific prompts, sampling strategies, and hardware configurations.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="sampling strategies">
  <data key="d0">sampling strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Sampling strategies like nucleus sampling with p=0.95 and different temperatures influence the diversity and quality of generated outputs.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="cost of generation">
  <data key="d0">cost of generation</data>
  <data key="d1">Limitations</data>
  <data key="d2">The high monetary cost associated with generating multiple outputs using models like GPT-3.5 and GPT-4 limits the number of samples used for evaluation.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="hardware configurations">
  <data key="d0">hardware configurations</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Hardware setups such as NVIDIA A100 GPU, CUDA, HIP, and MPI configurations influence inference and compilation processes.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="code correctness">
  <data key="d0">code correctness</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of whether the generated code produces correct outputs.&lt;SEP&gt;Correctness is assessed by whether generated code produces expected outputs when run, as recorded by the test harness.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="execution time">
  <data key="d0">execution time</data>
  <data key="d1">Results</data>
  <data key="d2">Execution time measures how long generated code takes to run, used to evaluate performance.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="compile status">
  <data key="d0">compile status</data>
  <data key="d1">Results</data>
  <data key="d2">Compile status indicates whether the generated code successfully compiled, affecting its usability.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="test harness">
  <data key="d0">test harness</data>
  <data key="d1">Tools</data>
  <data key="d2">The ParEval test harness automates the compilation, execution, correctness testing, and performance measurement of generated code.&lt;SEP&gt;The test harness is a testing framework that runs the generated code on specific inputs and checks if outputs match expected results to assess correctness.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="performance evaluation">
  <data key="d0">performance evaluation</data>
  <data key="d1">Results</data>
  <data key="d2">The process of measuring correctness, execution time, and compile status to evaluate code quality.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parallel code generation">
  <data key="d0">parallel code generation</data>
  <data key="d1">Application</data>
  <data key="d2">Parallel code generation involves producing code that can utilize multiple processes or threads, often for performance optimization.&lt;SEP&gt;The process of generating code capable of executing in parallel across multiple processing units, important for efficiency in high-performance computing.&lt;SEP&gt;The study investigates how well state-of-the-art LLMs can generate parallel code, comparing different models' performance in producing correct and efficient parallel programs.&lt;SEP&gt;The task of generating code that can run concurrently across multiple processing units, important for efficiency in computing.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-d1ed35f0489d99675c43fbbee04d61fd&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="sequential code generation">
  <data key="d0">sequential code generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Sequential code generation produces code that runs in a single process without parallelism, often used as a baseline.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="performance metrics for sequential code">
  <data key="d0">performance metrics for sequential code</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics like speedup1@𝑘 are used to evaluate how efficient the generated sequential code is compared to human implementations.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="study setup">
  <data key="d0">study setup</data>
  <data key="d1">Study Designs</data>
  <data key="d2">The setup involves generating code with various models, prompts, and configurations, then evaluating with the test harness.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="evaluation methodology">
  <data key="d0">evaluation methodology</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The evaluation involves compiling, running, and testing correctness and performance of generated code using scripts and hardware setups.</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="output">
  <data key="d0">output</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The output refers to the result produced by a program or code, including correctness, runtime metrics, and performance evaluations.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="the prompt">
  <data key="d0">the prompt</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The prompt is the initial instruction or question provided to guide the code generation or analysis process.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="generated code">
  <data key="d0">generated code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Generated code is the computer program produced in response to the prompt, intended to perform specific tasks or computations.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="pass@1 score">
  <data key="d0">pass@1 score</data>
  <data key="d1">Results</data>
  <data key="d2">A metric indicating the correctness of generated code, with higher scores reflecting better accuracy in code translation and generation.&lt;SEP&gt;A metric indicating the percentage of correct code generated on the first attempt by LLMs for various models and problem types.&lt;SEP&gt;A metric used to evaluate the correctness of generated code, which improves when LLMs are provided with correct implementations during translation.&lt;SEP&gt;A score indicating the percentage of times the top generated output is correct, used to evaluate model accuracy in code generation.&lt;SEP&gt;Pass@1 score is a metric indicating the percentage of prompts for which the top generated output is correct, used to evaluate model performance in code generation tasks.&lt;SEP&gt;The pass@1 score measures the accuracy of LLMs in generating correct code on the first attempt for various models and problem types, revealing that structured, dense problems are easier for LLMs to solve than unstructured, sparse ones.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="serial code">
  <data key="d0">serial code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Serial code refers to non-parallel, sequential code used as a baseline or comparison point in evaluating parallel code generation.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parallel execution models">
  <data key="d0">parallel execution models</data>
  <data key="d1">Tools</data>
  <data key="d2">Parallel execution models are programming paradigms or frameworks that enable code to run concurrently across multiple cores or processors, relevant for assessing parallel code effectiveness.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="binary">
  <data key="d0">binary</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The generated binary is the executable file produced after compiling the generated code, which is then run by the test harness to evaluate correctness and performance.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="correctness check">
  <data key="d0">correctness check</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The correctness check involves verifying if the generated code produces the same results as the sequential baseline, often using specific criteria or metrics.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="sequential baseline">
  <data key="d0">sequential baseline</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The sequential baseline consists of handwritten, optimal implementations used as a standard to compare the generated code's correctness and performance.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="problem sizes">
  <data key="d0">problem sizes</data>
  <data key="d1">Variables</data>
  <data key="d2">Problem sizes are the specific input sizes chosen for testing, designed to ensure reasonable execution times (less than three minutes) for the code.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="execution time limit">
  <data key="d0">execution time limit</data>
  <data key="d1">Limitations</data>
  <data key="d2">The code is considered incorrect if it takes longer than three minutes to run, setting a performance constraint.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parallel programming model">
  <data key="d0">parallel programming model</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The parallel programming model refers to the framework or paradigm (e.g., OpenMP, MPI) that the generated code is expected to utilize for parallel execution.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="OpenMP">
  <data key="d0">OpenMP</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A parallel programming API used to enable multi-threaded execution of the loop in `aggregate_metrics` for performance enhancement.&lt;SEP&gt;A shared memory parallel programming API that simplifies parallelization by using pragmas, making it easier for models to generate code for shared memory systems.&lt;SEP&gt;OpenMP is a shared memory parallel programming API that simplifies parallelization of serial code with pragmas, making it easier for LLMs to generate.&lt;SEP&gt;OpenMP is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, used as a parallel programming model in the evaluation.&lt;SEP&gt;OpenMP is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, used as an execution model for parallel programming.&lt;SEP&gt;OpenMP is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran.&lt;SEP&gt;An API supporting multi-platform shared memory multiprocessing programming in C++, enabling parallel execution of code.&lt;SEP&gt;An API that supports multi-platform shared memory multiprocessing programming in C, used for parallel programming.&lt;SEP&gt;OpenMP is a parallel programming model that provides shared-memory multiprocessing capabilities, widely used for optimizing performance in HPC and scientific computing.&lt;SEP&gt;OpenMP is a widely adopted API for shared-memory parallel programming in C, C++, and Fortran, with version 5.2 released in 2021.&lt;SEP&gt;OpenMP is a widely used API for shared-memory parallel programming in C, C++, and Fortran, with version 5.2 released in 2021.&lt;SEP&gt;OpenMP is a parallel programming model and API for shared-memory architectures, used to facilitate multi-threaded programming and performance benchmarking.&lt;SEP&gt;OpenMP is a parallel programming model for shared-memory architectures, enabling developers to write multi-threaded applications using directives.&lt;SEP&gt;OpenMP is a parallel programming model that enables multi-threaded execution in shared-memory architectures through compiler directives and runtime library routines.&lt;SEP&gt;OpenMP is a parallel programming model used to execute tasks concurrently on shared-memory architectures, improving performance for suitable problem types.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e&lt;SEP&gt;chunk-0e2888ba86c90678f67636c1c02fe25c&lt;SEP&gt;chunk-5f6ee49c82aaaf22c167cac791d43a0f&lt;SEP&gt;chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="MPI (Message Passing Interface)">
  <data key="d0">MPI (Message Passing Interface)</data>
  <data key="d1">Tools</data>
  <data key="d2">MPI is a mature, versatile programming model supporting explicit message passing for distributed memory systems, enabling collective operations like reduction.&lt;SEP&gt;MPI is a standardized and portable message-passing system designed to function on parallel computing architectures, supporting distributed memory programming.&lt;SEP&gt;MPI is a standardized and portable message-passing system designed to function on parallel computing architectures, used in the evaluation.&lt;SEP&gt;MPI is a standardized message-passing system used to implement distributed memory programming, supporting explicit communication between nodes.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="AMD MI50 GPU">
  <data key="d0">AMD MI50 GPU</data>
  <data key="d1">Tools</data>
  <data key="d2">An AMD GPU used for running parallel kernels in the evaluation environment.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="pass@1 scores">
  <data key="d0">pass@1 scores</data>
  <data key="d1">Results</data>
  <data key="d2">The pass@1 scores for each model indicate their accuracy in generating correct code outputs, with higher scores reflecting better performance.</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Open-source models">
  <data key="d0">Open-source models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Models like Phind-V2 that are publicly available and open for modification, compared against proprietary models for performance in code tasks.&lt;SEP&gt;Open-source models are publicly available language models like Phind-V2, which are compared against closed-source models in performance.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Closed-source models">
  <data key="d0">Closed-source models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Closed-source models such as GPT-3.5 and GPT-4 are proprietary models evaluated for their code generation performance.&lt;SEP&gt;Proprietary models such as GPT-3.5 and GPT-4, evaluated for their code generation accuracy and efficiency.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CUDA/HIP">
  <data key="d0">CUDA/HIP</data>
  <data key="d1">Tools</data>
  <data key="d2">CUDA and HIP are GPU programming frameworks for parallel processing, with models generally performing worse on these compared to serial or OpenMP models.&lt;SEP&gt;GPU programming frameworks for parallel processing, with models generally performing less effectively on these due to complexity and training data limitations.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="MPI/MPI+OpenMP">
  <data key="d0">MPI/MPI+OpenMP</data>
  <data key="d1">Tools</data>
  <data key="d2">MPI and MPI+OpenMP are parallel execution models for distributed and shared memory systems, respectively, with lower pass@1 scores in models.&lt;SEP&gt;Parallel execution models for distributed and shared memory systems, respectively, which pose different challenges for code generation by models.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="pass@k for 𝑘&gt;1">
  <data key="d0">pass@k for 𝑘&gt;1</data>
  <data key="d1">Results</data>
  <data key="d2">Analyzing pass@k for values greater than 1 helps understand how multiple attempts improve the likelihood of correct code generation by LLMs.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Results of pass@k for various models">
  <data key="d0">Results of pass@k for various models</data>
  <data key="d1">Results</data>
  <data key="d2">Data showing the performance of different LLMs at generating correct code within k attempts, indicating upper bounds and plateau points.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model Performance on Different Parallel Models">
  <data key="d0">Model Performance on Different Parallel Models</data>
  <data key="d1">Results</data>
  <data key="d2">Data indicating that models are most accurate on serial and OpenMP, less so on CUDA/HIP and MPI, with performance variations across model sizes.</data>
  <data key="d3">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Structured, Dense Problems">
  <data key="d0">Structured, Dense Problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Types of computational problems characterized by data parallelism, including transform, reduce, and search.&lt;SEP&gt;Types of computational problems, such as transform, reduction, and search, that are data-parallel and generally easier for LLMs to generate solutions for.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Unstructured, Sparse Problems">
  <data key="d0">Unstructured, Sparse Problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Problems like sparse linear algebra, FFT, geometry, and sort that are more challenging for LLMs to parallelize.&lt;SEP&gt;Types of computational problems, including sparse linear algebra, FFT, geometry, and sort, which are more challenging for LLMs to parallelize effectively.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Transform Problems">
  <data key="d0">Transform Problems</data>
  <data key="d1">Results</data>
  <data key="d2">Problems involving data parallel transformations, generally easier for LLMs to solve.&lt;SEP&gt;Transform problems are the easiest for LLMs to parallelize and generate correct solutions, with GPT-3.5 being an exception where it ranks second.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sparse Linear Algebra">
  <data key="d0">Sparse Linear Algebra</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A problem type involving computations on sparse matrices, identified as the most difficult for LLMs to parallelize due to complexity.&lt;SEP&gt;Problems involving sparse matrices, identified as the most difficult for LLMs to parallelize.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="FFT (Fast Fourier Transform)">
  <data key="d0">FFT (Fast Fourier Transform)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A computational problem involving frequency analysis, identified as challenging for LLMs to generate correct parallel code.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Graph Problems">
  <data key="d0">Graph Problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Problems involving graph structures, which some larger LLMs like StarCoder-Base and CodeLlama can handle better.&lt;SEP&gt;Problems involving graph structures, which some larger LLMs like StarCoder-Base and CodeLlama handle better, but still pose difficulty for others.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Experiment Results">
  <data key="d0">Experiment Results</data>
  <data key="d1">Results</data>
  <data key="d2">The study finds that the more different a parallel programming model’s code is from its serial version, the harder it is for LLMs to generate correct code, especially with MPI.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Serial Code">
  <data key="d0">Serial Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Serial code refers to sequential programming without parallelism, serving as the baseline for comparing parallel models.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Reduction">
  <data key="d0">Reduction</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A class of problems that involve aggregating data, which LLMs tend to solve well.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Search">
  <data key="d0">Search</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Problems involving searching algorithms, generally simpler for LLMs to generate solutions.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Stencil, Histogram, Dense Linear Algebra">
  <data key="d0">Stencil, Histogram, Dense Linear Algebra</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Structured dense problems that some LLMs like Phind-V2 and GPTs handle effectively.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="FFT">
  <data key="d0">FFT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Fast Fourier Transform, a computational problem that is challenging for LLMs.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sort">
  <data key="d0">Sort</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sorting algorithms, which are surprisingly difficult for LLMs despite high-level abstractions.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Scan">
  <data key="d0">Scan</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Parallel prefix sum operations, challenging for LLMs to generate correct code.</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Spatiotemporal Information">
  <data key="d0">Spatiotemporal Information</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Data related to the spatial and temporal aspects of the objects of study, capturing location and time-based dynamics.</data>
  <data key="d3">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Across Thread Counts">
  <data key="d0">Across Thread Counts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The concept of across thread counts pertains to evaluating the performance and efficiency of parallel computing models across different numbers of threads, highlighting how computational resources are utilized.&lt;SEP&gt;The concept of across thread counts pertains to evaluating the performance and efficiency of parallel computing models across different numbers of threads.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CL-7B">
  <data key="d0">CL-7B</data>
  <data key="d1">Models</data>
  <data key="d2">CL-7B is a language model used for code generation and performance evaluation in parallel computing contexts, specifically designed for high-performance code tasks.&lt;SEP&gt;CL-7B is a language model used for code generation and performance evaluation in parallel computing contexts.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CL-13B">
  <data key="d0">CL-13B</data>
  <data key="d1">Models</data>
  <data key="d2">CL-13B is a language model employed for code translation and performance assessment in parallel computing environments, capable of generating and translating code across models.&lt;SEP&gt;CL-13B is a language model used for code translation and performance assessment in parallel computing environments.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CL-34B">
  <data key="d0">CL-34B</data>
  <data key="d1">Models</data>
  <data key="d2">CL-34B is a large language model used for code translation and performance evaluation in high-performance computing tasks.&lt;SEP&gt;CL-34B is a large language model used for code translation, generation, and performance evaluation in high-performance computing tasks, with a focus on scalability and correctness.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Phind-V2">
  <data key="d0">Phind-V2</data>
  <data key="d1">Models</data>
  <data key="d2">Phind-V2 is a 34B parameter model used for code generation, included in performance comparisons.&lt;SEP&gt;Phind-V2 is a 34B parameter model, fine-tuned on a proprietary dataset, and was the top performer on the BigCode leaderboard at its release, used for advanced code modeling.&lt;SEP&gt;Phind-V2 is a language model evaluated for efficiency and effectiveness in generating parallel code across different execution models, particularly excelling in MPI prompts.&lt;SEP&gt;Phind-V2 is a language model evaluated for efficiency and effectiveness in generating parallel code across different execution models.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="GPT-3.5">
  <data key="d0">GPT-3.5</data>
  <data key="d1">GPT-3.5</data>
  <data key="d2">A large language model used as a baseline to evaluate the effectiveness of DSP Y-optimized pipelines.&lt;SEP&gt;A large language model used for evaluation and analysis purposes in the research paper.&lt;SEP&gt;A large language model used for evaluation purposes in the research.&lt;SEP&gt;GPT-3.5 is an advanced language model used for code generation, translation, and performance evaluation in parallel computing contexts, with high pass@1 scores indicating accuracy.&lt;SEP&gt;GPT-3.5 is an advanced language model used for code generation, translation, and performance evaluation in parallel computing contexts.&lt;SEP&gt;A state-of-the-art language model developed by OpenAI, used as a benchmark for comparison in AI performance and capabilities.&lt;SEP&gt;A version of OpenAI's GPT model, used as a baseline for performance comparison.&lt;SEP&gt;GPT-3.5 is a state-of-the-art commercial large language model from OpenAI, accessible via API, used for various advanced language tasks including code generation.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0&lt;SEP&gt;chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Figure 6">
  <data key="d0">Figure 6</data>
  <data key="d1">Results</data>
  <data key="d2">Figure 6 shows pass@1 scores and speedup metrics for different models, highlighting GPT-4's superior performance and efficiency in parallel code execution.&lt;SEP&gt;Figure 6 shows pass@1 scores and speedup metrics for different models, highlighting GPT-4's superior performance and the trend of efficiency and speedup across models and parallel prompts.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="speedup 𝑛@1">
  <data key="d0">speedup 𝑛@1</data>
  <data key="d1">Variables</data>
  <data key="d2">speedup 𝑛@1 measures how much faster parallel code runs compared to the sequential baseline, reflecting the performance gains achieved through parallelization.&lt;SEP&gt;speedup 𝑛@1 quantifies the factor by which parallel code outperforms sequential baseline, indicating performance gains.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="efficiency 𝑛@1">
  <data key="d0">efficiency 𝑛@1</data>
  <data key="d1">Variables</data>
  <data key="d2">efficiency 𝑛@1 measures how effectively parallel resources are utilized relative to maximum possible speedup.&lt;SEP&gt;efficiency 𝑛@1 measures how effectively parallel resources are utilized, representing the ratio of achieved speedup to the maximum possible speedup, indicating resource utilization efficiency.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parallel code">
  <data key="d0">parallel code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Parallel code refers to programs designed to execute computations concurrently across multiple threads or processes, evaluated for correctness, efficiency, and scalability in the study.&lt;SEP&gt;Parallel code refers to programs designed to execute computations concurrently across multiple threads or processes, evaluated for efficiency and scalability.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code Translation">
  <data key="d0">Code Translation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Code translation involves converting serial code into parallel code across different execution models, assessing the models' scalability and performance.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Inefficiency of Parallel Code">
  <data key="d0">Inefficiency of Parallel Code</data>
  <data key="d1">Results</data>
  <data key="d2">The study finds that the parallel code produced by language models is generally inefficient, with low utilization of parallel resources despite correctness.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code translation">
  <data key="d0">Code translation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Code translation involves converting code from one execution model to another (e.g., serial to OpenMP, CUDA to Kokkos), assessing the models' scalability, correctness, and translation accuracy.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel code generation">
  <data key="d0">Parallel code generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Parallel code generation is the process of automatically creating parallel programs using language models, evaluated for correctness, efficiency, and scalability.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model performance">
  <data key="d0">Model performance</data>
  <data key="d1">Results</data>
  <data key="d2">Model performance is assessed through metrics like pass@1, speedup, and efficiency, indicating the accuracy, effectiveness, and scalability of code generated or translated by models.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="inefficiency of parallel code">
  <data key="d0">inefficiency of parallel code</data>
  <data key="d1">Results</data>
  <data key="d2">The study finds that the parallel code produced by language models is generally inefficient, with low resource utilization despite correctness, indicating room for improvement.</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Efficiency@1">
  <data key="d0">Efficiency@1</data>
  <data key="d1">Results</data>
  <data key="d2">A metric measuring the efficiency of generated code; generally similar for translation and generation, with some exceptions.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Speedup@1">
  <data key="d0">Speedup@1</data>
  <data key="d1">Results</data>
  <data key="d2">A metric assessing the scaling behavior of generated parallel code, with observed improvements in certain translation scenarios.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Open- and closed-source models">
  <data key="d0">Open- and closed-source models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Types of LLMs evaluated, with closed-source models outperforming open-source ones in generating parallel code.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Correct implementations">
  <data key="d0">Correct implementations</data>
  <data key="d1">Methodology</data>
  <data key="d2">Providing correct code in one execution model (e.g., serial) helps LLMs generate correct parallel code.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel code">
  <data key="d0">Parallel code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code that executes tasks concurrently across multiple processors or cores, including MPI, OpenMP, and Kokkos.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Serial code">
  <data key="d0">Serial code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sequential code that runs without parallelization, serving as the baseline for translating to parallel models.&lt;SEP&gt;Sequential code used as a baseline to improve parallel code generation.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Runtime performance">
  <data key="d0">Runtime performance</data>
  <data key="d1">Variables</data>
  <data key="d2">Performance metrics of generated parallel code, including efficiency and speedup.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Benchmark">
  <data key="d0">Benchmark</data>
  <data key="d1">Tools</data>
  <data key="d2">ParEval serves as a benchmark to evaluate and improve LLMs' capabilities in parallel code generation.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="efficiency@1">
  <data key="d0">efficiency@1</data>
  <data key="d1">Results</data>
  <data key="d2">A metric measuring the runtime efficiency of generated parallel code, comparing performance between translation and native generation.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="speedup@1">
  <data key="d0">speedup@1</data>
  <data key="d1">Results</data>
  <data key="d2">A metric quantifying the scaling behavior and performance improvement of parallel code when resources increase.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="benchmark">
  <data key="d0">benchmark</data>
  <data key="d1">Tools</data>
  <data key="d2">Benchmarking involves using standardized tests like ParEval to compare different models' capabilities in parallel code generation, assessing correctness, memory, and throughput.&lt;SEP&gt;Benchmarking involves using standardized tests like ParEval to compare different models' capabilities in parallel code generation, focusing on correctness, resource use, and throughput.&lt;SEP&gt;ParEval is a benchmarking framework designed to evaluate and improve LLMs' capabilities in parallel code generation.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="correct implementation">
  <data key="d0">correct implementation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Accurate and efficient code written in serial or other models, used to guide LLMs in generating correct parallel code.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="correctness and performance">
  <data key="d0">correctness and performance</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Improving both the correctness and runtime performance of parallel code generated by LLMs is a key goal.</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="research">
  <data key="d0">research</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Artificial Intelligence, Parallel Computing, Software Engineering, Machine Learning, High-Performance Computing</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="National Energy Research Scientific Computing Center">
  <data key="d0">National Energy Research Scientific Computing Center</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A U.S. Department of Energy Office of Science User Facility providing high-performance computing resources for research activities.&lt;SEP&gt;A U.S. Department of Energy Office of Science User Facility providing high-performance computing resources for scientific research activities.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="NERSC">
  <data key="d0">NERSC</data>
  <data key="d1">Tools</data>
  <data key="d2">A high-performance computing resource used for computational research activities.&lt;SEP&gt;A high-performance computing resource used for large-scale scientific computations and data processing.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="DDR-ERCAP0025593">
  <data key="d0">DDR-ERCAP0025593</data>
  <data key="d1">Variables</data>
  <data key="d2">A specific award identifier associated with resource allocation at NERSC.&lt;SEP&gt;An award identifier for resource allocation at NERSC used to support computational research.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Big Code Models Leaderboard">
  <data key="d0">Big Code Models Leaderboard</data>
  <data key="d1">Platform</data>
  <data key="d2">A benchmark platform for evaluating the performance of large code models, hosted on Hugging Face.&lt;SEP&gt;A benchmarking platform hosted on Hugging Face to evaluate the performance of large code models across various tasks and datasets.&lt;SEP&gt;An online leaderboard hosted on Hugging Face showcasing top-performing big code models.&lt;SEP&gt;An online leaderboard hosted on Hugging Face showcasing top-performing large code models and their evaluations.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HIP Documentation">
  <data key="d0">HIP Documentation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Documentation providing guidelines and technical details for HIP (Heterogeneous-compute Interface for Portability) framework.&lt;SEP&gt;Official technical documentation detailing the HIP framework for portability and performance across heterogeneous computing platforms.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Zero-Shot Replication Framework">
  <data key="d0">Zero-Shot Replication Framework</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework enabling the replication of experiments without prior training, accessible via GitHub.&lt;SEP&gt;A framework enabling the replication of experiments without prior training, facilitating reproducibility in research, hosted on GitHub.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Wasi Uddin Ahmad">
  <data key="d0">Wasi Uddin Ahmad</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research exploring transformer-based approaches for source code summarization.&lt;SEP&gt;Research exploring transformer-based models for source code summarization, focusing on the effectiveness of such models in generating concise summaries.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Saikat Chakraborty">
  <data key="d0">Saikat Chakraborty</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research focusing on learning code summarization from small and local datasets.&lt;SEP&gt;Study investigating learning approaches for code summarization from small and local datasets to improve model efficiency.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Baishakhi Ray">
  <data key="d0">Baishakhi Ray</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on techniques for automatic code summarization and understanding using neural models.&lt;SEP&gt;Research related to code summarization techniques and models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Kai-Wei Chang">
  <data key="d0">Kai-Wei Chang</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on source code summarization using transformer-based models.&lt;SEP&gt;Study examining transformer-based approaches for source code summarization and their performance.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="SantaCoder">
  <data key="d0">SantaCoder</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A code generation model designed to generate code without reaching for overly ambitious goals, emphasizing practical code synthesis.&lt;SEP&gt;A neural code generation model designed to produce code snippets, emphasizing practicality over reaching for overly ambitious goals.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jacob Austin">
  <data key="d0">Jacob Austin</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on program synthesis using large language models to generate code from specifications.&lt;SEP&gt;Research on program synthesis using large language models, aiming to generate code from specifications effectively.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Augustus Odena">
  <data key="d0">Augustus Odena</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on capabilities of large language models in program synthesis tasks.&lt;SEP&gt;Study assessing the capabilities of large language models in program synthesis tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Maxwell I. Nye">
  <data key="d0">Maxwell I. Nye</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural approaches to code generation and synthesis.&lt;SEP&gt;Research on neural approaches to code generation, focusing on improving synthesis accuracy and efficiency.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Maarten Bosma">
  <data key="d0">Maarten Bosma</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research investigating large language models' ability to generate and understand code across multiple programming languages.&lt;SEP&gt;Research on large language models applied to code synthesis and understanding.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Henryk Michalewski">
  <data key="d0">Henryk Michalewski</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural program synthesis techniques.&lt;SEP&gt;Study exploring neural program synthesis techniques and their effectiveness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="David Dohan">
  <data key="d0">David Dohan</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural network architectures and training methods for code synthesis.&lt;SEP&gt;Research on neural network architectures for code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Ellen Jiang">
  <data key="d0">Ellen Jiang</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on large language models for program synthesis.&lt;SEP&gt;Study evaluating large language models in code generation and understanding tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Carrie J. Cai">
  <data key="d0">Carrie J. Cai</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code synthesis, focusing on improving accuracy and scalability.&lt;SEP&gt;Research on neural models for code understanding and synthesis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Michael Terry">
  <data key="d0">Michael Terry</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on large language models' capabilities in program synthesis.&lt;SEP&gt;Study assessing the performance of large language models in code generation tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Quoc V. Le">
  <data key="d0">Quoc V. Le</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research exploring the capabilities of neural models in code synthesis and their transferability across tasks.&lt;SEP&gt;Research on neural network models for code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Charles Sutton">
  <data key="d0">Charles Sutton</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on program synthesis and neural code models.&lt;SEP&gt;Study on neural program synthesis techniques and evaluation metrics.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Language Models are Few-Shot Learners">
  <data key="d0">Language Models are Few-Shot Learners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A foundational paper proposing that large language models can perform tasks with minimal task-specific training data.&lt;SEP&gt;A foundational paper proposing that large language models can perform various tasks, including code generation, with minimal task-specific examples.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Federico Cassano">
  <data key="d0">Federico Cassano</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on scalable benchmarking approaches for neural code generation across multiple programming languages.&lt;SEP&gt;Research on scalable, multilingual benchmarks for neural code generation models, aiming to evaluate performance across diverse programming languages.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="John Gouwar">
  <data key="d0">John Gouwar</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluating neural models for code generation and understanding.&lt;SEP&gt;Study evaluating neural code generation models' performance and robustness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Daniel Nguyen">
  <data key="d0">Daniel Nguyen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on code generation performance and model scalability.&lt;SEP&gt;Research on neural models' ability to generate correct and efficient code across multiple languages.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sydney Nguyen">
  <data key="d0">Sydney Nguyen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on multi-language code generation benchmarking.&lt;SEP&gt;Study focusing on benchmarking neural code models in multilingual settings.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Luna Phipps-Costin">
  <data key="d0">Luna Phipps-Costin</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluation frameworks for neural code generation models, emphasizing scalability and accuracy.&lt;SEP&gt;Research on evaluation frameworks for neural code models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Donald Pinckney">
  <data key="d0">Donald Pinckney</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural model performance metrics.&lt;SEP&gt;Study analyzing the performance metrics and evaluation techniques for neural code models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Ming-Ho Yee">
  <data key="d0">Ming-Ho Yee</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation and understanding in multiple programming languages.&lt;SEP&gt;Research on neural code generation in multiple languages.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yangtian Zi">
  <data key="d0">Yangtian Zi</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on benchmarking neural models for code generation.&lt;SEP&gt;Study assessing the benchmarking and evaluation of neural models for code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Carolyn Jane Anderson">
  <data key="d0">Carolyn Jane Anderson</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on datasets and evaluation strategies for neural code models.&lt;SEP&gt;Research on evaluation datasets for code models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Molly Q Feldman">
  <data key="d0">Molly Q Feldman</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code model scalability.&lt;SEP&gt;Study on the scalability and effectiveness of neural code generation models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Arjun Guha">
  <data key="d0">Arjun Guha</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation evaluation.&lt;SEP&gt;Research on neural models' ability to generate, understand, and evaluate code snippets in multiple languages.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Michael Greenberg">
  <data key="d0">Michael Greenberg</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code model benchmarking.&lt;SEP&gt;Study evaluating neural models' performance in code synthesis and understanding tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Abhinav Jangda">
  <data key="d0">Abhinav Jangda</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation evaluation frameworks.&lt;SEP&gt;Research on neural code models' scalability and evaluation across diverse datasets.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="MultiPL-E">
  <data key="d0">MultiPL-E</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A comprehensive, scalable benchmark dataset for evaluating neural code generation models across multiple programming languages, supporting multilingual assessment.&lt;SEP&gt;A scalable, multilingual benchmark for evaluating neural code generation models across various programming languages.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Le Chen">
  <data key="d0">Le Chen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on applying large language models to data race detection to enhance parallel code correctness.&lt;SEP&gt;Research on data race detection using large language models to improve parallel code correctness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xianzhong Ding">
  <data key="d0">Xianzhong Ding</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on applying language models to high-performance computing tasks, including data race detection.&lt;SEP&gt;Study investigating the use of language models in high-performance computing, specifically for data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Murali Emani">
  <data key="d0">Murali Emani</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on language models' effectiveness in high-performance computing environments.&lt;SEP&gt;Research on the effectiveness of language models in detecting data races in parallel code environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Tristan Vanderbruggen">
  <data key="d0">Tristan Vanderbruggen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on applying language models to data race detection and HPC applications.&lt;SEP&gt;Study exploring language model applications in high-performance computing, including data race detection and parallel program analysis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pei Hung Lin">
  <data key="d0">Pei Hung Lin</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on language models for HPC and data race detection.&lt;SEP&gt;Research on leveraging language models for parallel code analysis and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Chunhua Liao">
  <data key="d0">Chunhua Liao</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on language models' application in parallel computing environments.&lt;SEP&gt;Study evaluating language models' capabilities in high-performance and parallel computing contexts.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Bronis de Supinski">
  <data key="d0">Bronis de Supinski</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on applying language models to optimize parallel code performance and correctness in HPC.&lt;SEP&gt;Research on high-performance computing and language model applications.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mark Chen">
  <data key="d0">Mark Chen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Mark Chen develops and trains models aimed at solving complex problems, including math word problems.&lt;SEP&gt;Mark Chen works on training AI models for solving complex problems, including math word problems.&lt;SEP&gt;Research on evaluating large language models trained on code to assess capabilities and limitations.&lt;SEP&gt;Study assessing the evaluation of large language models trained on code for their ability to assist in parallel programming and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jerry Tworek">
  <data key="d0">Jerry Tworek</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Jerry Tworek contributes to AI research on model training, verification, and problem-solving applications.&lt;SEP&gt;Jerry Tworek researches model training, verification, and solving capabilities in AI systems.&lt;SEP&gt;Research on large language models' performance in code understanding and generation.&lt;SEP&gt;Research on the performance of neural models in code understanding and generation relevant to HPC environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Heewoo Jun">
  <data key="d0">Heewoo Jun</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Heewoo Jun participates in research on AI verification and problem-solving capabilities.&lt;SEP&gt;Heewoo Jun researches AI verification and training techniques for models solving mathematical problems.&lt;SEP&gt;Research on evaluation of neural code models.&lt;SEP&gt;Study evaluating neural models' effectiveness in high-performance computing tasks, including data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Qiming Yuan">
  <data key="d0">Qiming Yuan</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code models and their evaluation.&lt;SEP&gt;Research on neural code models' ability to improve parallel code correctness and efficiency.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Henrique Ponde de Oliveira Pinto">
  <data key="d0">Henrique Ponde de Oliveira Pinto</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code models and benchmarking.&lt;SEP&gt;Study exploring neural models' application in high-performance computing environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jared Kaplan">
  <data key="d0">Jared Kaplan</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural language models for code.&lt;SEP&gt;Research on neural language models' capacity for code synthesis in parallel and high-performance computing contexts.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Harri Edwards">
  <data key="d0">Harri Edwards</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code synthesis.&lt;SEP&gt;Study evaluating neural models' performance on code understanding and generation tasks relevant to parallel computing.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yuri Burda">
  <data key="d0">Yuri Burda</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on large language models' capabilities in code generation, including applications in HPC.&lt;SEP&gt;Research on large language models' capabilities in code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nicholas Joseph">
  <data key="d0">Nicholas Joseph</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluation of large language models trained on code.&lt;SEP&gt;Study assessing the evaluation of neural models' effectiveness in code synthesis and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Greg Brockman">
  <data key="d0">Greg Brockman</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code synthesis and their application to parallel computing environments.&lt;SEP&gt;Research on neural models for code synthesis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Alex Ray">
  <data key="d0">Alex Ray</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on large language models' performance in code tasks.&lt;SEP&gt;Study evaluating neural models' performance in generating and understanding parallel code.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Raul Puri">
  <data key="d0">Raul Puri</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code model evaluation.&lt;SEP&gt;Research on neural code models' ability to assist in data race detection and parallel program correctness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Gretchen Krueger">
  <data key="d0">Gretchen Krueger</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code understanding.&lt;SEP&gt;Study on neural models' effectiveness in code understanding and synthesis for high-performance computing.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Michael Petrov">
  <data key="d0">Michael Petrov</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation capabilities relevant to parallel and HPC environments.&lt;SEP&gt;Research on neural code generation capabilities.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Heidy Khlaaf">
  <data key="d0">Heidy Khlaaf</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code understanding.&lt;SEP&gt;Study evaluating neural models' application in high-performance and parallel computing tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Girish Sastry">
  <data key="d0">Girish Sastry</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code models.&lt;SEP&gt;Research on neural models for code understanding and synthesis in HPC contexts.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pamela Mishkin">
  <data key="d0">Pamela Mishkin</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluation frameworks for neural code models.&lt;SEP&gt;Study on neural code models' performance in parallel code analysis and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Brooke Chan">
  <data key="d0">Brooke Chan</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code evaluation.&lt;SEP&gt;Research on neural models' ability to improve parallel code correctness and efficiency.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Scott Gray">
  <data key="d0">Scott Gray</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code benchmarking.&lt;SEP&gt;Study evaluating neural models for code understanding and synthesis in high-performance computing.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nick Ryder">
  <data key="d0">Nick Ryder</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code understanding.&lt;SEP&gt;Research on neural models for data race detection and parallel code correctness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mikhail Pavlov">
  <data key="d0">Mikhail Pavlov</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation performance.&lt;SEP&gt;Study assessing neural code models' performance in parallel programming environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Alethea Power">
  <data key="d0">Alethea Power</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code evaluation metrics.&lt;SEP&gt;Research on neural models' scalability and effectiveness in high-performance computing tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Lukasz Kaiser">
  <data key="d0">Lukasz Kaiser</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Lukasz Kaiser specializes in deep learning and AI training methodologies, including verification techniques.&lt;SEP&gt;Lukasz Kaiser specializes in deep learning architectures and verification techniques for AI models.&lt;SEP&gt;Research on neural network architectures for code.&lt;SEP&gt;Study evaluating neural network architectures optimized for parallel code synthesis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mohammad Bavarian">
  <data key="d0">Mohammad Bavarian</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Mohammad Bavarian contributes to research on AI problem-solving and verification techniques.&lt;SEP&gt;Mohammad Bavarian works on AI verification, problem-solving, and training models to solve math word problems.&lt;SEP&gt;Research on neural models for code generation.&lt;SEP&gt;Research on neural models' capacity to generate correct parallel code and detect data races.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Clemens Winter">
  <data key="d0">Clemens Winter</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code synthesis.&lt;SEP&gt;Study on neural approaches to parallel code synthesis and correctness verification.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Philippe Tillet">
  <data key="d0">Philippe Tillet</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research exploring neural models' application in high-performance parallel computing.&lt;SEP&gt;Research on large language models in code tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Felipe Petroski Such">
  <data key="d0">Felipe Petroski Such</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code understanding.&lt;SEP&gt;Study evaluating neural models' performance in parallel code synthesis and correctness.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Fotiou Chantzis">
  <data key="d0">Fotiou Chantzis</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluation of neural code models.&lt;SEP&gt;Research on neural models' effectiveness in data race detection and parallel program analysis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Elizabeth Barnes">
  <data key="d0">Elizabeth Barnes</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code benchmarking.&lt;SEP&gt;Study on evaluation metrics for neural models applied to parallel code understanding.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Ariel Herbert-Voss">
  <data key="d0">Ariel Herbert-Voss</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on large language models' capabilities in code synthesis for high-performance computing.&lt;SEP&gt;Research on large language models' capabilities in coding tasks.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="William Hebgen Guss">
  <data key="d0">William Hebgen Guss</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code evaluation.&lt;SEP&gt;Study evaluating neural models' performance in parallel code generation and analysis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Alex Nichol">
  <data key="d0">Alex Nichol</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code understanding.&lt;SEP&gt;Research on neural models' ability to improve parallel code correctness and reduce data races.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Alex Paino">
  <data key="d0">Alex Paino</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models for code synthesis.&lt;SEP&gt;Study assessing neural code models' effectiveness in parallel computing environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nikolas Tezak">
  <data key="d0">Nikolas Tezak</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on evaluation of neural code models.&lt;SEP&gt;Research on neural code generation and data race detection in parallel programs.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jie Tang">
  <data key="d0">Jie Tang</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code evaluation.&lt;SEP&gt;Study evaluating neural models' performance in code synthesis for high-performance applications.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Igor Babuschkin">
  <data key="d0">Igor Babuschkin</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural language models for code.&lt;SEP&gt;Research on neural language models' application to parallel code and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Andrew N. Carr">
  <data key="d0">Andrew N. Carr</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code generation capabilities.&lt;SEP&gt;Study evaluating neural models' performance in parallel code synthesis and correctness verification.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jan Leike">
  <data key="d0">Jan Leike</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Jan Leike is a researcher involved in evaluating large language models trained on code, contributing to studies on AI performance and capabilities.&lt;SEP&gt;Jan Leike is a researcher involved in evaluating large language models trained on code, focusing on assessing model performance and capabilities.&lt;SEP&gt;Research on safety and evaluation of large language models for code.&lt;SEP&gt;Research on safety and robustness of neural models in parallel code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Josh Achiam">
  <data key="d0">Josh Achiam</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Josh Achiam is a researcher contributing to the evaluation and development of large language models, particularly in code understanding and problem solving.&lt;SEP&gt;Josh Achiam participates in research on large language models and their applications, including mathematical problem solving.&lt;SEP&gt;Research on neural models for code understanding.&lt;SEP&gt;Study on neural models' effectiveness in parallel code understanding and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Vedant Misra">
  <data key="d0">Vedant Misra</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural code evaluation and robustness.&lt;SEP&gt;Research on neural models' robustness and accuracy in parallel code synthesis.&lt;SEP&gt;Vedant Misra is involved in research on language models and their evaluation, focusing on code generation and understanding.&lt;SEP&gt;Vedant Misra researches large language models, especially their training, evaluation, and application in code generation.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Evan Morikawa">
  <data key="d0">Evan Morikawa</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Evan Morikawa contributes to studies on AI models, particularly in the context of language and code processing.&lt;SEP&gt;Evan Morikawa is involved in research on large language models, focusing on their evaluation and performance in coding tasks.&lt;SEP&gt;Research on neural code models and their training.&lt;SEP&gt;Study evaluating neural models' performance in parallel programming environments.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Matthew Knight">
  <data key="d0">Matthew Knight</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Matthew Knight contributes to research on large language models, particularly in their evaluation on code and mathematical tasks.&lt;SEP&gt;Matthew Knight is involved in evaluating AI models' performance and capabilities, especially in coding tasks.&lt;SEP&gt;Research on neural models for code understanding.&lt;SEP&gt;Study assessing neural models' effectiveness in parallel code synthesis and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Miles Brundage">
  <data key="d0">Miles Brundage</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Miles Brundage researches AI safety, evaluation, and the societal implications of large language models.&lt;SEP&gt;Miles Brundage researches AI safety, societal impacts, and evaluation of large language models trained on code.&lt;SEP&gt;Research on safety and alignment issues in neural models for parallel code generation.&lt;SEP&gt;Research on safety and alignment of large language models.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mira Murati">
  <data key="d0">Mira Murati</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Mira Murati works on AI development and evaluation, focusing on model safety and performance assessment.&lt;SEP&gt;Mira Murati works on AI development, safety, and evaluation, focusing on large language models and their applications.&lt;SEP&gt;Research on neural model deployment and evaluation.&lt;SEP&gt;Study evaluating neural models' deployment and performance in high-performance parallel computing.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Katie Mayer">
  <data key="d0">Katie Mayer</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Katie Mayer contributes to research on large language models, particularly in their evaluation and application.&lt;SEP&gt;Katie Mayer is involved in research on large language models, including their evaluation on coding tasks.&lt;SEP&gt;Research on evaluation metrics for neural code models.&lt;SEP&gt;Research on evaluation metrics for neural models in parallel code synthesis.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Peter Welinder">
  <data key="d0">Peter Welinder</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Peter Welinder contributes to the development and evaluation of large language models, with a focus on their capabilities and safety.&lt;SEP&gt;Peter Welinder is involved in AI research, including model evaluation and benchmarking.&lt;SEP&gt;Research on large language models' performance in coding.&lt;SEP&gt;Study on neural models' ability to generate efficient parallel code.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Bob McGrew">
  <data key="d0">Bob McGrew</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Bob McGrew researches large language models, focusing on their training, evaluation, and deployment.&lt;SEP&gt;Research on neural model evaluation and benchmarking.&lt;SEP&gt;Research on neural models' robustness in parallel code generation and data race detection.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sam McCandlish">
  <data key="d0">Sam McCandlish</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural language models' capabilities and limitations.&lt;SEP&gt;Research on neural models' capabilities and limitations in parallel code generation and analysis.&lt;SEP&gt;Sam McCandlish contributes to research on the training and evaluation of large language models, especially in code understanding.&lt;SEP&gt;Sam McCandlish works on large language model training, evaluation, and capabilities, especially in code understanding.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Ilya Sutskever">
  <data key="d0">Ilya Sutskever</data>
  <data key="d1">Researcher</data>
  <data key="d2">A key figure in language model development, published in 2018.&lt;SEP&gt;A prominent researcher in language model development, published in 2018.&lt;SEP&gt;Ilya Sutskever is a co-founder of OpenAI and a leading figure in developing large language models and their evaluation methodologies.&lt;SEP&gt;Ilya Sutskever is a co-founder of OpenAI and a leading researcher in large language models, focusing on their development and evaluation.&lt;SEP&gt;Research on neural network development for code tasks.&lt;SEP&gt;Study on neural network architectures optimized for parallel code synthesis.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb&lt;SEP&gt;chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Wojciech Zaremba">
  <data key="d0">Wojciech Zaremba</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Research on neural models and evaluation in code generation.&lt;SEP&gt;Research on neural models' effectiveness in parallel code generation and correctness verification.&lt;SEP&gt;Wojciech Zaremba contributes to the research and development of large language models, including their evaluation methodologies.&lt;SEP&gt;Wojciech Zaremba works on AI model training and evaluation, focusing on language models and their capabilities.</data>
  <data key="d3">chunk-409a22222a0092123b63b598c4b30bad&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:arXiv:2107.03374">
  <data key="d0">arXiv:arXiv:2107.03374</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A preprint repository hosting the study evaluating large language models trained on code, providing datasets and research findings.&lt;SEP&gt;A preprint repository hosting the study evaluating large language models trained on code, providing the research paper and dataset for the study.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Karl Cobbe">
  <data key="d0">Karl Cobbe</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focused on training verifiers to solve math word problems, enhancing reasoning abilities of models.&lt;SEP&gt;Karl Cobbe conducts research on AI verification and solving mathematical problems, contributing to the development of methods for AI problem-solving.&lt;SEP&gt;Karl Cobbe conducts research on AI verification, mathematical problem solving, and training models to solve math word problems.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Matthias Plappert">
  <data key="d0">Matthias Plappert</data>
  <data key="d1">Researcher</data>
  <data key="d2">Matthias Plappert contributes to training and verification methodologies for AI models, including solving math problems.&lt;SEP&gt;Matthias Plappert works on AI training and evaluation, focusing on verification and problem-solving.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="John Schulman">
  <data key="d0">John Schulman</data>
  <data key="d1">Researcher</data>
  <data key="d2">John Schulman is a key figure in reinforcement learning and AI verification research.&lt;SEP&gt;John Schulman is involved in reinforcement learning and verification techniques for AI models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2110.14168">
  <data key="d0">arXiv:2110.14168</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A preprint repository hosting research on training verifiers to solve math word problems, including datasets and methodologies.&lt;SEP&gt;A preprint repository hosting the study on training verifiers to solve math word problems, providing datasets and methodology.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xueying Du">
  <data key="d0">Xueying Du</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xueying Du develops benchmarks and evaluation datasets for large language models, focusing on code generation and mathematical problem solving.&lt;SEP&gt;Xueying Du researches evaluation benchmarks and methods for large language models, especially in code generation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mingwei Liu">
  <data key="d0">Mingwei Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Mingwei Liu contributes to benchmarking and evaluation of language models on coding tasks.&lt;SEP&gt;Mingwei Liu researches evaluation benchmarks and performance analysis of language models on code and math tasks.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Kaixin Wang">
  <data key="d0">Kaixin Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Kaixin Wang develops evaluation benchmarks for large language models on code generation tasks.&lt;SEP&gt;Kaixin Wang works on developing evaluation benchmarks for code generation by language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Hanlin Wang">
  <data key="d0">Hanlin Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Hanlin Wang participates in assessment of language models' code capabilities.&lt;SEP&gt;Hanlin Wang works on assessing language models' capabilities in code and problem-solving contexts.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Junwei Liu">
  <data key="d0">Junwei Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Junwei Liu is involved in research on code generation and model evaluation.&lt;SEP&gt;Junwei Liu researches evaluation datasets for code and math problem solving in language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yixuan Chen">
  <data key="d0">Yixuan Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yixuan Chen contributes to benchmarking and analysis of language models' code performance.&lt;SEP&gt;Yixuan Chen contributes to benchmarking and performance evaluation of language models on coding and mathematical tasks.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jiayi Feng">
  <data key="d0">Jiayi Feng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jiayi Feng develops datasets and evaluation methods for assessing code generation and problem-solving capabilities.&lt;SEP&gt;Jiayi Feng works on evaluation datasets for code generation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Chaofeng Sha">
  <data key="d0">Chaofeng Sha</data>
  <data key="d1">Researcher</data>
  <data key="d2">Chaofeng Sha develops benchmarks for assessing language models on class-level code generation tasks.&lt;SEP&gt;Chaofeng Sha researches benchmarks for evaluating language models' performance on class-level code generation and problem solving.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xin Peng">
  <data key="d0">Xin Peng</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xin Peng contributes to evaluation datasets and benchmarks for code generation by language models.&lt;SEP&gt;Xin Peng participates in research on code evaluation benchmarks.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yiling Lou">
  <data key="d0">Yiling Lou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Yiling Lou contributes to the development of evaluation benchmarks for language models in coding.&lt;SEP&gt;Yiling Lou develops and applies evaluation benchmarks for large language models in coding and problem solving.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2308.01861">
  <data key="d0">arXiv:2308.01861</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A preprint hosting the ClassEval benchmark for evaluating language models on class-level code generation tasks.&lt;SEP&gt;A preprint hosting the ClassEval benchmark for evaluating large language models on class-level code generation tasks.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Leo Gao">
  <data key="d0">Leo Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Leo Gao is involved in creating and analyzing large datasets for language modeling, including the Pile dataset for diverse text.&lt;SEP&gt;Leo Gao is involved in dataset creation and evaluation for language modeling, including large text corpora.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Stella Biderman">
  <data key="d0">Stella Biderman</data>
  <data key="d1">Researcher</data>
  <data key="d2">Stella Biderman contributes to dataset development and analysis for language models.&lt;SEP&gt;Stella Biderman works on datasets and evaluation metrics for language models, supporting benchmarking efforts.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sid Black">
  <data key="d0">Sid Black</data>
  <data key="d1">Researcher</data>
  <data key="d2">Sid Black contributes to dataset development and evaluation metrics for large language models.&lt;SEP&gt;Sid Black works on datasets and evaluation metrics for large language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Laurence Golding">
  <data key="d0">Laurence Golding</data>
  <data key="d1">Researcher</data>
  <data key="d2">Laurence Golding researches datasets for language modeling.&lt;SEP&gt;Laurence Golding researches large datasets for language modeling and evaluation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Travis Hoppe">
  <data key="d0">Travis Hoppe</data>
  <data key="d1">Researcher</data>
  <data key="d2">Travis Hoppe contributes to data curation for language models.&lt;SEP&gt;Travis Hoppe develops datasets and tools for language model evaluation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Charles Foster">
  <data key="d0">Charles Foster</data>
  <data key="d1">Researcher</data>
  <data key="d2">Charles Foster works on datasets and evaluation frameworks for language models.&lt;SEP&gt;Charles Foster works on datasets and evaluation methods for language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jason Phang">
  <data key="d0">Jason Phang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jason Phang develops datasets and benchmarks for assessing large language models.&lt;SEP&gt;Jason Phang develops datasets and benchmarks for evaluating language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Horace He">
  <data key="d0">Horace He</data>
  <data key="d1">Researcher</data>
  <data key="d2">Horace He contributes to dataset curation and evaluation methods for language models.&lt;SEP&gt;Horace He participates in dataset creation for language model evaluation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Anish Thite">
  <data key="d0">Anish Thite</data>
  <data key="d1">Researcher</data>
  <data key="d2">Anish Thite contributes to dataset development and model evaluation.&lt;SEP&gt;Anish Thite researches dataset creation and evaluation techniques for language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Noa Nabeshima">
  <data key="d0">Noa Nabeshima</data>
  <data key="d1">Researcher</data>
  <data key="d2">Noa Nabeshima works on large datasets for language modeling.&lt;SEP&gt;Noa Nabeshima works on large-scale datasets for language modeling and evaluation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Shawn Presser">
  <data key="d0">Shawn Presser</data>
  <data key="d1">Researcher</data>
  <data key="d2">Shawn Presser develops datasets and evaluation metrics for language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Connor Leahy">
  <data key="d0">Connor Leahy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Connor Leahy contributes to dataset curation and evaluation of language models.&lt;SEP&gt;Connor Leahy contributes to dataset curation and evaluation of large language models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="DeepDev-PERF">
  <data key="d0">DeepDev-PERF</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A dataset and approach for improving software performance through deep learning, used in research on software engineering.&lt;SEP&gt;DeepDev-PERF is a dataset and approach aimed at improving software performance using deep learning techniques.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="William Godoy">
  <data key="d0">William Godoy</data>
  <data key="d1">Researcher</data>
  <data key="d2">William Godoy evaluates AI models for high-performance computing (HPC) applications, including code generation.&lt;SEP&gt;William Godoy evaluates AI models for high-performance computing (HPC), including their ability to generate HPC parallel programming kernels.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pedro Valero-Lara">
  <data key="d0">Pedro Valero-Lara</data>
  <data key="d1">Researcher</data>
  <data key="d2">Pedro Valero-Lara is a researcher involved in assessing the performance and applicability of AI-generated code in HPC contexts.&lt;SEP&gt;Pedro Valero-Lara is an author and researcher contributing to the evaluation of AI-generated kernels in HPC.&lt;SEP&gt;Pedro Valero-Lara researches HPC performance and AI model evaluation in scientific computing.&lt;SEP&gt;Pedro Valero-Lara researches HPC performance, focusing on AI model application and evaluation in scientific computing.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Keita Teranishi">
  <data key="d0">Keita Teranishi</data>
  <data key="d1">Researcher</data>
  <data key="d2">Keita Teranishi assesses AI models for HPC environments and parallel programming.&lt;SEP&gt;Keita Teranishi assesses AI models' effectiveness in HPC environments and parallel programming models.&lt;SEP&gt;Keita Teranishi contributes to the evaluation of AI-generated kernels, focusing on implementation quality across programming models.&lt;SEP&gt;Keita Teranishi is involved in the study on AI-assisted HPC kernel generation, focusing on implementation and evaluation.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Prasanna Balaprakash">
  <data key="d0">Prasanna Balaprakash</data>
  <data key="d1">Researcher</data>
  <data key="d2">Prasanna Balaprakash is an author contributing to the assessment of AI capabilities in generating HPC kernels.&lt;SEP&gt;Prasanna Balaprakash is involved in analyzing the proficiency and limitations of AI-generated HPC kernels.&lt;SEP&gt;Prasanna Balaprakash studies HPC performance bugs and evaluates AI models' performance in high-performance computing contexts.&lt;SEP&gt;Prasanna Balaprakash works on performance evaluation of AI models in high-performance computing.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jeffrey Vetter">
  <data key="d0">Jeffrey Vetter</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jeffrey Vetter researches HPC systems and evaluates AI models' performance in scientific and parallel computing.&lt;SEP&gt;Jeffrey Vetter researches HPC systems and the application of AI models in scientific computing.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="as per ">
  <data key="d0">as per </data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A dataset and evaluation framework for HPC parallel programming model kernel generation, used for assessing AI model performance in HPC.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Jian Gu">
  <data key="d0">Jian Gu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jian Gu develops foundation models for automatic code summarization and analysis.&lt;SEP&gt;Jian Gu develops foundation models for automatic code summarization, focusing on software engineering and code understanding.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pasquale Salza">
  <data key="d0">Pasquale Salza</data>
  <data key="d1">Researcher</data>
  <data key="d2">Pasquale Salza researches AI models for code analysis and summarization, contributing to software engineering tools.&lt;SEP&gt;Pasquale Salza researches AI models for code analysis and summarization.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Harald C. Gall">
  <data key="d0">Harald C. Gall</data>
  <data key="d1">Researcher</data>
  <data key="d2">Harald C. Gall contributes to research on AI models for software analysis and code summarization.&lt;SEP&gt;Harald C. Gall works on AI models for software analysis, including code summarization and understanding.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)">
  <data key="d0">IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)</data>
  <data key="d1">Conference</data>
  <data key="d2">SANER is a conference where research on foundation models for automatic code summarization was presented.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sakib Haque">
  <data key="d0">Sakib Haque</data>
  <data key="d1">Researcher</data>
  <data key="d2">Sakib Haque develops semantic similarity metrics for evaluating source code summaries.&lt;SEP&gt;Sakib Haque works on semantic similarity metrics for evaluating source code summaries.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Zachary Eberhart">
  <data key="d0">Zachary Eberhart</data>
  <data key="d1">Researcher</data>
  <data key="d2">Zachary Eberhart contributes to research on code summarization evaluation metrics.&lt;SEP&gt;Zachary Eberhart researches evaluation metrics for source code summarization and semantics.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Aakash Bansal">
  <data key="d0">Aakash Bansal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Aakash Bansal researches methods for evaluating source code summaries.&lt;SEP&gt;Aakash Bansal works on metrics and evaluation techniques for source code summarization.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Collin McMillan">
  <data key="d0">Collin McMillan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Collin McMillan develops tools and metrics for assessing source code summaries.&lt;SEP&gt;Collin McMillan develops tools and metrics for code summarization evaluation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE/ACM 30th International Conference on Program Comprehension (ICPC)">
  <data key="d0">IEEE/ACM 30th International Conference on Program Comprehension (ICPC)</data>
  <data key="d1">Conference</data>
  <data key="d2">ICPC is a venue where semantic similarity metrics for code summarization were presented.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Semantics Similarity Metrics">
  <data key="d0">Semantics Similarity Metrics</data>
  <data key="d1">Methodology</data>
  <data key="d2">Metrics used to evaluate the quality of source code summaries based on semantic similarity.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Neural Text Degeneration">
  <data key="d0">Neural Text Degeneration</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Neural Text Degeneration describes the phenomenon where neural language models produce repetitive, low-diversity, or low-quality text outputs, impacting generation performance.&lt;SEP&gt;Neural Text Degeneration refers to the phenomenon where neural language models produce repetitive or low-quality text outputs, impacting text generation quality.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Edward J. Hu">
  <data key="d0">Edward J. Hu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Edward Hu researches adaptation techniques for large language models, including LoRA, and their applications.&lt;SEP&gt;Edward Hu researches model adaptation techniques such as LoRA for large language models, focusing on efficient fine-tuning.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="LoRA: Low-Rank Adaptation of Large Language Models">
  <data key="d0">LoRA: Low-Rank Adaptation of Large Language Models</data>
  <data key="d1">Methodology</data>
  <data key="d2">LoRA is a methodology for efficiently adapting large language models through low-rank matrix updates, enabling effective fine-tuning with fewer parameters.&lt;SEP&gt;LoRA is a technique for efficiently adapting large language models using low-rank matrices, enabling better fine-tuning.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2106.09685">
  <data key="d0">arXiv:2106.09685</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A preprint repository hosting the LoRA methodology paper, detailing low-rank adaptation techniques for large models.&lt;SEP&gt;A preprint repository hosting the LoRA paper, detailing the low-rank adaptation methodology for large models.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Tal Kadosh">
  <data key="d0">Tal Kadosh</data>
  <data key="d1">Researcher</data>
  <data key="d2">Tal Kadosh researches adaptations of language models for high-performance computing (HPC) applications.&lt;SEP&gt;Tal Kadosh researches transforming large language models for high-performance computing (HPC) applications, including domain-specific adaptations.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Niranjan Hasabnis">
  <data key="d0">Niranjan Hasabnis</data>
  <data key="d1">Researcher</data>
  <data key="d2">Niranjan Hasabnis contributes to research on transforming LLMs for HPC environments.&lt;SEP&gt;Niranjan Hasabnis works on adapting language models for HPC code transformation and optimization.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Vy A. Vo">
  <data key="d0">Vy A. Vo</data>
  <data key="d1">Researcher</data>
  <data key="d2">Vy A. Vo develops methods for transforming and adapting large language models for HPC environments.&lt;SEP&gt;Vy A. Vo works on model adaptation techniques for HPC code transformation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nadav Schneider">
  <data key="d0">Nadav Schneider</data>
  <data key="d1">Researcher</data>
  <data key="d2">Nadav Schneider researches model transformation and adaptation for specific application domains.&lt;SEP&gt;Nadav Schneider researches model transformation techniques to optimize large language models for HPC code.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Neva Krien">
  <data key="d0">Neva Krien</data>
  <data key="d1">Researcher</data>
  <data key="d2">Neva Krien contributes to optimizing language models for high-performance computing.&lt;SEP&gt;Neva Krien contributes to the adaptation of language models for high-performance computing tasks.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Abdul Wasay">
  <data key="d0">Abdul Wasay</data>
  <data key="d1">Researcher</data>
  <data key="d2">Abdul Wasay researches methods to adapt large language models for specialized domains.&lt;SEP&gt;Abdul Wasay works on domain-specific adaptation of language models for scientific and HPC applications.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nesreen Ahmed">
  <data key="d0">Nesreen Ahmed</data>
  <data key="d1">Researcher</data>
  <data key="d2">Nesreen Ahmed researches application-driven adaptation of AI models in scientific computing and HPC.&lt;SEP&gt;Nesreen Ahmed works on applying AI models to scientific and high-performance computing contexts.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Ted Willke">
  <data key="d0">Ted Willke</data>
  <data key="d1">Researcher</data>
  <data key="d2">Ted Willke develops strategies for transforming and adapting language models for HPC and scientific applications.&lt;SEP&gt;Ted Willke researches AI model adaptation and transformation for HPC applications.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2308.09440">
  <data key="d0">arXiv:2308.09440</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A preprint describing methods for transforming LLMs for HPC code, including datasets and evaluation frameworks.&lt;SEP&gt;A preprint detailing methods for transforming LLMs for HPC code, including datasets and evaluation approaches.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Md Abul Kalam Azad">
  <data key="d0">Md Abul Kalam Azad</data>
  <data key="d1">Researcher</data>
  <data key="d2">Md Abul Kalam Azad conducts empirical research on performance bugs in high-performance computing (HPC) environments, analyzing software issues.&lt;SEP&gt;Md Abul Kalam Azad conducts empirical studies on high-performance computing (HPC) performance bugs.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Nafees Iqbal">
  <data key="d0">Nafees Iqbal</data>
  <data key="d1">Researcher</data>
  <data key="d2">Nafees Iqbal researches HPC software bugs and their impact on performance.&lt;SEP&gt;Nafees Iqbal researches HPC software bugs, focusing on their characteristics and impact on system performance.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Foyzul Hassan">
  <data key="d0">Foyzul Hassan</data>
  <data key="d1">Researcher</data>
  <data key="d2">Foyzul Hassan conducts empirical studies on HPC performance bugs, aiming to improve software reliability.&lt;SEP&gt;Foyzul Hassan contributes to empirical studies of HPC performance bugs.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Probir Roy">
  <data key="d0">Probir Roy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Probir Roy studies software bugs in HPC environments.&lt;SEP&gt;Probir Roy studies software bugs in HPC systems, focusing on bug characterization and mitigation.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)">
  <data key="d0">IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)</data>
  <data key="d1">Conference</data>
  <data key="d2">A major conference where empirical studies on HPC performance bugs and software repositories were presented.&lt;SEP&gt;A venue where empirical studies on HPC performance bugs were presented.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Learning to Reduce False Positives in Analytic Bug Detectors">
  <data key="d0">Learning to Reduce False Positives in Analytic Bug Detectors</data>
  <data key="d1">Methodology</data>
  <data key="d2">A methodology aimed at improving bug detection accuracy by reducing false positives in analytic bug detectors.&lt;SEP&gt;A research approach aimed at improving bug detection accuracy in software, reducing false positives.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Anant Kharkar">
  <data key="d0">Anant Kharkar</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Anant Kharkar is an author involved in research on reducing false positives in analytic bug detectors, contributing to the understanding of software quality assurance.&lt;SEP&gt;Anant Kharkar researches methods for reducing false positives in bug detection tools.&lt;SEP&gt;Anant Kharkar researches techniques for reducing false positives in software bug detection tools to improve reliability.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Roshanak Zilouchian Moghaddam">
  <data key="d0">Roshanak Zilouchian Moghaddam</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Roshanak Zilouchian Moghaddam develops methods for bug detection and software analysis to improve software quality.&lt;SEP&gt;Roshanak Zilouchian Moghaddam is a researcher focusing on software engineering and bug detection methodologies.&lt;SEP&gt;Roshanak Zilouchian Moghaddam is a researcher focusing on software engineering, bug detection methodologies, and reducing false positives in analytic tools.&lt;SEP&gt;Roshanak Zilouchian Moghaddam works on software bug detection and analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Colin B. Clement">
  <data key="d0">Colin B. Clement</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Colin B. Clement develops deep learning approaches for software analysis.&lt;SEP&gt;Colin B. Clement is a researcher specializing in machine learning applications in software engineering and bug detection.&lt;SEP&gt;Colin B. Clement is a researcher working on machine learning applications in software engineering.&lt;SEP&gt;Colin B. Clement researches deep learning approaches for software analysis, including bug detection.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Neel Sundaresan">
  <data key="d0">Neel Sundaresan</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Neel SUNDaresan works on analytic bug detectors and techniques to improve bug detection accuracy.&lt;SEP&gt;Neel Sundaresan contributes to research on analytic bug detectors and software quality.&lt;SEP&gt;Neel Sundaresan is a researcher focusing on software analysis, bug detection, and machine learning techniques in software engineering.&lt;SEP&gt;Neel Sundaresan is involved in research on software analysis and bug detection methodologies.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Chen Wu">
  <data key="d0">Chen Wu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Chen Wu develops software analysis techniques, including bug detection and evaluation methodologies.&lt;SEP&gt;Chen Wu researches software analysis techniques, including bug detection.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:10.1145/3605731.3605886">
  <data key="d0">arXiv:10.1145/3605731.3605886</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A dataset and evaluation framework for HPC kernel generation, used to assess AI models' performance in HPC environments.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE SANER 2022">
  <data key="d0">IEEE SANER 2022</data>
  <data key="d1">Conference</data>
  <data key="d2">The IEEE International Conference on Software Analysis, Evolution, and Reengineering where research on code summarization models was presented.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="ICPC 2022">
  <data key="d0">ICPC 2022</data>
  <data key="d1">Conference</data>
  <data key="d2">The IEEE/ACM International Conference on Program Comprehension where research on semantic metrics for code summaries was presented.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="International Conference on Learning Representations (ICLR)">
  <data key="d0">International Conference on Learning Representations (ICLR)</data>
  <data key="d1">Conference</data>
  <data key="d2">A major conference where research on neural text degeneration was presented, highlighting issues in neural language model outputs.</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Matthew Jin">
  <data key="d0">Matthew Jin</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Matthew Jin is a researcher contributing to studies on analytic bug detection and software engineering.&lt;SEP&gt;Matthew Jin is a researcher contributing to studies on bug detection and software analysis methodologies.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xiaoyu Liu">
  <data key="d0">Xiaoyu Liu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Xiaoyu Liu is involved in research related to software analysis and bug detection techniques.&lt;SEP&gt;Xiaoyu Liu is involved in research related to software analysis, bug detection techniques, and improving software reliability.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xin Shi">
  <data key="d0">Xin Shi</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Xin Shi is a researcher contributing to software engineering research, particularly in bug detection.&lt;SEP&gt;Xin Shi is a researcher working on software engineering, bug detection, and analytic tools for software quality assurance.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Denis Kocetkov">
  <data key="d0">Denis Kocetkov</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Denis Kocetkov is a researcher contributing to open-source code datasets and software analysis.&lt;SEP&gt;Denis Kocetkov is a researcher involved in large-scale open-source code datasets and their use in machine learning for software analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Raymond Li">
  <data key="d0">Raymond Li</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Raymond Li is a researcher focusing on large-scale source code datasets and their applications.&lt;SEP&gt;Raymond Li is a researcher working on large source code datasets and their applications in software engineering and machine learning.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Loubna Ben Allal">
  <data key="d0">Loubna Ben Allal</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Loubna Ben Allal is a researcher focusing on source code licensing, datasets, and legal aspects of open-source software.&lt;SEP&gt;Loubna Ben Allal is involved in research on source code licensing and datasets.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Chenghao Mou">
  <data key="d0">Chenghao Mou</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Chenghao Mou is a researcher contributing to source code datasets and software analysis methodologies.&lt;SEP&gt;Chenghao Mou is involved in research on large source code datasets, software analysis methodologies, and machine learning applications.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Carlos Muñoz Ferrandis">
  <data key="d0">Carlos Muñoz Ferrandis</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Carlos Muñoz Ferrandis is a researcher working on large open-source code datasets and their applications in software engineering.&lt;SEP&gt;Carlos Muñoz Ferrandis is involved in research on open-source software datasets.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yacine Jernite">
  <data key="d0">Yacine Jernite</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher supporting transformer augmentation with memory and similarity-based retrieval.&lt;SEP&gt;A researcher working on neural models with memory augmentation.&lt;SEP&gt;Yacine Jernite is a researcher specializing in natural language processing, code analysis, and AI models applied to software engineering.&lt;SEP&gt;Yacine Jernite is a researcher working on natural language processing and software code analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Margaret Mitchell">
  <data key="d0">Margaret Mitchell</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Margaret Mitchell is a researcher focusing on AI ethics and large-scale data analysis.&lt;SEP&gt;Margaret Mitchell is a researcher focusing on AI ethics, large datasets, and machine learning fairness in software tools.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Sean Hughes">
  <data key="d0">Sean Hughes</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Sean Hughes is a researcher working on AI models, natural language processing, and their applications in software engineering.&lt;SEP&gt;Sean Hughes is involved in AI research related to language models and software engineering.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Thomas Wolf">
  <data key="d0">Thomas Wolf</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Thomas Wolf is a researcher developing deep learning models for language understanding and software analysis.&lt;SEP&gt;Thomas Wolf is a researcher working on deep learning models and their applications in software analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Dzmitry Bahdanau">
  <data key="d0">Dzmitry Bahdanau</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Dzmitry Bahdanau is a researcher specializing in neural networks and natural language processing.&lt;SEP&gt;Dzmitry Bahdanau is a researcher specializing in neural networks, attention mechanisms, and their applications in code analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Leandro von Werra">
  <data key="d0">Leandro von Werra</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Leandro von Werra is a researcher working on large language models and their application to software engineering datasets.&lt;SEP&gt;Leandro von Werra is involved in AI research focusing on language models and software datasets.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Harm de Vries">
  <data key="d0">Harm de Vries</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Harm de Vries is a researcher focusing on machine learning models, code understanding, and software analysis techniques.&lt;SEP&gt;Harm de Vries is a researcher working on machine learning models for code understanding.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="StarCoder">
  <data key="d0">StarCoder</data>
  <data key="d1">Tools</data>
  <data key="d2">StarCoder is a large language model designed for source code generation and understanding, aiming to assist developers.&lt;SEP&gt;StarCoder is a large language model designed for source code generation, understanding, and assisting developers in programming tasks.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="VerilogEval">
  <data key="d0">VerilogEval</data>
  <data key="d1">Tools</data>
  <data key="d2">VerilogEval is a benchmark and evaluation tool for assessing language models' performance in Verilog hardware description language code generation.&lt;SEP&gt;VerilogEval is a benchmark tool for evaluating large language models' performance in Verilog code generation.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="LLM4VV">
  <data key="d0">LLM4VV</data>
  <data key="d1">Tools</data>
  <data key="d2">LLM4VV is a large language model-driven test suite developed for compiler validation and software correctness testing.&lt;SEP&gt;LLM4VV is a testing framework for validating compiler behavior using large language models.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="NVIDIA CUDA">
  <data key="d0">NVIDIA CUDA</data>
  <data key="d1">Tools</data>
  <data key="d2">CUDA is a parallel computing platform and API from NVIDIA for high-performance GPU programming, widely used in scientific computing and deep learning.&lt;SEP&gt;CUDA is a parallel computing platform and API model developed by NVIDIA for GPU programming, used in high-performance computing tasks.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="OpenAI API">
  <data key="d0">OpenAI API</data>
  <data key="d1">Tools</data>
  <data key="d2">OpenAI API provides programmatic access to OpenAI's language models for various applications including code generation and analysis.&lt;SEP&gt;OpenAI API provides programmatic access to OpenAI's language models, enabling integration into software tools for code generation, analysis, and AI services.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="OpenAI Python API Library">
  <data key="d0">OpenAI Python API Library</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python library facilitating interaction with OpenAI's models for tasks like code generation and analysis.&lt;SEP&gt;OpenAI Python API library is a software library that facilitates interaction with OpenAI's models from Python, supporting automation and development.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="OpenMP 4.0">
  <data key="d0">OpenMP 4.0</data>
  <data key="d1">Tools</data>
  <data key="d2">OpenMP 4.0 is a specification for parallel programming in shared-memory environments, enabling developers to write portable high-performance code.&lt;SEP&gt;OpenMP 4.0 is a specification for parallel programming in shared-memory environments, used to develop high-performance applications.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="PyTorch">
  <data key="d0">PyTorch</data>
  <data key="d1">Tools</data>
  <data key="d2">A deep learning library providing abstractions for building and training neural networks, inspiring DSPy module design.&lt;SEP&gt;An open-source deep learning library providing imperative programming style for building and training neural networks.&lt;SEP&gt;PyTorch is an open-source deep learning library providing imperative programming style for developing neural networks.&lt;SEP&gt;PyTorch is an open-source deep learning library supporting imperative programming for neural network development and research.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-ebb9fff9b6ca895234d2fcb26b588fe4&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Phind-CodeLlama-34B-v2">
  <data key="d0">Phind-CodeLlama-34B-v2</data>
  <data key="d1">Tools</data>
  <data key="d2">A large language model specialized for code understanding and generation, available via Hugging Face.&lt;SEP&gt;Phind-CodeLlama-34B-v2 is a large language model specialized for code understanding, code generation, and developer assistance, available via Hugging Face.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Source Code Datasets">
  <data key="d0">Source Code Datasets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Large collections of source code used as training data for machine learning models, benchmarks, and software analysis research.&lt;SEP&gt;Large collections of source code used for training and evaluating machine learning models in software engineering.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Source Code">
  <data key="d0">Source Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Actual source code files, scripts, or repositories examined in research for bug detection, code synthesis, and analysis.&lt;SEP&gt;The actual source code files examined in research for bug detection, code generation, and analysis.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Performance Deep Learning">
  <data key="d0">Performance Deep Learning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Performance Deep Learning refers to the application of deep learning techniques to improve performance in various computational tasks, including model training, optimization, and evaluation, often involving large datasets and complex models.&lt;SEP&gt;Performance Deep Learning refers to the application of deep learning techniques to improve performance in various computational tasks, often involving model training, optimization, and evaluation.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:1912.01703 [cs.LG]">
  <data key="d0">arXiv:1912.01703 [cs.LG]</data>
  <data key="d1">Study Design</data>
  <data key="d2">An arXiv preprint documenting research related to performance deep learning, likely including methodologies, results, or theoretical frameworks.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Phind">
  <data key="d0">Phind</data>
  <data key="d1">Tools</data>
  <data key="d2">Phind-CodeLlama-34B-v2 is a language model tool designed for code generation and understanding, hosted on Hugging Face.&lt;SEP&gt;Phind-CodeLlama-34B-v2 is a large language model tool designed for code understanding and generation, hosted on Hugging Face, used in research and application development.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Cedric Richter and Heike Wehrheim">
  <data key="d0">Cedric Richter and Heike Wehrheim</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who authored a study on learning from developer mistakes, focusing on bug localization and repair from real bug fixes.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Learning to localize and repair real bugs from real bug fixes">
  <data key="d0">Learning to localize and repair real bugs from real bug fixes</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">The study investigates whether models can learn to identify and fix bugs based on real-world bug fixes, assessing the ability to improve software reliability.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Baptiste Rozière, Jonas Gehring, et al.">
  <data key="d0">Baptiste Rozière, Jonas Gehring, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a paper introducing Code Llama, a foundation model for code, emphasizing large-scale language models for programming tasks.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code Llama: Open Foundation Models for Code">
  <data key="d0">Code Llama: Open Foundation Models for Code</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework for large-scale foundation models designed for code understanding, generation, and AI-assisted programming, emphasizing open access and adaptability.&lt;SEP&gt;A model framework for code understanding and generation based on large-scale foundation models, designed to enhance code-related AI applications.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="M. Snir">
  <data key="d0">M. Snir</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author of a comprehensive reference book on MPI, the Message Passing Interface, a core methodology for parallel and distributed computing.&lt;SEP&gt;Author of a comprehensive reference on MPI (Message Passing Interface), a core methodology for parallel computing.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="MPI–the Complete Reference: The MPI core">
  <data key="d0">MPI–the Complete Reference: The MPI core</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A detailed book covering MPI, the standard for message-passing in parallel computing environments.&lt;SEP&gt;A detailed book covering MPI, the standard for message-passing in parallel computing, essential for high-performance computing applications.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xiangru Tang, Bill Qian, Rick Gao, et al.">
  <data key="d0">Xiangru Tang, Bill Qian, Rick Gao, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who developed BioCoder, a benchmark for bioinformatics code generation, incorporating contextual pragmatic knowledge.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge">
  <data key="d0">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge</data>
  <data key="d1">Study Design</data>
  <data key="d2">A benchmark dataset and evaluation framework aimed at assessing the ability of models to generate bioinformatics code with pragmatic and contextual understanding.&lt;SEP&gt;A benchmark dataset and evaluation framework for testing code generation models in bioinformatics, emphasizing pragmatic and contextual understanding.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Hugo Touvron et al.">
  <data key="d0">Hugo Touvron et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Authors of a paper on Llama 2, an open foundation and fine-tuned chat model, advancing NLP and code understanding capabilities.&lt;SEP&gt;Authors of a paper on Llama 2, an open foundation and fine-tuned chat model, advancing large language models for NLP and code understanding.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Llama 2: Open Foundation and Fine-Tuned Chat Models">
  <data key="d0">Llama 2: Open Foundation and Fine-Tuned Chat Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework for large language models that are open-sourced and fine-tuned for chat and code tasks, improving natural language processing.&lt;SEP&gt;A large language model framework that is open-sourced, fine-tuned for chat and code tasks, supporting NLP and AI applications.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Christian R. Trott, Damien Lebrun-Grandié, et al.">
  <data key="d0">Christian R. Trott, Damien Lebrun-Grandié, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who authored a paper on Kokkos 3, a programming model extension designed for exascale computing, focusing on high-performance parallel programming.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Kokkos 3: Programming Model Extensions for the Exascale Era">
  <data key="d0">Kokkos 3: Programming Model Extensions for the Exascale Era</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An extension to the Kokkos programming model aimed at enabling scalable, efficient computation on exascale systems, emphasizing parallel programming techniques.&lt;SEP&gt;An extension to the Kokkos programming model that supports scalable, efficient parallel programming on exascale systems, emphasizing portability and performance.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, et al.">
  <data key="d0">Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers comparing Llama-2 and GPT-3 large language models for high-performance computing (HPC) kernel generation, assessing model performance and suitability.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation">
  <data key="d0">Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation</data>
  <data key="d1">Study Design</data>
  <data key="d2">A comparative study evaluating the effectiveness of two large language models in generating HPC kernels, focusing on performance metrics and practical applications.&lt;SEP&gt;A systematic comparison evaluating the effectiveness, accuracy, and efficiency of Llama-2 and GPT-3 in generating high-performance computing kernels.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Hao Yu, Bo Shen, Dezhi Ran, et al.">
  <data key="d0">Hao Yu, Bo Shen, Dezhi Ran, et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who developed CoderEval, a benchmark for pragmatic code generation using generative pre-trained models, evaluating code quality and contextual understanding.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models">
  <data key="d0">CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models</data>
  <data key="d1">Study Design</data>
  <data key="d2">A benchmark framework designed to evaluate the pragmatic and contextual accuracy of code generated by large language models.&lt;SEP&gt;A benchmark suite designed to evaluate the pragmatic and contextual accuracy of code generated by large language models, emphasizing real-world coding tasks.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Library. arXiv:1912.01703 [cs.LG]">
  <data key="d0">Library. arXiv:1912.01703 [cs.LG]</data>
  <data key="d1">Study Design</data>
  <data key="d2">An arXiv preprint that documents research related to performance deep learning, likely including methodologies, results, and theoretical frameworks.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes">
  <data key="d0">Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A research study investigating whether models can learn to identify and fix bugs based on real-world bug fixes, aiming to improve software reliability and automation.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve">
  <data key="d0">Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve</data>
  <data key="d1">Researcher</data>
  <data key="d2">A large team of researchers who authored a paper on Code Llama, an open foundation model for code understanding and generation, highlighting advances in AI models for programming tasks.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein">
  <data key="d0">Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who developed BioCoder, a benchmark dataset for bioinformatics code generation, integrating pragmatic and contextual knowledge to evaluate models.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Christian R. Trott, Damien Lebrun-Grandié, Daniel Arndt, Jan Ciesko, Vinh Dang, Nathan Ellingwood, Rahulkumar Gayatri, Evan Harvey, Daisy S. Hollman, Dan Ibanez, Nevin Liber, Jonathan Madsen, Jeff Miles, David Poliakoff, Amy Powell, Sivasankaran Rajamanickam, Mikael Simberg, Dan Sunderland, Bruno Turcksin, Jeremiah Wilke">
  <data key="d0">Christian R. Trott, Damien Lebrun-Grandié, Daniel Arndt, Jan Ciesko, Vinh Dang, Nathan Ellingwood, Rahulkumar Gayatri, Evan Harvey, Daisy S. Hollman, Dan Ibanez, Nevin Liber, Jonathan Madsen, Jeff Miles, David Poliakoff, Amy Powell, Sivasankaran Rajamanickam, Mikael Simberg, Dan Sunderland, Bruno Turcksin, Jeremiah Wilke</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who authored a paper on Kokkos 3, an extension of the Kokkos programming model, aimed at enabling scalable, high-performance programming for exascale supercomputers.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, William F. Godoy, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter">
  <data key="d0">Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, William F. Godoy, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers conducting a comparative study of Llama-2 and GPT-3 large language models for HPC kernel generation, assessing their performance and suitability for scientific computing.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, Qianxiang Wang">
  <data key="d0">Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, Qianxiang Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researchers who developed CoderEval, a benchmark framework for pragmatic code generation using generative pre-trained models, focusing on real-world coding scenarios.</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Evaluation frameworks">
  <data key="d0">Evaluation frameworks</data>
  <data key="d3">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d2">Benchmarks like HumanEval and MBPP are used to evaluate the performance and capabilities of code-generating language models.&lt;SEP&gt;Benchmarks such as HumanEval, MBPP, DS-1000, GSM8K, and CoderEval are used to evaluate the performance of models on code generation tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Component counting">
  <data key="d0">Component counting</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d2">Component counting is a fundamental concept in geometric analysis, involving quantifying parts of a geometric structure.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Array Mapping">
  <data key="d0">Array Mapping</data>
  <data key="d3">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d2">Mapping a constant function to array elements is a data transformation technique used in computational algorithms.&lt;SEP&gt;Mapping a constant function to each element of an array is a data transformation process used in computational tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model comparison">
  <data key="d0">Model comparison</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d2">Models are compared based on their performance on datasets like HumanEval and BigCode, using defined metrics."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model architecture">
  <data key="d0">Model architecture</data>
  <data key="d3">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d2">Different architectures like SantaCoder and CodeLlama influence model capabilities and performance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Performance Optimization">
  <data key="d0">Performance Optimization</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d2">Equation (4) estimates the maximum achievable speedup over varying resource counts, guiding optimization of parallel code performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Speedup max@k">
  <data key="d0">Speedup max@k</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d2">Speedup max@k provides an upper bound on speedup across resource configurations, indicating peak potential of generated code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Performance Bound">
  <data key="d0">Performance Bound</data>
  <data key="d3">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d2">Equation (4) estimates the maximum possible speedup over different resource counts, guiding understanding of the upper performance limit for generated code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code evaluation">
  <data key="d0">Code evaluation</data>
  <data key="d3">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d2">The ParEval test harness compiles and runs generated code, recording correctness and execution time to evaluate code quality.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="performance of LLMs">
  <data key="d0">performance of LLMs</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d2">Fine-tuning improves the models' ability to generate accurate parallel code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="code execution">
  <data key="d0">code execution</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d2">Problem sizes are chosen to ensure code runs within a reasonable time limit, typically less than three minutes.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="code">
  <data key="d0">code</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d2">Codes exceeding the three-minute threshold are labeled as incorrect, establishing a performance constraint.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="NVIDIA A100 GPU">
  <data key="d0">NVIDIA A100 GPU</data>
  <data key="d3">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d2">The NVIDIA A100 GPU is used for high-performance kernel execution during evaluation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Difficulty">
  <data key="d0">Difficulty</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d2">Sparse linear algebra problems are the most difficult for LLMs to parallelize, likely due to their inherent complexity."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Kokkos Search Problems">
  <data key="d0">Kokkos Search Problems</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d2">GPT-4 does not perform well on Kokkos search problems despite excelling elsewhere, indicating model limitations."|&lt;SEP&gt;GPT-4 does not perform well on Kokkos search problems despite excelling in other areas, indicating specific model limitations."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Code Complexity">
  <data key="d0">Code Complexity</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d2">Code complexity refers to the level of intricacy involved in programming tasks, affecting the difficulty of obtaining acceptable results.&lt;SEP&gt;Code complexity refers to the level of intricacy involved in programming tasks, which influences the difficulty of achieving acceptable results in computational tasks.&lt;SEP&gt;The greater the difference between serial and parallel code in a given model, the more difficult it is for LLMs to generate correct code."|</data>
  <data key="d1">Core Concepts</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Generation Difficulty">
  <data key="d0">Code Generation Difficulty</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d2">MPI's complexity and divergence from serial code make it the hardest for LLMs to generate correct code."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="pass@1 Score">
  <data key="d0">pass@1 Score</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d2">Transform problems have the highest pass@1 scores across LLMs, indicating they are the easiest to parallelize."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel Models">
  <data key="d0">Parallel Models</data>
  <data key="d3">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d2">The greater the difference between serial and parallel code in a model, the harder it is for LLMs to generate correct code."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Models and Prompts">
  <data key="d0">Models and Prompts</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">Figure 7 shows the efficiency@1 for different models across MPI, OpenMP, and Kokkos prompts, illustrating comparative performance and resource utilization across models and models' effectiveness in different contexts."|&gt;"performance comparison, resource utilization&lt;SEP&gt;Figure 7 shows the efficiency@1 for different models across MPI, OpenMP, and Kokkos prompts, illustrating comparative performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Models and Performance">
  <data key="d0">Models and Performance</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">Figure 6 demonstrates pass@1 scores and speedup for models, emphasizing GPT-4's superior performance and the trend of efficiency and speedup across models and parallel prompts."|&gt;"model performance, accuracy&lt;SEP&gt;Figure 6 demonstrates pass@1 scores and speedup for models, emphasizing GPT-4's superior performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Resource Counts and Performance">
  <data key="d0">Resource Counts and Performance</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">Figure 8 depicts maximum speedup and efficiency across resource counts, analyzing how parallel code performance scales with resource availability and identifying plateauing behaviors."|&gt;"scalability, resource utilization&lt;SEP&gt;Figure 8 depicts maximum speedup and efficiency across resource counts, analyzing scalability and resource utilization.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Model Translation">
  <data key="d0">Model Translation</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">Figure 9 compares pass@1 scores for code translation between models and execution contexts, assessing translation accuracy.&lt;SEP&gt;Figure 9 compares pass@1 scores for translating code between models and execution contexts, assessing the models' ability to accurately convert code from serial to OpenMP, MPI, and CUDA to Kokkos, thereby evaluating translation effectiveness."|&gt;"translation accuracy, model capability</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Parallel Code Performance">
  <data key="d0">Parallel Code Performance</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">speedup 𝑛@1 measures how much faster parallel code runs compared to sequential, indicating performance gains.&lt;SEP&gt;speedup 𝑛@1 measures how much faster parallel code runs compared to the sequential baseline, indicating the effectiveness of parallelization and the performance gains achieved."|&gt;"performance metrics, effectiveness</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="code translation">
  <data key="d0">code translation</data>
  <data key="d3">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d2">Code translation is a methodology to convert code from one execution model to another, evaluating the translation accuracy and impact on performance."|&gt;"method evaluation, scalability</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="parEval">
  <data key="d0">parEval</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d2">ParEval benchmarks LLMs' ability to generate parallel code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Providing correct implementations">
  <data key="d0">Providing correct implementations</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d2">Supplying correct code examples enhances LLMs' correctness in code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="open-source models">
  <data key="d0">open-source models</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d2">Closed-source models outperform open-source models in generating correct and scalable parallel code."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="providing correct implementations">
  <data key="d0">providing correct implementations</data>
  <data key="d3">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d2">Supplying correct code examples in serial or other models helps LLMs produce correct parallel code."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2101.00027">
  <data key="d0">arXiv:2101.00027</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">The Pile dataset is hosted on arXiv as a large diverse text dataset for language modeling, supporting various research activities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Luyu Gao">
  <data key="d0">Luyu Gao</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">A researcher working on analyzing and revising language model outputs using language models themselves.&lt;SEP&gt;PAL (Program-aided Language Models) is described in an arXiv preprint as a methodology for enhancing language models with program assistance.</data>
  <data key="d1">Researcher</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2211.10435">
  <data key="d0">arXiv:2211.10435</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">PAL (Program-aided Language Models) is described in an arXiv preprint as a methodology for enhancing language models with program assistance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="ICPP-W 2023">
  <data key="d0">ICPP-W 2023</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">William Godoy's evaluation of OpenAI Codex for HPC is presented at the ICPP-W 2023 conference, sharing results on model performance in HPC contexts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE ICPC 2022">
  <data key="d0">IEEE ICPC 2022</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">Sakib Haque's research on semantic similarity metrics for code summaries was presented at ICPC 2022, contributing to evaluation techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="MSR 2023">
  <data key="d0">MSR 2023</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">Empirical study on HPC performance bugs was presented at MSR 2023, providing datasets and analysis on software bugs in HPC.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="IEEE/ACM 44th Software Engineering">
  <data key="d0">IEEE/ACM 44th Software Engineering</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">Research on reducing false positives in bug detectors was presented at ICSE 2022, sharing datasets and techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Edward Hu">
  <data key="d0">Edward Hu</data>
  <data key="d3">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d2">The LoRA adaptation technique is detailed in an arXiv preprint, explaining low-rank adaptation methods for large models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="The Stack Dataset">
  <data key="d0">The Stack Dataset</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Denis Kocetkov is associated with the creation or utilization of the 'The Stack' large source code dataset, which provides extensive code for analysis and training models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="DS-1000 Benchmark">
  <data key="d0">DS-1000 Benchmark</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Yuhang Lai and colleagues developed DS-1000, a benchmark dataset for data science code generation tasks.&lt;SEP&gt;Yuhang Lai and colleagues developed the DS-1000 benchmark dataset for data science code generation tasks, providing a reliable evaluation resource.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Yuhang Lai et al.">
  <data key="d0">Yuhang Lai et al.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Yuhang Lai and colleagues developed DS-1000, a benchmark dataset for data science code generation tasks.&lt;SEP&gt;Yuhang Lai and colleagues developed the DS-1000 benchmark dataset for data science code generation tasks, providing a reliable evaluation resource.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Large Source Code Dataset">
  <data key="d0">Large Source Code Dataset</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Raymond Li and team contributed to or analyzed a large-scale source code dataset for machine learning and software engineering research.&lt;SEP&gt;Raymond Li and team contributed to or analyzed large-scale source code datasets for machine learning applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Raymond Li et al.">
  <data key="d0">Raymond Li et al.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Raymond Li and team contributed to or analyzed a large-scale source code dataset for machine learning and software engineering research.&lt;SEP&gt;Raymond Li and team contributed to or analyzed large-scale source code datasets for machine learning applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Mingjie Liu et al.">
  <data key="d0">Mingjie Liu et al.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Mingjie Liu and colleagues developed VerilogEval, a benchmark for evaluating language models in Verilog hardware description code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="VerilogEval Benchmark">
  <data key="d0">VerilogEval Benchmark</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Mingjie Liu and colleagues developed VerilogEval, a benchmark for evaluating language models in Verilog hardware description code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Christian Munley et al.">
  <data key="d0">Christian Munley et al.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Christian Munley and team developed LLM4VV, a large language model-driven test suite for compiler validation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="LLM for Compiler Validation">
  <data key="d0">LLM for Compiler Validation</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Christian Munley and team developed LLM4VV, a large language model-driven test suite for compiler validation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Daniel Nichols et al.">
  <data key="d0">Daniel Nichols et al.</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">Daniel Nichols and colleagues used large language models to simulate and analyze parallel programming constructs.&lt;SEP&gt;Daniel Nichols and colleagues used large language models to simulate, analyze, and optimize parallel programming constructs, advancing understanding of parallel computing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="GPT-4 Technical Report">
  <data key="d0">GPT-4 Technical Report</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d2">A comprehensive documentation detailing the architecture, training, and capabilities of the GPT-4 language model.&lt;SEP&gt;OpenAI published the GPT-4 technical report detailing the architecture and capabilities of the GPT-4 model.</data>
  <data key="d1">Study Designs</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="OpenAI API and Python Library">
  <data key="d0">OpenAI API and Python Library</data>
  <data key="d3">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d2">OpenAI provides APIs and Python libraries that enable developers to incorporate GPT-4 and other models into their software tools for code generation, analysis, and automation.&lt;SEP&gt;OpenAI provides APIs and libraries for accessing and utilizing their language models in software applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="arXiv:2308.12950 [cs.CL]">
  <data key="d0">arXiv:2308.12950 [cs.CL]</data>
  <data key="d3">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d2">The arXiv publication details the architecture, capabilities, and applications of Code Llama models, contributing to open-source AI for code."|&lt;"research dissemination, model description&lt;SEP&gt;The arXiv publication details the development and capabilities of the Code Llama models, contributing to the field of large language models for code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</node>
<node id="Kernel Generation">
  <data key="d0">Kernel Generation</data>
  <data key="d1">Methodology</data>
  <data key="d2">Kernel Generation refers to the process of creating computational kernels, which are fundamental code units used in numerical and scientific computing, often generated or optimized using AI tools like Codex.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="William F. Godoy">
  <data key="d0">William F. Godoy</data>
  <data key="d1">Researcher</data>
  <data key="d2">Godoy and colleagues evaluated the performance and portability of high-level programming models Julia, Python/Numba, and Kokkos on exascale nodes, assessing their scalability and efficiency.&lt;SEP&gt;Godoy and colleagues evaluated the performance and portability of high-level programming models like Julia, Python/Numba, and Kokkos on exascale computing nodes, assessing their efficiency in high-performance computing.&lt;SEP&gt;William F. Godoy is a researcher and author leading the evaluation study on AI-assisted HPC kernel generation, analyzing outputs and limitations.&lt;SEP&gt;William F. Godoy is an author and researcher involved in evaluating AI-assisted code generation for HPC applications.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jeffrey S. Vetter">
  <data key="d0">Jeffrey S. Vetter</data>
  <data key="d1">Researcher</data>
  <data key="d2">Jeffrey S. Vetter is a researcher assessing the impact of AI tools like Codex on HPC software development and performance.&lt;SEP&gt;Jeffrey S. Vetter is an author and researcher analyzing the application of generative AI in high-performance computing.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Oak Ridge National Laboratory">
  <data key="d0">Oak Ridge National Laboratory</data>
  <data key="d1">Organization</data>
  <data key="d2">A research institution where the evaluation of AI-assisted HPC kernel generation was conducted, providing infrastructure and support.&lt;SEP&gt;The Oak Ridge National Laboratory is a research institution where the study on AI-assisted HPC kernel generation was conducted.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="High-Performance Computing (HPC)">
  <data key="d0">High-Performance Computing (HPC)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">HPC involves the use of supercomputers and parallel processing techniques to perform large-scale scientific and numerical computations, such as AXPY, GEMV, GEMM, SpMV, JacobiStencil, and CG kernels.&lt;SEP&gt;HPC involves the use of supercomputers and parallel processing to solve complex computational problems, including numerical kernels like AXPY, GEMV, GEMM, SpMV, JacobiStencil, and CG.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Numerical Kernels">
  <data key="d0">Numerical Kernels</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Fundamental computational routines in scientific computing, including AXPY, GEMV, GEMM, SpMV, JacobiStencil, and CG, which are the focus of AI-generated implementation and evaluation.&lt;SEP&gt;Numerical kernels are fundamental computational routines used in scientific computing, such as AXPY, GEMV, GEMM, SpMV, JacobiStencil, and CG, which are evaluated for AI-generated implementation quality.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Programming Models">
  <data key="d0">Programming Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Frameworks and APIs that enable parallel programming in HPC, including C++, Fortran, Python, Julia, with specific models like OpenMP, OpenACC, Kokkos, SYCL, CUDA, HIP, Threads, CUDA.jl, AMDGPU.jl, KernelAbstractions.jl.&lt;SEP&gt;Programming models include C++, Fortran, Python, and Julia, supported by various APIs like OpenMP, OpenACC, Kokkos, SYCL, CUDA, HIP, and others, used to develop HPC kernels.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GitHub Copilot">
  <data key="d0">GitHub Copilot</data>
  <data key="d1">Tools</data>
  <data key="d2">An AI-powered code assistant that provides code suggestions and completions, aimed at improving software development productivity.&lt;SEP&gt;An AI-powered code assistant that suggests code snippets and completions to programmers.&lt;SEP&gt;An AI-powered code generation tool designed to assist programmers by suggesting code snippets and solutions.&lt;SEP&gt;An AI-powered code suggestion tool designed to assist programmers by providing code snippets and completions.&lt;SEP&gt;An AI-powered code suggestion tool designed to assist programmers by providing code snippets and suggestions.&lt;SEP&gt;GitHub Copilot is an AI-powered code assistant plugin based on OpenAI Codex, used to generate programming code within development environments like Visual Studio Code.&lt;SEP&gt;GitHub Copilot is an AI-powered code assistant plugin built on OpenAI Codex, integrated into development environments like Visual Studio Code to assist in code generation for HPC kernels.&lt;SEP&gt;GitHub Copilot is an AI-powered code generation tool that assists programmers by suggesting code snippets and completions, with security assessments conducted in recent research.&lt;SEP&gt;GitHub Copilot is an AI-powered code generation tool that utilizes OpenAI Codex to assist in writing HPC kernel code via Visual Studio Code.&lt;SEP&gt;GitHub Copilot is an AI-powered code synthesis tool that suggests code snippets, and its security and performance are evaluated in recent research.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252&lt;SEP&gt;chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Proficiency Metric">
  <data key="d0">Proficiency Metric</data>
  <data key="d1">Variables</data>
  <data key="d2">A metric designed to quantify the quality, relevance, and correctness of AI-generated code suggestions, used to compare outputs across different prompts and models.&lt;SEP&gt;The proficiency metric quantifies the quality and relevance of the AI-generated code suggestions based on initial outputs, used to compare different programming models and prompts.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Language-supported Programming Models">
  <data key="d0">Language-supported Programming Models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The study tests various programming models supported by languages like C++, Fortran, Python, and Julia, to evaluate AI-generated kernel code across different environments.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Evaluation of OpenAI Codex for HPC Parallel Programming">
  <data key="d0">Evaluation of OpenAI Codex for HPC Parallel Programming</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">This document evaluates the capabilities, limitations, and applications of OpenAI Codex in generating HPC kernels, focusing on its effectiveness across various programming models and languages.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Models Kernel Generation">
  <data key="d0">Models Kernel Generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of creating numerical kernels using AI tools like OpenAI Codex to automate and optimize code development for high-performance computing applications.</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Languages">
  <data key="d0">Languages</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">C++, Fortran, Python, Julia, the programming languages evaluated in the study for AI code suggestion quality.&lt;SEP&gt;C++, Fortran, Python, Julia—programming languages evaluated for AI suggestion quality and correctness.&lt;SEP&gt;Programming languages supported in the study, such as C++, Fortran, Python, and Julia, used to generate HPC kernels with AI assistance.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="potential">
  <data key="d0">potential</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Potential encompasses the possibilities and capabilities of the new technology to impact scientific computing, HPC, and AI-assisted code generation.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="OpenAI Codex">
  <data key="d0">OpenAI Codex</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">OpenAI Codex is an AI language model capable of generating code snippets and solutions across various programming languages based on prompts.&lt;SEP&gt;OpenAI Codex is an AI language model capable of generating code snippets and solutions across various programming languages based on user prompts, used in automating code generation.&lt;SEP&gt;OpenAI Codex is an advanced language model based on GPT-3, specifically designed for code understanding and generation, enabling automated creation of scientific kernels for HPC applications.&lt;SEP&gt;OpenAI Codex is an advanced language model descended from GPT-3, designed for code generation and understanding, with applications in scientific kernels for HPC.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Scientific Kernels for HPC">
  <data key="d0">Scientific Kernels for HPC</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Scientific kernels are fundamental computational routines such as AXPY, GEMV, GEMM, SpMV, 3D Jacobi, and conjugate gradients, used to evaluate the impact of LLMs on HPC performance and correctness.&lt;SEP&gt;Scientific kernels are fundamental computational routines used in high-performance computing, such as AXPY, GEMV, GEMM, SpMV, 3D Jacobi, and conjugate gradients, which are evaluated for optimization and correctness.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Metrics for Quality">
  <data key="d0">Metrics for Quality</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Metrics are proposed to evaluate the quality of generated code, including correctness, trade-offs, and overall value, to benchmark LLM performance in HPC contexts.&lt;SEP&gt;Metrics are proposed to score the quality of generated outputs, including correctness, trade-offs, and overall value of large language models in HPC contexts.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Broader Computer Science Research">
  <data key="d0">Broader Computer Science Research</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Related efforts include analyzing the impact of LLMs like GPT-3 on NLP, code synthesis, and automation, highlighting recent attention to these topics.&lt;SEP&gt;Related efforts include research on GPT-3's impact on NLP, code synthesis, automation, and the recent attention to these topics in the broader AI and CS community.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Semi-supervised and Few-shot Learning">
  <data key="d0">Semi-supervised and Few-shot Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">These models describe learning paradigms where models perform tasks with limited labeled data by leveraging prior knowledge, crucial for prompt-based code generation.&lt;SEP&gt;These models describe techniques where models learn from limited data, using prior knowledge to improve performance, especially relevant for prompt-based code generation.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Limitations of ML in Code Generation">
  <data key="d0">Limitations of ML in Code Generation</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges include variability in code quality, dataset dependency, difficulty handling complex long operations, and reliance on large datasets like GitHub repositories.&lt;SEP&gt;Challenges include variability in quality, dataset dependency, difficulty handling complex long operations, and issues with code correctness and interpretability, especially with large datasets like GitHub.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Impact on HPC Practitioners">
  <data key="d0">Impact on HPC Practitioners</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The study aims to evaluate how current LLM capabilities can be adopted by HPC practitioners for kernel development, optimization, and testing, influencing future workflows.&lt;SEP&gt;The study aims to understand how current LLM capabilities can be adopted by HPC practitioners, influencing kernel development, optimization, and testing.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Future Directions">
  <data key="d0">Future Directions</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Future research includes improving prompt techniques, enhancing model accuracy, expanding datasets, and broadening applications of LLMs in scientific computing.&lt;SEP&gt;Future research includes improving prompt techniques, enhancing model accuracy, expanding datasets, and exploring broader applications of LLMs in scientific computing.&lt;SEP&gt;The work aims to inform future development of HPC AI developer tools by understanding data and modeling challenges.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Potential">
  <data key="d0">Potential</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Potential encompasses the possibilities, capabilities, and future impact of the new technology in scientific computing, HPC, and AI-assisted code generation.</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Sobania et al.">
  <data key="d0">Sobania et al.</data>
  <data key="d1">Research Study</data>
  <data key="d2">A study comparing Copilot and automatic generators using genetic programming applied to PSB2 program synthesis benchmarks, assessing differences in performance.&lt;SEP&gt;Researchers who conducted an analysis comparing Copilot and genetic programming-based automatic code generators on program synthesis benchmarks, focusing on performance differences.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Copilot">
  <data key="d0">Copilot</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An AI-powered code generation tool based on GPT models, used for assisting programming tasks, with considerations of output quality, security vulnerabilities, and educational impact.&lt;SEP&gt;An AI-powered code generation tool based on GPT models, used to assist programmers by suggesting code snippets, with evaluations of its effectiveness, security vulnerabilities, and educational impact.&lt;SEP&gt;Copilot is an AI-powered code generation tool that assists in producing code snippets across various languages and paradigms, especially useful in scientific and HPC contexts.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PSB2 Program Synthesis Benchmarks">
  <data key="d0">PSB2 Program Synthesis Benchmarks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A collection of benchmark problems used to evaluate program synthesis methods, including AI code generators like Copilot.&lt;SEP&gt;A set of benchmark problems used to evaluate the performance of program synthesis methods, including AI tools like Copilot and genetic programming.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Imai">
  <data key="d0">Imai</data>
  <data key="d1">Research Study</data>
  <data key="d2">An assessment of Copilot's ability to generate code lines faster but with lower quality compared to human programming.&lt;SEP&gt;An initial assessment indicating that Copilot can generate code faster but with lower quality compared to human programmers.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Human Pair Programming">
  <data key="d0">Human Pair Programming</data>
  <data key="d1">Study Design</data>
  <data key="d2">A collaborative programming approach where two programmers work together, used as a comparison standard for AI-generated code quality.&lt;SEP&gt;A collaborative programming approach where two programmers work together, used as a standard for evaluating the quality of code generated by AI.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="YouTistiren et al.">
  <data key="d0">YouTistiren et al.</data>
  <data key="d1">Research Study</data>
  <data key="d2">An evaluation of the correctness, validity, and efficiency of Copilot targeting the HumanEval problem dataset with high success rates.&lt;SEP&gt;Research assessing the correctness, validity, and efficiency of Copilot's code on the HumanEval dataset, with a high success rate.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Finnie-Ansley et al.">
  <data key="d0">Finnie-Ansley et al.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Report on Codex's high ranking in generating programming solutions compared to introductory students.&lt;SEP&gt;Research reporting that Codex ranks high in generating programming solutions compared to introductory students.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Denny et al.">
  <data key="d0">Denny et al.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Discussion of the pedagogical value of Copilot in testing answers and providing prompts in introductory programming.&lt;SEP&gt;Research discussing the pedagogical value of Copilot in testing answers and providing prompts in introductory programming courses.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Pedagogical Value">
  <data key="d0">Pedagogical Value</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The educational benefit and challenges of integrating AI tools like Copilot into programming education.&lt;SEP&gt;The educational benefits and challenges associated with integrating AI tools like Copilot into programming education.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Wermelinger">
  <data key="d0">Wermelinger</data>
  <data key="d1">Research Study</data>
  <data key="d2">Discussion on how teaching programming might evolve with the adoption of AI-assisted tools.&lt;SEP&gt;Discussion on how teaching programming will evolve with AI assistance.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Brennan and Lesage">
  <data key="d0">Brennan and Lesage</data>
  <data key="d1">Research Study</data>
  <data key="d2">Discussion of educational opportunities for undergraduates using AI-assisted programming tools, emphasizing the development of software development intuition.&lt;SEP&gt;Research highlighting educational opportunities for undergraduates using AI-assisted tools and emphasizing the importance of developing strong software development intuition.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Pierce et al.">
  <data key="d0">Pierce et al.</data>
  <data key="d1">Research Study</data>
  <data key="d2">Assessment of security vulnerabilities in Copilot's code contributions, highlighting the need for security-aware tooling.&lt;SEP&gt;Research assessing security vulnerabilities in Copilot's code contributions, finding high vulnerability rates and emphasizing the need for security-aware tooling.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Security Vulnerabilities">
  <data key="d0">Security Vulnerabilities</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Instances of potential security issues in AI-generated code, with implications for safe deployment.&lt;SEP&gt;Instances of security flaws in AI-generated code, highlighting risks and the need for mitigation strategies.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="AI Code-Generating Capabilities">
  <data key="d0">AI Code-Generating Capabilities</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The abilities and limitations of AI tools like Copilot in generating functional, secure, and efficient code.&lt;SEP&gt;The technical ability of AI tools like Copilot to generate functional, efficient, and secure code, with ongoing research into their limitations and potentials.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Technical Implications">
  <data key="d0">Technical Implications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The impact of AI code generation on software development practices, security considerations, and technological advancement.&lt;SEP&gt;The technical impact of AI code generation on software development, security, and performance.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Economic and Social Implications">
  <data key="d0">Economic and Social Implications</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Broader consequences of integrating AI code generators into human-computer interaction, education, and industry.&lt;SEP&gt;Broader societal effects, including changes in education, industry, and human-computer interaction driven by AI code generation.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="HPC Kernels">
  <data key="d0">HPC Kernels</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High-Performance Computing kernels, specific computational routines in scientific computing, with no current application of Copilot capabilities reported.&lt;SEP&gt;High-Performance Computing kernels, specific computational routines used in scientific computing, noted as not yet explored with Copilot.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Prompt Queries">
  <data key="d0">Prompt Queries</data>
  <data key="d1">Tools</data>
  <data key="d2">Structured prompts designed to elicit code suggestions from Copilot based on kernels and programming models.&lt;SEP&gt;Structured prompts used to elicit code suggestions from Copilot, based on kernels and programming models.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Assessment Metrics">
  <data key="d0">Assessment Metrics</data>
  <data key="d1">Variables</data>
  <data key="d2">Metrics designed to evaluate the correctness and quality of AI-generated code suggestions.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Popularity Metrics">
  <data key="d0">Popularity Metrics</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of repositories on GitHub and TIOBE index used to assess the availability of code examples per language.&lt;SEP&gt;Number of repositories on GitHub and TIOBE index used to estimate the availability of code examples per language.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Experiment Setup">
  <data key="d0">Experiment Setup</data>
  <data key="d1">Methodology</data>
  <data key="d2">Procedure for selecting prompts, generating suggestions, and evaluating their correctness across languages and models.&lt;SEP&gt;The process of selecting prompts, generating code suggestions, and evaluating their correctness across different languages and models.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Assessment Techniques">
  <data key="d0">Assessment Techniques</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Methods for analyzing the correctness of suggestions and the influence of prompt structure and language.</data>
  <data key="d3">chunk-5adc1bc605f86b4e3648d3a3ef16f8a3</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="OpenACC">
  <data key="d0">OpenACC</data>
  <data key="d1">Discipline</data>
  <data key="d2">A parallel programming standard designed to simplify coding for heterogeneous CPU/GPU systems.&lt;SEP&gt;A standard API designed to simplify parallel programming for heterogeneous CPU/GPU systems, facilitating code acceleration.&lt;SEP&gt;OpenACC is a programming standard and API for parallel computing, allowing directives-based GPU acceleration, with version 3.1 specified in 2020.&lt;SEP&gt;OpenACC is a programming standard and API for parallel computing, enabling directives-based GPU acceleration, with version 3.1 specified in 2020.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Thrust">
  <data key="d0">Thrust</data>
  <data key="d1">Methodology</data>
  <data key="d2">A C++ template library for parallel algorithms targeting GPU acceleration via CUDA, enabling high-level data parallelism.&lt;SEP&gt;A parallel algorithms library in C++ designed for GPU programming, built on CUDA.&lt;SEP&gt;Thrust is a C++ template library for CUDA, facilitating parallel algorithms and data structures for GPU programming.&lt;SEP&gt;Thrust is a C++ template library for CUDA, offering parallel algorithms and data structures to facilitate GPU programming.&lt;SEP&gt;Thrust is a parallel algorithms library resembling the C++ Standard Template Library, designed for GPU programming and high-performance computing.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="SyCL">
  <data key="d0">SyCL</data>
  <data key="d1">Methodology</data>
  <data key="d2">A C++-based programming model for heterogeneous computing, enabling code portability across different hardware.&lt;SEP&gt;A C++-based programming model for heterogeneous computing, supporting portable code execution across multiple hardware backends.&lt;SEP&gt;SyCL is a high-level programming abstraction layer for heterogeneous computing, enabling code portability across CPUs, GPUs, and other accelerators.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Post fix function">
  <data key="d0">Post fix function</data>
  <data key="d1">methodology</data>
  <data key="d2">A coding pattern where functions are called with a postfix notation, used here to specify code generation prompts for experiments.&lt;SEP&gt;A coding style where functions are called with a postfix notation, used in some programming contexts.&lt;SEP&gt;A prompt pattern used to specify the function style in code generation experiments.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GEMV">
  <data key="d0">GEMV</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">A kernel operation representing General Matrix-Vector multiplication, used as a benchmark in the study.&lt;SEP&gt;General Matrix-Vector multiplication kernel used as a computational benchmark to evaluate code correctness and model proficiency.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GEMM">
  <data key="d0">GEMM</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">A kernel operation representing General Matrix-Matrix multiplication, a key computational routine analyzed.&lt;SEP&gt;GEMM (General Matrix to Matrix Multiplication) is a fundamental operation in linear algebra used in various scientific and engineering computations, involving multiplying matrices.&lt;SEP&gt;General Matrix-Matrix multiplication kernel, a fundamental linear algebra operation analyzed in the study for code generation accuracy.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="SpMV">
  <data key="d0">SpMV</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">SpMV (Sparse Matrix-Vector multiplication) is a key operation in scientific computing involving multiplying a sparse matrix by a vector, crucial for large-scale simulations.&lt;SEP&gt;SpMV (Sparse Matrix-Vector multiplication) is an operation that multiplies a sparse matrix by a vector, crucial for large-scale scientific computations.&lt;SEP&gt;Sparse Matrix-Vector multiplication kernel, used to assess code correctness and model proficiency in sparse linear algebra.&lt;SEP&gt;Sparse Matrix-Vector multiplication, a computational kernel evaluated in the experiments.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jacobi">
  <data key="d0">Jacobi</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">An iterative algorithm for solving systems of linear equations, used here as a computational kernel.&lt;SEP&gt;An iterative method for solving systems of linear equations, used here as a kernel in evaluating code correctness.&lt;SEP&gt;Jacobi is an iterative method for solving systems of linear equations, often used in numerical analysis and scientific computing.&lt;SEP&gt;Jacobi method is an iterative algorithm for solving systems of linear equations, often used in numerical analysis.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="CG">
  <data key="d0">CG</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">Conjugate Gradient (CG) is an iterative algorithm for solving large sparse linear systems, especially in scientific and engineering applications.&lt;SEP&gt;Conjugate Gradient (CG) is an iterative method for solving large sparse systems of linear equations, especially symmetric positive-definite matrices.&lt;SEP&gt;Conjugate Gradient method, an iterative algorithm for solving large sparse linear systems, analyzed for correctness and proficiency.&lt;SEP&gt;Conjugate Gradient method, an iterative algorithm for solving large sparse linear systems, studied for its implementation complexity.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Prefix &lt;kernel&gt;">
  <data key="d0">Prefix &lt;kernel&gt;</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A prompt pattern used to specify the kernel type in code generation experiments.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Kernel">
  <data key="d0">Kernel</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A computational routine or operation (e.g., GEMV, GEMM, SpMV, Jacobi, CG) used as benchmarks in the experiments.&lt;SEP&gt;A kernel is a fundamental computational unit in parallel programming, often representing a function executed on a GPU or accelerator, such as in CUDA or OpenMP.&lt;SEP&gt;A kernel is a GPU function executed in parallel across many threads, representing the core computational unit in CUDA programming.&lt;SEP&gt;Core computational routines within benchmarks, such as the hotspot stencil or heartwall kernel, which are targeted for optimization.&lt;SEP&gt;Kernels are the core computational routines within benchmarks, such as the hotspot stencil or heartwall, which are optimized for performance.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-64e7fd510c0d613c9975fe3385d6f3a0&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Experiment">
  <data key="d0">Experiment</data>
  <data key="d1">Study Design</data>
  <data key="d2">The structured procedure involving prompts and evaluation metrics to assess code generation quality across different kernels and programming models.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Prompt Pattern">
  <data key="d0">Prompt Pattern</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The specific input format used to instruct code generation models, such as including &lt;kernel&gt; and &lt;programming model&gt; tags.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Correctness Level">
  <data key="d0">Correctness Level</data>
  <data key="d1">Variables</data>
  <data key="d2">A scale from 0 to 1 indicating the accuracy and correctness of the generated code, with detailed proficiency labels.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Generation Model">
  <data key="d0">Code Generation Model</data>
  <data key="d1">Tools</data>
  <data key="d2">AI-based systems like GitHub Copilot that generate code snippets based on input prompts for various programming tasks.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Analysis">
  <data key="d0">Analysis</data>
  <data key="d1">Study Analysis</data>
  <data key="d2">The process of interpreting the results to understand the effectiveness of code generation across kernels and models.</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Fortran">
  <data key="d0">Fortran</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Fortran is a domain-specific programming language primarily used in scientific computing and HPC, known for its legacy and efficiency in numerical tasks.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="numpy">
  <data key="d0">numpy</data>
  <data key="d1">Tools</data>
  <data key="d2">NumPy is a Python library for numerical computing, providing support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions.&lt;SEP&gt;NumPy is a Python library providing support for large multi-dimensional arrays and matrices, along with mathematical functions, widely used in scientific computing.&lt;SEP&gt;NumPy is a fundamental library for scientific computing in Python, providing support for large multi-dimensional arrays, matrices, and a collection of mathematical functions.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="cuPy">
  <data key="d0">cuPy</data>
  <data key="d1">Tools</data>
  <data key="d2">cuPy is a GPU-accelerated library for array computations in Python, offering a NumPy-compatible interface for NVIDIA CUDA-enabled GPUs.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="pyCUDA">
  <data key="d0">pyCUDA</data>
  <data key="d1">Tools</data>
  <data key="d2">pyCUDA is a Python library that allows direct access to CUDA APIs for writing custom GPU kernels and managing GPU resources from Python.&lt;SEP&gt;pyCUDA is a Python wrapper for CUDA allowing direct access to GPU programming and kernel execution, facilitating custom GPU-based computations.&lt;SEP&gt;pyCUDA is a Python wrapper for CUDA, allowing direct access to GPU programming and kernel execution from Python.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Numba">
  <data key="d0">Numba</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python library that compiles Python code to optimized machine code at runtime, improving performance for numerical applications.&lt;SEP&gt;Numba is a JIT compiler for Python that accelerates numerical and scientific computing by compiling Python code to optimized machine code at runtime.&lt;SEP&gt;Numba is a JIT compiler for Python that accelerates numerical functions by compiling Python code into machine code at runtime, used to optimize scientific code.&lt;SEP&gt;Numba is a JIT compiler for Python that accelerates numerical functions by compiling Python code to machine code at runtime.&lt;SEP&gt;Numba is a JIT compiler that translates a subset of Python and NumPy code into fast machine code, supporting GPU acceleration but with limitations, such as deprecated support for AMD GPUs.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-49dc2215d7813b7181583fe38186e482&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="High-level programming models">
  <data key="d0">High-level programming models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High-level programming models like Kokkos, Thrust, and SyCL are abstractions designed to improve portability and productivity in HPC but show varying performance across kernels.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Legacy HPC applications">
  <data key="d0">Legacy HPC applications</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Legacy HPC applications are older, established scientific codes often written in Fortran, still relevant and used in current research.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Benchmark repositories">
  <data key="d0">Benchmark repositories</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Benchmark repositories like HecBench are collections of computational kernels and datasets used to evaluate performance and code generation quality.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Parallel programming models">
  <data key="d0">Parallel programming models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallel programming models are frameworks and abstractions, such as OpenMP, CUDA, Kokkos, Thrust, and SyCL, that facilitate writing parallel code for various hardware architectures.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code quality">
  <data key="d0">Code quality</data>
  <data key="d1">Results</data>
  <data key="d2">Code quality refers to the correctness, efficiency, and readability of generated code, assessed through metrics and expert evaluation.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code generation">
  <data key="d0">Code generation</data>
  <data key="d1">Results</data>
  <data key="d2">Code generation is the process of automatically producing source code using AI tools like Copilot, evaluated based on accuracy, completeness, and performance.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Community size">
  <data key="d0">Community size</data>
  <data key="d1">Variables</data>
  <data key="d2">Community size influences the performance of high-level abstractions; larger communities tend to have more optimized and well-supported tools like Kokkos, Thrust, and SyCL.</data>
  <data key="d3">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="CuPy">
  <data key="d0">CuPy</data>
  <data key="d1">Tools</data>
  <data key="d2">CuPy is a GPU-accelerated library compatible with NumPy and SciPy, designed for high-performance numerical computations on Nvidia GPUs.&lt;SEP&gt;CuPy is a GPU-accelerated library compatible with NumPy, enabling high-performance array computations on NVIDIA GPUs.&lt;SEP&gt;CuPy is a GPU-accelerated library compatible with NumPy, enabling high-performance computations on NVIDIA GPUs, used in scientific and numerical tasks.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;kernel&gt;">
  <data key="d0">&lt;kernel&gt;</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The &lt;kernel&gt; pattern indicates code segments or functions designed for GPU or parallel execution, often used in high-performance computing.&lt;SEP&gt;The &lt;kernel&gt; pattern indicates code segments or functions designed for GPU or parallel execution, often used in performance-critical computations.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="def">
  <data key="d0">def</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The 'def' keyword in Python defines a function, encapsulating code for reuse and modularity, essential in developing computational kernels.&lt;SEP&gt;The 'def' keyword in Python defines a function, which encapsulates a block of code for reuse and modularity.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Julia">
  <data key="d0">Julia</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Julia is a high-level programming language used for scientific computing, evaluated for performance and portability in high-performance environments.&lt;SEP&gt;Julia is a high-performance programming language designed for scientific computing, emphasizing speed, ease of use, and mathematical capabilities.&lt;SEP&gt;Julia is a high-performance programming language designed for technical and scientific computing, emphasizing speed and ease of use.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="LLVM">
  <data key="d0">LLVM</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A compiler infrastructure framework for program analysis, transformation, and optimization across multiple languages and platforms.&lt;SEP&gt;An open-source compiler infrastructure framework that facilitates program analysis, transformation, and optimization across multiple languages and platforms.&lt;SEP&gt;LLVM (Low Level Virtual Machine) is a compiler infrastructure designed for building high-performance compilers and language tools.&lt;SEP&gt;LLVM (Low Level Virtual Machine) is a compiler infrastructure used for building high-performance compilers and language tools, supporting multiple languages including Julia.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53&lt;SEP&gt;chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="CUDA.jl">
  <data key="d0">CUDA.jl</data>
  <data key="d1">Tools</data>
  <data key="d2">CUDA.jl is a Julia package that provides support for NVIDIA GPU programming using CUDA, enabling GPU-accelerated computations.&lt;SEP&gt;CUDA.jl is a Julia package that provides support for NVIDIA GPU programming using CUDA, enabling GPU-accelerated scientific computations.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="AMDGPU.jl">
  <data key="d0">AMDGPU.jl</data>
  <data key="d1">Tools</data>
  <data key="d2">AMDGPU.jl is a Julia package for programming AMD GPUs, supporting cross-vendor GPU kernels and portability in scientific computing.&lt;SEP&gt;AMDGPU.jl is a Julia package for programming AMD GPUs, supporting portable GPU kernels across different hardware.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="KernelAbstractions.jl">
  <data key="d0">KernelAbstractions.jl</data>
  <data key="d1">Tools</data>
  <data key="d2">KernelAbstractions.jl is a Julia package that abstracts GPU kernel programming across multiple hardware vendors, enabling portable and flexible kernel development.&lt;SEP&gt;KernelAbstractions.jl is a Julia package that abstracts GPU kernel programming for multiple hardware vendors, facilitating portable kernel development.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="public repositories">
  <data key="d0">public repositories</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Public code repositories contain a vast amount of Python and numpy code, which influences the availability and success rate of code generation.&lt;SEP&gt;Public code repositories contain extensive Python and numpy code, which influence the availability of training data and the success rate of AI code generation.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="complexity">
  <data key="d0">complexity</data>
  <data key="d1">Variables</data>
  <data key="d2">Kernel complexity affects the difficulty of generating correct code, with more complex kernels like CG being harder to implement correctly.&lt;SEP&gt;Kernel complexity directly impacts the difficulty of generating correct code with AI models, with more complex kernels like CG being harder to implement accurately.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="proficiency level">
  <data key="d0">proficiency level</data>
  <data key="d1">Results</data>
  <data key="d2">The overall proficiency of AI code generation across languages and kernels is generally at a novice level, with C++ and Python performing slightly better than Julia or Fortran.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="targeted quality">
  <data key="d0">targeted quality</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Focusing on specific, well-defined problems and domain-specific prompts can improve AI code generation accuracy and reliability.&lt;SEP&gt;Focusing on specific, well-defined problems and syntax can improve AI code generation quality, emphasizing the importance of domain-specific prompts.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="V GEMM">
  <data key="d0">V GEMM</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">V GEMM refers to a specific variant or implementation of General Matrix to Matrix Multiplication, a fundamental linear algebra operation used in high-performance computing.</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Multistep or Multikernel Codes">
  <data key="d0">Multistep or Multikernel Codes</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Multistep or multikernel codes, such as CG, are specialized programming approaches that can be difficult to generate with high quality.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Keywords">
  <data key="d0">Keywords</data>
  <data key="d1">Variables</data>
  <data key="d2">Keywords are specific and sensitive terms used to improve AI-generated answer proficiency, requiring careful selection tailored to programming languages, models, or communities.&lt;SEP&gt;Keywords are specific and sensitive terms used to improve the proficiency of AI-generated answers, requiring careful selection for different programming languages or communities.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="HPC Numerical Kernels">
  <data key="d0">HPC Numerical Kernels</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">HPC numerical kernels are computational components targeted by AI tools like Codex and Copilot for parallel programming models.&lt;SEP&gt;HPC numerical kernels are specialized computational components targeted by AI tools for parallel programming models in HPC environments.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Taxonomy">
  <data key="d0">Taxonomy</data>
  <data key="d1">Taxonomies</data>
  <data key="d2">A taxonomy is a classification framework designed to evaluate the accuracy and trustworthiness of AI-generated results; the current taxonomy is initial and needs expansion for broader acceptance.&lt;SEP&gt;A taxonomy is a classification framework used to evaluate the accuracy and trustworthiness of AI-generated results; the current one is initial and needs expansion.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="LLM Technologies">
  <data key="d0">LLM Technologies</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Large Language Models (LLMs) like GPT-3 are advanced AI models that can generate human-like text and code, impacting HPC ecosystem integration.&lt;SEP&gt;Large Language Models like GPT-3 are advanced AI models capable of generating human-like text and code, influencing HPC ecosystem integration and development.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Human-in-the-loop">
  <data key="d0">Human-in-the-loop</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Human-in-the-loop involves incorporating human feedback into AI workflows to refine and improve code suggestions and outputs.&lt;SEP&gt;Human-in-the-loop involves integrating human feedback into AI workflows to refine and improve generated code or suggestions.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Metadata-rich Suggestions">
  <data key="d0">Metadata-rich Suggestions</data>
  <data key="d1">Tools</data>
  <data key="d2">Metadata-rich suggestions are detailed, context-aware outputs from AI tools designed to assist human decision-making.&lt;SEP&gt;Metadata-rich suggestions are detailed, context-aware outputs from AI tools designed to support human decision-making and code refinement.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="HPC Modernization Initiatives">
  <data key="d0">HPC Modernization Initiatives</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Projects like DARPA’s High Productivity Computing Systems and the US Department of Energy’s Exascale Computing Project aim to modernize HPC infrastructure and may incorporate AI tools.&lt;SEP&gt;Projects such as DARPA’s High Productivity Computing Systems and the US Department of Energy’s Exascale Computing Project aim to modernize HPC infrastructure and workflows, potentially integrating AI tools.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Ecosystem Features">
  <data key="d0">Ecosystem Features</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Features like build systems, packaging, validation &amp; verification, reproducibility, and CI/CD pipelines are crucial components of HPC software ecosystems.&lt;SEP&gt;Features such as building systems, packaging, validation, verification, reproducibility, and CI/CD pipelines are essential parts of HPC software ecosystems.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Revolutionary Capabilities">
  <data key="d0">Revolutionary Capabilities</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Refers to advanced AI-driven features that could redefine HPC domains, including automation and educational aspects.&lt;SEP&gt;Revolutionary capabilities refer to advanced AI-driven features, such as automation and intelligent management, that could redefine HPC software and workflows.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Educational Aspects">
  <data key="d0">Educational Aspects</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">AI technologies and tools could significantly impact HPC education by providing new methods for training, learning, and knowledge dissemination.&lt;SEP&gt;AI technologies could significantly impact HPC education by providing new tools and methodologies for learning and training.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="High-Quality Multistep or Multikernel Codes">
  <data key="d0">High-Quality Multistep or Multikernel Codes</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Multistep or multikernel codes, such as CG, are complex programming approaches that pose challenges for generation of high-quality code.</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Information Processing Systems">
  <data key="d0">Information Processing Systems</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A collection of research papers and proceedings related to neural information processing and machine learning, edited by H. Larochelle et al., covering various topics and advancements in the field.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="H. Larochelle">
  <data key="d0">H. Larochelle</data>
  <data key="d1">Researchers/Editors</data>
  <data key="d2">A researcher involved in compiling and editing proceedings on information processing systems.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="M. Ranzato">
  <data key="d0">M. Ranzato</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher contributing to the compilation of proceedings on neural information processing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="R. Hadsell">
  <data key="d0">R. Hadsell</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher contributing to the compilation of proceedings on neural information processing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="M.F. Balcan">
  <data key="d0">M.F. Balcan</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher contributing to the compilation of proceedings on neural information processing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="H. Lin">
  <data key="d0">H. Lin</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher contributing to the compilation of proceedings on neural information processing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Curran Associates, Inc.">
  <data key="d0">Curran Associates, Inc.</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Publisher of the proceedings volume, facilitating dissemination of research on neural information processing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Vol. 33">
  <data key="d0">Vol. 33</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Volume number of the proceedings, indicating its place within a series.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">
  <data key="d0">https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</data>
  <data key="d1">Tools</data>
  <data key="d2">URL link to the full paper, providing access to detailed research content.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="H. Carter Edwards">
  <data key="d0">H. Carter Edwards</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher involved in high-performance computing and performance portability studies.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Christian R. Trott">
  <data key="d0">Christian R. Trott</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in high-performance computing and performance portability studies.&lt;SEP&gt;Christian R. Trott is an author involved in developing programming model extensions for exascale computing using Kokkos.&lt;SEP&gt;Christian R. Trott is an author involved in extending programming models for exascale computing, particularly through Kokkos 3.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450&lt;SEP&gt;chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Daniel Sunderland">
  <data key="d0">Daniel Sunderland</data>
  <data key="d1">Researchers</data>
  <data key="d2">A researcher involved in high-performance computing and performance portability studies.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="J. Parallel and Distrib. Comput.">
  <data key="d0">J. Parallel and Distrib. Comput.</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The journal publishing research on parallel and distributed computing, including the Kokkos framework.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.1016/j.jpdc.2014.07.003">
  <data key="d0">https://doi.org/10.1016/j.jpdc.2014.07.003</data>
  <data key="d1">Tools</data>
  <data key="d2">URL to the publication detailing Kokkos and related high-performance computing frameworks.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.">
  <data key="d0">Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a study evaluating large language models trained on code, exploring their capabilities and limitations.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="arXiv preprint arXiv:2107.03374">
  <data key="d0">arXiv preprint arXiv:2107.03374</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Preprint publication providing preliminary research findings on language models.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Valentin Churavy, Dilum Aluthge, Lucas C Wilcox, James Schloss, Simon Byrne, Maciej Waruszewski, Julian Samaroo, Ali Ramadhan, Meredith, Simeon Schaub, Jake Bolewski, Anton Smirnov, Charles Kawczynski, Chris Hill, Jinguo Liu, Oliver Schulz, Oscar, Páll Haraldsson, Takafumi Arakaki, and Tim Besard">
  <data key="d0">Valentin Churavy, Dilum Aluthge, Lucas C Wilcox, James Schloss, Simon Byrne, Maciej Waruszewski, Julian Samaroo, Ali Ramadhan, Meredith, Simeon Schaub, Jake Bolewski, Anton Smirnov, Charles Kawczynski, Chris Hill, Jinguo Liu, Oliver Schulz, Oscar, Páll Haraldsson, Takafumi Arakaki, and Tim Besard</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of JuliaGPU/KernelAbstractions.jl, a software library for GPU programming and abstractions.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="JuliaGPU/KernelAbstractions.jl: v0.8.3">
  <data key="d0">JuliaGPU/KernelAbstractions.jl: v0.8.3</data>
  <data key="d1">Tools</data>
  <data key="d2">A Julia package providing abstractions for GPU kernel programming, facilitating high-performance computing on GPUs.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.5281/zenodo.6742177">
  <data key="d0">https://doi.org/10.5281/zenodo.6742177</data>
  <data key="d1">Tools</data>
  <data key="d2">URL linking to the software repository or documentation for JuliaGPU/KernelAbstractions.jl.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt, Wannes Meert">
  <data key="d0">Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt, Wannes Meert</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a systematic review on code generation using machine learning, summarizing current approaches and challenges.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Generation Using Machine Learning: A Systematic Review">
  <data key="d0">Code Generation Using Machine Learning: A Systematic Review</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A comprehensive review analyzing various machine learning methods applied to code generation tasks.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="IEEE Access 10 (2022), 82434–82455">
  <data key="d0">IEEE Access 10 (2022), 82434–82455</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The journal publication where the systematic review is published, providing detailed insights.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Paul Denny, Viraj Kumar, Nasser Giacaman">
  <data key="d0">Paul Denny, Viraj Kumar, Nasser Giacaman</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors exploring prompt engineering for solving computer science problems using natural language interfaces like Copilot.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language">
  <data key="d0">Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research examining how prompt engineering affects the effectiveness of AI-powered code assistants in introductory programming.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.1145/3545945.3569823">
  <data key="d0">https://doi.org/10.1145/3545945.3569823</data>
  <data key="d1">Tools</data>
  <data key="d2">URL to the paper discussing prompt engineering techniques for AI code assistants.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jack Dongarra, Robert Graybill, William Harrod, Robert Lucas, Ewing Lusk, Piotr Luszczek, Janice Mcmahon, Allan Snavely, Jeffrey Vetter, Katherine Yelick, Sadaf Alam, Roy Campbell, Laura Carrington, Tzu-Yi Chen, Omid Khalili, Jeremy Meredith, Mustafa Tikir">
  <data key="d0">Jack Dongarra, Robert Graybill, William Harrod, Robert Lucas, Ewing Lusk, Piotr Luszczek, Janice Mcmahon, Allan Snavely, Jeffrey Vetter, Katherine Yelick, Sadaf Alam, Roy Campbell, Laura Carrington, Tzu-Yi Chen, Omid Khalili, Jeremy Meredith, Mustafa Tikir</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors involved in DARPA’s HPCS Program and related high-performance computing models, tools, and languages.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="DARPA’s HPCS Program: History, Models, Tools, Languages">
  <data key="d0">DARPA’s HPCS Program: History, Models, Tools, Languages</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A comprehensive overview of high-performance computing systems, models, and software tools developed under DARPA's initiative.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.1016/S0065-2458(08)00001-6">
  <data key="d0">https://doi.org/10.1016/S0065-2458(08)00001-6</data>
  <data key="d1">Tools</data>
  <data key="d2">URL to the publication detailing the history and components of DARPA’s HPCS program.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jack Dongarra et al. 2011">
  <data key="d0">Jack Dongarra et al. 2011</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of the exascale software project roadmap, outlining future directions for high-performance computing.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="The International Exascale Software Project roadmap">
  <data key="d0">The International Exascale Software Project roadmap</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A strategic plan for developing exascale computing software and infrastructure.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.1177/1094342010391989">
  <data key="d0">https://doi.org/10.1177/1094342010391989</data>
  <data key="d1">Tools</data>
  <data key="d2">URL to the exascale software roadmap publication.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Li Fei-Fei, R. Fergus, and P. Perona">
  <data key="d0">Li Fei-Fei, R. Fergus, and P. Perona</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a study on one-shot learning for object categories, proposing methods for learning from minimal data.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="One-shot learning of object categories">
  <data key="d0">One-shot learning of object categories</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research exploring techniques for training models to recognize objects from a single example.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="IEEE Transactions on Pattern Analysis and Machine Intelligence 28, 4 (2006), 594–611">
  <data key="d0">IEEE Transactions on Pattern Analysis and Machine Intelligence 28, 4 (2006), 594–611</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The journal publication presenting the research on one-shot learning methods.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Michael Fink">
  <data key="d0">Michael Fink</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focusing on object classification from limited examples using relevance metrics.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Object Classification from a Single Example Utilizing Class Relevance Metrics">
  <data key="d0">Object Classification from a Single Example Utilizing Class Relevance Metrics</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research detailing methods for classifying objects based on minimal training data and relevance metrics.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Advances in Neural Information Processing Systems, Vol. 17">
  <data key="d0">Advances in Neural Information Processing Systems, Vol. 17</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Conference proceedings where Fink's work was published.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather">
  <data key="d0">James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors investigating the implications of OpenAI Codex on introductory programming education.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming">
  <data key="d0">The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research analyzing how AI code generation tools impact learning and teaching in computer science.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="ACE ’22">
  <data key="d0">ACE ’22</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The ACE ’22 is a conference organized by the Association for Computing Machinery, held in New York, NY, USA, from October 10 to 19, 2022, focusing on computing research and developments.&lt;SEP&gt;The conference where the study was presented, focusing on computing education.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252&lt;SEP&gt;chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="https://doi.org/10.1145/3511861.3511863">
  <data key="d0">https://doi.org/10.1145/3511861.3511863</data>
  <data key="d1">Tools</data>
  <data key="d2">URL to the conference paper discussing AI code assistants in education.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Luciano Floridi and Massimo Chiriatti">
  <data key="d0">Luciano Floridi and Massimo Chiriatti</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors analyzing GPT-3’s nature, scope, limits, and consequences.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GPT-3: Its nature, scope, limits, and consequences">
  <data key="d0">GPT-3: Its nature, scope, limits, and consequences</data>
  <data key="d1">Study Questions/Hypotheses</data>
  <data key="d2">Research exploring the capabilities, limitations, and societal implications of GPT-3 language model.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Minds and Machines 30 (2020), 681–694">
  <data key="d0">Minds and Machines 30 (2020), 681–694</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The journal publication presenting Floridi and Chiriatti's analysis of GPT-3.</data>
  <data key="d3">chunk-7d708fb91b624e6e44a1f601647c9fac</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Association for Computing Machinery">
  <data key="d0">Association for Computing Machinery</data>
  <data key="d1">Discipline</data>
  <data key="d2">A professional organization dedicated to advancing computing as a science and profession, organizing conferences like ACE ’22.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Luciano Floridi">
  <data key="d0">Luciano Floridi</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Floridi is a philosopher and researcher who authored a paper discussing GPT-3, its nature, scope, limits, and societal consequences, contributing to AI ethics and philosophy.&lt;SEP&gt;Floridi is a philosopher and researcher who authored a paper discussing the nature, scope, limits, and consequences of GPT-3, contributing to understanding AI language models.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Massimo Chiriatti">
  <data key="d0">Massimo Chiriatti</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Chiriatti co-authored a paper with Floridi on GPT-3, exploring its attributes and societal impact.&lt;SEP&gt;Chiriatti co-authored the paper with Floridi on GPT-3, exploring its attributes, implications, and societal impact.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Julia, Python/Numba, Kokkos">
  <data key="d0">Julia, Python/Numba, Kokkos</data>
  <data key="d1">Tools</data>
  <data key="d2">These are high-level programming models used to develop scalable and portable software for exascale computing environments.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Thomas Helmuth">
  <data key="d0">Thomas Helmuth</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Helmuth and Kelly developed PSB2, a benchmark suite for program synthesis, used to evaluate and compare program synthesis techniques.&lt;SEP&gt;Helmuth and Kelly developed PSB2, a benchmark suite for program synthesis, used to evaluate and compare synthesis algorithms.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PSB2">
  <data key="d0">PSB2</data>
  <data key="d1">Tools</data>
  <data key="d2">The Second Program Synthesis Benchmark Suite is a set of standardized tests to assess program synthesis algorithms.&lt;SEP&gt;The Second Program Synthesis Benchmark Suite provides standardized tests for assessing program synthesis techniques.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Julia Hirschberg">
  <data key="d0">Julia Hirschberg</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Hirschberg and Manning contributed to advances in natural language processing, improving algorithms and understanding in NLP.&lt;SEP&gt;Hirschberg and Manning contributed to advances in natural language processing, improving understanding and techniques in NLP.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Christopher D. Manning">
  <data key="d0">Christopher D. Manning</data>
  <data key="d1">Researcher</data>
  <data key="d2">A key author involved in research on answering open-domain questions, published in 2019.&lt;SEP&gt;A key contributor to research on iterative query generation and question answering, published in 2019 and 2020.&lt;SEP&gt;Manning collaborated with Hirschberg on NLP advancements, focusing on language understanding and computational linguistics.&lt;SEP&gt;Manning collaborated with Hirschberg on NLP research, focusing on language understanding and computational linguistics.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b&lt;SEP&gt;chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Saki Imai">
  <data key="d0">Saki Imai</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Imai investigated whether GitHub Copilot can serve as a substitute for human pair programming, evaluating its effectiveness and limitations.&lt;SEP&gt;Imai investigated whether GitHub Copilot can substitute for human pair programming, evaluating its effectiveness.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Zheming Jin">
  <data key="d0">Zheming Jin</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Jin authored a technical report on the Rodinia benchmarks implemented in SYCL, a programming model for heterogeneous computing performance evaluation.&lt;SEP&gt;Jin authored a technical report on the Rodinia benchmarks implemented in SYCL, a programming model for heterogeneous computing.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Hecbench">
  <data key="d0">Hecbench</data>
  <data key="d1">Tools</data>
  <data key="d2">Hecbench is a benchmarking suite for evaluating high-performance computing hardware and software performance.&lt;SEP&gt;Hecbench is a benchmarking suite for evaluating high-performance computing hardware and software, available on GitHub.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Srinath Kailasa, Tingyu Wang, Lorena A. Barba, Timo Betcke">
  <data key="d0">Srinath Kailasa, Tingyu Wang, Lorena A. Barba, Timo Betcke</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">This group developed PyExaFMM, a high-performance software exercise using Python and Numba for scalable scientific computations.&lt;SEP&gt;This group developed PyExaFMM, a high-performance software framework using Python and Numba for scientific computations involving fast multipole methods.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PyExaFMM">
  <data key="d0">PyExaFMM</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python-based software framework designed for high-performance multipole methods in scientific computing.&lt;SEP&gt;PyExaFMM is a Python-based high-performance library for scientific computing, focusing on scalable multipole methods.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Andreas Klöckner">
  <data key="d0">Andreas Klöckner</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Klöckner developed pycuda and pyopencl, libraries that enable scripting and runtime code generation for GPU programming, facilitating high-performance applications.&lt;SEP&gt;Klöckner developed pycuda and pyopencl, scripting libraries for GPU programming, enabling runtime code generation for NVIDIA GPUs.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="pycuda">
  <data key="d0">pycuda</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python library that provides access to NVIDIA's CUDA parallel computing platform for GPU acceleration.&lt;SEP&gt;pycuda is a Python library that provides access to NVIDIA CUDA for GPU acceleration, enabling runtime code generation.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="pyopencl">
  <data key="d0">pyopencl</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python library that provides access to OpenCL, enabling cross-platform GPU and parallel computing.&lt;SEP&gt;pyopencl is a Python library that provides access to OpenCL, allowing GPU and parallel computing across platforms.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Tobias Knopp">
  <data key="d0">Tobias Knopp</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Knopp contributed to experimental multi-threading support in Julia, enhancing its capabilities for high-performance and parallel computing.&lt;SEP&gt;Knopp contributed to experimental multi-threading support in Julia, enhancing its capabilities for high-performance computing.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Douglas Kothe, Stephen Lee, Irene Qualters">
  <data key="d0">Douglas Kothe, Stephen Lee, Irene Qualters</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">They discussed exascale computing in the US, analyzing challenges and strategies for large-scale scientific computation.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Exascale Computing">
  <data key="d0">Exascale Computing</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Exascale computing refers to supercomputers capable of at least one exaFLOP performance, critical for advanced scientific simulations.&lt;SEP&gt;Refers to the next generation of supercomputers capable of at least one exaFLOP performance, critical for large-scale scientific simulations.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Siu Kwan Lam, Antoine Pitrou, Stanley Seibert">
  <data key="d0">Siu Kwan Lam, Antoine Pitrou, Stanley Seibert</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">They developed Numba, a JIT compiler for Python based on LLVM, to accelerate scientific and numerical computations.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Dongdong Lu, Jie Wu, Yongxiang Sheng, Peng Liu, Mengmeng Yang">
  <data key="d0">Dongdong Lu, Jie Wu, Yongxiang Sheng, Peng Liu, Mengmeng Yang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">This group analyzed the popularity of programming languages in open source communities, providing insights into language adoption and trends.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Nhan Nguyen, Sarah Nadi">
  <data key="d0">Nhan Nguyen, Sarah Nadi</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">They empirically evaluated GitHub Copilot’s code suggestions to determine its effectiveness and impact on software development.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Event">
  <data key="d0">Event</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The event refers to the specific occasion or gathering, in this case, the ACE ’22 conference held in New York, focusing on computing research.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Australia">
  <data key="d0">Australia</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Australia is mentioned as the location associated with the event, providing geographical context.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Python/Numba">
  <data key="d0">Python/Numba</data>
  <data key="d1">Tools</data>
  <data key="d2">Python combined with Numba, a JIT compiler, is used for high-performance scientific computing, evaluated for efficiency on exascale systems.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="SYCL">
  <data key="d0">SYCL</data>
  <data key="d1">Tools</data>
  <data key="d2">SYCL is a programming model for heterogeneous computing, enabling portable code across diverse hardware.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Douglas Kothe">
  <data key="d0">Douglas Kothe</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Kothe, Lee, and Qualters discussed exascale computing in the US, addressing challenges and strategies for achieving large-scale scientific computing performance.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Stephen Lee">
  <data key="d0">Stephen Lee</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Lee contributed to the analysis of exascale computing challenges and strategies in the US context.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Irene Qualters">
  <data key="d0">Irene Qualters</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Qualters contributed to the discussion on exascale computing challenges and the future of large-scale scientific simulations.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Siu Kwan Lam">
  <data key="d0">Siu Kwan Lam</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Lam, Pitrou, and Seibert developed Numba, a JIT compiler for Python that accelerates numerical code, enabling high-performance scientific computing.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Antoine Pitrou">
  <data key="d0">Antoine Pitrou</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Pitrou contributed to the development of Numba, enhancing Python's performance for scientific applications.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Stanley Seibert">
  <data key="d0">Stanley Seibert</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Seibert contributed to the Numba project, focusing on performance optimization for Python in scientific computing.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Dongdong Lu">
  <data key="d0">Dongdong Lu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Lu and colleagues analyzed the popularity of programming languages in open source communities, providing insights into language adoption trends.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jie Wu">
  <data key="d0">Jie Wu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Wu contributed to the analysis of programming language popularity in open source software.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Yongxiang Sheng">
  <data key="d0">Yongxiang Sheng</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Sheng contributed to the study on programming languages in open source communities.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Peng Liu">
  <data key="d0">Peng Liu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Liu was involved in analyzing programming language trends in open source projects.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Mengmeng Yang">
  <data key="d0">Mengmeng Yang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Yang contributed to the research on programming language popularity in open source software communities.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Nhan Nguyen">
  <data key="d0">Nhan Nguyen</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Nguyen empirically evaluated GitHub Copilot's code suggestions to assess its effectiveness and impact on developers.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Sarah Nadi">
  <data key="d0">Sarah Nadi</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Nadi collaborated with Nguyen on evaluating GitHub Copilot's performance and implications.</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Cupy">
  <data key="d0">Cupy</data>
  <data key="d1">Tools</data>
  <data key="d2">Cupy is a library compatible with NumPy designed for GPU calculations, facilitating high-performance numerical computing on Nvidia GPUs.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="NVIDIA">
  <data key="d0">NVIDIA</data>
  <data key="d1">Discipline</data>
  <data key="d2">NVIDIA is a leading technology company providing GPU hardware and software tools, including the CUDA Toolkit and Thrust library, for high-performance computing.&lt;SEP&gt;NVIDIA is a leading technology company specializing in GPU hardware and software development, providing tools such as CUDA and Thrust for high-performance computing.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="CUDA Toolkit">
  <data key="d0">CUDA Toolkit</data>
  <data key="d1">Tools</data>
  <data key="d2">The CUDA Toolkit is a software development kit from NVIDIA that enables programming and optimization of GPU computations, currently at version 11.7.0.&lt;SEP&gt;The CUDA Toolkit is a software development kit provided by NVIDIA for programming and optimizing GPU computations, currently at version 11.7.0.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Hammond Pearce et al. 2022 Study">
  <data key="d0">Hammond Pearce et al. 2022 Study</data>
  <data key="d1">Study Designs</data>
  <data key="d2">This study assesses the security of GitHub Copilot’s code contributions, analyzing potential vulnerabilities and security implications.&lt;SEP&gt;This study assesses the security of GitHub Copilot’s code contributions, analyzing potential vulnerabilities and security risks associated with AI-generated code.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Preferred Networks and Inc. Preferred Infrastructure">
  <data key="d0">Preferred Networks and Inc. Preferred Infrastructure</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">These organizations develop and support CuPy, a GPU-accelerated numerical library that enhances scientific computing workflows on Nvidia GPUs.&lt;SEP&gt;These organizations provide CuPy, a GPU-accelerated library for numerical computations, supporting high-performance scientific computing.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="JuliaGPU/AMDGPU.jl">
  <data key="d0">JuliaGPU/AMDGPU.jl</data>
  <data key="d1">Tools</data>
  <data key="d2">JuliaGPU/AMDGPU.jl is a Julia package for GPU programming, enabling efficient computation on AMD and other GPUs, version 0.4.1.&lt;SEP&gt;JuliaGPU/AMDGPU.jl is a Julia package that enables GPU programming on AMD and other GPUs, supporting high-performance parallel computations.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Automated Generation of Programming Exercises">
  <data key="d0">Automated Generation of Programming Exercises</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This research explores the feasibility and effectiveness of using LLMs to automatically generate programming exercises, explanations, and educational content.&lt;SEP&gt;This research investigates how large language models can automatically generate programming exercises and explanations to aid learning and coding education.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Github Copilot">
  <data key="d0">Github Copilot</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">As a code synthesis tool powered by LLMs, GitHub Copilot's performance is compared with genetic programming to evaluate its effectiveness in code generation.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Bjarne Stroustrup 2013 The C++ Programming Language">
  <data key="d0">Bjarne Stroustrup 2013 The C++ Programming Language</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">This seminal book explains the C++ programming language, covering syntax, semantics, and best practices for software development.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Python Programming Language">
  <data key="d0">Python Programming Language</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Python is a high-level, interpreted programming language known for its simplicity and versatility, widely used in scientific computing, web development, and automation.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="NumPy">
  <data key="d0">NumPy</data>
  <data key="d1">Tools</data>
  <data key="d2">NumPy is a core Python library for numerical computation, providing efficient array structures and mathematical operations for scientific and engineering applications.&lt;SEP&gt;NumPy is a fundamental Python library for numerical computation, providing efficient array structures and mathematical functions for scientific computing.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="SciPy">
  <data key="d0">SciPy</data>
  <data key="d1">Tools</data>
  <data key="d2">SciPy builds on NumPy to offer additional scientific computing functionalities, including optimization, integration, and signal processing.&lt;SEP&gt;SciPy extends NumPy's capabilities, offering additional modules for optimization, integration, and scientific computing tasks.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Extreme Heterogeneity 2018">
  <data key="d0">Extreme Heterogeneity 2018</data>
  <data key="d1">Discipline</data>
  <data key="d2">A report discussing the challenges and opportunities in computational science within highly heterogeneous computing environments, emphasizing the importance of productive approaches.&lt;SEP&gt;This report discusses computational science in the context of heterogeneous computing environments, emphasizing productivity and performance in diverse hardware architectures.&lt;SEP&gt;This report discusses computational science in the context of heterogeneous computing environments, emphasizing productivity, performance, and scientific workflows across diverse hardware architectures.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35&lt;SEP&gt;chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Yaqing Wang, Quanming Yao, James">
  <data key="d0">Yaqing Wang, Quanming Yao, James</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">This work explores the application of machine learning and advanced algorithms in high-performance computing contexts, aiming to improve computational efficiency.&lt;SEP&gt;This work investigates the application of machine learning and advanced algorithms to improve computational efficiency and scientific productivity in high-performance computing.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Synthesis Performance">
  <data key="d0">Code Synthesis Performance</data>
  <data key="d1">Results</data>
  <data key="d2">The performance of AI tools like GitHub Copilot and genetic programming in synthesizing code is evaluated, comparing their efficiency and accuracy.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Software Security">
  <data key="d0">Software Security</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Software security involves protecting code and systems from vulnerabilities, with recent studies assessing the security implications of AI-generated code such as GitHub Copilot.</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="T. Peterka, M. Strout, J. Wilke">
  <data key="d0">T. Peterka, M. Strout, J. Wilke</data>
  <data key="d1">authors</data>
  <data key="d2">Researchers who authored the report on Extreme Heterogeneity, focusing on productive computational science in heterogeneous systems.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="DOE ASCR Workshop on Extreme Heterogeneity">
  <data key="d0">DOE ASCR Workshop on Extreme Heterogeneity</data>
  <data key="d1">Study Design</data>
  <data key="d2">A workshop organized by the Department of Energy's Office of Science to explore issues related to extreme heterogeneity in computational science.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Technical Report">
  <data key="d0">Technical Report</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A formal document presenting research findings and discussions on computational science in heterogeneous systems.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Yaqing Wang, Quanming Yao, James T. Kwok, Lionel M. Ni">
  <data key="d0">Yaqing Wang, Quanming Yao, James T. Kwok, Lionel M. Ni</data>
  <data key="d1">authors</data>
  <data key="d2">Researchers who conducted a survey on Few-Shot Learning, exploring how models generalize from limited examples.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Generalizing from a Few Examples">
  <data key="d0">Generalizing from a Few Examples</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">A survey investigating methods and effectiveness of machine learning models that learn effectively from a small number of training examples.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Few-Shot Learning">
  <data key="d0">Few-Shot Learning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A machine learning paradigm where models are trained to generalize well from only a few examples, addressing data scarcity issues.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="ACM Comput. Surv.">
  <data key="d0">ACM Comput. Surv.</data>
  <data key="d1">Evidence Type</data>
  <data key="d2">A scholarly journal publishing comprehensive surveys and reviews in computer science, including the survey on Few-Shot Learning.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Michel Wermelinger">
  <data key="d0">Michel Wermelinger</data>
  <data key="d1">author</data>
  <data key="d2">Researcher exploring the use of GitHub Copilot to assist in solving programming problems.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Using GitHub Copilot to Solve Simple Programming Problems">
  <data key="d0">Using GitHub Copilot to Solve Simple Programming Problems</data>
  <data key="d1">Methodology</data>
  <data key="d2">A study demonstrating how GitHub Copilot can be employed to address straightforward programming tasks, assessing its utility.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Proceedings of the 54th ACM Technical Symposium on Computer Science Education">
  <data key="d0">Proceedings of the 54th ACM Technical Symposium on Computer Science Education</data>
  <data key="d1">Study Design</data>
  <data key="d2">Conference proceedings documenting research on computer science education and tools like GitHub Copilot.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Burak Yetistiren, Isik Ozsoy, Eray Tuzun">
  <data key="d0">Burak Yetistiren, Isik Ozsoy, Eray Tuzun</data>
  <data key="d1">authors</data>
  <data key="d2">Researchers assessing the quality of code generated by GitHub Copilot.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Assessing the Quality of GitHub Copilot’s Code Generation">
  <data key="d0">Assessing the Quality of GitHub Copilot’s Code Generation</data>
  <data key="d1">Study Design</data>
  <data key="d2">An evaluation study measuring the effectiveness and accuracy of code produced by GitHub Copilot in various programming scenarios.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="PROMISE 2022">
  <data key="d0">PROMISE 2022</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An international conference focusing on predictive models and data analytics in software engineering, including studies on AI code generation.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Reproducibility and Replicability">
  <data key="d0">Reproducibility and Replicability</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges in reproducing experimental results in AI and computational science due to evolving technologies and experimental setups.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Reproducibility and Repl icability">
  <data key="d0">Reproducibility and Repl icability</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges in reproducing experimental results in AI and computational science due to evolving technologies and experimental setups.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Evaluation of OpenAI Codex">
  <data key="d0">Evaluation of OpenAI Codex</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d2">William F. Godoy evaluates the effectiveness and limitations of OpenAI Codex in generating HPC kernels across multiple programming models.&lt;SEP&gt;William F. Godoy leads the evaluation, analyzing the quality, limitations, and applicability of AI-generated HPC kernels across different programming environments."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Language Support">
  <data key="d0">Language Support</data>
  <data key="d3">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d2">The effectiveness of AI-generated kernels varies depending on the programming language and model maturity; C++ and CUDA perform well, HIP less so; Julia models perform acceptably."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Related efforts">
  <data key="d0">Related efforts</data>
  <data key="d3">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d2">Related research highlights the growing focus on LLMs' impacts across NLP, code synthesis, and automation."|&gt;"Research Context&lt;SEP&gt;Related research highlights the increasing focus on LLMs' impact on NLP, code synthesis, and automation in computer science."|&gt;"Research Context</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;experiment&gt;">
  <data key="d0">&lt;experiment&gt;</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d2">The GEMV kernel is used as a benchmark in the experiments to evaluate code correctness and proficiency levels."|&gt;"evaluation, benchmarking</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;code generation model&gt;">
  <data key="d0">&lt;code generation model&gt;</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d2">The prompt pattern specifies how input instructions are formatted to guide code generation."|&gt;"prompt design, input format</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;study&gt;">
  <data key="d0">&lt;study&gt;</data>
  <data key="d3">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d2">The results summarize the accuracy levels achieved across different kernels and models."|&gt;"findings, evaluation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;Tools&gt;">
  <data key="d0">&lt;Tools&gt;</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d2">Julia is used for scientific computing and offers features like LLVM-based compilation, supporting high-performance kernel development.&lt;SEP&gt;Julia supports GPU programming through packages like CUDA.jl and AMDGPU.jl, enabling cross-vendor GPU kernels.&lt;SEP&gt;Julia's syntax and features are influenced by Fortran and designed for mathematical and performance-focused applications.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;Results&gt;">
  <data key="d0">&lt;Results&gt;</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d2">OpenAI Codex's code generation quality decreases as kernel complexity increases, with better results on simpler kernels and languages with more public code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;Objects of Study&gt;">
  <data key="d0">&lt;Objects of Study&gt;</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d2">Public repositories provide the source code that influences the AI's ability to generate accurate kernels, especially in popular languages.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="&lt;Variables&gt;">
  <data key="d0">&lt;Variables&gt;</data>
  <data key="d3">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d2">Kernel complexity directly impacts the difficulty of generating correct code with AI models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="HPC Ecosystem">
  <data key="d0">HPC Ecosystem</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d2">Emerging LLMs such as GPT-3 influence the development and integration of AI tools within HPC ecosystems.&lt;SEP&gt;Emerging LLMs such as GPT-3 influence the integration and development of HPC software ecosystems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="AI Tools">
  <data key="d0">AI Tools</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d2">Modernization projects are exploring the incorporation of AI tools to enhance HPC infrastructure and workflows.&lt;SEP&gt;Modernization projects are exploring the integration of AI tools to enhance workflows and infrastructure in HPC.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="AI Technologies">
  <data key="d0">AI Technologies</data>
  <data key="d3">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d2">AI-driven tools could redefine HPC education by providing advanced training and learning methodologies.&lt;SEP&gt;AI-driven tools could transform HPC education by providing innovative training and learning solutions.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Minds and Machines">
  <data key="d0">Minds and Machines</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Floridi and Chiriatti's paper explores GPT-3's nature, scope, limits, and societal consequences, linking the model to philosophical and technological discussions."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Advances">
  <data key="d0">Advances</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Hirschberg and Manning's work contributed to progress in NLP techniques and understanding."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Substitute for Human Programming">
  <data key="d0">Substitute for Human Programming</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Imai's empirical study assesses whether Copilot can replace or assist human pair programming effectively."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="SYCL Benchmarks">
  <data key="d0">SYCL Benchmarks</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Jin's technical report discusses the implementation of Rodinia benchmarks in SYCL for heterogeneous computing performance evaluation."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Scientific Computing">
  <data key="d0">Scientific Computing</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Kailasa et al. developed PyExaFMM for high-performance scientific simulations using Python and Numba."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="GPU Programming">
  <data key="d0">GPU Programming</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Klöckner's pycuda library enables scripting and runtime code generation for NVIDIA GPUs."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Large-Scale Computing">
  <data key="d0">Large-Scale Computing</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Kothe, Lee, and Qualters discuss the challenges and strategies for achieving exascale performance in US scientific computing."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Community Trends">
  <data key="d0">Community Trends</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Lu, Wu, Sheng, Liu, and Yang analyze language popularity, informing trends in open source software."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Code Suggestions">
  <data key="d0">Code Suggestions</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Nguyen and Nadi empirically evaluate Copilot's code suggestions for effectiveness and developer impact."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Floridi">
  <data key="d0">Floridi</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Floridi and Chiriatti's paper discusses GPT-3's nature, scope, limits, and societal consequences, linking philosophical analysis to AI technology."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Godoy, Valero-Lara, Dettling, Trefftz, Jorquera, Sheehy, Miller, Gonzalez-Tallada, Vetter, Churavy">
  <data key="d0">Godoy, Valero-Lara, Dettling, Trefftz, Jorquera, Sheehy, Miller, Gonzalez-Tallada, Vetter, Churavy</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They evaluate the performance and portability of Julia, Python/Numba, and Kokkos on exascale nodes, assessing efficiency and scalability."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Performance and Portability">
  <data key="d0">Performance and Portability</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">These tools are evaluated for their scalability, efficiency, and portability in high-performance computing environments."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Helmuth, Kelly">
  <data key="d0">Helmuth, Kelly</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They developed PSB2 to benchmark program synthesis algorithms, facilitating standardized evaluation."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Program Synthesis Benchmark">
  <data key="d0">Program Synthesis Benchmark</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They developed PSB2 to benchmark program synthesis algorithms, facilitating standardized evaluation."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Hirschberg, Manning">
  <data key="d0">Hirschberg, Manning</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They contributed to advances in NLP, improving understanding, algorithms, and applications in language processing."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Jin">
  <data key="d0">Jin</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Jin's technical report discusses implementing Rodinia benchmarks in SYCL to evaluate heterogeneous computing performance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Kailasa, Wang, Barba, Betcke">
  <data key="d0">Kailasa, Wang, Barba, Betcke</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They developed PyExaFMM, a high-performance Python and Numba-based software for scientific multipole computations."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Klöckner">
  <data key="d0">Klöckner</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Klöckner's pycuda and pyopencl libraries enable scripting, runtime code generation, and GPU acceleration in Python."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Knopp">
  <data key="d0">Knopp</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Knopp contributed to multi-threading support in Julia, enhancing its capabilities for high-performance parallel computing."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Kothe, Lee, Qualters">
  <data key="d0">Kothe, Lee, Qualters</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They discuss challenges, strategies, and future directions for exascale supercomputing in the United States."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Lu, Wu, Sheng, Liu, Yang">
  <data key="d0">Lu, Wu, Sheng, Liu, Yang</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">They analyze the popularity and trends of programming languages in open source communities."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Nguyen, Nadi">
  <data key="d0">Nguyen, Nadi</data>
  <data key="d3">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d2">Nguyen and Nadi empirically evaluate Copilot's code suggestions, assessing effectiveness and developer impact."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Discipline">
  <data key="d0">Discipline</data>
  <data key="d3">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d2">OpenACC offers a standardized API for GPU acceleration, complementing CUDA-based tools in high-performance computing environments.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Yaqing Wang et al.">
  <data key="d0">Yaqing Wang et al.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d2">The survey provides an overview of methods enabling models to generalize from limited data, contributing to the broader understanding of machine learning generalization.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Burak Yetistiren et al.">
  <data key="d0">Burak Yetistiren et al.</data>
  <data key="d3">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d2">The evaluation assesses how well Copilot's generated code meets quality standards in software engineering.&lt;SEP&gt;The research investigates the effectiveness and limitations of AI-generated code in real-world scenarios.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</node>
<node id="Knowledge-Intensive NLP Tasks">
  <data key="d0">Knowledge-Intensive NLP Tasks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Tasks requiring models to access, manipulate, and produce factual and detailed knowledge, such as question answering and fact verification.&lt;SEP&gt;Tasks that require models to access, manipulate, and produce factual and detailed knowledge, such as question answering and fact verification.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Pre-trained Language Models">
  <data key="d0">Pre-trained Language Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Models trained on large datasets to learn language representations and store factual knowledge within their parameters.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Non-parametric Memory">
  <data key="d0">Non-parametric Memory</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External memory components, such as dense vector indices of Wikipedia, that models access to retrieve relevant information during generation.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Seq2Seq Model">
  <data key="d0">Seq2Seq Model</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sequence-to-sequence models used as the parametric memory component in RAG, capable of generating language based on learned parameters.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wikipedia">
  <data key="d0">Wikipedia</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large, collaboratively edited online encyclopedia used as the external knowledge base for retrieval in RAG.&lt;SEP&gt;Wikipedia acts as the primary source of evidence for classifying claims in FEVER.&lt;SEP&gt;Wikipedia is a comprehensive online encyclopedia used as a source of factual information.&lt;SEP&gt;Wikipedia serves as the primary evidence source for classifying claims in the FEVER framework.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-4bb8bb2fc231e52a13ed0e91512e55eb&lt;SEP&gt;chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="State-of-the-art Results">
  <data key="d0">State-of-the-art Results</data>
  <data key="d1">Results</data>
  <data key="d2">The RAG models outperform existing models on three open domain question answering tasks, setting new performance benchmarks.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open Domain Question Answering (QA)">
  <data key="d0">Open Domain Question Answering (QA)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A task where models answer questions based on a broad, unstructured knowledge base, such as Wikipedia.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Diverse and Factual Language">
  <data key="d0">Diverse and Factual Language</data>
  <data key="d1">Results</data>
  <data key="d2">RAG models generate language that is more specific, diverse, and factually accurate compared to parametric-only models.</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Sequence-to-Sequence (seq2seq) Models">
  <data key="d0">Sequence-to-Sequence (seq2seq) Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Neural network models designed to convert input sequences into output sequences, widely used in translation, summarization, and question answering tasks.&lt;SEP&gt;Seq2seq models are a type of neural network architecture used in NLP tasks for generating output sequences based on input sequences, often employed in translation, summarization, and question answering.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval-Augmented Generation (RAG)">
  <data key="d0">Retrieval-Augmented Generation (RAG)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework combining retrieval of relevant documents with generative models to produce more factual and knowledge-rich outputs, especially for knowledge-intensive tasks.&lt;SEP&gt;RAG is a hybrid approach that combines parametric memory from pre-trained seq2seq models with non-parametric memory via a neural retriever, enabling knowledge-intensive generation by retrieving relevant documents and generating responses conditioned on them.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Pre-trained Transformer Models (e.g., BART, T5)">
  <data key="d0">Pre-trained Transformer Models (e.g., BART, T5)</data>
  <data key="d1">Tools</data>
  <data key="d2">Transformers like BART and T5 serve as the backbone for the generator component in RAG, providing powerful sequence generation capabilities and pre-trained knowledge.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Dense Passage Retriever (DPR)">
  <data key="d0">Dense Passage Retriever (DPR)</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural retriever that fetches relevant text passages from large corpora like Wikipedia, conditioned on input queries, enabling external knowledge access in RAG.&lt;SEP&gt;DPR is a neural retriever that fetches relevant documents from a large corpus like Wikipedia, conditioned on input queries, to serve as non-parametric memory in RAG models.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="State-of-the-Art Results">
  <data key="d0">State-of-the-Art Results</data>
  <data key="d1">Results</data>
  <data key="d2">Performance benchmarks indicating that RAG models outperform previous approaches on datasets like Natural Questions, WebQuestions, CuratedTrec, and TriviaQA, demonstrating the effectiveness of combining retrieval with generation.&lt;SEP&gt;RAG models achieve superior performance compared to previous models on datasets like Natural Questions, WebQuestions, CuratedTrec, and TriviaQA, demonstrating the effectiveness of combining retrieval with generation.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Unconstrained Generation">
  <data key="d0">Unconstrained Generation</data>
  <data key="d1">Results</data>
  <data key="d2">Generation without strict extraction constraints, producing responses that are more factual, specific, and diverse compared to extractive methods.&lt;SEP&gt;Unconstrained generation refers to models generating responses without strict extractive constraints, leading to more factual, specific, and diverse answers than traditional extractive methods.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="FEVER Fact Verification">
  <data key="d0">FEVER Fact Verification</data>
  <data key="d1">Results</data>
  <data key="d2">A task evaluating the model's ability to verify factual claims, where RAG achieves results within 4.3% of the top pipeline models, indicating high effectiveness.&lt;SEP&gt;The RAG approach achieves results within 4.3% of the best pipeline models, indicating strong performance in verifying factual claims.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Models' Knowledge Updating">
  <data key="d0">Models' Knowledge Updating</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The external non-parametric memory can be replaced or updated, allowing models to incorporate new information as the world changes, leading to more current responses.&lt;SEP&gt;The non-parametric memory component allows for updating the model's knowledge as the world changes by replacing or augmenting the external memory, enabling more current and accurate responses.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open-Domain Extractive Question Answering">
  <data key="d0">Open-Domain Extractive Question Answering</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A task in natural language processing where systems extract relevant information directly from a large corpus to answer questions, typically focusing on retrieving exact text spans.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Hybrid Parametric and Non-Parametric Memory">
  <data key="d0">Hybrid Parametric and Non-Parametric Memory</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A combined memory approach that integrates pre-trained (parametric) knowledge with external (non-parametric) retrieval-based memory to enhance NLP models' knowledge access and reasoning capabilities.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Pre-trained Models (e.g., BART, T5)">
  <data key="d0">Pre-trained Models (e.g., BART, T5)</data>
  <data key="d1">Tools</data>
  <data key="d2">Transformer-based models trained on large corpora to generate fluent language outputs, serving as the backbone for sequence generation in RAG.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Probabilistic Model">
  <data key="d0">Probabilistic Model</data>
  <data key="d1">Theoretical Framework</data>
  <data key="d2">A modeling approach where the retrieval and generation components are trained jointly in a probabilistic manner, marginalizing over latent variables like retrieved documents.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Top-K Approximation">
  <data key="d0">Top-K Approximation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique used to marginalize over the top K retrieved documents in RAG models, either on a per-output or per-token basis, to approximate the distribution over responses.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG-Sequence Model">
  <data key="d0">RAG-Sequence Model</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A variant of RAG where the same retrieved document is used to generate the entire output sequence, marginalizing over multiple documents.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG-Token Model">
  <data key="d0">RAG-Token Model</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A retrieval-augmented generative model that retrieves relevant documents for each target token and marginalizes over them to produce answers, allowing flexible content selection from multiple documents.&lt;SEP&gt;A retrieval-augmented generative model that retrieves relevant documents for each token and marginalizes over them, enabling content selection from multiple documents during answer generation.&lt;SEP&gt;A variant of RAG where different retrieved documents can be used for each token in the output, allowing more flexible and context-specific generation.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b&lt;SEP&gt;chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Training and Decoding Procedures">
  <data key="d0">Training and Decoding Procedures</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Processes for jointly training the retriever and generator components and generating outputs during inference in RAG models.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge Sources">
  <data key="d0">Knowledge Sources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External corpora like Wikipedia used as non-parametric memory sources for retrieval in RAG models.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Top-K Document Retrieval">
  <data key="d0">Top-K Document Retrieval</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval process that fetches the top K most relevant documents based on input queries, used in RAG to provide context for generation.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Marginalization">
  <data key="d0">Marginalization</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">A probabilistic technique used to integrate over multiple latent variables (e.g., retrieved documents) to compute the overall likelihood of generated outputs.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Factual, Specific, and Diverse Responses">
  <data key="d0">Factual, Specific, and Diverse Responses</data>
  <data key="d1">Results</data>
  <data key="d2">Qualitative improvements observed in responses generated by RAG models, indicating enhanced factual correctness and variety.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="External Knowledge Source Replacement">
  <data key="d0">External Knowledge Source Replacement</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The ability to replace or update the external memory allows models to adapt to new information, making them more flexible and current.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge-Intensive Generation">
  <data key="d0">Knowledge-Intensive Generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Generation tasks that benefit from external knowledge retrieval to produce accurate, factual, and contextually relevant responses.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Performance Benchmarks">
  <data key="d0">Performance Benchmarks</data>
  <data key="d1">Results</data>
  <data key="d2">Quantitative measures demonstrating the superiority of RAG over previous models across multiple datasets and tasks.</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Model Architecture">
  <data key="d0">Model Architecture</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Model architecture defines the structural design of a neural network, such as llama or mixture-of-experts, affecting its scalability, efficiency, and capacity.&lt;SEP&gt;The structural design of RAG, integrating retriever and generator components with probabilistic training, enabling flexible knowledge access and generation.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retriever (DPR)">
  <data key="d0">Retriever (DPR)</data>
  <data key="d1">Tools</data>
  <data key="d2">A dense retrieval component based on BERT encoders that retrieves relevant documents by calculating dense vector representations and solving a Maximum Inner Product Search (MIPS) problem, used to find documents containing answers to questions.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Generator (BART)">
  <data key="d0">Generator (BART)</data>
  <data key="d1">Tools</data>
  <data key="d2">A pre-trained sequence-to-sequence transformer model that generates output sequences by concatenating input and retrieved content, trained with denoising objectives.&lt;SEP&gt;A sequence-to-sequence transformer model used to generate outputs by concatenating input and retrieved content, pre-trained with denoising objectives, and fine-tuned for specific generation tasks.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Decoding Methods">
  <data key="d0">Decoding Methods</data>
  <data key="d1">Methodology</data>
  <data key="d2">Procedures such as RAG-Token and RAG-Sequence decoding that approximate the most probable output sequences by different strategies, including beam search and thorough or fast decoding approximations.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="z2top-k(p(·|x))p⌘(z|x)NY">
  <data key="d0">z2top-k(p(·|x))p⌘(z|x)NY</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A mathematical expression representing a probabilistic model involving top-k document retrieval and marginalization over latent variables, used in the context of retrieval-augmented generation models.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="ip✓(yi|x, z, y 1:i1)">
  <data key="d0">ip✓(yi|x, z, y 1:i1)</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable representing the probability of a target token yi given input x, latent variables z, and previous output tokens y 1:i-1, within the probabilistic model.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="retriever">
  <data key="d0">retriever</data>
  <data key="d1">Tools</data>
  <data key="d2">A component based on DPR (Dense Passage Retrieval) that retrieves relevant documents using dense vector representations produced by BERT encoders, facilitating knowledge retrieval for generation.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="DPR (Dense Passage Retrieval)">
  <data key="d0">DPR (Dense Passage Retrieval)</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval system employing bi-encoder architecture with BERT to produce dense document and query representations, solving a MIPS problem to find top-k relevant documents.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="p⌘(z|x)">
  <data key="d0">p⌘(z|x)</data>
  <data key="d1">Variables</data>
  <data key="d2">The probability distribution over documents z given input x, modeled by DPR, used to select relevant documents for the generator.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="p✓(yi|x, z, y 1:i1)">
  <data key="d0">p✓(yi|x, z, y 1:i1)</data>
  <data key="d1">Variables</data>
  <data key="d2">The probability of generating token yi given input x, retrieved document z, and previous tokens, used during sequence generation.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Training">
  <data key="d0">Training</data>
  <data key="d1">Methodology</data>
  <data key="d2">A joint training process that optimizes both the retriever and generator components by minimizing negative marginal log-likelihood, using stochastic gradient descent, with fixed document encoder to reduce computational costs.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Decoding">
  <data key="d0">Decoding</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Procedures such as RAG-Token and RAG-Sequence decoding that approximate the most probable output sequences using beam search, thorough decoding, or fast decoding strategies.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG-Sequence">
  <data key="d0">RAG-Sequence</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A decoding approach that runs beam search for each document and estimates output probabilities by combining document probabilities with generator scores, allowing sequence generation across multiple documents.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Thorough Decoding">
  <data key="d0">Thorough Decoding</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A decoding method that performs comprehensive scoring by running additional forward passes for hypotheses not generated during initial beam search, providing more accurate output probability estimates.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fast Decoding">
  <data key="d0">Fast Decoding</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A decoding procedure designed to avoid redundant forward passes during generation by utilizing knowledge about candidate sets, thereby improving efficiency in language model inference.&lt;SEP&gt;An approximate decoding method that reduces computation by ignoring hypotheses not generated during initial beam search, improving efficiency in sequence generation.</data>
  <data key="d3">chunk-20aafa58f3d7697ab6ba5015b31cda9b&lt;SEP&gt;chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Experiments">
  <data key="d0">Experiments</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A series of tests conducted to evaluate the performance of RAG across various knowledge-intensive tasks, using a standardized Wikipedia dump as the knowledge source.&lt;SEP&gt;The experiments involve testing RAG's performance on multiple knowledge-intensive tasks using a large Wikipedia dataset as the knowledge source.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wikipedia dump (December 2018)">
  <data key="d0">Wikipedia dump (December 2018)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large-scale collection of Wikipedia articles split into 100-word chunks, serving as the non-parametric knowledge source for retrieval in experiments.&lt;SEP&gt;The Wikipedia dump from December 2018 serves as the knowledge base for retrieval in the experiments.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document encoder">
  <data key="d0">Document encoder</data>
  <data key="d1">Tools</data>
  <data key="d2">A component that computes embeddings for each Wikipedia document, facilitating efficient retrieval via similarity search.&lt;SEP&gt;A component that encodes Wikipedia documents into embeddings for similarity search, enabling retrieval of relevant knowledge."|</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="MIPS index with FAISS">
  <data key="d0">MIPS index with FAISS</data>
  <data key="d1">Tools</data>
  <data key="d2">A similarity search index built using FAISS with Hierarchical Navigable Small World approximation, enabling fast retrieval of relevant documents during training and testing.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge-intensive tasks">
  <data key="d0">Knowledge-intensive tasks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Tasks that require external knowledge sources like Wikipedia to answer questions or generate responses, including open-domain QA, abstractive QA, question generation, and fact verification.&lt;SEP&gt;Tasks that require leveraging external knowledge sources, such as Wikipedia, to answer questions or generate responses, exemplified by open-domain QA, abstractive QA, question generation, and fact verification.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open-domain Question Answering (QA)">
  <data key="d0">Open-domain Question Answering (QA)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A task where models answer questions by retrieving relevant documents and generating answers, evaluated on datasets like NQ, TriviaQA, WQ, and CuratedTrec."|&lt;SEP&gt;A task where questions are answered using retrieved documents from Wikipedia, compared against extractive and closed-book approaches, evaluated with Exact Match scores across datasets like NQ, TriviaQA, WebQuestions, and CuratedTrec.&lt;SEP&gt;Open-domain QA is a task where systems answer questions based on a broad and unstructured knowledge base, such as Wikipedia, without domain restrictions.&lt;SEP&gt;Open-domain QA is a task where systems answer questions based on broad, unstructured knowledge sources such as Wikipedia, without specific domain restrictions.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Abstractive Question Answering">
  <data key="d0">Abstractive Question Answering</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A task where models generate free-form answers to questions, assessed using MSMARCO NLG task, relying on retrieved passages to produce natural language responses.&lt;SEP&gt;A task where models generate natural language answers that may not be directly extractable from documents, relying on retrieved knowledge."|</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jeopardy Question Generation">
  <data key="d0">Jeopardy Question Generation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A generation task where models produce Jeopardy-style questions based on entity facts, evaluated with Q-BLEU-1 and human judgments for factuality and specificity.&lt;SEP&gt;A specific task where models generate questions in the style of Jeopardy, used to evaluate generative capabilities.&lt;SEP&gt;A task designed to evaluate models' ability to generate relevant and accurate questions based on input data, used here to compare RAG and BART performance.&lt;SEP&gt;A task where models generate Jeopardy-style questions based on entity facts, evaluating factuality and specificity through human assessments."|&lt;SEP&gt;A task where models generate questions in the style of Jeopardy to assess generative performance.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-99d4d675d4565ed5a3ce6fb12d24e2ee&lt;SEP&gt;chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fact Veriﬁcation (FEVER)">
  <data key="d0">Fact Veriﬁcation (FEVER)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A classification task determining whether claims are supported, refuted, or unverifiable based on evidence retrieved from Wikipedia, involving evidence retrieval and reasoning.&lt;SEP&gt;A task where models retrieve evidence from Wikipedia to support or refute claims, involving evidence retrieval and reasoning to classify claims as supported, refuted, or unverifiable."|</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Y">
  <data key="d0">Y</data>
  <data key="d1">Variables</data>
  <data key="d2">Y is the object of study in the decoding procedure, focusing on how candidate sets are generated and utilized.&lt;SEP&gt;Y represents variables that are involved in the decoding process, specifically related to the generation of candidate sets during beam search.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Yhas">
  <data key="d0">Yhas</data>
  <data key="d1">Variables</data>
  <data key="d2">Yhas denotes the set of candidate variables or options generated during the decoding process, influencing the efficiency of the model.&lt;SEP&gt;Yhas refers to the candidate set Y that is generated during beam search, which is crucial for the decoding process.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="additional forward passes">
  <data key="d0">additional forward passes</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Additional forward passes are extra computations performed after initial decoding to improve accuracy or generate further outputs, which the described procedure aims to avoid.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="candidate set Y">
  <data key="d0">candidate set Y</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The candidate set Y is a collection of potential outputs or tokens generated during decoding, central to the efficiency of the process.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="decoding procedure">
  <data key="d0">decoding procedure</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The decoding procedure, termed 'Fast Decoding,' is a methodology designed to optimize inference efficiency by reducing redundant computations.</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="FAISS">
  <data key="d0">FAISS</data>
  <data key="d1">Tools</data>
  <data key="d2">FAISS is a library used to build the MIPS index for fast approximate nearest neighbor search of document embeddings."|</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Hierarchical Navigable Small World (HNSW)">
  <data key="d0">Hierarchical Navigable Small World (HNSW)</data>
  <data key="d1">Tools</data>
  <data key="d2">HNSW is an algorithm used within FAISS to perform efficient approximate nearest neighbor searches for document retrieval."|</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="FEVER">
  <data key="d0">FEVER</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">FEVER is a dataset and evaluation framework designed for fact extraction and verification, used to assess the correspondence between retrieved documents and gold evidence.&lt;SEP&gt;FEVER is a fact verification framework that involves classifying whether a claim is supported, refuted, or unverifiable based on evidence from Wikipedia, requiring retrieval and reasoning.&lt;SEP&gt;FEVER is a framework for fact verification that involves classifying claims as supported, refuted, or unverifiable by retrieving evidence from Wikipedia and reasoning over it.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Natural Language Claim">
  <data key="d0">Natural Language Claim</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A natural language claim is a statement that can be supported or refuted through evidence retrieval and reasoning processes in the FEVER task.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Problem">
  <data key="d0">Retrieval Problem</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">FEVER maps to a retrieval challenge where relevant evidence must be retrieved from Wikipedia to support classification.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Entailment Reasoning">
  <data key="d0">Entailment Reasoning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Entailment reasoning involves analyzing retrieved evidence to determine if it supports, refutes, or is insufficient for a claim.&lt;SEP&gt;FEVER involves reasoning over retrieved evidence to determine if a claim is supported, refuted, or unverifiable.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Claim-Class Pairs">
  <data key="d0">Claim-Class Pairs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Claim-class pairs are training and evaluation data consisting of claims and their associated labels (supports, refutes, not enough info).&lt;SEP&gt;Pairs of claims and their corresponding class labels used for training and evaluation in FEVER.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Class Labels">
  <data key="d0">Class Labels</data>
  <data key="d1">Variables</data>
  <data key="d2">The class labels 'supports', 'refutes', and 'not enough info' categorize the relationship between claims and evidence in FEVER.&lt;SEP&gt;The labels 'supports', 'refutes', and 'not enough info' are used to categorize claims in FEVER.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Label Accuracy">
  <data key="d0">Label Accuracy</data>
  <data key="d1">Results</data>
  <data key="d2">Label accuracy measures how correctly models classify claims into the three classes in FEVER.&lt;SEP&gt;The performance metric used to evaluate models on FEVER tasks, measuring the correctness of claim classifications.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG Models">
  <data key="d0">RAG Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Retrieval-Augmented Generation (RAG) models are advanced language models that combine retrieval mechanisms with generative capabilities to improve factual accuracy and diversity.&lt;SEP&gt;Retrieval-Augmented Generation (RAG) models combine retrieval of evidence with generative capabilities to perform classification and question answering tasks.&lt;SEP&gt;Retrieval-Augmented Generation (RAG) models combine retrieval of evidence with generative capabilities to perform classification and question answering.&lt;SEP&gt;Retrieval-Augmented Generation models that combine document retrieval with generative capabilities to improve factual accuracy.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd&lt;SEP&gt;chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="State-of-the-Art Models">
  <data key="d0">State-of-the-Art Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Latest advanced code LLMs, such as HPC-Coder-V2 series, evaluated for parallel code generation capabilities.&lt;SEP&gt;Latest advanced language models (e.g., HPC-Coder-V2 series) designed for parallel code generation, evaluated against benchmarks.&lt;SEP&gt;RAG models outperform previous models on multiple datasets, setting new benchmarks in fact verification and open-domain QA.&lt;SEP&gt;RAG models outperform previous models on multiple open-domain QA tasks, setting new performance benchmarks.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open-domain Question Answering">
  <data key="d0">Open-domain Question Answering</data>
  <data key="d1">Study Design</data>
  <data key="d2">A task involving answering questions using a broad knowledge base, evaluated using datasets like NQ, TQA, WQ, and CT.&lt;SEP&gt;Open-domain QA involves answering questions using broad knowledge sources, evaluated on datasets like NQ, TQA, WQ, CT.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Generation and Classification Test Scores">
  <data key="d0">Generation and Classification Test Scores</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics evaluating the performance of models like RAG on tasks such as MS-MARCO, FEVER, and others, including Bleu, Rouge-L, and accuracy scores.&lt;SEP&gt;Performance metrics such as Bleu, Rouge-L, and accuracy scores evaluate model effectiveness on QA tasks.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Claim">
  <data key="d0">Claim</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A claim is a statement in natural language that needs to be verified as supported, refuted, or unverifiable based on evidence.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Model Variants">
  <data key="d0">Model Variants</data>
  <data key="d1">Tools</data>
  <data key="d2">Different variants of RAG, such as RAG-Token and RAG-Sequence, are used to evaluate different approaches within retrieval-augmented generation.</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fact Verification">
  <data key="d0">Fact Verification</data>
  <data key="d1">Study Design</data>
  <data key="d2">A process to assess the factual correctness of generated responses, exemplified by FEVER dataset evaluations.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Q-BLEU-1">
  <data key="d0">Q-BLEU-1</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A metric used to evaluate the quality and relevance of generated questions or answers, particularly in Jeopardy question generation.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document Posterior p(zi|x, yi)">
  <data key="d0">Document Posterior p(zi|x, yi)</data>
  <data key="d1">Variables</data>
  <data key="d2">The probability distribution over documents conditioned on input and generated tokens, used to analyze how models select relevant documents during generation.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Human Evaluation">
  <data key="d0">Human Evaluation</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment involving human evaluators indicating that RAG is more factual and specific than BART in generated responses.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 1: 'His works are considered classics of American literature...'">
  <data key="d0">Document 1: 'His works are considered classics of American literature...'</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A historical document providing context on literature and authorship.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 2: 'Artists of the 1920s &quot;Lost Generation&quot; expatriate community...'">
  <data key="d0">Document 2: 'Artists of the 1920s "Lost Generation" expatriate community...'</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A historical document describing literary and artistic communities of the 1920s.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 3: 'His wartime experiences formed the basis for his novel &quot;A Farewell to Arms&quot; (1929)...'">
  <data key="d0">Document 3: 'His wartime experiences formed the basis for his novel "A Farewell to Arms" (1929)...'</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A biographical/historical document related to the author Ernest Hemingway.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 4: 'Artists of the 1920s &quot;Lost Generation&quot; expatriate community...'">
  <data key="d0">Document 4: 'Artists of the 1920s "Lost Generation" expatriate community...'</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A historical context on expatriate artists and writers of the 1920s.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 5: 'The Sun Also Rises is a novel by this author of &quot;A Farewell to Arms&quot;'">
  <data key="d0">Document 5: 'The Sun Also Rises is a novel by this author of "A Farewell to Arms"'</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A factual document about literary works and authorship.</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Purgatorio">
  <data key="d0">Purgatorio</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Purgatorio is the second part of Dante Alighieri's Divine Comedy, depicting the soul's purification process in the afterlife.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Paradiso">
  <data key="d0">Paradiso</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Paradiso is the third part of Dante's Divine Comedy, illustrating the ascent to heaven and divine enlightenment.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Thorne">
  <data key="d0">Thorne</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Thorne is a researcher whose work involves classifying claims as true or false using models like RoBERTa.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Vlachos">
  <data key="d0">Vlachos</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Vlachos is a researcher collaborating with Thorne in classifying claims and evidence.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RoBERTa">
  <data key="d0">RoBERTa</data>
  <data key="d1">Tools</data>
  <data key="d2">A transformer-based language model used for classifying claims as true or false in fact verification tasks.&lt;SEP&gt;RoBERTa is a transformer-based language model used for text classification tasks, such as determining claim veracity.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG">
  <data key="d0">RAG</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Retrieval-Augmented Generation (RAG) is a methodology that combines document retrieval with generative models to improve factual accuracy and evidence retrieval.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Gold Evidence Sentence">
  <data key="d0">Gold Evidence Sentence</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A gold evidence sentence is a verified piece of evidence used as a benchmark for model performance in claim verification tasks.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document Titles">
  <data key="d0">Document Titles</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Document titles refer to the titles of articles retrieved or annotated as evidence in the FEVER dataset.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Top k Documents">
  <data key="d0">Top k Documents</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The highest-ranked documents retrieved by models like RAG, used to assess retrieval effectiveness.&lt;SEP&gt;Top k documents are the highest-ranked documents retrieved by RAG used to evaluate retrieval performance.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Generation Diversity">
  <data key="d0">Generation Diversity</data>
  <data key="d1">Results</data>
  <data key="d2">A metric assessing the variety in generated text, calculated via ratios of distinct ngrams, indicating diversity in models like RAG and BART.&lt;SEP&gt;Generation diversity measures the variety and novelty in generated text, assessed by ratios of distinct ngrams.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Ablations">
  <data key="d0">Retrieval Ablations</data>
  <data key="d1">Study Design</data>
  <data key="d2">Ablation studies where the retriever component is frozen or replaced to assess its impact on model performance.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="BM25">
  <data key="d0">BM25</data>
  <data key="d1">Tools</data>
  <data key="d2">A traditional information retrieval algorithm used as a baseline to compare against learned retrievers in RAG.&lt;SEP&gt;An information retrieval algorithm based on the probabilistic relevance framework, used for ranking documents based on query relevance.&lt;SEP&gt;BM25 is a traditional information retrieval algorithm used as a baseline for document retrieval in the context of RAG.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Index Hot-Swapping">
  <data key="d0">Index Hot-Swapping</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Index hot-swapping involves updating the knowledge base or index used by RAG to reflect new information without retraining the model.&lt;SEP&gt;The process of updating the knowledge base or index used by RAG with newer data without retraining, allowing knowledge updates at test time.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="World Leaders">
  <data key="d0">World Leaders</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A list of individuals holding key political positions, used to evaluate the effectiveness of index updates and knowledge freshness in RAG.&lt;SEP&gt;A list of individuals holding significant political positions, used to evaluate index updating and knowledge freshness.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Dec 2016 Wikipedia Dump">
  <data key="d0">Dec 2016 Wikipedia Dump</data>
  <data key="d1">Study Design</data>
  <data key="d2">A snapshot of Wikipedia used to build a knowledge index for RAG, allowing testing of knowledge updates over time.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Claim Verification">
  <data key="d0">Claim Verification</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The process of assessing whether a given claim is true or false based on evidence sentences, as explored through models like RAG and RoBERTa.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document Retrieval">
  <data key="d0">Document Retrieval</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The process of fetching relevant documents or evidence to support or refute claims, central to models like RAG.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Evidence Overlap">
  <data key="d0">Evidence Overlap</data>
  <data key="d1">Variables</data>
  <data key="d2">The measure of the intersection between retrieved documents and gold evidence, used to evaluate retrieval accuracy.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Claim as True or False">
  <data key="d0">Claim as True or False</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The classification task of determining the truthfulness of claims based on evidence.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="FEVER Dataset">
  <data key="d0">FEVER Dataset</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A benchmark dataset for fact extraction and verification, used to evaluate models' ability to identify supporting evidence.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Ablation Studies">
  <data key="d0">Retrieval Ablation Studies</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Experiments where the retriever component is frozen or replaced to evaluate its impact on overall model performance.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wikipedia Dump Dec 2016">
  <data key="d0">Wikipedia Dump Dec 2016</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A snapshot of Wikipedia used to create an older knowledge index for RAG, enabling testing of knowledge update mechanisms.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wikipedia Dump Dec 2018">
  <data key="d0">Wikipedia Dump Dec 2018</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A more recent snapshot of Wikipedia used to build an updated index for RAG, facilitating comparison of knowledge accuracy over time.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Factuality">
  <data key="d0">Factuality</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Factuality pertains to the degree to which generated content accurately reflects real-world facts, a key advantage of RAG models.&lt;SEP&gt;The accuracy of models like RAG and BART in generating factually correct responses, assessed through human evaluations.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Specificity">
  <data key="d0">Specificity</data>
  <data key="d1">Results</data>
  <data key="d2">The degree to which generated responses are detailed and precise, compared across models like RAG and BART.</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="2016 Index">
  <data key="d0">2016 Index</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The 2016 index is used as a benchmark to evaluate the accuracy of leader identification in 2016.&lt;SEP&gt;The 2016 index is used as a reference measure to evaluate the accuracy of leader identification across different years.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="2018 Index">
  <data key="d0">2018 Index</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The 2018 index serves as a benchmark for assessing the accuracy of leader identification in 2018.&lt;SEP&gt;The 2018 index serves as a benchmark to evaluate the accuracy of leader identification in 2018.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="2016 World Leaders">
  <data key="d0">2016 World Leaders</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The group of world leaders from 2016 used in comparison with indices for accuracy measurement.&lt;SEP&gt;The set of world leaders from 2016 used to assess index accuracy.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="2018 World Leaders">
  <data key="d0">2018 World Leaders</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The group of world leaders from 2018 used in comparison with indices for accuracy measurement.&lt;SEP&gt;The set of world leaders from 2018 used to assess index accuracy.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Indices">
  <data key="d0">Indices</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Indices are reference measures used to evaluate the correctness of leader identification across different years.&lt;SEP&gt;Reference measures used to evaluate leader matching accuracy across different years.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Mismatched Indices">
  <data key="d0">Mismatched Indices</data>
  <data key="d1">Results</data>
  <data key="d2">Instances where indices fail to correctly identify leaders, resulting in low accuracy scores.&lt;SEP&gt;The instances where the indices do not correctly match the actual leaders, leading to low accuracy scores.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG (Retrieval-Augmented Generation)">
  <data key="d0">RAG (Retrieval-Augmented Generation)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A hybrid model that updates world knowledge by replacing non-parametric memory with retrieved documents.&lt;SEP&gt;A method that updates an AI model's world knowledge by replacing its non-parametric memory with retrieved documents.&lt;SEP&gt;RAG combines parametric memory (like trained language models) with non-parametric retrieval mechanisms to generate more factual and controllable responses in NLP tasks.&lt;SEP&gt;RAG combines parametric memory (like trained language models) with non-parametric retrieval mechanisms to generate more factual, controllable responses in NLP tasks.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc&lt;SEP&gt;chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieving More Documents">
  <data key="d0">Retrieving More Documents</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process in RAG models where increasing the number of retrieved documents at test time can influence performance metrics like accuracy and runtime.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Figure 3">
  <data key="d0">Figure 3</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">A figure illustrating how retrieval quantity affects model performance metrics across different models and tasks.&lt;SEP&gt;A figure illustrating the impact of number of retrieved documents on model performance across different models and metrics.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Related Work">
  <data key="d0">Related Work</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A review of previous research on retrieval-enhanced NLP tasks, general-purpose NLP architectures, learned retrieval methods, memory-based architectures, and retrieve-and-edit approaches.&lt;SEP&gt;Survey of previous research on retrieval-enhanced NLP tasks, general-purpose NLP models, learned retrieval, memory architectures, and retrieve-and-edit methods.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Single-Task Retrieval">
  <data key="d0">Single-Task Retrieval</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Models showing how retrieval improves performance across various NLP tasks such as question answering, fact checking, dialogue, translation, and language modeling.&lt;SEP&gt;Models that show how retrieval improves performance across various NLP tasks such as question answering, fact checking, and dialogue.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="General-Purpose Architectures">
  <data key="d0">General-Purpose Architectures</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Pre-trained language models like GPT-2, BART, T5 that achieve high performance without retrieval but can be augmented with retrieval modules.&lt;SEP&gt;Pre-trained language models like GPT-2, BART, and T5 that achieve strong performance without retrieval, yet can be expanded with retrieval modules.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Learned Retrieval">
  <data key="d0">Learned Retrieval</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Approaches that optimize document retrieval using neural language models, reinforcement learning, or latent variables to improve downstream task performance.&lt;SEP&gt;Methods that optimize document retrieval using neural models, reinforcement learning, or latent variable approaches to enhance downstream tasks.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Memory-based Architectures">
  <data key="d0">Memory-based Architectures</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Architectures that treat document indices as external memory, enabling models to attend over raw text or embeddings for improved factual consistency and interpretability.&lt;SEP&gt;Architectures that treat document indices as external memory, enabling models to attend over raw text or embeddings, improving factual accuracy and interpretability.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieve-and-Edit">
  <data key="d0">Retrieve-and-Edit</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches that retrieve similar training pairs or content and then edit them to generate outputs, successful in machine translation and semantic parsing.&lt;SEP&gt;Approaches that retrieve similar training pairs or content and then edit them to produce outputs, applied in machine translation and semantic parsing.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Discussion">
  <data key="d0">Discussion</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A reflection on the hybrid generation models with access to both parametric and non-parametric memory, emphasizing the interpretability and dynamic update capabilities of the document index.&lt;SEP&gt;Reflection on hybrid models combining parametric and non-parametric memory, emphasizing interpretability and dynamic updating of document indices.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Mismatches">
  <data key="d0">Mismatches</data>
  <data key="d1">Limitations</data>
  <data key="d2">Errors arising from index mismatches that reduce overall accuracy.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieving more documents">
  <data key="d0">Retrieving more documents</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adjusting the number of documents retrieved at test time to influence model performance and runtime.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Test time retrieval">
  <data key="d0">Test time retrieval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of retrieving documents during testing to improve model outcomes, affecting performance metrics.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Performance metrics">
  <data key="d0">Performance metrics</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics such as Rouge-L and Bleu-1 used to evaluate the quality of outputs as a function of retrieved documents.&lt;SEP&gt;Metrics such as throughput, accuracy (pass@1), and resource usage used to evaluate and compare model effectiveness across different configurations.</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad&lt;SEP&gt;chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Semantic Parsing">
  <data key="d0">Semantic Parsing</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Semantic Parsing involves converting natural language into formal representations to facilitate understanding and reasoning in NLP systems.&lt;SEP&gt;Semantic Parsing is a computational approach that involves converting natural language into formal representations to facilitate understanding and reasoning in NLP systems.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Latent Retrieval">
  <data key="d0">Latent Retrieval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Latent retrieval involves learning implicit representations to improve the effectiveness of retrieving relevant evidence, often integrated with neural models.&lt;SEP&gt;Latent retrieval refers to learning implicit representations to improve the effectiveness of retrieving relevant evidence, often integrated with neural models.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="State of the Art Results">
  <data key="d0">State of the Art Results</data>
  <data key="d1">Results</data>
  <data key="d2">Achieving the highest performance metrics on open-domain QA benchmarks, demonstrating the effectiveness of the RAG models.&lt;SEP&gt;Achieving the highest performance metrics on open-domain QA benchmarks, demonstrating the effectiveness of the proposed RAG models.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Index">
  <data key="d0">Retrieval Index</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The retrieval index is a data structure that stores and organizes evidence documents, enabling efficient hot-swapping and updating without retraining the entire model.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Bias">
  <data key="d0">Bias</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Bias in external sources such as Wikipedia can influence model outputs, necessitating strategies for bias mitigation.&lt;SEP&gt;Bias involves systematic prejudice or skew in data sources like Wikipedia, which can influence model outputs and pose societal risks.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Societal Impact">
  <data key="d0">Societal Impact</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The societal impact includes benefits like more factual and controllable NLP systems, but also risks like misuse for misinformation or automation of jobs.&lt;SEP&gt;The societal impact of RAG includes improved factual accuracy and interpretability, but also potential misuse such as generating misleading content or automation of jobs.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Bias Mitigation">
  <data key="d0">Bias Mitigation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Strategies to reduce bias and misinformation in language models, including employing AI to detect and counteract misleading content.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Factual Hallucination">
  <data key="d0">Factual Hallucination</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Factual hallucination is the phenomenon where language models generate false or misleading facts, which RAG seeks to minimize.&lt;SEP&gt;Factual hallucination is when a language model generates incorrect or misleading facts, which RAG aims to reduce by grounding responses in retrieved evidence.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Bias and Misinformation">
  <data key="d0">Bias and Misinformation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Bias and misinformation concern the presence of skewed or false information in external sources like Wikipedia, affecting model reliability.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Factual Grounding">
  <data key="d0">Factual Grounding</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Factual grounding describes the process of anchoring generated responses to retrieved evidence to ensure accuracy.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="External Knowledge Sources">
  <data key="d0">External Knowledge Sources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">External knowledge sources like Wikipedia provide factual data that ground RAG's responses, reducing hallucinations.</data>
  <data key="d3">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Association for Computational Linguistics">
  <data key="d0">Association for Computational Linguistics</data>
  <data key="d1">Discipline</data>
  <data key="d2">An organization dedicated to advancing research and development in computational linguistics and natural language processing.&lt;SEP&gt;An organization dedicated to the advancement of research, development, and dissemination in the fields of computational linguistics and natural language processing.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Eunsol Choi">
  <data key="d0">Eunsol Choi</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in question answering and long document comprehension studies.&lt;SEP&gt;A researcher involved in question answering, long document comprehension, and language understanding studies.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Daniel Hewlett">
  <data key="d0">Daniel Hewlett</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to question answering methodologies and long document processing.&lt;SEP&gt;A researcher contributing to studies on question answering for long documents.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Alexandre Lacoste">
  <data key="d0">Alexandre Lacoste</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher associated with advancements in language comprehension models.&lt;SEP&gt;A researcher involved in question answering research, especially in long documents.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jonathan Berant">
  <data key="d0">Jonathan Berant</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to question answering and language understanding research.&lt;SEP&gt;A researcher specializing in semantic parsing, question answering, and language understanding.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics">
  <data key="d0">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceeding documenting research on question answering for long documents.&lt;SEP&gt;A conference proceedings documenting research on question answering, long document comprehension, and related NLP methodologies.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Christopher Clark">
  <data key="d0">Christopher Clark</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focused on multi-paragraph reading comprehension and natural language understanding.&lt;SEP&gt;A researcher focused on multi-paragraph reading comprehension techniques.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Matt Gardner">
  <data key="d0">Matt Gardner</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in developing effective reading comprehension models.&lt;SEP&gt;A researcher working on models for effective reading comprehension and question answering.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="arXiv:1710.10723">
  <data key="d0">arXiv:1710.10723</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint describing methods for multi-paragraph reading comprehension in NLP.&lt;SEP&gt;A preprint describing models for multi-paragraph reading comprehension.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jacob Devlin">
  <data key="d0">Jacob Devlin</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher known for developing BERT, a pre-training model for language understanding.&lt;SEP&gt;A researcher known for developing BERT, a pre-training model that advances language understanding and question answering.&lt;SEP&gt;Jacob Devlin is known for his work on language models and question answering benchmarks.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Ming-Wei Chang">
  <data key="d0">Ming-Wei Chang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A researcher contributing to deep bidirectional transformer models.&lt;SEP&gt;A researcher contributing to knowledge grounding in neural conversation models.&lt;SEP&gt;A researcher involved in transformer-based models for language understanding.&lt;SEP&gt;Ming-Wei Chang is a researcher specializing in natural language processing and retrieval-augmented models.&lt;SEP&gt;Ming-Wei Chang works on information retrieval and natural language processing models.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kenton Lee">
  <data key="d0">Kenton Lee</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A researcher involved in language understanding and model pre-training.&lt;SEP&gt;A researcher working on bidirectional transformers and language models.&lt;SEP&gt;Kenton Lee is a researcher focusing on retrieval techniques for open-domain question answering.&lt;SEP&gt;Kenton Lee is a researcher involved in language modeling and retrieval techniques.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kristina Toutanova">
  <data key="d0">Kristina Toutanova</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to deep language understanding models, including BERT.&lt;SEP&gt;A researcher working on language models and understanding.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics">
  <data key="d0">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceeding detailing BERT and related models.&lt;SEP&gt;A conference proceeding detailing advances in language models like BERT and their applications.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://www.aclweb.org/anthology/N19-1423">
  <data key="d0">https://www.aclweb.org/anthology/N19-1423</data>
  <data key="d1">Tool</data>
  <data key="d2">URL linking to the BERT paper and related resources.&lt;SEP&gt;URL linking to the BERT research paper.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Emily Dinan">
  <data key="d0">Emily Dinan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on knowledge-powered conversational agents and dialogue systems.&lt;SEP&gt;A researcher working on knowledge-powered conversational agents.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Stephen Roller">
  <data key="d0">Stephen Roller</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to conversational AI and knowledge-grounded dialogue models.&lt;SEP&gt;A researcher involved in developing conversational AI systems.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kurt Shuster">
  <data key="d0">Kurt Shuster</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to dialogue systems.&lt;SEP&gt;A researcher involved in neural conversational models and knowledge integration.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Angela Fan">
  <data key="d0">Angela Fan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focusing on neural story generation and question answering.&lt;SEP&gt;A researcher involved in augmenting transformers with memory mechanisms.&lt;SEP&gt;A researcher working on augmenting transformers with memory mechanisms, including KNN-based approaches.&lt;SEP&gt;A researcher working on neural story generation, hierarchical models, and question answering.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Michael Auli">
  <data key="d0">Michael Auli</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in language modeling and generation.&lt;SEP&gt;A researcher specializing in language modeling, story generation, and neural architectures.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jason Weston">
  <data key="d0">Jason Weston</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in neural dialogue systems and story generation.&lt;SEP&gt;A researcher working on conversational agents and language understanding.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics">
  <data key="d0">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceeding on hierarchical neural story generation.&lt;SEP&gt;A conference proceeding on neural story generation and hierarchical language models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://www.aclweb.org/anthology/P18-1082">
  <data key="d0">https://www.aclweb.org/anthology/P18-1082</data>
  <data key="d1">Tool</data>
  <data key="d2">URL linking to the neural story generation paper.&lt;SEP&gt;URL to the paper on neural story generation and hierarchical models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Ethan Perez">
  <data key="d0">Ethan Perez</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to neural language models.&lt;SEP&gt;A researcher involved in neural memory modules and language modeling.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="David Grangier">
  <data key="d0">David Grangier</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in language modeling and neural architectures.&lt;SEP&gt;A researcher working on neural architectures and transformer enhancements.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes">
  <data key="d0">Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes</data>
  <data key="d1">Researchers</data>
  <data key="d2">A group of researchers working on transformer augmentation and neural models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://openreview.net/forum?id=H1gx1CNKPH">
  <data key="d0">https://openreview.net/forum?id=H1gx1CNKPH</data>
  <data key="d1">Tool</data>
  <data key="d2">URL linking to the paper on transformers with KNN-based memory.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski">
  <data key="d0">Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers exploring entity as experts and sparse memory access with entity supervision.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://arxiv.org/abs/2004.07202">
  <data key="d0">https://arxiv.org/abs/2004.07202</data>
  <data key="d1">Tool</data>
  <data key="d2">URL to the paper on entities as experts and sparse memory access with entity supervision.&lt;SEP&gt;URL to the paper on entities as experts and sparse memory access.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentu Yih, Michel Galley">
  <data key="d0">Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentu Yih, Michel Galley</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers involved in knowledge-grounded neural conversation models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710">
  <data key="d0">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710</data>
  <data key="d1">Tool</data>
  <data key="d2">URL to the knowledge-grounded neural conversation model paper.&lt;SEP&gt;URL to the paper on knowledge-grounded neural conversation models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, Owain Evans">
  <data key="d0">Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, Owain Evans</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers studying when AI will exceed human performance, based on expert opinions.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="http://arxiv.org/abs/1705.08807">
  <data key="d0">http://arxiv.org/abs/1705.08807</data>
  <data key="d1">Tool</data>
  <data key="d2">URL to the paper on AI exceeding human performance and expert predictions.&lt;SEP&gt;URL to the paper on AI exceeding human performance.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jiatao Gu">
  <data key="d0">Jiatao Gu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on search engine guided neural machine translation.&lt;SEP&gt;Jiatao Gu is a researcher involved in developing search engine guided neural machine translation techniques.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Yong Wang">
  <data key="d0">Yong Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in neural machine translation and search-guided models.&lt;SEP&gt;A researcher involved in search-guided neural translation models.&lt;SEP&gt;Yong Wang is a researcher contributing to neural machine translation research.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kyunghyun Cho">
  <data key="d0">Kyunghyun Cho</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to neural translation models with search guidance.&lt;SEP&gt;A researcher contributing to neural translation models.&lt;SEP&gt;Kyunghyun Cho is a researcher known for work in neural network models and machine translation.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Victor O.K. Li">
  <data key="d0">Victor O.K. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focusing on language translation and search engine integration.&lt;SEP&gt;A researcher working on integrating search engines into neural translation systems.&lt;SEP&gt;Victor O.K. Li is a researcher involved in artificial intelligence and machine translation studies.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Proceedings of the AAAI Conference on Artificial Intelligence, 2018">
  <data key="d0">Proceedings of the AAAI Conference on Artificial Intelligence, 2018</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference publication detailing search engine guided neural machine translation models.&lt;SEP&gt;A conference publication detailing search engine-guided neural machine translation.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282">
  <data key="d0">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282</data>
  <data key="d1">Tool</data>
  <data key="d2">URL linking to the search engine guided neural machine translation paper.&lt;SEP&gt;URL to the paper on search engine guided neural machine translation.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Thibault Févry">
  <data key="d0">Thibault Févry</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher exploring entities as experts and sparse memory access with entity supervision.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Livio Baldini Soares">
  <data key="d0">Livio Baldini Soares</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher supporting entity supervision and sparse memory models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Nicholas FitzGerald">
  <data key="d0">Nicholas FitzGerald</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on entity-based neural memory and sparse access mechanisms.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Marjan Ghazvininejad">
  <data key="d0">Marjan Ghazvininejad</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focused on knowledge-grounded neural conversation models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Chris Brockett">
  <data key="d0">Chris Brockett</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on neural dialogue models grounded in external knowledge.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Bill Dolan">
  <data key="d0">Bill Dolan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher supporting knowledge-grounded dialogue systems.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jianfeng Gao">
  <data key="d0">Jianfeng Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in neural conversation models with knowledge grounding.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wentu Yih">
  <data key="d0">Wentu Yih</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on integrating external knowledge into neural dialogue models.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Michel Galley">
  <data key="d0">Michel Galley</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher focusing on knowledge-grounded neural conversation systems.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Katja Grace">
  <data key="d0">Katja Grace</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher studying expert predictions and timelines for AI surpassing human intelligence.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="John Salvatier">
  <data key="d0">John Salvatier</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to expert surveys on AI progress.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Owain Evans">
  <data key="d0">Owain Evans</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in evaluating predictions about AI capabilities.</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Search engine guided neural machine translation">
  <data key="d0">Search engine guided neural machine translation</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural machine translation approach that incorporates search engine techniques to improve translation quality.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="32nd AAAI Conference on Artificial Intelligence">
  <data key="d0">32nd AAAI Conference on Artificial Intelligence</data>
  <data key="d1">Study Designs</data>
  <data key="d2">An academic conference where research papers on artificial intelligence, including neural machine translation, are presented.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kelvin Guu">
  <data key="d0">Kelvin Guu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in pre-training retrieval-augmented language models to improve retrieval and reasoning capabilities.&lt;SEP&gt;Kelvin Guu is a researcher involved in language modeling and sentence generation research.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Tatsunori B. Hashimoto">
  <data key="d0">Tatsunori B. Hashimoto</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Tatsunori B. Hashimoto is a researcher working on structured output prediction and language models.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Yonatan Oren">
  <data key="d0">Yonatan Oren</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Yonatan Oren is a researcher contributing to retrieval-augmented language models and structured prediction frameworks.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Percy Liang">
  <data key="d0">Percy Liang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Percy Liang is a researcher specializing in natural language processing and structured output prediction.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="REALM: Retrieval-augmented language model pre-training">
  <data key="d0">REALM: Retrieval-augmented language model pre-training</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">REALM is a language model pre-training framework that incorporates retrieval mechanisms to enhance language understanding and generation.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="retrieve-and-edit framework">
  <data key="d0">retrieve-and-edit framework</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework for predicting structured outputs by retrieving relevant information and editing it accordingly.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Nabil Hossain">
  <data key="d0">Nabil Hossain</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Nabil Hossain is a researcher working on text generation and retrieval-based methods.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Simple and effective retrieve-edit-rerank text generation">
  <data key="d0">Simple and effective retrieve-edit-rerank text generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A text generation approach that involves retrieving relevant data, editing it, and reranking outputs to improve quality.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jeff Johnson">
  <data key="d0">Jeff Johnson</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Jeff Johnson is a researcher focused on similarity search at large scales using GPU acceleration.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Zora Tung">
  <data key="d0">Zora Tung</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Zora Tung is a researcher contributing to retrieval and language modeling research.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Panupong Pasupat">
  <data key="d0">Panupong Pasupat</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Panupong Pasupat is a researcher working on language understanding and retrieval methods.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Billion-scale similarity search with GPUs">
  <data key="d0">Billion-scale similarity search with GPUs</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for performing large-scale similarity search efficiently using GPU hardware acceleration.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Urvashi Khandelwal">
  <data key="d0">Urvashi Khandelwal</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Urvashi Khandelwal is a researcher exploring generalization in language models through memorization and nearest neighbor methods.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Omer Levy">
  <data key="d0">Omer Levy</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Omer Levy is a researcher working on language modeling and generalization techniques.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Dan Jurafsky">
  <data key="d0">Dan Jurafsky</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Dan Jurafsky is a linguist and researcher in natural language processing and machine learning.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Luke Zettlemoyer">
  <data key="d0">Luke Zettlemoyer</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on language models and tools, published in 2023.&lt;SEP&gt;A researcher working on language models and tools, published in 2023.&lt;SEP&gt;Luke Zettlemoyer is a researcher specializing in language understanding and modeling techniques.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mike Lewis">
  <data key="d0">Mike Lewis</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in research on language model capabilities, published in 2022.&lt;SEP&gt;A researcher involved in language model studies, published in 2022.&lt;SEP&gt;Mike Lewis is a researcher working on language modeling and retrieval-based NLP methods.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Generalization through memorization">
  <data key="d0">Generalization through memorization</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A hypothesis that language models can improve their generalization by memorizing and retrieving relevant data points.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Diederik P. Kingma">
  <data key="d0">Diederik P. Kingma</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Diederik P. Kingma is a researcher known for optimization algorithms in machine learning.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Adam: A method for stochastic optimization">
  <data key="d0">Adam: A method for stochastic optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adam is an optimization algorithm used to train neural networks efficiently by adaptively adjusting learning rates.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Tom Kwiatkowski">
  <data key="d0">Tom Kwiatkowski</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Tom Kwiatkowski is a researcher involved in question answering and NLP benchmarking.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Natural Questions: a Benchmark for Question Answering Research">
  <data key="d0">Natural Questions: a Benchmark for Question Answering Research</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A benchmark dataset and evaluation framework designed to advance research in question answering by providing a standard for comparison and assessment.&lt;SEP&gt;A dataset and benchmark designed to evaluate question answering systems' performance on real-world queries.</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Danielle Epstein">
  <data key="d0">Danielle Epstein</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Danielle Epstein is an author associated with research in natural language processing and question answering.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Matthew Kelcey">
  <data key="d0">Matthew Kelcey</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Matthew Kelcey is involved in research related to natural language understanding and evaluation methodologies.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kristina N. Toutanova">
  <data key="d0">Kristina N. Toutanova</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Kristina N. Toutanova specializes in natural language understanding, model training, and evaluation methodologies.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Andrew Dai">
  <data key="d0">Andrew Dai</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Andrew Dai is involved in research on neural language models and sequence modeling.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Quoc Le">
  <data key="d0">Quoc Le</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Quoc Le focuses on neural network training techniques and large-scale language models.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Slav Petrov">
  <data key="d0">Slav Petrov</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Slav Petrov works on natural language processing models and evaluation benchmarks.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Large memory layers with product keys">
  <data key="d0">Large memory layers with product keys</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural network architecture that incorporates large memory layers using product key mechanisms to improve model capacity and performance.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Latent retrieval for weakly supervised open domain question answering">
  <data key="d0">Latent retrieval for weakly supervised open domain question answering</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A retrieval-based methodology that leverages latent representations to improve open-domain question answering in weak supervision scenarios.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension">
  <data key="d0">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A pre-training model architecture that uses denoising objectives to enhance language understanding and generation capabilities.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="A diversity-promoting objective function for neural conversation models">
  <data key="d0">A diversity-promoting objective function for neural conversation models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">An objective function designed to increase diversity in neural conversation outputs, improving conversational relevance and variety.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons">
  <data key="d0">Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">An evaluation methodology that enhances dialogue system assessment through optimized questioning and multi-turn comparison techniques.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Robust neural machine translation with joint textual and phonetic embedding">
  <data key="d0">Robust neural machine translation with joint textual and phonetic embedding</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural translation approach that combines textual and phonetic embeddings to improve translation robustness.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Generating Wikipedia by summarizing long sequences">
  <data key="d0">Generating Wikipedia by summarizing long sequences</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A sequence summarization technique aimed at generating comprehensive Wikipedia articles from long input sequences.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs">
  <data key="d0">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An algorithmic approach for fast and reliable nearest neighbor search using hierarchical graph structures.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="The next decade in AI: four steps towards robust artificial intelligence">
  <data key="d0">The next decade in AI: four steps towards robust artificial intelligence</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A research framework proposing four strategic steps to achieve more robust and reliable artificial intelligence systems over the next decade.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="How decoding strategies affect the verifiability of generated text">
  <data key="d0">How decoding strategies affect the verifiability of generated text</data>
  <data key="d1">Study Designs</data>
  <data key="d2">An empirical study investigating the impact of different decoding strategies on the trustworthiness and verifiability of generated language models.&lt;SEP&gt;An empirical study investigating the impact of different decoding strategies on the trustworthiness and verifiability of generated language.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Mixed precision training">
  <data key="d0">Mixed precision training</data>
  <data key="d1">Tools</data>
  <data key="d2">A computational approach that employs mixed numerical precisions during training of neural networks to enhance speed and reduce memory usage.&lt;SEP&gt;A training technique that employs mixed-precision arithmetic to accelerate neural network training while maintaining model accuracy.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb&lt;SEP&gt;chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Towards exploiting background knowledge for building conversation systems">
  <data key="d0">Towards exploiting background knowledge for building conversation systems</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A study exploring how leveraging background knowledge can improve the effectiveness of conversational AI systems.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kuchaiev, Ganesh Venkatesh, and Hao Wu">
  <data key="d0">Kuchaiev, Ganesh Venkatesh, and Hao Wu</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Researchers associated with mixed precision training, a technique to optimize neural network training by using different numerical precisions to improve efficiency and performance.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra">
  <data key="d0">Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers focused on exploiting background knowledge to improve the development of conversation systems in natural language processing.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Background knowledge in NLP">
  <data key="d0">Background knowledge in NLP</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The foundational information and contextual data used to enhance the performance of conversation systems and language understanding models.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Preksha Nema and Mitesh M. Khapra">
  <data key="d0">Preksha Nema and Mitesh M. Khapra</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers working on developing better evaluation metrics for question generation systems in NLP.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Question generation evaluation metrics">
  <data key="d0">Question generation evaluation metrics</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Methods and criteria used to assess the quality and effectiveness of automatically generated questions in NLP tasks.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng">
  <data key="d0">Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers who created MS MARCO, a large dataset for machine reading comprehension designed to evaluate and improve question answering models.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="MS MARCO dataset">
  <data key="d0">MS MARCO dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A human-generated machine reading comprehension dataset used to benchmark and develop question answering systems.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Rodrigo Nogueira and Kyunghyun Cho">
  <data key="d0">Rodrigo Nogueira and Kyunghyun Cho</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers focusing on passage re-ranking techniques using BERT to improve information retrieval performance.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Passage re-ranking with BERT">
  <data key="d0">Passage re-ranking with BERT</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that applies BERT-based models to reorder retrieved passages to enhance the relevance of search results.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli">
  <data key="d0">Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli</data>
  <data key="d1">Research Team</data>
  <data key="d2">Developers of fairseq, an extensible toolkit for sequence modeling in NLP, supporting various models and tasks.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="fairseq toolkit">
  <data key="d0">fairseq toolkit</data>
  <data key="d1">Tools</data>
  <data key="d2">A software toolkit designed for efficient training and evaluation of sequence models, including translation, language modeling, and more.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho">
  <data key="d0">Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers investigating methods to find generalizable evidence and improve Q&amp;A models' ability to convince or verify information.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Evidence verification in NLP">
  <data key="d0">Evidence verification in NLP</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Techniques and approaches aimed at ensuring the reliability and factual correctness of responses generated by language models.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Yuxiang Wu, and Alexander Miller">
  <data key="d0">Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Yuxiang Wu, and Alexander Miller</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers exploring whether language models can serve as knowledge bases, capturing factual information within their parameters.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Language models as knowledge bases">
  <data key="d0">Language models as knowledge bases</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The idea that large-scale language models can store and retrieve factual knowledge, functioning similarly to structured knowledge bases.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel">
  <data key="d0">Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers studying how context influences language models’ factual predictions and knowledge retrieval.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Contextual influence on language models">
  <data key="d0">Contextual influence on language models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Understanding how different contextual inputs affect the factual accuracy and predictions of language models.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever">
  <data key="d0">Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers who developed the original GPT models, pioneering advances in language understanding through generative pre-training.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Generative Pre-Training (GPT)">
  <data key="d0">Generative Pre-Training (GPT)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A pre-training approach that enables models to learn language representations in an unsupervised manner, facilitating transfer to various downstream tasks.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever">
  <data key="d0">Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers who advanced language models as unsupervised multitask learners, capable of performing multiple NLP tasks without task-specific training.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Unsupervised multitask learning in language models">
  <data key="d0">Unsupervised multitask learning in language models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A training paradigm where language models learn from large unlabeled corpora to perform multiple NLP tasks simultaneously.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu">
  <data key="d0">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu</data>
  <data key="d1">Research Team</data>
  <data key="d2">Researchers known for developing the T5 model, which frames NLP tasks as text-to-text problems to unify various tasks under a single framework.</data>
  <data key="d3">chunk-4abd23114af78f5d3ff5bf7b1056dbdb</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Unsupervised Multitask Learners">
  <data key="d0">Unsupervised Multitask Learners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A framework describing language models that learn multiple tasks without explicit supervision, enabling versatile language understanding.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Text-to-Text Transformer">
  <data key="d0">Text-to-Text Transformer</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A neural network architecture that converts various NLP tasks into a text-to-text format, facilitating transfer learning and unified modeling.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge Packing in Language Models">
  <data key="d0">Knowledge Packing in Language Models</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates how much information and knowledge can be embedded within the parameters of a language model, impacting its capacity and performance.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Probabilistic Relevance Framework">
  <data key="d0">Probabilistic Relevance Framework</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A theoretical framework for information retrieval that models relevance probabilistically, exemplified by BM25 and its extensions.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Social Impacts of Language Models">
  <data key="d0">Social Impacts of Language Models</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Explores how the deployment and release strategies of language models affect society, including ethical and social considerations.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Fact Extraction and Verification Dataset (FEVER)">
  <data key="d0">Fact Extraction and Verification Dataset (FEVER)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large-scale dataset created to evaluate systems on their ability to extract facts and verify information from texts.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Mitigating Model Biases">
  <data key="d0">Mitigating Model Biases</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates methods like elastic weight consolidation to prevent catastrophic forgetting and reduce biases in sentence-pair classification models.&lt;SEP&gt;Systematic errors or prejudices in language models that can affect their outputs and fairness.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Attention Mechanism">
  <data key="d0">Attention Mechanism</data>
  <data key="d1">Tools</data>
  <data key="d2">A neural network component that dynamically focuses on relevant parts of input data, foundational to models like Transformers, enabling better context understanding.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Diverse Beam Search">
  <data key="d0">Diverse Beam Search</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A decoding strategy that enhances the diversity and quality of generated text by exploring multiple candidate sequences during inference.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="GLUE Benchmark">
  <data key="d0">GLUE Benchmark</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A multi-task benchmark suite for evaluating the performance of NLP models across various language understanding tasks.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="SuperGLUE Benchmark">
  <data key="d0">SuperGLUE Benchmark</data>
  <data key="d1">Study Designs</data>
  <data key="d2">An advanced benchmark designed to provide a more challenging and comprehensive evaluation of general-purpose language understanding systems.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="NLP Research">
  <data key="d0">NLP Research</data>
  <data key="d1">Disciplines</data>
  <data key="d2">The interdisciplinary field focused on developing computational models to understand, interpret, and generate human language.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Model Biases">
  <data key="d0">Model Biases</data>
  <data key="d1">Variables</data>
  <data key="d2">Systematic errors or prejudices in language models that can affect their outputs and fairness.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge in Language Models">
  <data key="d0">Knowledge in Language Models</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount and scope of information stored within a model's parameters, influencing its capacity to perform various tasks.</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Alché-Buc, E. Fox, and R. Garnett">
  <data key="d0">Alché-Buc, E. Fox, and R. Garnett</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Editors associated with a publication on neural information processing systems, indicating their role in advancing knowledge in neural information processing.&lt;SEP&gt;Editors of a publication presenting advances in neural information processing, contributing to the foundational understanding of neural data processing and systems.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Advances in Neural Information Processing Systems 32">
  <data key="d0">Advances in Neural Information Processing Systems 32</data>
  <data key="d1">Study Design</data>
  <data key="d2">A comprehensive conference proceeding compiling research papers, methodologies, and advancements in neural information processing, serving as a key event for disseminating cutting-edge research.&lt;SEP&gt;A conference proceeding that compiles research papers and advancements in neural information processing, serving as a platform for disseminating new methodologies and findings.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Shuohang Wang">
  <data key="d0">Shuohang Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to prompt reliability, published in 2022.&lt;SEP&gt;A researcher contributing to prompt reliability, published in 2022.&lt;SEP&gt;Author involved in developing reinforcement learning models for open-domain question answering, contributing to the methodologies in NLP.&lt;SEP&gt;Author of research on reinforcement learning for open-domain question answering, focusing on improving answer ranking and retrieval techniques in NLP.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b&lt;SEP&gt;chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mo Yu">
  <data key="d0">Mo Yu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Co-author contributing to reinforcement learning models and evidence aggregation methods for question answering systems.&lt;SEP&gt;Collaborator in research on reinforcement ranker-reader models and evidence aggregation for question answering, advancing NLP techniques.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Xiaoxiao Guo">
  <data key="d0">Xiaoxiao Guo</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to research on open-domain question answering, focusing on evidence aggregation and ranking models.&lt;SEP&gt;Researcher involved in evidence aggregation techniques to enhance answer re-ranking in open-domain question answering.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Zhiguo Wang">
  <data key="d0">Zhiguo Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in NLP studies related to question answering and evidence aggregation.&lt;SEP&gt;Researcher working on evidence aggregation and NLP model improvements for question answering.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Tim Klinger">
  <data key="d0">Tim Klinger</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to NLP research on question answering models.&lt;SEP&gt;Researcher contributing to evidence aggregation strategies in NLP for question answering.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Shiyu Chang">
  <data key="d0">Shiyu Chang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Contributor to NLP research focusing on question answering and evidence aggregation techniques.&lt;SEP&gt;Researcher working on evidence aggregation and answer ranking techniques in NLP.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Gerry Tesauro">
  <data key="d0">Gerry Tesauro</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in reinforcement learning and NLP model development.&lt;SEP&gt;Researcher known for developing or contributing to memory networks, which enhance reasoning and memory capabilities in NLP models.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Bowen Zhou">
  <data key="d0">Bowen Zhou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to NLP advancements in question answering systems.&lt;SEP&gt;Researcher involved in NLP advancements, particularly in question answering and evidence aggregation.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jing Jiang">
  <data key="d0">Jing Jiang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Author involved in open-domain question answering research, specifically in evidence aggregation.&lt;SEP&gt;Researcher focused on open-domain question answering, evidence aggregation, and answer reranking methodologies.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d0">Evidence aggregation for answer re-ranking in open-domain question answering</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique that combines multiple evidence sources to improve the accuracy and ranking of answers in open-domain QA systems.&lt;SEP&gt;A technique that combines multiple pieces of evidence to improve the ranking and accuracy of answers in open-domain question answering systems.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Memory networks">
  <data key="d0">Memory networks</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A neural network architecture designed to enhance reasoning and memory capabilities in NLP tasks, introduced by Weston et al.&lt;SEP&gt;A neural network architecture designed to improve reasoning and memory in NLP, facilitating complex question answering and reasoning tasks.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieve and refine models for dialogue">
  <data key="d0">Retrieve and refine models for dialogue</data>
  <data key="d1">Methodology</data>
  <data key="d2">A sequence generation approach that retrieves relevant information and refines responses for improved dialogue systems.&lt;SEP&gt;An approach that improves sequence generation in dialogue systems through retrieval and refinement techniques.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Huggingface’s transformers">
  <data key="d0">Huggingface’s transformers</data>
  <data key="d1">Tools</data>
  <data key="d2">A library providing state-of-the-art NLP models and architectures, facilitating research and application development in NLP.&lt;SEP&gt;A library providing state-of-the-art NLP models, enabling researchers to implement advanced language models efficiently.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Addressing semantic drift in question generation">
  <data key="d0">Addressing semantic drift in question generation</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">Investigates how to mitigate semantic drift in semi-supervised question generation to improve question answering performance.&lt;SEP&gt;Research exploring how to mitigate semantic drift in semi-supervised question generation to improve the quality and relevance of generated questions.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Reasoning over semantic-level graph for fact checking">
  <data key="d0">Reasoning over semantic-level graph for fact checking</data>
  <data key="d1">Methodology</data>
  <data key="d2">A reasoning approach that employs semantic graphs to verify facts and improve fact-checking accuracy.&lt;SEP&gt;A reasoning approach that utilizes semantic graphs to verify facts and improve fact-checking accuracy.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wanjun Zhong">
  <data key="d0">Wanjun Zhong</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher focusing on semantic reasoning and fact checking using graph-based methods.&lt;SEP&gt;Researcher working on semantic graph reasoning and fact verification techniques.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jingjing Xu">
  <data key="d0">Jingjing Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher involved in semantic graph reasoning for fact checking and evidence validation.&lt;SEP&gt;Researcher involved in semantic-level reasoning and fact verification studies.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jian Yin">
  <data key="d0">Jian Yin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to semantic reasoning and fact checking research.&lt;SEP&gt;Researcher engaged in semantic reasoning and fact checking research.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jiahai Wang">
  <data key="d0">Jiahai Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Researcher contributing to semantic reasoning and fact verification using graph-based methods.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval-augmented generation">
  <data key="d0">Retrieval-augmented generation</data>
  <data key="d3">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d2">Pre-trained language models serve as the parametric component in RAG, providing a foundation for knowledge storage and language generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Seq2seq Models">
  <data key="d0">Seq2seq Models</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d2">RAG builds upon seq2seq models by integrating retrieval mechanisms to enhance knowledge-based generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="DPR">
  <data key="d0">DPR</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d2">DPR provides relevant documents to RAG, acting as the non-parametric memory source for retrieval.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="BART/T5">
  <data key="d0">BART/T5</data>
  <data key="d3">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d2">BART and T5 serve as the generative backbone in RAG, conditioned on retrieved documents to produce responses.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d2">The document encoder processes Wikipedia chunks to produce embeddings for similarity search."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="document encoder">
  <data key="d0">document encoder</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d2">The Wikipedia dump provides the corpus from which documents are encoded into embeddings for retrieval."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="MIPS index">
  <data key="d0">MIPS index</data>
  <data key="d3">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d2">FAISS is used to build the MIPS index that enables fast approximate nearest neighbor search of document embeddings."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open-domain QA Tasks">
  <data key="d0">Open-domain QA Tasks</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d2">RAG models are applied to open-domain question answering, demonstrating superior performance over previous models."|&lt;SEP&gt;RAG models are applied to open-domain question answering, demonstrating superior performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Open-domain QA Models">
  <data key="d0">Open-domain QA Models</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d2">Different variants of RAG models, such as RAG-Token and RAG-Sequence, are evaluated for performance on QA tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG-Token and RAG-Sequence">
  <data key="d0">RAG-Token and RAG-Sequence</data>
  <data key="d3">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d2">Different variants of RAG models, such as RAG-Token and RAG-Sequence, are evaluated for performance on QA tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="RAG-Token">
  <data key="d0">RAG-Token</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">RAG-Token performs better than RAG-Sequence on Jeopardy question generation, indicating its superior effectiveness in this task.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document Posterior">
  <data key="d0">Document Posterior</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">The posterior distribution over documents helps the model determine which source is most relevant during response generation, especially when combining content from multiple documents.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 1">
  <data key="d0">Document 1</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides historical context on American literature, relevant for verifying facts about authors and works.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Factual Context">
  <data key="d0">Factual Context</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides historical context on American literature, relevant for verifying facts about authors and works.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 2">
  <data key="d0">Document 2</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides historical context on 1920s expatriate artists, relevant for verifying facts about literary movements.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 3">
  <data key="d0">Document 3</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides biographical details about Hemingway's wartime experiences and novel 'A Farewell to Arms'.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 4">
  <data key="d0">Document 4</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides context on 1920s expatriate artists, supporting verification of related facts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Document 5">
  <data key="d0">Document 5</data>
  <data key="d3">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d2">Provides information about the author of 'The Sun Also Rises' and related works.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="claim classification">
  <data key="d0">claim classification</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d2">RoBERTa is used to classify claims as true or false based on evidence sentences.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Gold Evidence">
  <data key="d0">Gold Evidence</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d2">The top retrieved documents are compared to gold evidence annotations to measure retrieval accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Mechanism">
  <data key="d0">Retrieval Mechanism</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d2">BM25 is used as a baseline retrieval method to compare with learned retrievers in RAG.&lt;SEP&gt;BM25 is used as a baseline retrieval method to compare with learned retrievers like dense retrieval in RAG.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Index Updates">
  <data key="d0">Index Updates</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d2">The list of world leaders is used to evaluate the accuracy of RAG's knowledge when using different Wikipedia indices.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wikipedia Dumps">
  <data key="d0">Wikipedia Dumps</data>
  <data key="d3">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d2">Different Wikipedia snapshots (2016 and 2018) are used to evaluate how well RAG updates and maintains current knowledge.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Effect of Retrieving More Documents">
  <data key="d0">Effect of Retrieving More Documents</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d2">Retrieving more documents at test time can improve model performance metrics but may also affect runtime.&lt;SEP&gt;Retrieving more documents at test time influences model performance and runtime, with performance improvements up to a peak.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Retrieval Quantity and Performance">
  <data key="d0">Retrieval Quantity and Performance</data>
  <data key="d3">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d2">Figure 3 depicts how increasing retrieved documents influences model performance across different metrics and models.&lt;SEP&gt;Figure 3 illustrates the relationship between number of retrieved documents and performance metrics for different models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Simple and Effective Multi-Paragraph Reading Comprehension">
  <data key="d0">Simple and Effective Multi-Paragraph Reading Comprehension</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Clark's work on comprehension models relates to the broader field of question answering and reading comprehension."|&lt;SEP&gt;Clark's work relates to question answering and reading comprehension models."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="BERT: Pre-training of Deep Bidirectional Transformers">
  <data key="d0">BERT: Pre-training of Deep Bidirectional Transformers</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Devlin's BERT model is a foundational tool for language understanding, relevant to question answering."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wizard of Wikipedia">
  <data key="d0">Wizard of Wikipedia</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Dinan's work on knowledge-powered conversational agents relates to question answering and dialogue systems."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Neural Story Generation">
  <data key="d0">Neural Story Generation</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Fan's research on hierarchical neural models supports language understanding and generation tasks including QA."|&lt;SEP&gt;Fan's research on hierarchical neural story generation relates to narrative creation and language modeling."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Transformers with KNN-based Memory">
  <data key="d0">Transformers with KNN-based Memory</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Fan's work on augmenting transformers with memory enhances language understanding and generation."|&lt;SEP&gt;Fan's work on augmenting transformers with memory mechanisms enhances question answering and language understanding."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Entities as Experts">
  <data key="d0">Entities as Experts</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Févry's research on entity-based sparse memory access relates to knowledge representation in QA."|&lt;SEP&gt;Févry's research on sparse memory access with entity supervision relates to entity-based knowledge representation."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Knowledge-Grounded Neural Conversation">
  <data key="d0">Knowledge-Grounded Neural Conversation</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Ghazvininejad's research on knowledge-grounded models relates to conversational AI."|&lt;SEP&gt;Ghazvininejad's research supports knowledge grounding in neural dialogue and QA systems."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="AI exceeding human performance">
  <data key="d0">AI exceeding human performance</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">Grace's research involves expert surveys estimating when AI will surpass human intelligence."|&lt;SEP&gt;Grace's research involves expert surveys estimating when AI will surpass human performance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Tool">
  <data key="d0">Tool</data>
  <data key="d3">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d2">URL to the paper discussing AI surpassing human performance."|&lt;SEP&gt;URL to the paper on AI surpassing human performance."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="generalization through memorization">
  <data key="d0">generalization through memorization</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d2">contributes to</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Natural Questions dataset">
  <data key="d0">Natural Questions dataset</data>
  <data key="d3">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d2">contributes to</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Guillaume Lample et al.">
  <data key="d0">Guillaume Lample et al.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">The benchmark dataset and research framework provided by this study serve as a standard for evaluating question answering systems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Guillaume Lample et al.&quot;|&lt;&quot;The architecture proposes large memory layers to enhance neural network capacity, which can be applied in question answering models.">
  <data key="d0">Guillaume Lample et al."|&lt;"The architecture proposes large memory layers to enhance neural network capacity, which can be applied in question answering models.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">model architecture, memory</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Kenton Lee, Ming-Wei Chang, and Kristina Toutanova">
  <data key="d0">Kenton Lee, Ming-Wei Chang, and Kristina Toutanova</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This methodology improves open-domain QA performance by utilizing latent retrieval techniques in weakly supervised settings.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="BART: Denoising sequence-to-sequence pre-training">
  <data key="d0">BART: Denoising sequence-to-sequence pre-training</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">BART's pre-training approach enhances language understanding and generation, impacting various NLP tasks including question answering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Mike Lewis et al.">
  <data key="d0">Mike Lewis et al.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">BART's pre-training approach enhances language understanding and generation, impacting various NLP tasks including question answering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan">
  <data key="d0">Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This objective function aims to increase diversity in generated conversations, improving system engagement and relevance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Acute-eval: Improved dialogue evaluation">
  <data key="d0">Acute-eval: Improved dialogue evaluation</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This evaluation methodology improves assessment accuracy of dialogue systems through optimized questions and multi-turn comparisons.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Margaret Li, Jason Weston, and Stephen Roller">
  <data key="d0">Margaret Li, Jason Weston, and Stephen Roller</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This evaluation methodology improves assessment accuracy of dialogue systems through optimized questions and multi-turn comparisons.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He">
  <data key="d0">Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This approach enhances translation robustness by combining textual and phonetic features.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Peter J. Liu et al.">
  <data key="d0">Peter J. Liu et al.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This methodology advances automatic content generation by effectively summarizing lengthy sequences into encyclopedic articles.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Yury A. Malkov and D. A. Yashunin">
  <data key="d0">Yury A. Malkov and D. A. Yashunin</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This algorithm improves the speed and robustness of nearest neighbor searches in high-dimensional data.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Gary Marcus">
  <data key="d0">Gary Marcus</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This research proposes strategic steps to develop more reliable and robust AI systems in the future.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Luca Massarelli, Fabio Petroni, et al.">
  <data key="d0">Luca Massarelli, Fabio Petroni, et al.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This empirical study examines how different decoding methods influence the trustworthiness of generated language.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Paulius Micikevicius et al.">
  <data key="d0">Paulius Micikevicius et al.</data>
  <data key="d3">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d2">This technique accelerates neural network training while preserving accuracy, applicable across various NLP models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Question Answering Tasks">
  <data key="d0">Question Answering Tasks</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d2">End-to-end memory networks are designed to improve reasoning over sequences, making them suitable for tasks like fact extraction and verification.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="NLP Models">
  <data key="d0">NLP Models</data>
  <data key="d3">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d2">GLUE provides a standardized platform to evaluate and compare the performance of NLP models across multiple tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="R3: Reinforced ranker-reader for open-domain question answering">
  <data key="d0">R3: Reinforced ranker-reader for open-domain question answering</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d2">Shuohang Wang authored the paper presenting the R3 model, which enhances answer retrieval in open-domain QA.&lt;SEP&gt;Shuohang Wang is the lead author of this paper, which introduces a reinforcement learning model for question answering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Jason Weston, Emily Dinan, and Alexander Miller">
  <data key="d0">Jason Weston, Emily Dinan, and Alexander Miller</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d2">These researchers collaborated on models that retrieve relevant information and refine responses for dialogue systems.&lt;SEP&gt;These researchers worked on improving sequence generation models for dialogue systems through retrieval and refinement techniques.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Thomas Wolf et al.">
  <data key="d0">Thomas Wolf et al.</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d2">Thomas Wolf and colleagues developed the transformers library, a tool for NLP model implementation.&lt;SEP&gt;Thomas Wolf and colleagues developed the transformers library, providing tools for NLP research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Shiyue Zhang and Mohit Bansal">
  <data key="d0">Shiyue Zhang and Mohit Bansal</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d2">They conducted research to address semantic drift issues in semi-supervised question generation for improved QA.&lt;SEP&gt;They conducted research to address semantic drift issues in semi-supervised question generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Wanjun Zhong, Jingjing Xu, Jiahai Wang, and Jian Yin">
  <data key="d0">Wanjun Zhong, Jingjing Xu, Jiahai Wang, and Jian Yin</data>
  <data key="d3">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d2">These researchers collaborated on methods that perform reasoning over semantic graphs to verify facts.&lt;SEP&gt;These researchers collaborated on methods to perform reasoning over semantic graphs for fact verification.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</node>
<node id="Parallel Pattern Language (PPL)">
  <data key="d0">Parallel Pattern Language (PPL)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A high-level descriptive pattern language designed to define parallelism and facilitate static global optimizations in high-performance computing (HPC) codes, enabling automatic assignment of workloads to heterogeneous architectures.&lt;SEP&gt;A high-level, descriptive language for defining parallelism and enabling static global optimizations in scientific software development.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory and Power Walls">
  <data key="d0">Memory and Power Walls</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Fundamental limitations in HPC systems caused by memory bandwidth and power consumption constraints, which restrict scaling and performance improvements.&lt;SEP&gt;Fundamental limitations in high-performance computing systems caused by memory bandwidth and power consumption constraints, which restrict system scaling and performance improvements.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Heterogeneous Hardware">
  <data key="d0">Heterogeneous Hardware</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Hardware systems combining CPUs, GPUs, and NUMA architectures used in HPC, characterized by varied memory and processing units, increasing development complexity.&lt;SEP&gt;Hardware systems combining CPUs, GPUs, and NUMA architectures used in HPC, characterized by varied processing units and memory hierarchies, increasing development complexity.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Non-Uniform Memory Architecture (NUMA)">
  <data key="d0">Non-Uniform Memory Architecture (NUMA)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A memory architecture where memory access latency varies depending on the memory location relative to the processor, affecting software optimization and performance.&lt;SEP&gt;A memory architecture where memory access time varies depending on the memory location relative to the processor, impacting software development and optimization.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Accelerator Offloading">
  <data key="d0">Accelerator Offloading</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of transferring computation tasks from the CPU to specialized hardware accelerators like GPUs to improve performance and energy efficiency.&lt;SEP&gt;The process of transferring computational tasks from the CPU to accelerators like GPUs to improve efficiency and performance in HPC applications.&lt;SEP&gt;Using hardware accelerators like GPUs to offload compute-intensive tasks, improving performance and efficiency in parallel applications.&lt;SEP&gt;Using hardware accelerators such as GPUs to offload computationally intensive tasks, improving performance and efficiency.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Code Generator">
  <data key="d0">Code Generator</data>
  <data key="d1">Tools</data>
  <data key="d2">A code generator automates the creation of optimized parallel code based on high-level descriptions and transformations, facilitating efficient GPU programming.&lt;SEP&gt;A code generator automatically produces optimized code from input representations, playing a key role in compiler and transformation pipelines.&lt;SEP&gt;A code generator automatically translates high-level source code into optimized, architecture-specific code, enabling performance and portability.&lt;SEP&gt;A code generator creates optimized code based on high-level descriptions and transformations, facilitating parallel code production for GPU architectures.&lt;SEP&gt;A software component that automatically produces source code based on high-level parallel patterns, facilitating optimization for heterogeneous architectures.&lt;SEP&gt;A software component that automatically produces source code based on high-level specifications, in this case generating optimized parallel code for diverse architectures.&lt;SEP&gt;A software component that automatically produces source code based on models or specifications, used here for the PPLprototype toolchain to facilitate code creation and optimization.&lt;SEP&gt;A software component that automatically produces source code based on specified inputs or models, used here for the PPLprototype toolchain.&lt;SEP&gt;The code generator aims to optimize performance in heterogeneous systems by supporting shared/distributed memory, workload pinning, data movement, and synchronization, using CUDA, MPI, and pthreads.&lt;SEP&gt;The code generator optimizes performance of heterogeneous systems by supporting shared/distributed memory, workload pinning, data movement, and synchronization, employing CUDA, MPI, and pthreads.&lt;SEP&gt;The code generator translates high-level parallel patterns into optimized code for CPU and GPU, aiming to improve performance and portability.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-5f6ee49c82aaaf22c167cac791d43a0f&lt;SEP&gt;chunk-b943bff32a36cd0ce9ab43893da92b5b&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Rodinia Benchmark Suite">
  <data key="d0">Rodinia Benchmark Suite</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A collection of benchmark kernels used to evaluate parallel programming tools and frameworks.&lt;SEP&gt;A collection of high-performance computing benchmarks used for evaluating the performance and scalability of scientific codes, particularly in heterogeneous environments.&lt;SEP&gt;A collection of high-performance computing benchmarks used to evaluate the performance and scalability of scientific applications on heterogeneous systems.&lt;SEP&gt;A comprehensive set of benchmarks for evaluating heterogeneous computing systems and architectures.&lt;SEP&gt;A set of benchmark kernels used to evaluate the performance and portability of parallel programming frameworks like the PPL.&lt;SEP&gt;A suite of benchmark applications designed to evaluate the performance and scalability of heterogeneous computing systems.&lt;SEP&gt;Rodinia is a suite of parallel benchmarks used to evaluate optimization techniques, performance portability, and scalability.&lt;SEP&gt;The Rodinia benchmark suite includes a collection of programs designed to evaluate the performance of parallel computing systems.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-5cd0b27c19a2314eb0b2f819a6edfa2e&lt;SEP&gt;chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global Optimizations">
  <data key="d0">Global Optimizations</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Application-wide code transformations that improve performance by optimizing workload distribution, memory access, and data flow across entire applications.&lt;SEP&gt;Application-wide code transformations that improve performance by optimizing workload distribution, memory access, and data flow across the entire application.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Patterns">
  <data key="d0">Parallel Patterns</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Control-flow constructs within the APT that define how computations are executed in parallel, influencing data dependencies and compiler analysis.&lt;SEP&gt;Parallel patterns abstract parallelism, removing boilerplate code, and enhancing programming productivity and maintainability.&lt;SEP&gt;Parallel patterns abstract parallelism, removing boilerplate code, which improves programming productivity and maintainability for domain experts.&lt;SEP&gt;Parallel patterns are common structures in parallel programming that facilitate analysis, optimization, and implementation of concurrent tasks.&lt;SEP&gt;Parallel patterns are control-flow structures within the APT that define how computations are executed in parallel, influencing data dependencies and optimization strategies.&lt;SEP&gt;Parallel patterns are high-level computational templates, such as multiply or subtract, that operate element-wise on arrays, enabling structured parallelism.&lt;SEP&gt;Parallel patterns are structured approaches to executing computations concurrently, such as map, reduce, or stencil patterns, which are fundamental for optimizing GPU workloads.&lt;SEP&gt;Parallel patterns describe structured approaches to executing tasks concurrently, such as reduction, map, or stencil patterns, crucial for optimizing GPU computations.&lt;SEP&gt;Reusable abstractions representing common parallel computation structures, used to define and analyze parallelism in scientific software.&lt;SEP&gt;Reusable abstractions representing common parallel computation structures, used to define, analyze, and optimize parallelism in scientific codes.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1&lt;SEP&gt;chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Static Global Optimizations">
  <data key="d0">Static Global Optimizations</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Compiler techniques that analyze and optimize code at compile time to improve performance across heterogeneous systems without runtime overhead.&lt;SEP&gt;Compiler techniques that analyze and transform code at compile time to enhance performance and portability across heterogeneous systems.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Assignment between Tasklets and Architecture">
  <data key="d0">Assignment between Tasklets and Architecture</data>
  <data key="d1">Results</data>
  <data key="d2">A mapping calculated during compile time that assigns small computational units (tasklets) to specific hardware resources, enabling efficient execution.&lt;SEP&gt;A mapping computed during compile time that assigns small computational units (tasklets) to specific hardware resources, enabling efficient execution.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Shared Memory, Distributed Memory, GPU Offloading">
  <data key="d0">Shared Memory, Distributed Memory, GPU Offloading</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Different memory and computation paradigms targeted by the code generator to optimize performance on heterogeneous HPC systems.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Overheads during Compile Time Optimization">
  <data key="d0">Overheads during Compile Time Optimization</data>
  <data key="d1">Limitations</data>
  <data key="d2">Performance costs associated with analyzing and transforming code during compilation, especially for dynamic algorithms and complex global optimizations.&lt;SEP&gt;The performance overheads incurred during static code analysis and transformation, especially for dynamic algorithms and complex global optimizations.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Evaluation against Rodinia Benchmarks">
  <data key="d0">Evaluation against Rodinia Benchmarks</data>
  <data key="d1">Study Design</data>
  <data key="d2">Experimental assessment of the prototype's ability to optimize and compile HPC applications, measuring speedups and scalability in various configurations.&lt;SEP&gt;Experimental assessment of the prototype's ability to optimize and compile HPC applications, measuring speedups, scalability, and effectiveness across different configurations.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="High-Level IR (Intermediate Representation)">
  <data key="d0">High-Level IR (Intermediate Representation)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A semantic abstraction capturing parallelism and data flow, enabling effective global optimization and code generation for heterogeneous architectures.&lt;SEP&gt;An abstract semantic representation capturing parallelism and data flow, enabling effective global optimization and code generation for heterogeneous architectures.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global Optimization via Linear Programming (LP)">
  <data key="d0">Global Optimization via Linear Programming (LP)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An optimization approach used to determine the best assignment of workloads to hardware resources, though currently limited in the prototype's implementation.&lt;SEP&gt;An optimization approach used to determine the best assignment of workloads to hardware resources, though currently limited in the prototype.</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Linear Programming (LP)">
  <data key="d0">Linear Programming (LP)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">LP is a mathematical optimization technique used for global optimization problems, involving linear relationships among variables to find the best solution.&lt;SEP&gt;LP is an optimization technique used to find the best outcome in a mathematical model with linear relationships, often applied in global optimization problems.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PPLprototype">
  <data key="d0">PPLprototype</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A prototype implementation of a parallel programming toolchain focusing on code generation, inlining, and loop unrolling for optimization.&lt;SEP&gt;A prototype implementation of a parallel programming toolchain that includes code generation, inlining, and loop unrolling to improve performance and scalability.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Abstract Pattern Tree (APT)">
  <data key="d0">Abstract Pattern Tree (APT)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A data structure representing code patterns, used within the PPL to enable inlining and loop unrolling for optimization.&lt;SEP&gt;A data structure representing patterns in code to facilitate optimization techniques like inlining and loop unrolling.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LULESH proxy-app">
  <data key="d0">LULESH proxy-app</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A proxy application used to assess scalability and performance of parallel computing setups.&lt;SEP&gt;A proxy application used to assess scalability and performance of the parallel code generated by the PPL toolchain.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallelization Patterns">
  <data key="d0">Parallelization Patterns</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Recurring structures in parallel algorithms, such as map, reduction, and stencil, used to optimize performance across hardware.&lt;SEP&gt;Standard recurring structures in parallel algorithms such as map, reduction, and stencil, which are exploited for efficient parallel execution.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Optimization of Parallel Programs">
  <data key="d0">Optimization of Parallel Programs</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Approaches and techniques, including polyhedral compilation and GPU-specific optimizations, aimed at improving performance and scalability of parallel applications.&lt;SEP&gt;Techniques aimed at improving the efficiency and scalability of parallel applications, including polyhedral compilation and specialized GPU compilation.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Polyhedral Compilers">
  <data key="d0">Polyhedral Compilers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A class of compiler techniques that optimize loop nests with polyhedral models for parallel execution.&lt;SEP&gt;Compiler techniques that optimize loop nests with polyhedral models, often used to enhance parallel performance in nested loops.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GPU Code Optimization">
  <data key="d0">GPU Code Optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies and compiler techniques aimed at improving GPU kernel performance, including graph algorithms and specialized code transformations.&lt;SEP&gt;Techniques and compiler strategies to enhance GPU performance, often focusing on graph algorithms and kernel optimization.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Source-to-Source Compiler">
  <data key="d0">Source-to-Source Compiler</data>
  <data key="d1">Tools</data>
  <data key="d2">A compiler that transforms source code from one form into another, maintaining high-level structure, exemplified by the PPL toolchain which generates C++ code from DSL.&lt;SEP&gt;A compiler that transforms source code written in one language or paradigm into another, facilitating portability and optimization, as exemplified by the PPLtool.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C++ Source Code">
  <data key="d0">C++ Source Code</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The output of the source-to-source compiler, compatible with C++17 capable compilers, used to generate executable code.&lt;SEP&gt;The output of the source-to-source compiler, compatible with C++17, ready for compilation and execution.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Domain Specific Language (DSL)">
  <data key="d0">Domain Specific Language (DSL)</data>
  <data key="d1">Tools</data>
  <data key="d2">A specialized programming language designed to express application-specific optimizations, used within the PPL to facilitate targeted code transformations.&lt;SEP&gt;A specialized programming language designed to express particular concepts succinctly, used in PPL for specific optimizations.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="OpenSBLI">
  <data key="d0">OpenSBLI</data>
  <data key="d1">Tools</data>
  <data key="d2">A code generator for fluid dynamics simulations on unstructured grids, demonstrating application-specific code generation for heterogeneous architectures.&lt;SEP&gt;A code generator for fluid dynamics simulations on unstructured grids, optimized for heterogeneous architectures.&lt;SEP&gt;An automated code-generation framework for heterogeneous computing architectures applied to compressible fluid dynamics on structured grids.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04&lt;SEP&gt;chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Kempf et al.'s Code Generator">
  <data key="d0">Kempf et al.'s Code Generator</data>
  <data key="d1">Tools</data>
  <data key="d2">A code generator for the discontinuous Galerkin method, enabling vectorization and high-performance numerical computation.&lt;SEP&gt;A tool that generates code for the discontinuous Galerkin method, enabling vectorization for specific numerical methods.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Lift">
  <data key="d0">Lift</data>
  <data key="d1">Tools</data>
  <data key="d2">A framework that uses OpenCL patterns to generate GPU code, supporting heterogeneous hardware and parallel execution.&lt;SEP&gt;A framework that utilizes parallel patterns specified in OpenCL to generate GPU code, supporting heterogeneous hardware.&lt;SEP&gt;Lift is a functional data-parallel intermediate representation (IR) designed for high-performance GPU code generation, enabling efficient parallel processing.&lt;SEP&gt;Lift is a functional data-parallel intermediate representation designed to facilitate high-performance GPU code generation, enabling efficient parallel processing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="YASK">
  <data key="d0">YASK</data>
  <data key="d1">Tools</data>
  <data key="d2">A code generator focusing on optimized stencil kernels within DSL approaches.&lt;SEP&gt;A code generator focusing on optimized stencil kernels within a DSL framework, targeting performance on various architectures.&lt;SEP&gt;YASK (Yet Another Stencil Kernel) is a framework designed for generating and tuning high-performance stencil code for scientific computing.&lt;SEP&gt;YASK (Yet Another Stencil Kernel) is a framework for high-performance stencil code generation and tuning used in scientific computing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450&lt;SEP&gt;chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Raja">
  <data key="d0">Raja</data>
  <data key="d1">Libraries</data>
  <data key="d2">A performance portability library for HPC applications, enabling efficient execution on diverse hardware architectures.&lt;SEP&gt;A performance portability library for HPC applications, targeting complete node-level parallelism.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="SDFGs and the DaCe Tool">
  <data key="d0">SDFGs and the DaCe Tool</data>
  <data key="d1">Tools</data>
  <data key="d2">A rule-based optimization framework for data-centric representations of computations, facilitating automated or semi-automated hotspot optimization.&lt;SEP&gt;A rule-based, semi-automatic optimization framework that applies transformations to hotspots in data-centric representations of computations to improve performance.</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Distributed Memory">
  <data key="d0">Distributed Memory</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Distributed memory refers to a system architecture where memory is physically distributed across multiple nodes, requiring explicit data movement and management for parallel computations.&lt;SEP&gt;Distributed memory systems have separate local memories on each node, requiring explicit data communication, often implemented using MPI.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="SDFGs">
  <data key="d0">SDFGs</data>
  <data key="d1">Tools</data>
  <data key="d2">SDFGs (Stateful DataFlow Graphs) are a representation used to optimize and manage data movement and computation in high-performance computing, enabling rule-based optimizations for hotspots.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="DaCe">
  <data key="d0">DaCe</data>
  <data key="d1">Tools</data>
  <data key="d2">DaCe (Data-Centric) is a tool that utilizes SDFGs for optimizing computational workflows, focusing on applying rule-based transformations to improve performance in HPC applications.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="HPC Expert">
  <data key="d0">HPC Expert</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An HPC expert is a specialist with domain knowledge who defines optimization rules and strategies for high-performance computing applications.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PPL (Parallel Pattern Language)">
  <data key="d0">PPL (Parallel Pattern Language)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">PPL is a pattern-based approach for representing and optimizing algorithms in parallel computing, using a modular toolchain to verify applicability in real-world environments.&lt;SEP&gt;PPL is an approach for static code optimization that enables automatic, global optimization across heterogeneous architectures from a single source.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Pattern-based Approach">
  <data key="d0">Pattern-based Approach</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A pattern-based approach models algorithms as compositions of parallel patterns, facilitating global optimizations and automatic code generation for high-performance applications.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Toolchain">
  <data key="d0">Toolchain</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A modular source-to-source compiler with five stages: parsing, APT generation, optimization, AMT generation, and code generation, designed to verify and implement pattern-based parallel algorithms.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parsing">
  <data key="d0">Parsing</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The parsing stage evaluates user input in DSL and hardware description, serving as the initial step in the compilation pipeline.&lt;SEP&gt;The parsing stage evaluates user input, including application sources in a domain-specific language (DSL) and hardware descriptions in JSON, serving as the initial step in the compilation pipeline.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="APT (Abstract Pattern Tree)">
  <data key="d0">APT (Abstract Pattern Tree)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The APT is a hierarchical representation of an algorithm built from parallel patterns, used to perform global optimizations and generate efficient code.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="AMT (Abstract Mapping Tree)">
  <data key="d0">AMT (Abstract Mapping Tree)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An extended hierarchical model that incorporates optimization and mapping data, representing distributed, heterogeneous, and concurrent execution with tasklet assignments, synchronization, and data transfer nodes.&lt;SEP&gt;The AMT extends the APT by incorporating optimization and mapping data, representing distributed, concurrent execution, and tasklet assignments across hardware components.&lt;SEP&gt;The AMT is generated from optimized APTs to include necessary synchronization and data transfers, serving as an intermediate representation for code generation.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8&lt;SEP&gt;chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="DSL (Domain-Specific Language)">
  <data key="d0">DSL (Domain-Specific Language)</data>
  <data key="d1">Tools</data>
  <data key="d2">A custom DSL inspired by CUDA notation, used to define parallel patterns and applications in a mathematical-algorithmic style, facilitating static array size specification and data dependency analysis.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hardware Language (HWL)">
  <data key="d0">Hardware Language (HWL)</data>
  <data key="d1">Tools</data>
  <data key="d2">A JSON-based description of target hardware, enabling the compiler to generate hardware-aware optimized code and map computations across cores and devices.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Function-inlining/Loop-unrolling">
  <data key="d0">Function-inlining/Loop-unrolling</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Code optimization techniques applied during code generation to inline functions and unroll loops, enhancing execution performance.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Optimization Strategy">
  <data key="d0">Optimization Strategy</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A rule-based optimization strategy involves applying predefined rules to identify and improve hotspots in computation, often performed by HPC experts, with options for automatic or semi-automatic execution.</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Execution Units">
  <data key="d0">Execution Units</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Execution units are defined as groups of processor cores that can be arbitrarily divided from the device, allowing for flexible parallelization and resource allocation.&lt;SEP&gt;Execution units are hardware components within a CPU that perform instruction processing, grouped loosely to facilitate parallel task execution.&lt;SEP&gt;Execution units are hardware components within a CPU that perform instruction processing; they are loosely grouped for parallel task execution.&lt;SEP&gt;Groups of processor cores defined by the program to enable flexible parallel execution, allowing for arbitrary division of device resources.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8&lt;SEP&gt;chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="APT (Abstract Program Tree)">
  <data key="d0">APT (Abstract Program Tree)</data>
  <data key="d1">hierarchical representation</data>
  <data key="d2">A hierarchical high-level representation of parallel code, categorizing nodes into expressions and statements, used for program analysis and optimization.&lt;SEP&gt;The APT is a high-level hierarchical model of parallel code, categorizing nodes into expressions and statements, used for program analysis and optimization.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Dependencies">
  <data key="d0">Data Dependencies</data>
  <data key="d1">data flow</data>
  <data key="d2">Data dependencies specify the relationships between expressions in the APT, which are essential for static dependency analysis and optimizing parallel execution.&lt;SEP&gt;Relationships between expressions in the APT that determine the order of computations and enable static dependency analysis for parallel execution.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global Optimization">
  <data key="d0">Global Optimization</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Application-level optimization techniques such as loop fusion and data flow optimization applied during compile time to improve performance.&lt;SEP&gt;Global optimization involves applying application-level changes such as loop fusion and data flow improvements, utilizing algorithms to enhance efficiency during compile time.&lt;SEP&gt;Global optimization involves mathematical and computational techniques to find the best possible solution across all feasible options, often used to optimize parallel execution patterns in GPU programming.&lt;SEP&gt;Global optimization involves techniques to find the best solution across all possible configurations, often used to enhance parallel pattern efficiency in GPU computing.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Synchronization Efficiency">
  <data key="d0">Synchronization Efficiency</data>
  <data key="d1">performance metric</data>
  <data key="d2">A performance metric quantifying how well dependency-based synchronization is minimized to maximize parallelism and reduce overhead.&lt;SEP&gt;Synchronization efficiency measures how well dependency-based synchronization is minimized to maximize parallelism and reduce overhead during execution.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Inter-Processor Dataflow Efficiency">
  <data key="d0">Inter-Processor Dataflow Efficiency</data>
  <data key="d1">performance metric</data>
  <data key="d2">A measure of how effectively data transfer and execution costs are minimized across multiple processing units, optimizing overall runtime.&lt;SEP&gt;This efficiency assesses the minimization of data transfer and execution costs across multiple processing units, optimizing overall runtime.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intra-Processor Dataflow Efficiency">
  <data key="d0">Intra-Processor Dataflow Efficiency</data>
  <data key="d1">performance metric</data>
  <data key="d2">A metric assessing data reuse within the same execution unit to improve cache performance and reduce data movement.&lt;SEP&gt;Intra-processor dataflow efficiency aims to maximize data reuse within the same execution unit, reducing cache misses and improving performance.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Tasklets">
  <data key="d0">Tasklets</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Partitioned units of work derived from parallel patterns, mapped onto hardware resources for optimized execution.&lt;SEP&gt;Tasklets are partitioned units of work derived from parallel patterns, which are mapped onto hardware resources and optimized for execution.&lt;SEP&gt;Tasklets are small units of work, represented as lambda functions, that can be scheduled and executed independently in parallel.&lt;SEP&gt;Tasklets are small, independent units of work that can be scheduled and executed in parallel, often represented as lambda functions for deferred execution.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8&lt;SEP&gt;chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Mapping and Assignments">
  <data key="d0">Mapping and Assignments</data>
  <data key="d1">resource allocation</data>
  <data key="d2">Data structures that specify how tasklets are allocated to specific hardware units such as cores or GPU groups, enabling resource optimization.&lt;SEP&gt;Mapping data assigns tasklets to specific hardware units such as cores or GPU groups, optimizing utilization and reducing synchronization.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Transfer and Synchronization">
  <data key="d0">Data Transfer and Synchronization</data>
  <data key="d1">communication activities</data>
  <data key="d2">Extensions to the data structure include nodes for data transfers and synchronization, derived iteratively based on data flow and mapping results.&lt;SEP&gt;Nodes within the AMT that represent data movement and synchronization activities, derived iteratively from data flow and mapping information, facilitating communication in heterogeneous systems.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GPU Memory Management">
  <data key="d0">GPU Memory Management</data>
  <data key="d1">hardware resource</data>
  <data key="d2">Special nodes in the AMT that model offloading computations to GPUs and managing GPU memory resources for efficient heterogeneous execution.&lt;SEP&gt;Special nodes represent offloaded parallel patterns and manage GPU memory, enabling heterogeneous execution across CPU and GPU.</data>
  <data key="d3">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="synchronization">
  <data key="d0">synchronization</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Synchronization is the process of coordinating the sequence of computations and data transfers across different hardware units, such as CPUs and GPUs, to ensure correct execution and data consistency in heterogeneous systems.&lt;SEP&gt;Synchronization refers to coordinating the sequence of computations and data transfers across different hardware units to ensure correct execution and data consistency, especially in heterogeneous systems involving CPUs and GPUs.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="data flow">
  <data key="d0">data flow</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Data flow describes the movement and transformation of data between different computation units, which is fundamental to understanding dependencies and optimizing performance in parallel computing.&lt;SEP&gt;Data flow refers to the movement and transformation of data between computation units, which is fundamental to understanding dependencies and optimizing performance in parallel and distributed systems.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Device local dependencies">
  <data key="d0">Device local dependencies</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Dependencies within local device memory can be resolved by synchronization assuming shared memory, enabling pattern-independent read/write accesses.&lt;SEP&gt;Device local dependencies can be resolved by synchronization assuming shared memory systems, allowing independent read and write accesses without additional synchronization.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Remote write accesses">
  <data key="d0">Remote write accesses</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Remote write accesses do not require synchronization because they do not cause write-after-write data races on separated device memory, but require invalidation of copies.&lt;SEP&gt;Remote write accesses do not require synchronization since they do not cause write-after-write data races on separated device memory, but necessitate invalidating copies of written data.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Remote read accesses">
  <data key="d0">Remote read accesses</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Remote read accesses require synchronization and data transfer, often creating copies on different devices to reduce network traffic, at the expense of increased memory usage.&lt;SEP&gt;Remote read accesses require synchronization and data transfer, often involving creating new data copies on different devices to reduce network traffic, at the cost of increased memory usage.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GPU memory">
  <data key="d0">GPU memory</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">GPUs have limited memory capacity, holding only data required for subsequent computations, which influences data management strategies in parallel processing.&lt;SEP&gt;GPUs have limited memory capacity, which means they only hold data required for subsequent computations, impacting data management and transfer strategies.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel patterns">
  <data key="d0">Parallel patterns</data>
  <data key="d1">Tools</data>
  <data key="d2">Parallel patterns are implemented using a combination of thread pools, CUDA, and MPI to support various synchronization and data transfer scenarios across hardware units.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hierarchical synchronization">
  <data key="d0">Hierarchical synchronization</data>
  <data key="d1">Methods</data>
  <data key="d2">Synchronization is managed at multiple levels: intra-device (within a device), intra-node (across accelerators and host), and inter-device (across distributed GPUs), ensuring data consistency across the system.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Shared memory parallelism">
  <data key="d0">Shared memory parallelism</data>
  <data key="d1">Discipline</data>
  <data key="d2">Shared memory parallelism is supported via POSIX threads, with threads pinned to specific cores to enable efficient workload distribution and minimize data transfer overhead.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic load balancing">
  <data key="d0">Dynamic load balancing</data>
  <data key="d1">Limitations</data>
  <data key="d2">Dynamic load balancing is not yet integrated but could be employed in future to handle irregular or adaptive workloads, such as graph algorithms or mesh refinement.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="distance">
  <data key="d0">distance</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The 'distance' between execution units determines the class of dependencies and the required synchronization mechanisms, influencing data transfer and coherence strategies.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel pattern language">
  <data key="d0">Parallel pattern language</data>
  <data key="d1">Tools</data>
  <data key="d2">The parallel pattern language (PMAM ’24) provides a framework for implementing parallel patterns with synchronization and data transfer support across hardware units.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hardware offloading">
  <data key="d0">Hardware offloading</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Offloading involves transferring computations from the CPU to accelerators like GPUs to improve performance and resource utilization.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Shared memory">
  <data key="d0">Shared memory</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Shared memory refers to memory accessible by multiple processing units, enabling fast data sharing and synchronization within a device.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intra-device synchronization">
  <data key="d0">Intra-device synchronization</data>
  <data key="d1">Methods</data>
  <data key="d2">Synchronization within a single device, such as within a GPU or CPU, to coordinate parallel tasks and ensure data consistency.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intra-node synchronization">
  <data key="d0">Intra-node synchronization</data>
  <data key="d1">Methods</data>
  <data key="d2">Synchronization across accelerators and host within the same node, managing data exchange and task coordination.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Inter-device synchronization">
  <data key="d0">Inter-device synchronization</data>
  <data key="d1">Methods</data>
  <data key="d2">Synchronization between distributed devices, such as multiple GPUs, often involving data transfers and coordination protocols.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Thread pool">
  <data key="d0">Thread pool</data>
  <data key="d1">Tools</data>
  <data key="d2">A thread pool manages task execution and synchronization across multiple threads, supporting workload pinning and parallel execution.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PThreads">
  <data key="d0">PThreads</data>
  <data key="d1">Tools</data>
  <data key="d2">POSIX threads (PThreads) provide a programming interface for creating and managing threads in shared memory systems, supporting parallel execution and synchronization.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Pinning of workload">
  <data key="d0">Pinning of workload</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Assigning specific tasks or work units to particular cores or processing units to optimize resource utilization and reduce overhead.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data movement">
  <data key="d0">Data movement</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies for transferring data between devices and memory hierarchies to support computation dependencies and optimize performance.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Support for patterns">
  <data key="d0">Support for patterns</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Implementation of various parallel execution patterns (e.g., reduction, task parallelism) across hardware architectures.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Support for shared, distributed memory, and offloading models">
  <data key="d0">Support for shared, distributed memory, and offloading models</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Ensuring the code generator supports multiple programming models to enable flexible and efficient heterogeneous system utilization.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Assignment of work to execution units">
  <data key="d0">Assignment of work to execution units</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Mechanisms for mapping computational tasks to specific hardware units, considering system structure and optimization goals.</data>
  <data key="d3">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Shared Memory Parallelism">
  <data key="d0">Shared Memory Parallelism</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Shared memory parallelism is a programming approach where multiple processing cores execute tasks simultaneously, utilizing shared memory resources to improve performance.&lt;SEP&gt;Shared memory parallelism refers to the programming approach where multiple processing cores execute tasks simultaneously, utilizing shared memory resources to enhance performance.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="POSIX Threads">
  <data key="d0">POSIX Threads</data>
  <data key="d1">Tools</data>
  <data key="d2">POSIX threads are a standardized API for creating and managing multiple threads within a process, enabling parallel execution.&lt;SEP&gt;POSIX threads are an API for creating and managing multiple threads within a process, enabling parallel execution.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="HWL">
  <data key="d0">HWL</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Hardware Layer (HWL) defines the hardware configuration, including the number of cores and their grouping, which influences parallel programming strategies.&lt;SEP&gt;The Hardware Layer (HWL) defines the hardware specifications, including the number of cores and their configuration, which influence parallel programming strategies.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Thread Pinning">
  <data key="d0">Thread Pinning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Thread pinning involves assigning specific threads to designated CPU cores to optimize cache usage and reduce context switching.&lt;SEP&gt;Thread pinning involves assigning threads to specific CPU cores to optimize cache locality and reduce context switches.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hyper-threading">
  <data key="d0">Hyper-threading</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Hyper-threading allows a single physical core to appear as multiple logical cores, but it is currently disregarded in this context.&lt;SEP&gt;Hyper-threading is a technology that allows a single CPU core to appear as multiple logical cores, but it is currently disregarded in this context.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Lambda Functions">
  <data key="d0">Lambda Functions</data>
  <data key="d1">Tools</data>
  <data key="d2">Lambda functions are anonymous functions used to encapsulate deferred tasks, allowing flexible scheduling and execution within a parallel framework.&lt;SEP&gt;Lambda functions are anonymous functions used to encapsulate deferred tasks, allowing flexible scheduling and execution within parallel frameworks.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Race">
  <data key="d0">Data Race</data>
  <data key="d1">Results</data>
  <data key="d2">Data races are undesirable conditions where multiple threads access shared data simultaneously, leading to inconsistent results; avoided through synchronization.&lt;SEP&gt;Data races occur when multiple threads access shared data simultaneously without proper synchronization, leading to inconsistent or incorrect results; avoided through synchronization mechanisms.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="pthread_setaffinity_np">
  <data key="d0">pthread_setaffinity_np</data>
  <data key="d1">Tools</data>
  <data key="d2">This Unix kernel callback function assigns threads to specific CPU cores, enabling thread pinning for optimized parallel execution.&lt;SEP&gt;This function assigns threads to specific CPU cores, enabling thread pinning for performance optimization.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Movement">
  <data key="d0">Data Movement</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Data transfers are explicitly managed via MPI communication or by the operating system, ensuring data consistency across nodes and accelerators.&lt;SEP&gt;Data transfers between nodes or devices are managed explicitly via MPI communication or by the operating system, ensuring data locality and consistency.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Synchronization">
  <data key="d0">Synchronization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Coordination points in code that ensure correct execution order, which can introduce overhead but are necessary for correctness.&lt;SEP&gt;Synchronization ensures correct execution order among GPU tasks, typically using CUDA streams or shared memory barriers to coordinate concurrent operations and data transfers.&lt;SEP&gt;Synchronization ensures correct execution order among GPU tasks, typically using streams or shared memory barriers to coordinate concurrent operations.&lt;SEP&gt;Synchronization mechanisms like barriers coordinate execution across threads and nodes, preventing data races and ensuring data consistency.&lt;SEP&gt;Synchronization mechanisms like barriers ensure coordinated execution across threads or nodes, preventing data races and ensuring data consistency.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Barrier Tasks">
  <data key="d0">Barrier Tasks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Barrier tasklets are synchronization points added to thread queues, ensuring that multiple threads reach a certain point before proceeding.&lt;SEP&gt;Barrier tasklets are synchronization points inserted into thread queues, ensuring all involved threads reach the barrier before continuing.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="pthread_barrier_wait">
  <data key="d0">pthread_barrier_wait</data>
  <data key="d1">Tools</data>
  <data key="d2">A POSIX function that blocks threads until a specified number have reached the barrier, facilitating synchronization.&lt;SEP&gt;A POSIX function that blocks threads until the specified number of threads have reached the barrier, enabling synchronization.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Pattern Implementation">
  <data key="d0">Pattern Implementation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Decomposition of computational patterns such as map, stencil, and reduce into parallel tasklets facilitates efficient execution.&lt;SEP&gt;Decomposition of computational patterns such as map, stencil, and reduce into parallel tasklets for efficient execution.&lt;SEP&gt;Pattern implementation in CUDA involves defining how computational tasks are divided among threads, including execution ranges and iteration assignment for load balancing.&lt;SEP&gt;Pattern implementation involves defining how computational tasks are divided among GPU threads, including setting execution ranges and workload distribution strategies.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Map Pattern">
  <data key="d0">Map Pattern</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A computational pattern where operations are performed independently on elements of a dataset, suitable for parallelization.&lt;SEP&gt;A pattern where operations are performed independently on each element of a dataset, suitable for parallel execution.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Stencil Pattern">
  <data key="d0">Stencil Pattern</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A pattern involving neighborhood-based computations over data grids, decomposed into tasklets for parallel processing.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Reduce Pattern">
  <data key="d0">Reduce Pattern</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A pattern that aggregates data by combining elements into a single value, decomposed into map and reduce steps for parallel execution.&lt;SEP&gt;A pattern that aggregates data by combining elements to produce a single summary value, decomposed into map and reduce steps for parallel execution.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GASPI">
  <data key="d0">GASPI</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">GASPI (Global Address Space Programming Interface) is a communication model for high-performance computing, alternative to MPI, supporting asynchronous communication.&lt;SEP&gt;GASPI (Global Address Space Programming Interface) is a communication model supporting asynchronous, one-sided communication in high-performance computing.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="OpenSHMEM">
  <data key="d0">OpenSHMEM</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">OpenSHMEM is a programming library providing one-sided communication primitives for parallel systems with shared memory semantics across distributed nodes.&lt;SEP&gt;OpenSHMEM is a programming library providing one-sided communication primitives for parallel systems, supporting shared address space semantics across distributed nodes.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="MPI Reduction">
  <data key="d0">MPI Reduction</data>
  <data key="d1">Results</data>
  <data key="d2">MPI reduction is an explicit collective operation used to combine data across multiple nodes, essential for implementing the reduce pattern in distributed environments.&lt;SEP&gt;MPI reduction is an explicit collective operation used to combine data across nodes, essential for implementing the reduce pattern in distributed environments.&lt;SEP&gt;MPI reduction operations are used to combine data efficiently across processes, which future improvements plan to leverage more effectively.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Thread Pool">
  <data key="d0">Thread Pool</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A thread pool manages a collection of threads to execute tasks concurrently, optimizing resource utilization and simplifying parallel execution.&lt;SEP&gt;A thread pool manages a collection of worker threads that execute tasks, including offloaded GPU kernels, for efficient resource utilization.&lt;SEP&gt;A thread pool manages a collection of worker threads that execute tasks, including offloaded GPU kernels, to optimize resource utilization.&lt;SEP&gt;A thread pool manages a group of worker threads to execute tasks concurrently, improving resource utilization and simplifying parallel execution management.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Locality">
  <data key="d0">Memory Locality</data>
  <data key="d1">Results</data>
  <data key="d2">Memory locality refers to accessing data stored close to the processor to improve performance, especially relevant in GPU programming for reducing latency.&lt;SEP&gt;Memory locality refers to accessing data stored close to the processor to reduce latency and improve performance, especially critical in GPU programming for efficient data transfer.&lt;SEP&gt;Memory locality refers to data placement and access patterns that optimize cache usage and reduce latency during offloading.&lt;SEP&gt;Memory locality refers to optimizing data placement and access patterns to reduce latency and improve performance during offloading.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Offloading">
  <data key="d0">Memory Offloading</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Offloading involves transferring data and computation from CPU to GPU or other accelerators to leverage their parallel processing capabilities.&lt;SEP&gt;Offloading involves transferring data and computation from the CPU to GPUs or other accelerators to leverage their parallel processing capabilities.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Allocation and Deallocation">
  <data key="d0">Memory Allocation and Deallocation</data>
  <data key="d1">Tools</data>
  <data key="d2">Explicit management of GPU memory through allocation and deallocation to optimize performance and memory usage.&lt;SEP&gt;Explicit management of GPU memory, including allocation and deallocation, to optimize performance and memory usage.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Movement">
  <data key="d0">Memory Movement</data>
  <data key="d1">Results</data>
  <data key="d2">Efficient data transfer between host and device is critical for high performance in accelerator offloading.&lt;SEP&gt;Efficient data transfer between host and device is critical for performance in accelerator offloading.</data>
  <data key="d3">chunk-b09f2e415a74aadae0bd890a74b69f3d</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="CUDA Calls">
  <data key="d0">CUDA Calls</data>
  <data key="d1">Tools</data>
  <data key="d2">CUDA calls are specific API functions like cudaMalloc, cudaFree, and cudaMemcpy used to allocate, deallocate, and transfer data on the GPU, essential for managing GPU resources and data movement.&lt;SEP&gt;CUDA calls are specific API functions used to allocate, free, and transfer memory or execute kernels on the GPU, essential for managing GPU resources.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="CUDA Streams">
  <data key="d0">CUDA Streams</data>
  <data key="d1">Tools</data>
  <data key="d2">CUDA streams are sequences of operations that execute asynchronously on the GPU, allowing overlapping of data transfers and kernel executions for performance gains.&lt;SEP&gt;CUDA streams are sequences of operations that execute asynchronously on the GPU, enabling overlapping of data transfers and kernel executions for performance optimization.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Execution Range">
  <data key="d0">Execution Range</data>
  <data key="d1">Variables</data>
  <data key="d2">The execution range specifies the number of iterations assigned to each thread, influencing workload balancing and parallel efficiency during kernel execution.&lt;SEP&gt;The execution range specifies the number of iterations assigned to each thread, influencing workload distribution and parallel efficiency.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Indexing">
  <data key="d0">Indexing</data>
  <data key="d1">Variables</data>
  <data key="d2">Indexing, such as tid = blockIdx.x * blockDim.x + threadIdx.x, is used to assign unique identifiers to threads within CUDA kernels, facilitating work sharing.&lt;SEP&gt;Indexing, such as tid = blockIdx.x * blockDim.x + threadIdx.x, uniquely identifies threads within CUDA kernels for work sharing.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intra-block and Inter-block Reduction">
  <data key="d0">Intra-block and Inter-block Reduction</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Reduction patterns aggregate data within and across CUDA thread blocks, often using shared memory and atomic operations to combine results.&lt;SEP&gt;Reduction patterns aggregate data within thread blocks (intra-block) and across multiple blocks (inter-block), often utilizing shared memory and atomic operations for combining results.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Shared Memory Synchronization">
  <data key="d0">Shared Memory Synchronization</data>
  <data key="d1">Methods</data>
  <data key="d2">Shared memory synchronization ensures data consistency among threads within a block, typically through synchronization primitives like __syncthreads().&lt;SEP&gt;Shared memory synchronization primitives like __syncthreads() are used to coordinate data access among threads within a block, ensuring data consistency during parallel execution.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Inlining">
  <data key="d0">Inlining</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Inlining replaces function calls with the function's code directly to enable further compiler optimizations like loop unrolling and flattening of hierarchical code structures.&lt;SEP&gt;Inlining replaces function calls with their actual code body to enable further compiler optimizations like loop unrolling and code flattening, improving performance.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Loop Unrolling">
  <data key="d0">Loop Unrolling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Loop unrolling expands loop iterations into repeated code blocks, reducing loop overhead and increasing instruction-level parallelism in GPU kernels.&lt;SEP&gt;Loop unrolling involves expanding loop iterations into repeated code blocks to decrease loop overhead and increase instruction-level parallelism in GPU kernels.&lt;SEP&gt;Loop unrolling is a technique to expand loops to reduce overhead and increase parallelism, though large loops can pose challenges.&lt;SEP&gt;Loop unrolling is an optimization technique that replicates loop bodies multiple times to reduce loop control overhead and increase parallelism.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f&lt;SEP&gt;chunk-e376cabd5c2bdbbf8ecdd095bfa831b5&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hierarchical Structure">
  <data key="d0">Hierarchical Structure</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Hierarchical structure refers to nested function calls or loops within code, which must be flattened for effective parallelization and optimization.&lt;SEP&gt;Hierarchical structure refers to nested functions or loops within code, which need to be flattened through inlining and unrolling for effective parallelization.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="MILP Specification">
  <data key="d0">MILP Specification</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Mixed Integer Linear Programming (MILP) specification defines the constraints and objective functions for optimization problems, ensuring that parallel code adheres to problem semantics and optimality.&lt;SEP&gt;Mixed Integer Linear Programming (MILP) specification defines the constraints and objectives for optimization problems, ensuring parallel patterns adhere to problem semantics.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Function Declaration">
  <data key="d0">Function Declaration</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A function declaration specifies the interface and code body of a function, used as a basis for inlining and code transformation during optimization.&lt;SEP&gt;A function declaration specifies the name, parameters, and body of a function, serving as the basis for inlining and code transformation during optimization processes.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Scope Overlaps">
  <data key="d0">Scope Overlaps</data>
  <data key="d1">Limitations</data>
  <data key="d2">Scope overlaps occur when variables or parameters share the same name across different code regions, complicating code transformation and requiring variable renaming to prevent conflicts.&lt;SEP&gt;Scope overlaps occur when variables or parameters share the same name in different code regions, complicating code transformation and requiring renaming to preserve correctness.</data>
  <data key="d3">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Function Arguments">
  <data key="d0">Function Arguments</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Function arguments are the inputs passed to functions, which can include nested parallel patterns that influence execution and dependencies.&lt;SEP&gt;Function arguments refer to the inputs passed to functions, which can include nested parallel patterns that influence execution and dependencies.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Nested Parallel Patterns">
  <data key="d0">Nested Parallel Patterns</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Nested parallel patterns are complex structures within code that allow parallel execution at multiple levels, requiring careful management to avoid data dependencies.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Write-After-Write Dependencies">
  <data key="d0">Write-After-Write Dependencies</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Write-after-write dependencies occur when multiple write operations to the same memory location are ordered, affecting parallel execution and correctness.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Write-After-Read Dependencies">
  <data key="d0">Write-After-Read Dependencies</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Write-after-read dependencies happen when a write operation occurs after a read, potentially leading to data hazards in parallel code.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Local Array">
  <data key="d0">Local Array</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A local array is a temporary storage used within functions to hold data, ensuring data isolation and avoiding side effects during parallel execution.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Return Node">
  <data key="d0">Return Node</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A ReturnNode signifies the point in a function where execution can terminate and return control, crucial for managing flow in parallel algorithms.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Jump Label">
  <data key="d0">Jump Label</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A jump label is a marker used to manage control flow, enabling jumps to specific points in code, especially in function call management and inlining.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Function Call">
  <data key="d0">Function Call</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A function call represents invoking a function, which can be inlined or referenced, affecting control flow and code generation.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hashing">
  <data key="d0">Hashing</data>
  <data key="d1">Tools</data>
  <data key="d2">Hashing is a technique used to generate unique identifiers for variables or data structures, facilitating variable replacement and data management in code transformations.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Deep Copy">
  <data key="d0">Deep Copy</data>
  <data key="d1">Tools</data>
  <data key="d2">Deep copying creates an exact duplicate of data structures, ensuring that modifications in copied data do not affect the original, important in parallel code transformations.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Variable Replacement">
  <data key="d0">Variable Replacement</data>
  <data key="d1">Tools</data>
  <data key="d2">Variable replacement involves substituting variable references with other values or identifiers, aiding in inlining and code optimization.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Inlined Function Call">
  <data key="d0">Inlined Function Call</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An inlined function call replaces a function invocation with the actual function code, improving performance and enabling further optimizations.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Algorithm 1: Simplified APT Function Inlining">
  <data key="d0">Algorithm 1: Simplified APT Function Inlining</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A specific algorithm designed to inline functions by replacing calls with code, managing jump labels, local variables, and return handling for optimized code execution.&lt;SEP&gt;A specific algorithm designed to inline functions by replacing calls with their code, managing jump labels, local variables, and return handling for optimized code execution.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Optimization Benchmarks">
  <data key="d0">Parallel Optimization Benchmarks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Benchmarks used to evaluate the effectiveness of parallel code optimizations across different hardware and software configurations.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Runtime Measurement">
  <data key="d0">Runtime Measurement</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Runtime measurement involves recording execution time of code segments to evaluate performance, often averaged over multiple runs.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Comparison of Hand-Written and Generated Code">
  <data key="d0">Comparison of Hand-Written and Generated Code</data>
  <data key="d1">Study Designs</data>
  <data key="d2">This comparison assesses the performance differences between manually optimized code and code produced automatically by tools.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="System Environment">
  <data key="d0">System Environment</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The evaluation environment includes specific hardware details such as CPU type, clock speed, memory, and system configuration, which influence performance results.</data>
  <data key="d3">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="5.953">
  <data key="d0">5.953</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value representing a measurement or data point in the dataset, likely related to performance or experimental results.&lt;SEP&gt;A numerical value representing a measurement or data point in the dataset.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="0.072">
  <data key="d0">0.072</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value indicating a specific measurement or parameter within the dataset.&lt;SEP&gt;A numerical value indicating a specific measurement or parameter.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="0.236">
  <data key="d0">0.236</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical measurement or result value, possibly representing timing, performance, or accuracy.&lt;SEP&gt;A numerical measurement or result value.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Generated">
  <data key="d0">Generated</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A process or output related to the creation of code or data, possibly indicating generated code or data outputs.&lt;SEP&gt;Refers to code, data, or outputs that are created automatically by a tool or process, indicating automation or code generation.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="0.913">
  <data key="d0">0.913</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value, possibly a measurement or metric related to performance or process.&lt;SEP&gt;A numerical value, possibly a performance metric or a timing measurement associated with generated code or processes.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="1.590">
  <data key="d0">1.590</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value, likely representing a performance measurement such as runtime or efficiency.&lt;SEP&gt;A numerical value, likely representing a performance metric or measurement.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="21.919">
  <data key="d0">21.919</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value, possibly indicating a timing or performance result.&lt;SEP&gt;A numerical value, potentially indicating total execution time, throughput, or a similar metric.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="0.043">
  <data key="d0">0.043</data>
  <data key="d1">Variables</data>
  <data key="d2">A small numerical value, possibly an error margin, standard deviation, or a measurement uncertainty.&lt;SEP&gt;A small numerical value, potentially a measurement or error margin.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="0.162">
  <data key="d0">0.162</data>
  <data key="d1">Variables</data>
  <data key="d2">A numerical value indicating a measurement or parameter.&lt;SEP&gt;A numerical value representing a measurement, perhaps related to performance, timing, or resource utilization.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="5.1 Environment">
  <data key="d0">5.1 Environment</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An overarching description of the computational environment, including hardware and software specifications used for measurements.&lt;SEP&gt;Describes the computational environment, including hardware and software configurations used for measurements and experiments.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="RWTH Aachen University">
  <data key="d0">RWTH Aachen University</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An academic institution where the experimental systems and measurements are conducted.&lt;SEP&gt;An academic institution where the systems and experiments are conducted.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="CLAIX18 systems">
  <data key="d0">CLAIX18 systems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The computing systems used for performing measurements and experiments.&lt;SEP&gt;The specific high-performance computing systems used for performing measurements and experiments.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Xeon Platinum 8160 24C 2.1GHz (Skylake)">
  <data key="d0">Xeon Platinum 8160 24C 2.1GHz (Skylake)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A CPU hardware component used in the systems, characterized by its model and specifications.&lt;SEP&gt;A specific CPU hardware component used in the systems.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="192 GB RAM">
  <data key="d0">192 GB RAM</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Memory hardware component in the systems.&lt;SEP&gt;Memory hardware component providing high-capacity RAM for computation.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intel Omni-Path 100G network fabric">
  <data key="d0">Intel Omni-Path 100G network fabric</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">High-speed network interconnect used to connect nodes in the HPC environment.&lt;SEP&gt;High-speed network interconnect used to connect nodes.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Rocky 8.9">
  <data key="d0">Rocky 8.9</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Operating system installed on the systems, managing hardware and software resources.&lt;SEP&gt;Operating system installed on the systems.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="NVIDIA V100 GPUs">
  <data key="d0">NVIDIA V100 GPUs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Graphics processing units used for GPU resources in the system.&lt;SEP&gt;Graphics processing units used for GPU-accelerated computations within the environment.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intel®OneAPI C/C++ Compiler 2022.1.0">
  <data key="d0">Intel®OneAPI C/C++ Compiler 2022.1.0</data>
  <data key="d1">Tools</data>
  <data key="d2">Compiler used for code compilation with optimization settings.&lt;SEP&gt;Compiler used for code compilation, enabling high-performance code generation.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="IntelMPI 2021.6.0">
  <data key="d0">IntelMPI 2021.6.0</data>
  <data key="d1">Tools</data>
  <data key="d2">MPI library used for parallel communication between processes.&lt;SEP&gt;Message Passing Interface library for parallel programming.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="CUDA 11.8">
  <data key="d0">CUDA 11.8</data>
  <data key="d1">Tools</data>
  <data key="d2">GPU computing platform used for code compilation and execution.&lt;SEP&gt;GPU computing platform used for compiling and running GPU-accelerated code.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Optimization level -O3">
  <data key="d0">Optimization level -O3</data>
  <data key="d1">Variables</data>
  <data key="d2">Compiler optimization setting aimed at maximizing code performance.&lt;SEP&gt;Compiler optimization setting applied to maximize performance.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global optimizations">
  <data key="d0">Global optimizations</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Optimization techniques applied during code compilation to improve performance without affecting correctness.&lt;SEP&gt;Optimization techniques performed without measurements or artificial synchronization to avoid influencing the process.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Artificial synchronization points">
  <data key="d0">Artificial synchronization points</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Artificial points inserted in code to synchronize processes, avoided during global optimizations.&lt;SEP&gt;Points inserted into code to synchronize processes, avoided during certain optimizations to prevent performance interference.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hand-inserted time measurements">
  <data key="d0">Hand-inserted time measurements</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Manual addition of timing data into code to ensure measurement accuracy.&lt;SEP&gt;Manual timing data added into code to ensure measurement accuracy.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Average wall-clock times">
  <data key="d0">Average wall-clock times</data>
  <data key="d1">Variables</data>
  <data key="d2">Mean duration of execution over multiple repetitions, used to evaluate performance.&lt;SEP&gt;Mean execution durations over multiple repetitions, used for performance assessment.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="40 repetitions">
  <data key="d0">40 repetitions</data>
  <data key="d1">Study Design</data>
  <data key="d2">Number of times experiments are repeated to obtain reliable average measurements.&lt;SEP&gt;Number of times the experiments or measurements are repeated to obtain average results.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Gurobi">
  <data key="d0">Gurobi</data>
  <data key="d1">Tools</data>
  <data key="d2">Optimization software used for solving mathematical programming problems, known to introduce instabilities.&lt;SEP&gt;Optimization solver software used for mathematical programming, known to introduce stochastic variability in solutions.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Random seed">
  <data key="d0">Random seed</data>
  <data key="d1">Variables</data>
  <data key="d2">Seed value used to initialize stochastic processes within Gurobi to mitigate variability.&lt;SEP&gt;Seed value used to initialize stochastic processes within Gurobi, affecting solution stability.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Multiple seeds (five)">
  <data key="d0">Multiple seeds (five)</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of different seed values used to reduce randomness effects in Gurobi solutions, enhancing result reliability.&lt;SEP&gt;Number of different seed values used to reduce randomness effects in Gurobi solutions.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Rodinia benchmarks">
  <data key="d0">Rodinia benchmarks</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A benchmark suite designed to evaluate HPC applications based on the Berkeley dwarfs, covering diverse computational patterns.&lt;SEP&gt;A benchmark suite for evaluating HPC applications, based on the Berkeley dwarfs.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Benchmark suite">
  <data key="d0">Benchmark suite</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A collection of representative HPC applications used for performance evaluation.&lt;SEP&gt;A collection of representative high-performance computing applications used for performance testing and validation.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="19 Rodinia OpenMP benchmarks">
  <data key="d0">19 Rodinia OpenMP benchmarks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Specific benchmarks within Rodinia used for testing parallel code performance.&lt;SEP&gt;Specific parallel benchmarks within Rodinia used to evaluate OpenMP-based code performance.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Port for PPLin previous work">
  <data key="d0">Port for PPLin previous work</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Adapted or previous version of code used as a baseline or comparison in experiments.&lt;SEP&gt;Adapted version of code used for comparison in experiments.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Code sizes/configurations">
  <data key="d0">Code sizes/configurations</data>
  <data key="d1">Variables</data>
  <data key="d2">Different dataset or problem sizes used in benchmarking.&lt;SEP&gt;Different dataset sizes or problem configurations used in benchmarking to assess scalability and performance.</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PPLin">
  <data key="d0">PPLin</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">PPLin is a methodology or framework used for porting and optimizing benchmarks, specifically applied to the Rodinia benchmark suite, involving adaptations, tuning, and performance analysis.&lt;SEP&gt;PPLin is a methodology or framework used for porting, optimizing, and analyzing the performance of benchmarks, involving code adaptation, tuning, and evaluation.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Rodinia">
  <data key="d0">Rodinia</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Rodinia is a benchmark suite of parallel applications used to evaluate optimization techniques and performance portability.&lt;SEP&gt;Rodinia is a benchmark suite used to evaluate high-performance computing performance and optimization techniques, consisting of various computational kernels and applications.&lt;SEP&gt;Rodinia is a benchmark suite used to evaluate performance and optimization approaches in high-performance computing.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Benchmarks">
  <data key="d0">Benchmarks</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Benchmarks refer to specific tests within the Rodinia suite, such as backprop, heartwall, hotspot, lavaMD, etc., used to measure performance improvements.&lt;SEP&gt;Specific benchmark applications within Rodinia, such as backprop, heartwall, hotspot, lavaMD, etc., used to measure performance improvements.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallelism">
  <data key="d0">Parallelism</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallelism involves executing multiple tasks simultaneously, enabled within dynamic control flow structures such as loops and branches to improve performance.&lt;SEP&gt;Parallelism refers to executing multiple operations simultaneously to reduce runtime, achieved through code tuning and hardware utilization.&lt;SEP&gt;The execution of multiple operations simultaneously to decrease runtime, achieved through code restructuring and hardware utilization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Static Overhead">
  <data key="d0">Static Overhead</data>
  <data key="d1">Limitations</data>
  <data key="d2">Fixed costs associated with code execution or synchronization that can be minimized through optimization, particularly beneficial for smaller datasets or applications.&lt;SEP&gt;Static overhead pertains to fixed computational or synchronization costs that can be reduced through optimization, especially beneficial for smaller applications.&lt;SEP&gt;Static overhead reduction refers to minimizing fixed computational costs in applications, especially beneficial for smaller applications, to improve performance.&lt;SEP&gt;Static overhead reduction refers to minimizing fixed computational costs in applications, which is particularly beneficial for smaller applications to improve performance.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GPU">
  <data key="d0">GPU</data>
  <data key="d1">Tools</data>
  <data key="d2">Graphics Processing Units are utilized in some benchmarks to accelerate computation, with the PPL approach selectively leveraging GPU resources.&lt;SEP&gt;Graphics Processing Units used to accelerate certain benchmarks, with the PPL approach selectively leveraging GPU resources based on dataset size and workload.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Multi-node Parallelism">
  <data key="d0">Multi-node Parallelism</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Distributing workloads across multiple nodes, as in particle and backprop benchmarks, to enhance performance and scalability.&lt;SEP&gt;Multi-node parallelism involves distributing workloads across multiple nodes, as seen in particle and backprop benchmarks, to enhance performance.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Control-Flow">
  <data key="d0">Dynamic Control-Flow</data>
  <data key="d1">Limitations</data>
  <data key="d2">Dynamic control-flow in benchmarks like bfs and streamcluster complicates optimization due to nested kernels and dependency analysis challenges.&lt;SEP&gt;Nested or dynamic control structures within kernels, such as in bfs and streamcluster, hinder optimization due to dependency analysis challenges.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Search Space">
  <data key="d0">Search Space</data>
  <data key="d1">Limitations</data>
  <data key="d2">Large search spaces, such as in cfd, nw, and pathfinder, hinder optimization due to complexity in scheduling and resource allocation.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Overhead">
  <data key="d0">Memory Overhead</data>
  <data key="d1">Limitations</data>
  <data key="d2">High memory consumption in some kernels, like myocyte, which restricts scalability and performance.&lt;SEP&gt;High memory requirements, as seen in the myocyte kernel, limit scalability and performance in some benchmarks.&lt;SEP&gt;Memory overhead pertains to extra memory used during computation, which future work aims to minimize by avoiding unnecessary local copies.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20&lt;SEP&gt;chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Code Rewriting">
  <data key="d0">Code Rewriting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Rewriting original code into element-wise iteration or other forms to facilitate optimization and parallel execution.&lt;SEP&gt;Transforming original code into alternative forms, such as element-wise iteration, to facilitate optimization and parallel execution.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Performance Analysis">
  <data key="d0">Performance Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Measuring runtime, speedup, and efficiency to evaluate the effectiveness of the PPL approach.&lt;SEP&gt;Performance analysis involves measuring runtimes, speedups, and efficiency gains to evaluate the effectiveness of the PPL approach.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Compiler Optimizations">
  <data key="d0">Compiler Optimizations</data>
  <data key="d1">Tools</data>
  <data key="d2">Optimizations from compilers like Intel OneAPI contribute to performance improvements, as seen in lavaMD.&lt;SEP&gt;Optimizations from compilers like Intel OneAPI that contribute to performance gains, as observed in lavaMD.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Large Search Space">
  <data key="d0">Large Search Space</data>
  <data key="d1">Limitations</data>
  <data key="d2">Complex scheduling problems in benchmarks like cfd, nw, and pathfinder, where the vast number of task dependencies makes optimization difficult.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Workload">
  <data key="d0">Dynamic Workload</data>
  <data key="d1">Limitations</data>
  <data key="d2">Workloads with branches, such as in LU-decomposition (lud), which create load imbalance and complicate static analysis.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Speedup Visualization">
  <data key="d0">Speedup Visualization</data>
  <data key="d1">Results</data>
  <data key="d2">Graphical representation of speedup data, illustrating performance improvements across benchmarks and configurations.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dependency Analysis">
  <data key="d0">Dependency Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Analyzing data and control dependencies within kernels to enable or limit optimization potential.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Scheduling">
  <data key="d0">Scheduling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Determining execution order and resource allocation for kernels and tasklets to optimize performance.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Transfers">
  <data key="d0">Data Transfers</data>
  <data key="d1">Variables</data>
  <data key="d2">Movement of data between host and device or across nodes, affecting overall performance and parallelization decisions.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dataset Size">
  <data key="d0">Dataset Size</data>
  <data key="d1">Variables</data>
  <data key="d2">Size of input data, influencing the choice of hardware resources and the effectiveness of optimization strategies.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Benchmark Results">
  <data key="d0">Benchmark Results</data>
  <data key="d1">Results</data>
  <data key="d2">Quantitative data showing runtime improvements, speedups, and performance trends across different benchmarks.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hardware Resources">
  <data key="d0">Hardware Resources</data>
  <data key="d1">Tools</data>
  <data key="d2">CPU cores, GPUs, and nodes utilized in the experiments, impacting the achievable speedups and scalability.</data>
  <data key="d3">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Backprop">
  <data key="d0">Backprop</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Backprop is a benchmark used to evaluate neural network training performance, often impacted by optimization techniques.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Srad">
  <data key="d0">Srad</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Srad (Stencil Red-Black) is a benchmark for stencil computations, relevant in performance evaluations.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LavaMD">
  <data key="d0">LavaMD</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">LavaMD is a molecular dynamics benchmark used to assess computational efficiency and optimization impact.&lt;SEP&gt;LavaMD is a molecular dynamics benchmark used to assess computational efficiency and the impact of optimizations.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C/C++ Support">
  <data key="d0">C/C++ Support</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Adding support for C/C++ in the PPL front-end to facilitate porting existing applications and improve reusability.&lt;SEP&gt;Adding support for C/C++ in the PPL front-end to improve reusability and ease porting of existing applications.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Annotations">
  <data key="d0">Annotations</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Using annotations similar to OpenMP to identify parallel patterns in source code, aiding parallelization.&lt;SEP&gt;Using annotations similar to OpenMP to identify parallel patterns in source code, facilitating parallelization.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Performance Slowdown (17%)">
  <data key="d0">Performance Slowdown (17%)</data>
  <data key="d1">Results</data>
  <data key="d2">The current code generator experiences a 17% slowdown compared to hand-optimized code for certain kernels, indicating room for further optimization.&lt;SEP&gt;The current code generator experiences a 17% slowdown compared to hand-optimized code for certain kernels, indicating room for optimization.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Speedup (14%)">
  <data key="d0">Speedup (14%)</data>
  <data key="d1">Results</data>
  <data key="d2">A 14% performance improvement over the parallel baseline demonstrates the potential of the approach.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Jacobi Kernel">
  <data key="d0">Jacobi Kernel</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Jacobi kernel is a computational routine used in iterative methods, with current optimization strategies yet to be fully implemented.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Monte Kernel">
  <data key="d0">Monte Kernel</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Monte kernel involves stochastic simulations, with optimization dependent on improving reduction implementations and shared memory strategies.&lt;SEP&gt;Monte kernel involves stochastic simulations, with optimization dependent on reduction implementation and shared memory strategies.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Behavior">
  <data key="d0">Dynamic Behavior</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Applications with dynamic workloads, such as b+tree and BFS, challenge static optimization approaches and require runtime adaptation.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Dependency Analysis">
  <data key="d0">Data Dependency Analysis</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Analyzing data dependencies during compile time to identify dynamic workloads and enable better optimization.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="SDFGs (Stateful DataFlow Graphs)">
  <data key="d0">SDFGs (Stateful DataFlow Graphs)</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">SDFGs enable detailed dependency analysis and aliasing elimination during compile time, aiding optimization of dynamic applications.&lt;SEP&gt;SDFGs enable detailed dependency analysis and aliasing elimination during compile time, facilitating optimization of dynamic applications.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dependency Chain">
  <data key="d0">Dependency Chain</data>
  <data key="d1">Variables</data>
  <data key="d2">A sequence of dependencies among variables that can be extracted to identify runtime arguments and control flow structures, supporting dynamic analysis.&lt;SEP&gt;A sequence of dependencies among variables that can be extracted to identify runtime arguments and control flow structures.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Legion">
  <data key="d0">Legion</data>
  <data key="d1">Tools</data>
  <data key="d2">A runtime system supporting execution of patterns/tasklets of unknown size, enabling dynamic load balancing and parallelism.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="StarPU">
  <data key="d0">StarPU</data>
  <data key="d1">Tools</data>
  <data key="d2">A runtime system that supports migration of tasklets during execution across distributed memory, useful for dynamic applications.&lt;SEP&gt;A runtime system that supports migration of tasklets during execution across distributed memory, useful for dynamic load balancing in distributed systems.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Tasklet Execution">
  <data key="d0">Parallel Tasklet Execution</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Execution of small, independent units of work (tasklets) that can be scheduled dynamically to optimize resource utilization.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Scalability of Tasklet Scheduling">
  <data key="d0">Scalability of Tasklet Scheduling</data>
  <data key="d1">Limitations</data>
  <data key="d2">Current scalability issues with exponential search space in certain benchmarks, making optimization and scheduling challenging.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Random Seed">
  <data key="d0">Random Seed</data>
  <data key="d1">Variables</data>
  <data key="d2">The random seed influences variable naming and impacts the duration of solving processes, affecting reproducibility and stability.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Beam Search">
  <data key="d0">Beam Search</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A search strategy that prunes the search space to improve predictability and stability in scheduling and optimization problems.&lt;SEP&gt;Beam search is a heuristic search technique that explores a limited subset of options at each step, used here for scheduling and optimization.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Pruning Strategies">
  <data key="d0">Pruning Strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Pruning strategies like beam search are techniques to reduce the search space in scheduling and optimization algorithms, improving efficiency.&lt;SEP&gt;Techniques like n-best pruning limit search space during optimization, balancing solution quality and computational effort.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Scheduling Problem">
  <data key="d0">Scheduling Problem</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The problem of efficiently scheduling tasks across multiple machines and stages, with applications in manufacturing and computational workflows.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LP-Solvers">
  <data key="d0">LP-Solvers</data>
  <data key="d1">Tools</data>
  <data key="d2">Linear programming solvers used for optimization problems; alternative approaches like beam search can offer more predictability.&lt;SEP&gt;Linear programming solvers used for optimization problems; alternative approaches like beam search can offer more predictable and stable solutions.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Model">
  <data key="d0">Data Model</data>
  <data key="d1">Variables</data>
  <data key="d2">The memory model of the code generator should be improved to avoid local memory copies, which impacts performance and efficiency.&lt;SEP&gt;The memory model of the code generator, which should be improved to avoid local memory copies and enhance efficiency.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Runtime-Dependent Applications">
  <data key="d0">Runtime-Dependent Applications</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Applications whose behavior depends on runtime data, requiring dynamic code generation and adaptation.</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="source">
  <data key="d0">source</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The source refers to the origin of data used during the optimization process, which is available throughout the optimization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="destination">
  <data key="d0">destination</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The destination indicates the target or endpoint in data flow during optimization, relevant in data management and control flow.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="amount of data">
  <data key="d0">amount of data</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount of data pertains to the volume or size of data involved in the optimization, affecting processing and resource allocation.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="parallelism">
  <data key="d0">parallelism</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallelism involves executing multiple computations simultaneously to improve efficiency, especially within dynamic control flow structures like loops and branches.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="dynamic control flow structures">
  <data key="d0">dynamic control flow structures</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Dynamic control flow structures include programming constructs such as loops and branches that influence program execution paths.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="loops">
  <data key="d0">loops</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Loops are control structures that repeat code blocks, whose unrolling can impact performance and parallelization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="branches">
  <data key="d0">branches</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Branches are decision points in code that affect control flow, relevant for optimization and parallel execution.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="streamcluster">
  <data key="d0">streamcluster</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Streamcluster is a benchmark application used to evaluate parallel processing capabilities and optimization techniques.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="cfd">
  <data key="d0">cfd</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">CFD (Computational Fluid Dynamics) refers to simulation applications modeling fluid flows, used here as an application for optimization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="memory model">
  <data key="d0">memory model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The memory model defines how data is stored, accessed, and managed in code generation, impacting performance and efficiency.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="local memory copies">
  <data key="d0">local memory copies</data>
  <data key="d1">Variables</data>
  <data key="d2">Local memory copies are duplicate data stored locally to reduce access latency, whose elimination can optimize memory usage.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="particle">
  <data key="d0">particle</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Particles are elements in physics simulations, such as in particle-based models, affected by memory management strategies.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="myocyte">
  <data key="d0">myocyte</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Myocytes are muscle cells, used here as objects in biological or physiological simulations impacted by data handling.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="global optimization step">
  <data key="d0">global optimization step</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A global optimization step aims to improve code by eliminating unnecessary local copies and enhancing overall performance.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="data size">
  <data key="d0">data size</data>
  <data key="d1">Variables</data>
  <data key="d2">Data size influences the choice between local copies and reference access, affecting performance and memory overhead.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="potential parallelism">
  <data key="d0">potential parallelism</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Potential parallelism refers to the degree to which computations can be executed simultaneously, influencing optimization decisions.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LULESH">
  <data key="d0">LULESH</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">LULESH is a mini-application simulating wave propagation, used to analyze parallelization and optimization techniques.&lt;SEP&gt;LULESH is a physical simulation proxy application modeling wave propagation, used to analyze the effectiveness of parallelization and optimization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="physical simulation">
  <data key="d0">physical simulation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Physical simulation models real-world phenomena like wave propagation, requiring computational methods for approximation.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="weak scaling">
  <data key="d0">weak scaling</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Weak scaling tests how well a parallel application performs as the problem size increases proportionally with resources.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="MPI+OpenMP">
  <data key="d0">MPI+OpenMP</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">MPI+OpenMP combines message passing with shared-memory parallelism, enabling hybrid parallel programming across distributed and shared-memory architectures.&lt;SEP&gt;MPI+OpenMP is a hybrid parallel programming model combining message passing and shared memory paradigms for high-performance applications.&lt;SEP&gt;MPI+OpenMP is a hybrid programming model combining message passing and shared memory paradigms to support high-performance parallel applications.</data>
  <data key="d3">chunk-727ea750122a5bed30ad781918446611&lt;SEP&gt;chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="iterative solvers">
  <data key="d0">iterative solvers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Iterative solvers are algorithms that approximate solutions to complex equations over multiple iterations, used in physics simulations.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="parallel patterns">
  <data key="d0">parallel patterns</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Parallel patterns are common structures in parallel programming that facilitate analysis and optimization of concurrent execution.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="code generator">
  <data key="d0">code generator</data>
  <data key="d1">Tools</data>
  <data key="d2">A code generator automatically produces optimized code from high-level source code, enabling performance improvements and portability.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="intermediate representation">
  <data key="d0">intermediate representation</data>
  <data key="d1">Tools</data>
  <data key="d2">Intermediate representations serve as an abstraction layer in compilation, aiding optimization and code analysis.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="heterogeneous architectures">
  <data key="d0">heterogeneous architectures</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Heterogeneous architectures include systems combining CPUs, GPUs, and other accelerators for high-performance computing.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="speedup">
  <data key="d0">speedup</data>
  <data key="d1">Results</data>
  <data key="d2">Speedup measures how much faster a program runs after optimization compared to baseline performance.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="concurrency">
  <data key="d0">concurrency</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Concurrency refers to executing multiple computations simultaneously, which can be revealed or enhanced through optimization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="speedups of up to 12.43">
  <data key="d0">speedups of up to 12.43</data>
  <data key="d1">Results</data>
  <data key="d2">The prototype achieved speedups of up to 12.43 times over baseline benchmarks by eliminating overhead and increasing parallelism.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="kmeans, hotspot3D">
  <data key="d0">kmeans, hotspot3D</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Kmeans and hotspot3D are benchmarks where additional concurrency was identified, leading to performance improvements.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="MPI reduction">
  <data key="d0">MPI reduction</data>
  <data key="d1">Tools</data>
  <data key="d2">MPI reduction is a collective operation in MPI used to combine data across processes efficiently, which the future improvements aim to better utilize.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="memory overhead">
  <data key="d0">memory overhead</data>
  <data key="d1">Variables</data>
  <data key="d2">Memory overhead refers to additional memory used during computation, which future work seeks to reduce by avoiding local copies.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="pruning strategies">
  <data key="d0">pruning strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Pruning strategies are techniques to reduce the search space in algorithms, such as beam search, to improve efficiency and scalability.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="beam search">
  <data key="d0">beam search</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Beam search is a heuristic search algorithm that explores a limited set of options at each step, used here for scheduling optimization.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C/C++">
  <data key="d0">C/C++</data>
  <data key="d1">Tools</data>
  <data key="d2">C and C++ are programming languages whose support will be added to ease porting and testing of applications.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="dynamic load balancers">
  <data key="d0">dynamic load balancers</data>
  <data key="d1">Tools</data>
  <data key="d2">Dynamic load balancers distribute workload during runtime to optimize resource utilization in dynamic applications.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="performance portability">
  <data key="d0">performance portability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Performance portability ensures that code performs efficiently across various hardware architectures.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data">
  <data key="d0">Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The data encompasses source, destination, and amount of data involved in the optimization process, which are available during the optimization to facilitate analysis and control flow.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Control Flow Structures">
  <data key="d0">Control Flow Structures</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Control flow structures like loops and branches determine the execution path of programs and can be optimized for parallel execution.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Application">
  <data key="d0">Application</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Streamcluster and cfd are applications used to evaluate optimization techniques and parallelization capabilities.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Model">
  <data key="d0">Memory Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The memory model defines how data is stored, accessed, and managed during code execution, impacting performance and memory overhead.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Local Memory Copies">
  <data key="d0">Local Memory Copies</data>
  <data key="d1">Variables</data>
  <data key="d2">Local memory copies are duplicate data stored locally to reduce access latency, whose elimination can optimize memory usage and performance.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global Optimization Step">
  <data key="d0">Global Optimization Step</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A global optimization step aims to eliminate unnecessary local copies by analyzing data access patterns to improve code efficiency.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Data Size">
  <data key="d0">Data Size</data>
  <data key="d1">Variables</data>
  <data key="d2">The size of data influences decisions between local copies and reference access, affecting memory overhead and performance.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallelism Potential">
  <data key="d0">Parallelism Potential</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Potential parallelism refers to the extent to which computations can be executed concurrently, influencing optimization strategies.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Wave Propagation">
  <data key="d0">Wave Propagation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Wave propagation is the physical phenomenon simulated in LULESH, involving iterative computations over time steps.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Weak Scaling">
  <data key="d0">Weak Scaling</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Weak scaling assesses how well a parallel application performs as the problem size increases proportionally with resources.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Iterative Solvers">
  <data key="d0">Iterative Solvers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Iterative solvers are algorithms that approximate solutions through repeated iterations, used in physics simulations like LULESH.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Intermediate Representation">
  <data key="d0">Intermediate Representation</data>
  <data key="d1">Tools</data>
  <data key="d2">Intermediate representations serve as an abstraction layer in compilation, facilitating optimization and analysis of code.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Heterogeneous Architectures">
  <data key="d0">Heterogeneous Architectures</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Heterogeneous architectures combine CPUs, GPUs, and accelerators to support high-performance computing across diverse hardware.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Additional Concurrency">
  <data key="d0">Additional Concurrency</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Additional concurrency refers to uncovering more parallel execution opportunities within applications, leading to performance gains, as seen in kmeans and hotspot3D.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Support for C/C++">
  <data key="d0">Support for C/C++</data>
  <data key="d1">Tools</data>
  <data key="d2">Adding support for C/C++ reduces porting effort and broadens applicability of the optimization framework.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Load Balancers">
  <data key="d0">Dynamic Load Balancers</data>
  <data key="d1">Tools</data>
  <data key="d2">Dynamic load balancers distribute workload during runtime to improve performance and resource utilization in dynamic applications.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Performance Portability">
  <data key="d0">Performance Portability</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Performance portability ensures that optimized code maintains efficiency across different hardware architectures.</data>
  <data key="d3">chunk-e578b0d9bfe757587fab19ecc63b4d20</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Performance Portability on Heterogeneous Architectures">
  <data key="d0">Performance Portability on Heterogeneous Architectures</data>
  <data key="d1">&lt;Core Concepts</data>
  <data key="d2">Refers to the ability of software or algorithms to run efficiently across various hardware architectures without modification.&lt;SEP&gt;Refers to the capability of software or algorithms to operate efficiently across different hardware architectures without needing significant modification.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Proceedings of the ACM Digital Library">
  <data key="d0">Proceedings of the ACM Digital Library</data>
  <data key="d1">&lt;Study Designs</data>
  <data key="d2">A collection of academic papers and conference proceedings documenting research activities.&lt;SEP&gt;A collection of academic papers and conference proceedings documenting various research activities and studies in high-performance computing.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PMAM ’24">
  <data key="d0">PMAM ’24</data>
  <data key="d1">&lt;Event</data>
  <data key="d2">An academic conference focused on high performance computing, networking, storage, and analysis, held in March 2024 in Edinburgh, UK.&lt;SEP&gt;An international conference held in March 2024 in Edinburgh, UK, focusing on high performance computing, networking, storage, and analysis.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Tal Ben-Nun et al.">
  <data key="d0">Tal Ben-Nun et al.</data>
  <data key="d1">&lt;Research Questions/Hypotheses</data>
  <data key="d2">Investigates data-centric parallel programming techniques and their effectiveness on heterogeneous architectures.&lt;SEP&gt;Investigates methods and frameworks to improve performance portability and efficiency in heterogeneous computing environments.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="DaCe-Data Centric Parallel Programming">
  <data key="d0">DaCe-Data Centric Parallel Programming</data>
  <data key="d1">&lt;Theories/Models</data>
  <data key="d2">A framework or model for data-centric parallel programming aimed at improving performance portability.&lt;SEP&gt;A programming model and framework aimed at enabling data-centric, performance-portable parallel programming across diverse architectures.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Filtered Beam Search Method">
  <data key="d0">Filtered Beam Search Method</data>
  <data key="d1">&lt;Methodology</data>
  <data key="d2">A search algorithm designed to optimize permutation flowshop scheduling problems by minimizing penalties and waiting times.&lt;SEP&gt;An optimization algorithm for permutation flowshop scheduling problems, minimizing penalties and waiting times, relevant for task scheduling in HPC.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Power Wall">
  <data key="d0">Power Wall</data>
  <data key="d1">&lt;Core Concepts</data>
  <data key="d2">Describes the fundamental limitation in increasing processor clock speeds due to power consumption constraints.&lt;SEP&gt;The fundamental limitation on increasing processor clock speeds due to power consumption constraints, influencing hardware design.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Münster Skeleton Library Muesli">
  <data key="d0">Münster Skeleton Library Muesli</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A library providing skeletal programming tools to facilitate structured parallel computation management.&lt;SEP&gt;A library providing tools for skeletal programming and structured management of parallel computations.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Algorithmic Skeletons">
  <data key="d0">Algorithmic Skeletons</data>
  <data key="d1">&lt;Core Concepts</data>
  <data key="d2">A high-level abstraction for structured parallel programming, enabling management of complex parallel algorithms.&lt;SEP&gt;High-level abstractions for managing parallel algorithms, enabling structured and efficient parallel program design.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="CUDA Toolkit and Libraries">
  <data key="d0">CUDA Toolkit and Libraries</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A collection of software tools and libraries for developing high-performance GPU applications, primarily on NVIDIA hardware.&lt;SEP&gt;A suite of software tools and libraries for developing high-performance applications on NVIDIA GPUs using CUDA architecture.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GCC 13.2 Manual">
  <data key="d0">GCC 13.2 Manual</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">Official documentation for the GNU Compiler Collection version 13.2, guiding compilation and optimization processes.&lt;SEP&gt;Official documentation for the GNU Compiler Collection version 13.2, providing compiler and toolchain information.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Top500 List">
  <data key="d0">Top500 List</data>
  <data key="d1">&lt;Objects of Study</data>
  <data key="d2">A ranking of the world's most powerful supercomputers, updated periodically to reflect performance capabilities.&lt;SEP&gt;A ranking of the world's most powerful supercomputers, used for benchmarking and performance comparison.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Locality-Aware Scheduling">
  <data key="d0">Locality-Aware Scheduling</data>
  <data key="d1">&lt;Methodology</data>
  <data key="d2">A scheduling approach that improves task placement efficiency by considering data locality, enhancing performance on modern architectures.&lt;SEP&gt;A scheduling approach that optimizes task placement based on data locality to improve runtime efficiency on modern architectures.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Polly">
  <data key="d0">Polly</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A polyhedral optimization framework that performs advanced loop transformations on low-level intermediate representations to enhance performance.&lt;SEP&gt;A polyhedral optimization framework that performs advanced loop transformations to improve code performance.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Gurobi Optimizer">
  <data key="d0">Gurobi Optimizer</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A commercial optimization software for solving large-scale linear, integer, and quadratic programming problems efficiently.&lt;SEP&gt;A commercial optimization solver used for solving large-scale linear, quadratic, and integer programming problems efficiently.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="OpenSHMEM Using MPI-3">
  <data key="d0">OpenSHMEM Using MPI-3</data>
  <data key="d1">&lt;Methodology</data>
  <data key="d2">An implementation approach for shared memory programming models using MPI-3's one-sided communication features.&lt;SEP&gt;An implementation approach that leverages MPI-3's one-sided communication features to enable scalable shared memory programming models.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Optimizing Parallel Reduction in CUDA">
  <data key="d0">Optimizing Parallel Reduction in CUDA</data>
  <data key="d1">&lt;Methodology</data>
  <data key="d2">Techniques for enhancing the efficiency of reduction operations in CUDA programming to improve parallel performance.&lt;SEP&gt;Techniques for improving the efficiency of reduction operations in CUDA programming, critical for high-performance parallel algorithms.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Scalable Communication Protocols">
  <data key="d0">Scalable Communication Protocols</data>
  <data key="d1">&lt;Methodologies</data>
  <data key="d2">Protocols designed for efficient data exchange in distributed systems with dynamic sparse data, enabling scalable communication in HPC environments.&lt;SEP&gt;Protocols designed for efficient data exchange in dynamic sparse data scenarios across distributed systems.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="oneAPI Threading Building Blocks">
  <data key="d0">oneAPI Threading Building Blocks</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A C++ library from Intel that provides portable multithreading and task parallelism support for high-performance applications.&lt;SEP&gt;A C++ template library for task-based parallelism and multithreading, provided by Intel for portable high-performance code.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LULESH 2.0">
  <data key="d0">LULESH 2.0</data>
  <data key="d1">&lt;Objects of Study</data>
  <data key="d2">A hydrodynamics simulation code used as a benchmark to evaluate the performance and scalability of high-performance computing systems.&lt;SEP&gt;A hydrodynamics simulation code used to evaluate computational performance and scalability of high-performance computing systems.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Automatic Code Generation for Discontinuous Galerkin Methods">
  <data key="d0">Automatic Code Generation for Discontinuous Galerkin Methods</data>
  <data key="d1">&lt;Methodology</data>
  <data key="d2">Techniques for automatically generating optimized code for high-performance numerical methods on modern architectures.&lt;SEP&gt;Techniques for automating the generation of optimized code for high-performance numerical methods on modern architectures.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="RAJA Performance Portability Layer">
  <data key="d0">RAJA Performance Portability Layer</data>
  <data key="d1">&lt;Tools</data>
  <data key="d2">A software layer developed at LLNL that enables performance-portable code development across diverse architectures.&lt;SEP&gt;A software layer developed at LLNL that enables performance-portable programming across diverse hardware architectures by abstracting parallel execution and data management.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Large-Scale Stencil Computation">
  <data key="d0">Large-Scale Stencil Computation</data>
  <data key="d1">&lt;Objects of Study</data>
  <data key="d2">A numerical computation pattern involving repeated data updates over multi-dimensional grids, commonly used in scientific simulations, and optimized for many-core processors.&lt;SEP&gt;A numerical computation pattern involving repeated data updates over multi-dimensional grids, optimized for many-core processors.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Mingzhen Li, Yi Liu, Hailong Yang, Yongmin Hu, Qingxiao Sun, Bangduo Chen, Xin You, Xiaoyan Liu, Zhongzhi Luan, Depei Qian">
  <data key="d0">Mingzhen Li, Yi Liu, Hailong Yang, Yongmin Hu, Qingxiao Sun, Bangduo Chen, Xin You, Xiaoyan Liu, Zhongzhi Luan, Depei Qian</data>
  <data key="d1">Research Authors</data>
  <data key="d2">Researchers who authored the study on automatic code generation and optimization of large-scale stencil computation on many-core processors, contributing to advancements in parallel processing techniques.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Automatic code generation and optimization of large-scale stencil computation on many-core processors">
  <data key="d0">Automatic code generation and optimization of large-scale stencil computation on many-core processors</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A methodology or framework aimed at automating the creation and enhancement of code for large-scale stencil computations, focusing on efficiency on many-core architectures.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Proceedings of the 50th International Conference on Parallel Processing">
  <data key="d0">Proceedings of the 50th International Conference on Parallel Processing</data>
  <data key="d1">Study Design</data>
  <data key="d2">A conference proceedings where the research was published, indicating peer-reviewed dissemination of scientific findings in parallel processing.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Large-scale stencil computation">
  <data key="d0">Large-scale stencil computation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A computational pattern involving repetitive calculations over multi-dimensional grids, significant in scientific simulations and high-performance computing.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Many-core processors">
  <data key="d0">Many-core processors</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Hardware architecture consisting of numerous processing cores designed to perform parallel computations efficiently.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Code optimization">
  <data key="d0">Code optimization</data>
  <data key="d1">Methodology</data>
  <data key="d2">Techniques aimed at improving the efficiency, speed, and resource utilization of computational code, particularly in high-performance computing contexts.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Processing">
  <data key="d0">Parallel Processing</data>
  <data key="d1">Disciplines</data>
  <data key="d2">A field of computer science focused on executing multiple computations simultaneously to enhance performance.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Performance Variability in Mixed-Integer Programming">
  <data key="d0">Performance Variability in Mixed-Integer Programming</data>
  <data key="d1">Research Topic</data>
  <data key="d2">A study examining the fluctuations in performance outcomes within mixed-integer programming problems, relevant to optimization methodologies.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="INFORMS">
  <data key="d0">INFORMS</data>
  <data key="d1">Organization</data>
  <data key="d2">The Institute for Operations Research and the Management Sciences, an organization promoting research in analytics and optimization.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Korali">
  <data key="d0">Korali</data>
  <data key="d1">Tools</data>
  <data key="d2">A high-performance computing framework for stochastic optimization and Bayesian uncertainty quantification.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Memory Wall">
  <data key="d0">Memory Wall</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A fundamental bottleneck in computer architecture where memory access speeds limit overall system performance.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Compiler scalability">
  <data key="d0">Compiler scalability</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies and techniques for improving compiler performance and efficiency when handling large programs.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Efficiency of Algorithmic Structures">
  <data key="d0">Efficiency of Algorithmic Structures</data>
  <data key="d1">Study Design</data>
  <data key="d2">A research poster evaluating the performance of different algorithmic structures in high-performance computing.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="A Theoretical Model for Global Optimization of Parallel Algorithms">
  <data key="d0">A Theoretical Model for Global Optimization of Parallel Algorithms</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A conceptual framework designed to optimize parallel algorithms globally, improving their efficiency and effectiveness.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Polygeist: Raising C to Polyhedral MLIR">
  <data key="d0">Polygeist: Raising C to Polyhedral MLIR</data>
  <data key="d1">Tools</data>
  <data key="d2">A compiler tool that transforms C code into polyhedral representations within MLIR for optimization purposes.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="MPI: A Message-Passing Interface Standard">
  <data key="d0">MPI: A Message-Passing Interface Standard</data>
  <data key="d1">Tools</data>
  <data key="d2">A standardized protocol for message passing in parallel computing environments, facilitating communication among distributed processes.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Automatically scheduling halide image processing pipelines">
  <data key="d0">Automatically scheduling halide image processing pipelines</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for optimizing the execution order of image processing tasks on GPUs to improve performance.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Pthreads programming">
  <data key="d0">Pthreads programming</data>
  <data key="d1">Tools</data>
  <data key="d2">A POSIX standard API for multi-threaded programming, enabling concurrent execution on shared-memory systems.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Thrust: The C++ Parallel Algorithms Library">
  <data key="d0">Thrust: The C++ Parallel Algorithms Library</data>
  <data key="d1">Tools</data>
  <data key="d2">A library providing parallel algorithms in C++ for GPU and CPU acceleration.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="OpenCL">
  <data key="d0">OpenCL</data>
  <data key="d1">Tools</data>
  <data key="d2">An open standard for parallel programming of heterogeneous systems, supporting diverse hardware architectures.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="OpenMP 5.1 Specification">
  <data key="d0">OpenMP 5.1 Specification</data>
  <data key="d1">Tools</data>
  <data key="d2">A standard API for shared-memory parallel programming, enabling multi-platform high-performance computing.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Filtered beam search in scheduling">
  <data key="d0">Filtered beam search in scheduling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A scheduling algorithm technique that employs beam search to efficiently explore scheduling options.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="A compiler for throughput optimization of graph algorithms on GPUs">
  <data key="d0">A compiler for throughput optimization of graph algorithms on GPUs</data>
  <data key="d1">Tools</data>
  <data key="d2">A compiler designed to enhance the throughput of graph processing algorithms on GPU hardware.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C2Rust Manual">
  <data key="d0">C2Rust Manual</data>
  <data key="d1">Tools</data>
  <data key="d2">Documentation for C2Rust, a tool for translating C code to Rust for safer and more efficient programming.&lt;SEP&gt;The C2Rust Manual is a documentation resource for the C2Rust tool, which facilitates automatic translation of C code to Rust, aiding in software modernization and safety.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450&lt;SEP&gt;chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="github .com/LLNL/">
  <data key="d0">github .com/LLNL/</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The GitHub repository hosted by LLNL containing various research projects and codebases related to high-performance computing and scientific computing.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="RAJA">
  <data key="d0">RAJA</data>
  <data key="d1">Tools</data>
  <data key="d2">RAJA is a performance-portable programming interface that enables developers to write high-performance portable code for diverse hardware architectures, facilitating easier implementation of optimized computational kernels.</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="IEEE">
  <data key="d0">IEEE</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">IEEE is a professional organization that develops standards, promotes research, and organizes conferences in electrical and electronic engineering fields.&lt;SEP&gt;IEEE is a professional organization that publishes standards, conducts conferences, and promotes research in electrical and electronic engineering.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="30–40">
  <data key="d0">30–40</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Refers to a data range or scope, possibly indicating the span of data, time, or context covered in the document.&lt;SEP&gt;Refers to a range or span, possibly indicating data range, time period, or scope within the context of the document.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Michel Steuwer">
  <data key="d0">Michel Steuwer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Michel Steuwer is an author contributing to research on high-performance GPU code generation using functional data-parallel IR.&lt;SEP&gt;Michel Steuwer is an author who contributed to research on functional data-parallel intermediate representations for GPU code generation, focusing on high-performance computing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Toomas Remmelg">
  <data key="d0">Toomas Remmelg</data>
  <data key="d1">Researcher</data>
  <data key="d2">Toomas Remmelg is an author involved in research related to code generation and optimization for high-performance computing.&lt;SEP&gt;Toomas Remmelg is an author involved in research related to code generation and optimization techniques for high-performance GPU programming.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Christophe Dubach">
  <data key="d0">Christophe Dubach</data>
  <data key="d1">Researcher</data>
  <data key="d2">Christophe Dubach is an author contributing to the development of Lift, a functional IR for data-parallel GPU code generation.&lt;SEP&gt;Christophe Dubach is an author working on functional data-parallel IR for GPU code generation.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Damien Lebrun-Grandié">
  <data key="d0">Damien Lebrun-Grandié</data>
  <data key="d1">Researcher</data>
  <data key="d2">Damien Lebrun-Grandié is an author working on programming model extensions like Kokkos 3 to enable scalable, portable programming for exascale systems.&lt;SEP&gt;Damien Lebrun-Grandié is an author working on programming models for high-performance and exascale computing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Daniel Arndt">
  <data key="d0">Daniel Arndt</data>
  <data key="d1">Researcher</data>
  <data key="d2">Daniel Arndt is an author contributing to research on programming models for exascale systems.&lt;SEP&gt;Daniel Arndt is an author contributing to the development of programming model extensions such as Kokkos 3 for exascale computing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Kokkos 3">
  <data key="d0">Kokkos 3</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Kokkos 3 is an extension of the Kokkos programming model aimed at enabling scalable programming for exascale computing systems.&lt;SEP&gt;Kokkos 3 is an extension of the Kokkos programming model, designed to support exascale-era high-performance computing with scalable and portable abstractions.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Programming Model Extensions">
  <data key="d0">Programming Model Extensions</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Extensions to existing programming models, like Kokkos 3, that facilitate scalable, portable, and efficient programming for exascale architectures.&lt;SEP&gt;Extensions to programming models like Kokkos 3 that facilitate scalable, portable, and efficient programming for exascale architectures.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Lukas Trümper">
  <data key="d0">Lukas Trümper</data>
  <data key="d1">Researcher</data>
  <data key="d2">Lukas Trümper is an author focused on automating the mapping of parallel pattern-based algorithms onto heterogeneous architectures.&lt;SEP&gt;Lukas Trümper is an author researching automatic mapping techniques for parallel pattern-based algorithms onto heterogeneous architectures.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Julian Miller">
  <data key="d0">Julian Miller</data>
  <data key="d1">Researcher</data>
  <data key="d2">Julian Miller is an author involved in research on automating the deployment of parallel algorithms across diverse hardware architectures.&lt;SEP&gt;Julian Miller is an author involved in research on parallel algorithms and their implementation on diverse hardware architectures.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Christian Terboven">
  <data key="d0">Christian Terboven</data>
  <data key="d1">Researcher</data>
  <data key="d2">Christian Terboven is an author working on computational frameworks and algorithms for high-performance computing.&lt;SEP&gt;Christian Terboven is an author working on frameworks that support the automatic mapping of parallel patterns onto various architectures.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Matthias S. Müller">
  <data key="d0">Matthias S. Müller</data>
  <data key="d1">Researcher</data>
  <data key="d2">Matthias S. Müller is an author contributing to research on parallel algorithms and heterogeneous computing architectures.&lt;SEP&gt;Matthias S. Müller is an author focused on optimizing parallel algorithms for high-performance and heterogeneous computing environments.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Automatic Mapping of Parallel Pattern-Based Algorithms">
  <data key="d0">Automatic Mapping of Parallel Pattern-Based Algorithms</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology aimed at automating the translation and deployment of parallel algorithms onto heterogeneous architectures to improve efficiency and portability.&lt;SEP&gt;A methodology for automatically translating parallel pattern-based algorithms onto heterogeneous computing architectures to improve efficiency and portability.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Springer International Publishing">
  <data key="d0">Springer International Publishing</data>
  <data key="d1">Discipline</data>
  <data key="d2">A publisher specializing in scientific, technical, and medical books and journals, disseminating research in computer science and engineering.&lt;SEP&gt;Springer International Publishing is a publisher that disseminates scientific research, including topics related to high-performance computing and computer science.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Tyler Whitney">
  <data key="d0">Tyler Whitney</data>
  <data key="d1">Researcher</data>
  <data key="d2">Tyler Whitney is an author involved in developing the Parallel Patterns Library (PPL) for parallel programming in C++.&lt;SEP&gt;Tyler Whitney is an author involved in developing the Parallel Patterns Library (PPL), which provides abstractions for parallel programming in C++.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Kent Sharkey">
  <data key="d0">Kent Sharkey</data>
  <data key="d1">Researcher</data>
  <data key="d2">Kent Sharkey is an author contributing to research on parallel programming frameworks and libraries.&lt;SEP&gt;Kent Sharkey is an author contributing to the development and research of the Parallel Patterns Library (PPL) for parallel algorithm implementation.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Next Turn">
  <data key="d0">Next Turn</data>
  <data key="d1">Tools</data>
  <data key="d2">Next Turn is a platform or tool supporting parallel programming, providing frameworks or libraries for developing and deploying parallel algorithms.&lt;SEP&gt;Next Turn is a tool or platform related to parallel programming, possibly providing support or frameworks for parallel algorithm implementation.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Colin Robertson">
  <data key="d0">Colin Robertson</data>
  <data key="d1">Researcher</data>
  <data key="d2">Colin Robertson is an author involved in parallel computing research.&lt;SEP&gt;Colin Robertson is an author involved with Next Turn, contributing to parallel computing solutions.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Mike Jones">
  <data key="d0">Mike Jones</data>
  <data key="d1">Researcher</data>
  <data key="d2">Mike Jones is an author working on parallel programming tools and frameworks.&lt;SEP&gt;Mike Jones is an author working on tools or frameworks related to Next Turn for parallel processing.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Mike Blome">
  <data key="d0">Mike Blome</data>
  <data key="d1">Researcher</data>
  <data key="d2">Mike Blome contributes to the development of Next Turn's parallel computing tools or frameworks.&lt;SEP&gt;Mike Blome is an author contributing to research on parallel algorithms and tools.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Gordon Hogenson">
  <data key="d0">Gordon Hogenson</data>
  <data key="d1">Researcher</data>
  <data key="d2">Gordon Hogenson is an author involved in high-performance computing research.&lt;SEP&gt;Gordon Hogenson is an author involved in research or development activities related to Next Turn's platform.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Saisang Cai">
  <data key="d0">Saisang Cai</data>
  <data key="d1">Researcher</data>
  <data key="d2">Saisang Cai is an author contributing to parallel computing solutions within Next Turn.&lt;SEP&gt;Saisang Cai is an author working on parallel computing frameworks.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Patterns Library (PPL)">
  <data key="d0">Parallel Patterns Library (PPL)</data>
  <data key="d1">Tools</data>
  <data key="d2">The Parallel Patterns Library (PPL) is a C++ library that provides abstractions for parallel programming, enabling developers to write parallel algorithms more easily.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Samuel Williams">
  <data key="d0">Samuel Williams</data>
  <data key="d1">Researcher</data>
  <data key="d2">Samuel Williams is an author known for developing the Roofline performance model, which visualizes the performance of multicore architectures.&lt;SEP&gt;Samuel Williams is an author known for work on performance modeling of multicore architectures using the Roofline model.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Andrew Waterman">
  <data key="d0">Andrew Waterman</data>
  <data key="d1">Researcher</data>
  <data key="d2">Andrew Waterman is an author involved in research on performance analysis and optimization of multicore systems.&lt;SEP&gt;Andrew Waterman is an author utilizing the Roofline model to analyze and optimize multicore system performance.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="David Patterson">
  <data key="d0">David Patterson</data>
  <data key="d1">Researcher</data>
  <data key="d2">David Patterson is an author associated with the development and application of the Roofline performance model for performance analysis.&lt;SEP&gt;David Patterson is an author contributing to computer architecture and performance modeling research.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Roofline">
  <data key="d0">Roofline</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Roofline is a visual performance model that helps analyze and optimize the performance of multicore architectures by relating computational intensity and achievable performance.&lt;SEP&gt;The Roofline model is a visual performance analysis tool that relates computational intensity to achievable performance on multicore architectures.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Charles Yount">
  <data key="d0">Charles Yount</data>
  <data key="d1">Researcher</data>
  <data key="d2">Charles Yount is an author involved in developing frameworks like YASK for high-performance stencil computations.&lt;SEP&gt;Charles Yount is an author involved in high-performance computing frameworks like YASK for stencil code generation.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Josh Tobin">
  <data key="d0">Josh Tobin</data>
  <data key="d1">Researcher</data>
  <data key="d2">Josh Tobin is an author working on stencil code-generation frameworks for high-performance computing.&lt;SEP&gt;Josh Tobin is an author working on the development and optimization of YASK, a framework for high-performance stencil computations.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Alexander Breuer">
  <data key="d0">Alexander Breuer</data>
  <data key="d1">Researcher</data>
  <data key="d2">Alexander Breuer is an author contributing to HPC stencil code-generation and optimization.&lt;SEP&gt;Alexander Breuer is an author contributing to the development and optimization of YASK for scientific HPC applications.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Stencil Code-Generation">
  <data key="d0">Stencil Code-Generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Stencil code-generation involves creating optimized code for stencil computations, crucial in scientific simulations, with tools like YASK facilitating this process.&lt;SEP&gt;Stencil code-generation involves creating optimized computational kernels for scientific simulations, with tools like YASK facilitating this process.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C2Rust Development Team">
  <data key="d0">C2Rust Development Team</data>
  <data key="d1">Tools</data>
  <data key="d2">The C2Rust Manual is a documentation resource for the C2Rust tool, which automates translation of C code to Rust, aiding in safety and modernization of codebases.</data>
  <data key="d3">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Pattern Language">
  <data key="d0">Parallel Pattern Language</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d2">The code generator is an implementation of the Parallel Pattern Language, producing optimized source code for heterogeneous HPC systems.&lt;SEP&gt;The code generator is an implementation of the Parallel Pattern Language, producing optimized source code for shared memory, distributed memory, and GPU offloading."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Global Optimization via Linear Programming">
  <data key="d0">Global Optimization via Linear Programming</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d2">The high-level IR facilitates global optimization processes, including LP-based workload assignment."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="High-Level IR">
  <data key="d0">High-Level IR</data>
  <data key="d3">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d2">The high-level IR facilitates global optimization processes, including LP-based workload assignment."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="LP">
  <data key="d0">LP</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d2">LP models are used to perform global optimization in the current prototype, addressing issues in the code generator and optimization processes.&lt;SEP&gt;LP models are used within the current prototype to address issues in the code generator and to guide optimization strategies for global problem solving."|&gt;"optimization, modeling</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="PPL">
  <data key="d0">PPL</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d2">The PPL toolchain is evaluated against kernels from the Rodinia suite to assess performance and applicability.&lt;SEP&gt;The PPL toolchain is evaluated against kernels from the Rodinia suite to validate its performance and applicability across diverse workloads."|&gt;"benchmarking, evaluation</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="DSL">
  <data key="d0">DSL</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d2">The custom DSL used in PPL allows for targeted optimizations beyond syntactical translation, focusing on application-specific tuning."|&gt;"domain-specific optimization, language design&lt;SEP&gt;The custom DSL used in PPL enables application-specific optimizations, allowing targeted code transformations beyond syntactic translation."|&gt;"domain-specific optimization, language design</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Libraries">
  <data key="d0">Libraries</data>
  <data key="d3">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d2">Kokkos provides performance portability for HPC applications across different hardware architectures, supporting node-level parallelism."|&gt;"performance portability, HPC</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="APT Generation">
  <data key="d0">APT Generation</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d2">Parsing evaluates user input in DSL and hardware description, enabling subsequent generation of the hierarchical APT representation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="APT">
  <data key="d0">APT</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d2">The APT is subjected to global optimizations, such as reordering and explicit hardware mapping, to improve performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="AMT Generation">
  <data key="d0">AMT Generation</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d2">Optimization results inform the generation of the AMT, which adds necessary synchronization and data transfer details for code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="AMT">
  <data key="d0">AMT</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d2">The AMT serves as the basis for generating optimized C++ code, incorporating techniques like function-inlining and loop-unrolling.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Hardware Language">
  <data key="d0">Hardware Language</data>
  <data key="d3">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d2">The hardware description in JSON guides the code generation process to produce hardware-aware optimized code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Measurements">
  <data key="d0">Measurements</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d2">All measurements are performed on the CLAIX18 systems, which are equipped with specified hardware components."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Systems">
  <data key="d0">Systems</data>
  <data key="d3">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d2">All measurements are performed on the CLAIX18 systems, which are equipped with specified hardware components."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Applications">
  <data key="d0">Applications</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d2">Applications with dynamic workloads like b+tree and BFS require runtime adaptation beyond static approaches."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Workloads">
  <data key="d0">Dynamic Workloads</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d2">Applications with dynamic workloads like b+tree and BFS require runtime adaptation beyond static approaches."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Runtime Arguments">
  <data key="d0">Runtime Arguments</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d2">Extracted dependency chains can identify variables that influence control flow and parallel patterns at runtime."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Dynamic Applications">
  <data key="d0">Dynamic Applications</data>
  <data key="d3">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d2">Legion supports executing patterns of unknown size, aiding dynamic load balancing."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Proceedings of the ACM Digital Library">
  <data key="d0">&lt;&gt;Proceedings of the ACM Digital Library</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">The document reports on research related to performance portability across various architectures, including methodologies and tools for achieving this goal.&lt;SEP&gt;The document reports research efforts focused on achieving and evaluating performance portability across diverse hardware architectures using various methodologies and tools.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Tal Ben-Nun et al.">
  <data key="d0">&lt;&gt;Tal Ben-Nun et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Provides a data-centric model for parallel programming that aims to improve performance portability and efficiency.&lt;SEP&gt;Provides a data-centric programming model designed to improve performance portability and efficiency in heterogeneous systems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Ernesto G Birgin et al.">
  <data key="d0">&lt;&gt;Ernesto G Birgin et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A scheduling method designed to optimize permutation flowshop problems, relevant for scheduling tasks in high-performance computing.&lt;SEP&gt;A scheduling methodology aimed at optimizing permutation flowshop problems, relevant for task scheduling in high-performance computing environments.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Pradip Bose">
  <data key="d0">&lt;&gt;Pradip Bose</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Describes a fundamental hardware limitation that influences the design and performance potential of computing architectures.&lt;SEP&gt;Describes the power consumption limitations that influence hardware performance and architectural design.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Shuai Che et al.">
  <data key="d0">&lt;&gt;Shuai Che et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A benchmarking suite used to evaluate the performance and scalability of heterogeneous computing systems across various workloads.&lt;SEP&gt;A benchmarking suite used to evaluate the performance of heterogeneous computing systems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Philipp Ciechanowicz et al.">
  <data key="d0">&lt;&gt;Philipp Ciechanowicz et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Provides tools for skeletal programming, aiding in structured parallel computation management.&lt;SEP&gt;Provides tools to facilitate structured parallel programming via skeletal abstractions, aiding in performance optimization.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Murray Cole">
  <data key="d0">&lt;&gt;Murray Cole</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A conceptual framework for managing parallel algorithms through structured abstractions, facilitating performance optimization.&lt;SEP&gt;A high-level conceptual framework for managing parallel algorithms, promoting structured and efficient parallel programming.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Massimiliano Fatica">
  <data key="d0">&lt;&gt;Massimiliano Fatica</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Tools essential for developing, optimizing, and deploying GPU-accelerated high-performance applications.&lt;SEP&gt;Tools for developing and optimizing GPU-accelerated applications, integral for performance portability.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Free Software Foundation">
  <data key="d0">&lt;&gt;Free Software Foundation</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Documentation guiding the compilation and optimization process for performance-critical code.&lt;SEP&gt;Documentation providing guidance on compiling and optimizing code for performance and portability.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Prometeus GmbH">
  <data key="d0">&lt;&gt;Prometeus GmbH</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Provides rankings of supercomputers, useful for benchmarking and evaluating high-performance systems.&lt;SEP&gt;Provides rankings of the most powerful supercomputers, serving as benchmarks for evaluating HPC performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Maxime Gonthier et al.">
  <data key="d0">&lt;&gt;Maxime Gonthier et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A scheduling methodology that improves task placement efficiency by considering data locality, boosting performance on modern architectures.&lt;SEP&gt;An approach to improve task scheduling efficiency by considering data locality, enhancing performance on modern architectures.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Tobias Grosser et al.">
  <data key="d0">&lt;&gt;Tobias Grosser et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Performs polyhedral optimizations to enhance loop execution efficiency, contributing to performance improvements in parallel code.&lt;SEP&gt;Performs polyhedral optimizations to improve loop execution efficiency in parallel programs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Gurobi Optimization, LLC">
  <data key="d0">&lt;&gt;Gurobi Optimization, LLC</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A solver used for large-scale optimization problems, supporting high-performance decision-making tasks.&lt;SEP&gt;An optimization solver used to solve large-scale linear, quadratic, and integer programming problems efficiently, supporting performance tuning.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Jeff R. Hammond et al.">
  <data key="d0">&lt;&gt;Jeff R. Hammond et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A methodology for implementing shared memory programming models using MPI-3 features, facilitating scalable parallelism.&lt;SEP&gt;A methodology that leverages MPI-3's one-sided communication to implement scalable shared memory programming models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Mark Harris et al.">
  <data key="d0">&lt;&gt;Mark Harris et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Techniques for enhancing reduction operations in CUDA, improving parallel computation efficiency.&lt;SEP&gt;Techniques to improve the efficiency of reduction operations in CUDA, which are critical for many parallel algorithms.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Torsten Hoefler et al.">
  <data key="d0">&lt;&gt;Torsten Hoefler et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Protocols designed for efficient data exchange in distributed systems with dynamic sparse data, enabling scalable high-performance computing.&lt;SEP&gt;Protocols designed to facilitate efficient data exchange in distributed systems with dynamic sparse data, critical for scalable HPC.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Intel Corporation">
  <data key="d0">&lt;&gt;Intel Corporation</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A library for portable multithreading and task parallelism, supporting performance portability.&lt;SEP&gt;A portable C++ library for multithreading and task parallelism, supporting performance portability across architectures.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Karlin et al.">
  <data key="d0">&lt;&gt;Karlin et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A hydrodynamics simulation code used as a benchmark to evaluate system performance, scalability, and portability in HPC systems.&lt;SEP&gt;A hydrodynamics simulation code used to evaluate system performance and scalability in high-performance computing.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Dominic Kempf et al.">
  <data key="d0">&lt;&gt;Dominic Kempf et al.</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Techniques for automating code creation to optimize numerical methods on modern architectures.&lt;SEP&gt;Techniques for automating high-performance code generation for numerical methods, facilitating portability and efficiency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Chris Lattner and Vikram Adve">
  <data key="d0">&lt;&gt;Chris Lattner and Vikram Adve</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A compiler infrastructure enabling program analysis and transformation, essential for optimizing performance across architectures.&lt;SEP&gt;A compiler infrastructure framework that enables program analysis, transformation, and optimization to improve performance across architectures.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Lawrence Livermore National Laboratory">
  <data key="d0">&lt;&gt;Lawrence Livermore National Laboratory</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A software layer facilitating performance-portable programming across diverse hardware architectures.&lt;SEP&gt;A software layer that abstracts hardware details to support performance-portable code development across diverse architectures.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Various datasets">
  <data key="d0">&lt;&gt;Various datasets</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">A common computational pattern in scientific simulations, used to evaluate performance and optimization strategies on many-core processors.&lt;SEP&gt;A computational pattern used to evaluate performance of numerical algorithms on many-core processors, impacting scientific simulations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="&lt;&gt;Various datasets and workloads">
  <data key="d0">&lt;&gt;Various datasets and workloads</data>
  <data key="d3">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d2">Used to evaluate performance, scalability, and portability of HPC techniques across different hardware and problem sizes.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Heterogeneous computing architectures">
  <data key="d0">Heterogeneous computing architectures</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">OpenSBLI is applied to heterogeneous computing architectures for fluid dynamics simulations, illustrating its use in diverse hardware environments.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Bayesian Uncertainty Quantification">
  <data key="d0">Bayesian Uncertainty Quantification</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">Korali provides a framework for Bayesian uncertainty quantification, enabling probabilistic analysis in high-performance computing tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Algorithms">
  <data key="d0">Parallel Algorithms</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">The study evaluates the efficiency of certain algorithmic structures in optimizing parallel algorithms.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C to Polyhedral MLIR">
  <data key="d0">C to Polyhedral MLIR</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">Polygeist transforms C code into polyhedral MLIR for optimization, bridging programming languages and compiler frameworks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Polygeist">
  <data key="d0">Polygeist</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">Polygeist transforms C code into polyhedral MLIR for optimization, bridging programming languages and compiler frameworks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Parallel Computing">
  <data key="d0">Parallel Computing</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">MPI provides message-passing capabilities essential for parallel computing across distributed systems.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Filtering beam search">
  <data key="d0">Filtering beam search</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">Filtered beam search is used to improve scheduling efficiency by exploring promising options while pruning less optimal ones.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="GPU graph algorithms compiler">
  <data key="d0">GPU graph algorithms compiler</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">The compiler optimizes throughput of graph algorithms on GPUs, enhancing performance in data-parallel tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="Throughput optimization">
  <data key="d0">Throughput optimization</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">The compiler optimizes throughput of graph algorithms on GPUs, enhancing performance in data-parallel tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="C2Rust">
  <data key="d0">C2Rust</data>
  <data key="d3">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d2">C2Rust facilitates translating C code to Rust, improving safety and performance in systems programming.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</node>
<node id="DSP Y">
  <data key="d0">DSP Y</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A programming model designed for composing, optimizing, and executing language model pipelines through declarative text transformation graphs and parameterized modules.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Compiling Declarative Language">
  <data key="d0">Compiling Declarative Language</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of translating high-level DSP Y programs into optimized, executable pipelines for language models, enhancing performance and flexibility.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self-Improving Pipelines">
  <data key="d0">Self-Improving Pipelines</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Automated, adaptive systems that can modify and improve their own processing pipelines based on performance metrics.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Language Models (LMs)">
  <data key="d0">Language Models (LMs)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">AI models trained to generate or understand human language, used in various tasks including prompting, in-context learning, and tool invocation.&lt;SEP&gt;AI models trained to understand and generate human language, including models like GPT, T5, and Llama, which are central to the discussed techniques.&lt;SEP&gt;Artificial intelligence models capable of understanding, generating, and reasoning with human language, used within pipelines for complex NLP tasks.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompt Templates">
  <data key="d0">Prompt Templates</data>
  <data key="d1">Tools</data>
  <data key="d2">Predefined large text templates used to instruct language models for specific tasks, often lengthy and manually crafted, as seen in existing libraries like LangChain.&lt;SEP&gt;Predefined prompt formats used to standardize and streamline prompt creation for language models.&lt;SEP&gt;Predefined, manually crafted string prompts used to guide language models, which are often brittle and lack scalability.&lt;SEP&gt;Prompt templates are predefined input formats that guide the language model to generate specific outputs, such as translating code or creating programming problems.&lt;SEP&gt;Prompt templates are predefined input formats used to guide the language model in generating specific outputs, such as code translation or problem solving.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f&lt;SEP&gt;chunk-5ff9371dcd9f0dcbeb25583bbba6123c&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy Modules">
  <data key="d0">DSPy Modules</data>
  <data key="d1">Tools</data>
  <data key="d2">Parameterized, modular components within DSP Y that encapsulate prompting, finetuning, augmentation, and reasoning techniques, capable of learning from demonstrations.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Text Transformation Graphs">
  <data key="d0">Text Transformation Graphs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Directed graphs representing sequences of text processing and model invocation steps, enabling systematic pipeline construction.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mathematical Word Problems">
  <data key="d0">Mathematical Word Problems</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Complex reasoning tasks involving math, addressed by DSP Y pipelines to demonstrate reasoning capabilities.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Multi-hop Retrieval">
  <data key="d0">Multi-hop Retrieval</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Information retrieval tasks requiring multiple reasoning steps, tackled by DSP Y pipelines to improve accuracy.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Complex Question Answering">
  <data key="d0">Complex Question Answering</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Tasks involving multi-step reasoning to answer intricate questions, addressed by DSP Y pipelines.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Llama2-13b-Chat">
  <data key="d0">Llama2-13b-Chat</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An open-source, smaller language model used to demonstrate DSP Y's performance in resource-constrained settings.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy Program">
  <data key="d0">DSPy Program</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A written script in DSP Y that expresses and optimizes language model pipelines for various complex NLP tasks.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Case Studies">
  <data key="d0">Case Studies</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Experiments demonstrating DSP Y's capability to reason about math problems, perform multi-hop retrieval, and answer complex questions.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Expert-Created Demonstrations">
  <data key="d0">Expert-Created Demonstrations</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Manually designed prompt chains used as benchmarks to compare DSP Y pipelines' performance.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompting Techniques">
  <data key="d0">Prompting Techniques</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Methods like Chain of Thought and ReAct that guide models through reasoning processes, forming the basis for pipeline design.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Pipeline Optimization">
  <data key="d0">Pipeline Optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Automated techniques within DSP Y to improve pipeline performance based on metrics and task requirements.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Open and Small LMs">
  <data key="d0">Open and Small LMs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Resource-efficient language models like 770M-parameter T5 and Llama2-13b-chat used to evaluate DSP Y in constrained environments.</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy">
  <data key="d0">DSPy</data>
  <data key="d1">Tools</data>
  <data key="d2">A framework for composing and executing programs involving retrieval, reasoning, and answer generation in question answering tasks.&lt;SEP&gt;A framework for composing, executing, and managing programs involving retrieval, reasoning, and answer generation in complex question answering.&lt;SEP&gt;A modular programming language or toolkit used to define, compile, and optimize programs (modules) for language models, capable of replacing prompt strings with concise modules.&lt;SEP&gt;A new programming framework introduced for designing AI systems using pipelines of pretrained language models and tools, emphasizing rapid development, modularity, and systematic leveraging of LMs.&lt;SEP&gt;A new programming model introduced for designing AI systems through pipelines of pretrained language models and tools, emphasizing rapid development and systematic leveraging of LMs.&lt;SEP&gt;A programming framework designed for composing, optimizing, and evaluating language models and related programs, supporting modules like teleprompters and ensemble techniques.&lt;SEP&gt;A programming framework designed to compose, optimize, and evaluate language model-based programs, supporting modules like teleprompters, ensembles, and finetuning.&lt;SEP&gt;A programming framework for optimizing language model pipelines using high-level signatures, modular units, and automated optimization, reducing reliance on prompt engineering.&lt;SEP&gt;A programming framework that abstracts language model workflows into high-level signatures, modules, and teleprompters, enabling automated and modular pipeline optimization without manual prompt engineering.&lt;SEP&gt;A programming model designed to translate prompting techniques into parameterized declarative modules and optimize NLP pipelines using a compiler with strategies called teleprompters.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0&lt;SEP&gt;chunk-9596cd861216e68aa0fdb35edbc9a132&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85&lt;SEP&gt;chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chain of Thought">
  <data key="d0">Chain of Thought</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A prompting technique that involves step-by-step reasoning processes to improve model accuracy on complex tasks.&lt;SEP&gt;A prompting technique that involves step-by-step reasoning to improve model performance on complex tasks.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ReAct">
  <data key="d0">ReAct</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A multi-step agent approach for tool use in question answering, implemented as a module in DSPy, involving iterative reasoning and retrieval.&lt;SEP&gt;A multi-step reasoning and tool use framework for question answering, implemented as a module in DSPy involving iterative reasoning steps.&lt;SEP&gt;A prompting technique combining reasoning and acting to enhance language model capabilities.&lt;SEP&gt;A prompting technique that combines reasoning and acting steps to enhance language model reasoning capabilities.&lt;SEP&gt;ReAct is a reasoning framework combining reasoning and acting, used in AI to improve decision-making and task performance.&lt;SEP&gt;ReAct is a theoretical framework or model that combines reasoning and acting components, used to improve AI decision-making and task execution.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132&lt;SEP&gt;chunk-d47a0e3d5d118be72a9f7b527b440956&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Teleprompters">
  <data key="d0">Teleprompters</data>
  <data key="d1">Tools</data>
  <data key="d2">Components within DSPy that automatically optimize modules (via model selection, RL, Bayesian methods) to maximize task-specific metrics.&lt;SEP&gt;Optimization components in DSPy that adjust modules to maximize performance metrics, possibly using model selection or reinforcement learning.&lt;SEP&gt;Optimization strategies within DSPy that determine how modules learn from data to improve system performance.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Multi-hop question answering">
  <data key="d0">Multi-hop question answering</data>
  <data key="d1">Study Design</data>
  <data key="d2">A task involving reasoning over multiple pieces of information to answer complex questions, explored in case studies.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Math word problems">
  <data key="d0">Math word problems</data>
  <data key="d1">Study Design</data>
  <data key="d2">A dataset involving solving mathematical problems expressed in natural language, used to evaluate DSPy.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LLaMA2-13b-chat">
  <data key="d0">LLaMA2-13b-chat</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large language model with 13 billion parameters, used as a base model in experiments.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="T5-Large">
  <data key="d0">T5-Large</data>
  <data key="d1">Tools</data>
  <data key="d2">A 770M parameter language model used for finetuning in the multihop question answering program.&lt;SEP&gt;A pre-trained language model with 770 million parameters used for finetuning in multihop question answering programs.&lt;SEP&gt;A transformer-based language model with 770 million parameters, used in the evaluation.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self-improving NLP systems">
  <data key="d0">Self-improving NLP systems</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Systems capable of enhancing their own performance through modular prompts and optimization strategies.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Empirical and algorithmic contributions">
  <data key="d0">Empirical and algorithmic contributions</data>
  <data key="d1">Results</data>
  <data key="d2">The main findings demonstrating that DSPy can outperform systems with hand-crafted prompts and use smaller models effectively.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="layers">
  <data key="d0">layers</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Layers refer to modular components that can be assembled to build complex architectures, enabling flexible and scalable system design.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="model weights">
  <data key="d0">model weights</data>
  <data key="d1">Variables</data>
  <data key="d2">Model weights are parameters within a neural network that are trained using optimizers, replacing hand-tuning for better performance.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="optimizers">
  <data key="d0">optimizers</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Modules, also called teleprompters, designed to improve the performance and reliability of language models within the graph framework.&lt;SEP&gt;Optimizers are algorithms used to adjust model weights during training, facilitating effective learning processes.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy programming model">
  <data key="d0">DSPy programming model</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A modular programming framework that translates prompting techniques into parameterized modules, allowing flexible NLP pipeline construction.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sec 3">
  <data key="d0">Sec 3</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Refers to the section of the paper where the DSPy programming model is introduced and detailed.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="string-based prompting techniques">
  <data key="d0">string-based prompting techniques</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Techniques involving the use of natural language prompts to guide language models' behavior, including complex and task-dependent prompts like Chain of Thought and ReAct.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="declarative modules">
  <data key="d0">declarative modules</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Modules that carry natural-language typed signatures, representing text transformations, used within DSPy to abstract and implement NLP tasks.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="natural-language typed signatures">
  <data key="d0">natural-language typed signatures</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Typed signatures in DSPy modules that specify the input/output nature of each module using natural language for clarity and flexibility.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="parameterize">
  <data key="d0">parameterize</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of assigning learnable parameters to DSPy modules so they can adapt their behavior through demonstrations and training.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="bootstrapping">
  <data key="d0">bootstrapping</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A statistical resampling technique that involves repeatedly sampling from a dataset with replacement to estimate the stability or variance of a model or process, mentioned as a future technique for dynamic evaluation.&lt;SEP&gt;A technique where modules learn their desired behavior iteratively by using demonstrations within the pipeline, enhancing their performance.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="expressive define-by-run computational graphs">
  <data key="d0">expressive define-by-run computational graphs</data>
  <data key="d1">Tools</data>
  <data key="d2">A programming paradigm inspired by PyTorch that allows dynamic construction and execution of models, used in DSPy for flexible pipeline definition.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy compiler">
  <data key="d0">DSPy compiler</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A component that optimizes DSPy programs to improve quality or reduce cost by simulating and bootstrapping example traces and guiding module learning.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="optimization strategies">
  <data key="d0">optimization strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Strategies called teleprompters that determine how modules learn from data to enhance system performance.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="training inputs with optional labels">
  <data key="d0">training inputs with optional labels</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Data used to train and evaluate DSPy modules during compilation and optimization.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="validation metric">
  <data key="d0">validation metric</data>
  <data key="d1">Variables</data>
  <data key="d2">A metric used to evaluate the performance of DSPy programs during the optimization process.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self-improving multi-stage NLP systems">
  <data key="d0">self-improving multi-stage NLP systems</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">NLP systems that bootstrap their own performance improvements through modular prompts and optimization strategies, reducing reliance on manual prompt engineering.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="empirical and algorithmic contributions">
  <data key="d0">empirical and algorithmic contributions</data>
  <data key="d1">Results</data>
  <data key="d2">The main findings demonstrating DSPy's effectiveness in outperforming hand-crafted prompt systems and enabling smaller, efficient language models.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="case studies">
  <data key="d0">case studies</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Empirical evaluations involving specific NLP tasks like math problems and multi-hop QA to demonstrate DSPy's capabilities.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="small language models">
  <data key="d0">small language models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Models like llama2-13b-chat and T5-Large that are used to test DSPy's ability to improve performance with smaller, more efficient models.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="automatic prompt optimization">
  <data key="d0">automatic prompt optimization</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The process enabled by DSPy where prompts are automatically improved via compilation and optimization strategies, reducing manual effort.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="multi-chain reflection">
  <data key="d0">multi-chain reflection</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A technique involving multiple reasoning chains to improve model understanding and accuracy.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="retrieval-augmented question answering">
  <data key="d0">retrieval-augmented question answering</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A method that enhances QA systems by retrieving relevant information during reasoning, explored within DSPy case studies.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="experts">
  <data key="d0">experts</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Domain specialists involved in designing prompts, models, and evaluating NLP system performance.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="empirical evaluation">
  <data key="d0">empirical evaluation</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Systematic testing and measurement of DSPy's performance across various tasks and models.</data>
  <data key="d3">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Radford et al. 2018; Brown et al. 2020">
  <data key="d0">Radford et al. 2018; Brown et al. 2020</data>
  <data key="d1">Study Design</data>
  <data key="d2">A foundational research work exploring mechanisms for foundation model programming, emphasizing instruction tuning and prompting techniques.&lt;SEP&gt;Seminal research papers that investigate foundational mechanisms in foundation model programming, emphasizing the role of instruction tuning and prompting techniques.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Instruction Tuning">
  <data key="d0">Instruction Tuning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A method to elicit sophisticated behavior from language models by providing specific training instructions, enhancing model performance on downstream tasks.&lt;SEP&gt;A training process that involves providing models with specific instructions to improve their ability to perform tasks, especially in zero-shot or few-shot settings.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompting">
  <data key="d0">Prompting</data>
  <data key="d1">Methodology</data>
  <data key="d2">A method of eliciting specific responses from language models by providing carefully designed input prompts, often used in instruction tuning and in-context learning.&lt;SEP&gt;A technique used to elicit specific responses from language models by providing carefully crafted input prompts, often used in conjunction with instruction tuning.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Weak Supervision">
  <data key="d0">Weak Supervision</data>
  <data key="d1">Methodology</data>
  <data key="d2">A form of supervision that leverages less explicit, often heuristic-based signals to train models, reducing the need for task-specific or hand-crafted labels.&lt;SEP&gt;A training paradigm where models learn from heuristic, noisy, or indirect signals rather than explicit labels, reducing the need for task-specific annotations.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LM Pipelines">
  <data key="d0">LM Pipelines</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Complex workflows combining language models with tools and retrieval systems to perform multi-stage tasks, enabling more sophisticated AI applications.&lt;SEP&gt;Complex workflows combining language models with tools, retrieval systems, and other modules to perform multi-stage tasks.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Toolkits">
  <data key="d0">Toolkits</data>
  <data key="d1">Tools</data>
  <data key="d2">Software frameworks like LangChain, Semantic Kernel, LlamaIndex that facilitate connecting language models with various tools and managing prompt templates.&lt;SEP&gt;Software frameworks like LangChain, Semantic Kernel, LlamaIndex, which facilitate connecting language models with external tools and managing prompt templates.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Discrete Optimization and Reinforcement Learning (RL)">
  <data key="d0">Discrete Optimization and Reinforcement Learning (RL)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques applied to optimize prompts and model behaviors, potentially involving model selection, hyperparameter tuning, and feedback loops.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Signatures">
  <data key="d0">Signatures</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Natural language declarations that specify input-output behaviors of language model modules, facilitating abstraction and automation in pipeline design.&lt;SEP&gt;Natural language declarations that specify the input/output behavior of language model modules, enabling abstraction over specific prompts or finetuning.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Modules">
  <data key="d0">Modules</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Reusable components in DSPy representing stages in a pipeline that can be composed and optimized without hand-crafted prompts.&lt;SEP&gt;Reusable components in DSPy representing stages in a pipeline, which can be composed and optimized collectively, replacing hand-crafted prompts.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Model Selection Techniques">
  <data key="d0">Model Selection Techniques</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Approaches like cross-validation, reinforcement learning, Bayesian hyperparameter optimization used within DSPy to select optimal models or configurations for pipeline modules.&lt;SEP&gt;Approaches such as cross-validation, reinforcement learning, Bayesian hyperparameter optimization used within DSPy to select optimal models or configurations for modules and pipelines.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Benchmark Numbers and Qualitative Measures">
  <data key="d0">Benchmark Numbers and Qualitative Measures</data>
  <data key="d1">Results</data>
  <data key="d2">Metrics and assessments used to evaluate DSPy's effectiveness in constructing prompt-free, modular language model systems.&lt;SEP&gt;Metrics used to evaluate the effectiveness of DSPy in building modular, prompt-free language model systems.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="High-Level Programmatic Abstraction">
  <data key="d0">High-Level Programmatic Abstraction</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An approach that models language model workflows at a high level, enabling systematic exploration and optimization of systems without manual prompt engineering.&lt;SEP&gt;The approach of designing language model workflows at a high level, enabling systematic exploration and optimization without manual prompt engineering.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Foundation Model Programming">
  <data key="d0">Foundation Model Programming</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A set of techniques and mechanisms for developing large-scale AI models capable of understanding and generating human-like text, primarily driven by instruction tuning and prompting.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Discrete Optimization">
  <data key="d0">Discrete Optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques like grid search, Bayesian optimization, and reinforcement learning used to optimize prompts and model parameters within LM workflows.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Reinforcement Learning (RL)">
  <data key="d0">Reinforcement Learning (RL)</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A machine learning approach where models learn to optimize behaviors through feedback signals, applied here to prompt and pipeline optimization.</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy Signatures">
  <data key="d0">DSPy Signatures</data>
  <data key="d1">Methodologies</data>
  <data key="d2">DSPy Signatures define structured interfaces for prompting language models, enabling modular, adaptable, and self-improving prompts for various tasks.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Predict Module">
  <data key="d0">Predict Module</data>
  <data key="d1">Tools</data>
  <data key="d2">Predict is a core DSPy module that formats prompts based on signatures, manages demonstrations, and interacts with language models to produce structured outputs.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ChainOfThought">
  <data key="d0">ChainOfThought</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A module designed to incorporate step-by-step reasoning into model predictions, by modifying signatures to include rationale outputs.&lt;SEP&gt;ChainOfThought is a DSPy module that guides language models to think step-by-step before producing final answers, enhancing reasoning capabilities.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4&lt;SEP&gt;chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Structured Prompting">
  <data key="d0">Structured Prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Structured prompting involves designing prompts with clear formats and interfaces, such as signatures, to improve model performance and consistency.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Parameterization">
  <data key="d0">Parameterization</data>
  <data key="d1">Variables</data>
  <data key="d2">Parameterization in DSPy refers to defining flexible prompt templates and modules that can be customized for different tasks, models, and contexts.&lt;SEP&gt;The concept of defining specific parameters that customize the behavior of prompting techniques within the DSPy framework.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4&lt;SEP&gt;chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self-Improving Prompts">
  <data key="d0">Self-Improving Prompts</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Self-improving prompts are designed to iteratively enhance their effectiveness through structured feedback and adaptation within the DSPy framework.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Bootstrapping">
  <data key="d0">Bootstrapping</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Bootstrapping involves using demonstrations and structured templates to train or adapt prompts for better performance in language models.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Structured Formatting and Parsing">
  <data key="d0">Structured Formatting and Parsing</data>
  <data key="d1">Tools</data>
  <data key="d2">DSPy handles structured formatting and parsing of model outputs to ensure consistency and interpretability of generated data.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Meta-Prompting">
  <data key="d0">Meta-Prompting</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Meta-prompting refers to designing prompts that instruct models to generate or improve their own prompts or reasoning processes.</data>
  <data key="d3">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="forward the inputs to the sub-module">
  <data key="d0">forward the inputs to the sub-module</data>
  <data key="d1">Methodology</data>
  <data key="d2">A procedural step indicating the process of passing inputs to a sub-module within a computational system.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="12 return self.predict(**kwargs)">
  <data key="d0">12 return self.predict(**kwargs)</data>
  <data key="d1">Methodology</data>
  <data key="d2">A code statement representing the return of a prediction function with specific arguments, indicating a step in a computational pipeline.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="This is a fully-fledged module capable of learning effective few-shot prompting for any LM or task">
  <data key="d0">This is a fully-fledged module capable of learning effective few-shot prompting for any LM or task</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A comprehensive module designed to learn and implement few-shot prompting techniques across various language models and tasks.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Appendix C">
  <data key="d0">Appendix C</data>
  <data key="d1">Study Design</data>
  <data key="d2">A referenced section in research or documentation that contains examples or supplementary information related to prompting techniques.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="long reasoning prompts hand-written by sources">
  <data key="d0">long reasoning prompts hand-written by sources</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Pre-designed, manually crafted prompts used in research to evaluate or demonstrate prompting methods.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy parameterizes these prompting techniques">
  <data key="d0">DSPy parameterizes these prompting techniques</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">The act of configuring or customizing prompting methods through parameters within the DSPy system to adapt to different tasks or models.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LM call seeking to implement a particular signature">
  <data key="d0">LM call seeking to implement a particular signature</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A specific invocation of a language model with a defined input-output behavior or signature.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt instructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3) the demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning)">
  <data key="d0">parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt instructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3) the demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning)</data>
  <data key="d1">Variables</data>
  <data key="d2">The set of configurable parameters that define the behavior of a language model call, including model choice, prompt instructions, and demonstrations.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="We focus primarily on automatically generating and selecting useful demonstrations">
  <data key="d0">We focus primarily on automatically generating and selecting useful demonstrations</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">The study investigates whether automated methods can effectively generate and select demonstrations to improve model prompting.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Tools DSPy programs may use tools, which are modules that execute computation">
  <data key="d0">Tools DSPy programs may use tools, which are modules that execute computation</data>
  <data key="d1">Tools</data>
  <data key="d2">Various computational modules supported by DSPy, such as retrieval models and code execution modules, used to enhance pipeline functionality.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="retrieval models through a dspy.Retrieve module">
  <data key="d0">retrieval models through a dspy.Retrieve module</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval module within DSPy that fetches relevant passages or data to support downstream tasks.&lt;SEP&gt;A specific module within DSPy that enables retrieval of information from external sources or databases.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ColBERTv2, Pyserini, and Pinecone retrievers">
  <data key="d0">ColBERTv2, Pyserini, and Pinecone retrievers</data>
  <data key="d1">Tools</data>
  <data key="d2">Supported retrieval systems integrated with DSPy for information retrieval tasks.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="experimental dspy.SQL for executing SQL queries">
  <data key="d0">experimental dspy.SQL for executing SQL queries</data>
  <data key="d1">Tools</data>
  <data key="d2">A module for executing SQL commands within DSPy pipelines to interact with databases.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.PythonInterpreter for executing Python code in a sandbox">
  <data key="d0">dspy.PythonInterpreter for executing Python code in a sandbox</data>
  <data key="d1">Tools</data>
  <data key="d2">A module allowing safe execution of Python code within DSPy pipelines for dynamic computation.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Programs DSPy modules can be composed in arbitrary pipelines in a define-by-run interface">
  <data key="d0">Programs DSPy modules can be composed in arbitrary pipelines in a define-by-run interface</data>
  <data key="d1">Methodology</data>
  <data key="d2">The approach of constructing flexible, dynamically defined computational pipelines by composing DSPy modules at runtime.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="define-by-run interface">
  <data key="d0">define-by-run interface</data>
  <data key="d1">Methodology</data>
  <data key="d2">A programming paradigm where the structure of the pipeline is specified dynamically during execution, allowing for flexible pipeline design.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Inspired directly by PyTorch and Chainer">
  <data key="d0">Inspired directly by PyTorch and Chainer</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The design inspiration for DSPy’s pipeline construction, emphasizing modularity and dynamic composition.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="a simple but complete retrieval-augmented generation (RAG) system">
  <data key="d0">a simple but complete retrieval-augmented generation (RAG) system</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An example pipeline combining retrieval and generative components to answer questions.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="class RAG(dspy.Module)">
  <data key="d0">class RAG(dspy.Module)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A class definition in DSPy representing a retrieval-augmented generation pipeline.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="def __init__(self, num_passages=3)">
  <data key="d0">def __init__(self, num_passages=3)</data>
  <data key="d1">Methodology</data>
  <data key="d2">Initialization method setting up retrieval and generation modules within the RAG pipeline.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self.retrieve = dspy.Retrieve(k=num_passages)">
  <data key="d0">self.retrieve = dspy.Retrieve(k=num_passages)</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval module configured to fetch a specified number of passages for the RAG system.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self.generate_answer = dspy.ChainOfThought('context, question -&gt; answer')">
  <data key="d0">self.generate_answer = dspy.ChainOfThought('context, question -&gt; answer')</data>
  <data key="d1">Tools</data>
  <data key="d2">A generative component that produces answers based on retrieved context and questions, using ChainOfThought signature.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="def forward(self, question)">
  <data key="d0">def forward(self, question)</data>
  <data key="d1">Methodology</data>
  <data key="d2">The forward method defining how the RAG system processes a question through retrieval and generation steps.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="context = self.retrieve(question).passages">
  <data key="d0">context = self.retrieve(question).passages</data>
  <data key="d1">Variables</data>
  <data key="d2">The retrieved passages used as context for answer generation.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="return self.generate_answer(context=context, question=question)">
  <data key="d0">return self.generate_answer(context=context, question=question)</data>
  <data key="d1">Results</data>
  <data key="d2">The output of the RAG system, an answer generated based on retrieved passages and the question.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="To highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict">
  <data key="d0">To highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict</data>
  <data key="d1">Core Concept</data>
  <data key="d2">Using interchangeable modules to demonstrate flexibility and modularity in pipeline design.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="One can now simply write RAG()('Where is Guaraní spoken?')">
  <data key="d0">One can now simply write RAG()('Where is Guaraní spoken?')</data>
  <data key="d1">Application</data>
  <data key="d2">An example of deploying the RAG pipeline to answer a specific question about Guaraní language location.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Teleprompters can automate prompting for arbitrary pipelines">
  <data key="d0">Teleprompters can automate prompting for arbitrary pipelines</data>
  <data key="d1">Core Concept</data>
  <data key="d2">Tools that optimize and automate the process of generating prompts and pipeline configurations in DSPy.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="When compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program">
  <data key="d0">When compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program</data>
  <data key="d1">Methodology</data>
  <data key="d2">The process of using a teleprompter to optimize DSPy pipelines based on training data and evaluation metrics.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="training sets may be small, potentially a handful of examples">
  <data key="d0">training sets may be small, potentially a handful of examples</data>
  <data key="d1">Study Design</data>
  <data key="d2">The design consideration that allows effective training and optimization with minimal data.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="training examples may be incomplete, i.e., only input values are necessary">
  <data key="d0">training examples may be incomplete, i.e., only input values are necessary</data>
  <data key="d1">Variables</data>
  <data key="d2">The type of training data that includes only inputs, without labels for intermediate steps, facilitating label-efficient training.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Labels for the pipeline steps are not required, unless they need to be used in the metric">
  <data key="d0">Labels for the pipeline steps are not required, unless they need to be used in the metric</data>
  <data key="d1">Variables</data>
  <data key="d2">The optional labels for intermediate steps, used only if metrics depend on them.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Metrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs that balance multiple concerns">
  <data key="d0">Metrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs that balance multiple concerns</data>
  <data key="d1">Tools</data>
  <data key="d2">Evaluation functions used to measure the performance of pipelines, ranging from simple metrics to complex DSPy programs.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="The goal of optimization here is to effectively bootstrap few-shot demonstrations">
  <data key="d0">The goal of optimization here is to effectively bootstrap few-shot demonstrations</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">The hypothesis that teleprompters can use minimal data to optimize and bootstrap effective prompts and demonstrations.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="qa_trainset = [dspy.Example(question='What is the capital of France?', answer='Paris')]">
  <data key="d0">qa_trainset = [dspy.Example(question='What is the capital of France?', answer='Paris')]</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A small training dataset with question-answer pairs used for pipeline training or bootstrapping.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)">
  <data key="d0">teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)</data>
  <data key="d1">Tools</data>
  <data key="d2">An instance of a DSPy tool designed to bootstrap few-shot demonstrations based on a specified evaluation metric.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)">
  <data key="d0">compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)</data>
  <data key="d1">Results</data>
  <data key="d2">A compiled, optimized RAG pipeline that has been bootstrapped and trained using minimal data.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="answer_and_context_match(example, pred, trace=None)">
  <data key="d0">answer_and_context_match(example, pred, trace=None)</data>
  <data key="d1">Tools</data>
  <data key="d2">A custom evaluation function that checks if the answer matches exactly and if the context is reflected in the prediction, used for more comprehensive performance assessment.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="behavior like this might be more accurately checked by another DSPy program that checks for faithful grounding of answers">
  <data key="d0">behavior like this might be more accurately checked by another DSPy program that checks for faithful grounding of answers</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The idea of using specialized DSPy programs to evaluate the faithfulness and grounding of model outputs.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Metrics are fully supported and encouraged in DSPy">
  <data key="d0">Metrics are fully supported and encouraged in DSPy</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The framework's capability to incorporate a variety of evaluation metrics, including complex and custom ones.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Teleprompters can be composed by specifying a teacher program">
  <data key="d0">Teleprompters can be composed by specifying a teacher program</data>
  <data key="d1">Methodology</data>
  <data key="d2">The approach of building complex pipeline optimizations through composition of teleprompter modules and teacher programs.</data>
  <data key="d3">chunk-ccadaa64b5b17c98386463d79a2cfaca</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="evaluate.answer_exact_match">
  <data key="d0">evaluate.answer_exact_match</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A method to assess whether a predicted answer exactly matches a reference answer, used for evaluating answer correctness in question-answering systems.&lt;SEP&gt;A method used to determine if two answers exactly match, often used for evaluating answer correctness in question-answering systems.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="example">
  <data key="d0">example</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A sample question and answer used to demonstrate the evaluation process, serving as a reference for answer matching.&lt;SEP&gt;Sample data used to demonstrate the evaluation process, such as specific questions and predicted answers.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="context_match">
  <data key="d0">context_match</data>
  <data key="d1">Variables</data>
  <data key="d2">A boolean indicator or metric that checks if the predicted answer is a substring of the context passage, used to determine answer fidelity.&lt;SEP&gt;A variable indicating whether the predicted answer correctly matches the context passage, used as a metric for answer fidelity.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="teleprompter">
  <data key="d0">teleprompter</data>
  <data key="d1">Tools</data>
  <data key="d2">An optimizer component within DSPy that enhances modules through prompting or finetuning, improving their performance and efficiency.&lt;SEP&gt;An optimizer component within DSPy that enhances program modules through prompting or finetuning, improving their performance and efficiency.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="BootstrapFinetune">
  <data key="d0">BootstrapFinetune</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A finetuning approach in DSPy where language model weights are updated using demonstrations to improve module performance.&lt;SEP&gt;A finetuning approach where language model weights are updated using demonstrations, aiming to improve model performance without requiring labeled data.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="predictor">
  <data key="d0">predictor</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A module or component in DSPy that generates predictions, such as answers or classifications, which can be optimized through demonstrations or parameter tuning.&lt;SEP&gt;A module or component within DSPy that generates predictions, such as answers or classifications, often subject to optimization and evaluation.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="candidate">
  <data key="d0">candidate</data>
  <data key="d1">Variables</data>
  <data key="d2">A potential demonstration, instruction, or parameter value generated during candidate generation, used to optimize model predictions.&lt;SEP&gt;A potential value or demonstration generated during the candidate generation stage in DSPy, used to optimize model performance.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="parameter">
  <data key="d0">parameter</data>
  <data key="d1">Variables</data>
  <data key="d2">A tunable setting within DSPy modules, such as demonstrations or instructions, optimized during the second stage.&lt;SEP&gt;A tunable setting within DSPy modules, such as demonstrations or instructions, that can be optimized during the second stage of the pipeline.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="metric">
  <data key="d0">metric</data>
  <data key="d1">Variables</data>
  <data key="d2">A quantitative measure used to evaluate the quality of predictions or traces during DSPy’s optimization process.&lt;SEP&gt;A quantitative measure used to evaluate the quality of predictions, traces, or model performance during optimization processes in DSPy.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="multi-stage traces">
  <data key="d0">multi-stage traces</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sequences of execution steps recorded during DSPy’s compilation and optimization, used to analyze and improve model performance across stages.&lt;SEP&gt;Sequences of execution steps tracked during DSPy’s compilation and optimization processes, used to evaluate model performance across stages.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ensemble">
  <data key="d0">ensemble</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A method of combining multiple models or answers to enhance performance and reliability in AI systems.&lt;SEP&gt;A method that combines multiple models or answers to enhance overall performance and reliability in AI tasks.&lt;SEP&gt;A technique in DSPy that runs multiple copies of a program in parallel and combines their outputs, enhancing robustness and accuracy.&lt;SEP&gt;A technique that runs multiple copies of a program in parallel and combines their outputs, often via voting, to improve robustness and accuracy.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="higher-order program optimization">
  <data key="d0">higher-order program optimization</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques that modify the control flow or structure of DSPy programs, such as creating ensembles or dynamic adjustments, to enhance performance.&lt;SEP&gt;Techniques to modify control flow or structure of DSPy programs, such as ensemble creation or dynamic adjustments, to improve overall performance.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="training examples">
  <data key="d0">training examples</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Data points or demonstrations used to bootstrap or finetune models within DSPy, serving as input for learning or optimization.&lt;SEP&gt;Data points used to bootstrap or finetune models within DSPy, serving as demonstrations for learning.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="signatures">
  <data key="d0">signatures</data>
  <data key="d1">Variables</data>
  <data key="d2">A formal specification of input-output behaviors for modules in DSPy, enabling systematic composition and reuse.&lt;SEP&gt;Definitions of input-output formats for modules in DSPy, used to validate and filter training examples during compilation.&lt;SEP&gt;Definitions of the input and output formats for modules in DSPy, used to validate and filter training examples during compilation.&lt;SEP&gt;Formal specifications of input-output behavior for DSPy modules, enabling systematic composition and reuse of components.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="hyperparameter tuning algorithms">
  <data key="d0">hyperparameter tuning algorithms</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Algorithms like random search or HyperOpt used to select optimal candidate parameters during the model optimization process in DSPy.&lt;SEP&gt;Algorithms like random search or Tree-structured Parzen Estimators applied to select optimal candidate parameters in DSPy.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="cross-validation">
  <data key="d0">cross-validation</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A method for assessing the performance of models by partitioning data into training and validation sets, used during finetuning in DSPy.&lt;SEP&gt;A method to evaluate model performance by partitioning data into training and validation sets, used during finetuning in DSPy.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="cost">
  <data key="d0">cost</data>
  <data key="d1">Variables</data>
  <data key="d2">A measure of computational or resource expenditure associated with DSPy modules or processes, relevant during optimization.&lt;SEP&gt;A measure of computational resources or efficiency associated with DSPy modules or processes, considered during optimization.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="training set">
  <data key="d0">training set</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A collection of data used to train or bootstrap models within DSPy, often comprising unlabeled or labeled examples.&lt;SEP&gt;A collection of data, often unlabeled or labeled, used to bootstrap or train models within DSPy.&lt;SEP&gt;The dataset used for training models, subdivided into 70% training and 30% validation splits, focusing on 'hard' examples.&lt;SEP&gt;The dataset used for training models, subdivided into 70% training and 30% validation, focusing on 'hard' examples.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4&lt;SEP&gt;chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="validation set">
  <data key="d0">validation set</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A subset of HotPotQA data used for testing model performance, consisting of 1000 samples for evaluation purposes.&lt;SEP&gt;A subset of HotPotQA's data reserved for testing model performance, with 1000 samples used for evaluation.&lt;SEP&gt;A subset of data used to evaluate and tune models during the training process in DSPy.</data>
  <data key="d3">chunk-8e85c7ed399116091462832c72381ba4&lt;SEP&gt;chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="modules">
  <data key="d0">modules</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Components of DSPy programs, used to build and compile algorithms or reasoning chains.&lt;SEP&gt;Components within the text transformation graphs that perform specific functions, such as processing or optimization, contributing to the overall system.&lt;SEP&gt;Discrete components implementing specific functions within the pipeline, facilitating rapid development and flexibility.&lt;SEP&gt;Discrete functional units within DSPy that implement specific tasks, enabling flexible and rapid system development.&lt;SEP&gt;Distinct components or units within DSPy that perform specific tasks, such as prediction, demonstration, or ensemble creation.&lt;SEP&gt;Reusable components within DSPy that encapsulate specific functions or reasoning steps, such as Predict, ChainOfThought, and MultiChainComparison.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2&lt;SEP&gt;chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f&lt;SEP&gt;chunk-8e85c7ed399116091462832c72381ba4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="majority voting">
  <data key="d0">majority voting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that combines multiple outputs or predictions from different models or modules to determine the most common or probable result, used here as a custom function to improve decision accuracy.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="programming frameworks">
  <data key="d0">programming frameworks</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Software frameworks designed to facilitate the development, testing, and deployment of machine learning models and pipelines, evaluated along dimensions like efficiency and intuitiveness.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="task-specific prompts">
  <data key="d0">task-specific prompts</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Hand-written prompts tailored to specific tasks that guide language models to produce desired outputs, identified as a key challenge in current LM pipelines.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="prompt strings">
  <data key="d0">prompt strings</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Traditional hand-crafted input prompts used to elicit responses from language models, contrasted with modular approaches in DSPy.&lt;SEP&gt;Traditional hand-crafted input prompts used to guide language models' responses, often requiring manual design and tuning.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="GSM8K dataset">
  <data key="d0">GSM8K dataset</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset of grade school math word problems used to evaluate the reasoning and accuracy of language models.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="modules in DSPy">
  <data key="d0">modules in DSPy</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Functional components like Predict, ChainOfThought, and MultiChainComparison used to build and optimize reasoning pipelines.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="program compilation">
  <data key="d0">program compilation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of transforming high-level DSPy programs into optimized, executable forms that can improve model performance and accuracy.&lt;SEP&gt;The process of transforming high-level DSPy programs into optimized, executable forms that can improve model performance.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="accuracy">
  <data key="d0">accuracy</data>
  <data key="d1">Results</data>
  <data key="d2">A measure of how correctly the language model's output matches the expected answer, used to evaluate the effectiveness of different program strategies.&lt;SEP&gt;Quantitative measure indicating the correctness of model outputs after applying various compilation strategies.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ensemble methods">
  <data key="d0">ensemble methods</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Combining multiple models or outputs to improve overall performance, as seen in ensemble strategies applied to DSPy modules.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="reproducibility">
  <data key="d0">reproducibility</data>
  <data key="d1">Study Design</data>
  <data key="d2">The ability to consistently replicate experimental results across different runs, configurations, or datasets, emphasized as a goal in the evaluation of modular pipelines.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="artful prompt construction">
  <data key="d0">artful prompt construction</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The practice of carefully designing prompts to elicit specific responses from language models, which the paper aims to reduce in favor of modular approaches.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="diverse task–program pairs">
  <data key="d0">diverse task–program pairs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Pairs of tasks and corresponding program modules used to evaluate the adaptability and performance of DSPy-based pipelines across different scenarios.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="performance characteristics">
  <data key="d0">performance characteristics</data>
  <data key="d1">Variables</data>
  <data key="d2">Attributes such as accuracy, efficiency, and robustness that describe how well a system functions under various conditions.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="complex pipelines">
  <data key="d0">complex pipelines</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Sequences of interconnected modules designed to handle nuanced or multi-step reasoning tasks, whose exploration is facilitated by the modularity of DSPy.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="test-time bootstrapping">
  <data key="d0">test-time bootstrapping</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A dynamic evaluation technique involving resampling or reinitialization during testing to assess model stability and performance.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="automatic backtracking-like logic">
  <data key="d0">automatic backtracking-like logic</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An automated process that allows models to revisit previous reasoning steps or decisions, enhancing accuracy and robustness.</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="et al. (2023)">
  <data key="d0">et al. (2023)</data>
  <data key="d1">Study References</data>
  <data key="d2">A scholarly reference indicating the source of the research, including the year of publication.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Section 4">
  <data key="d0">Section 4</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A specific part of the document discussing the modules used in DSPy programs, emphasizing their generic nature.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSPy programs">
  <data key="d0">DSPy programs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A set of computational modules designed for compiling, optimizing, and executing programs, particularly in the context of mathematical or reasoning tasks.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="compile">
  <data key="d0">compile</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A method that creates a customized program by running a teacher model on training data, collecting traces, and adding demonstrations to the student's model based on a metric.&lt;SEP&gt;The process of transforming DSPy modules or programs into optimized executable forms, often involving strategies like bootstrapping or ensembling.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="zero-shot">
  <data key="d0">zero-shot</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An evaluation approach where the program is executed without prior training or demonstration, relying solely on pre-existing knowledge.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="strategies for compiling">
  <data key="d0">strategies for compiling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Different techniques used to compile DSPy programs, including simple, bootstrap, nested, and ensemble methods.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LabeledFewShot">
  <data key="d0">LabeledFewShot</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A specific compilation strategy that samples a fixed number of demonstrations (k=8) from the training set to guide program execution.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="trainset">
  <data key="d0">trainset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The dataset used for training or sampling demonstrations, containing examples for program compilation.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="program">
  <data key="d0">program</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A DSPy module or sequence of modules representing a computational task or reasoning chain.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="question and answer">
  <data key="d0">question and answer</data>
  <data key="d1">Variables</data>
  <data key="d2">Fields in the dataset used as inputs and outputs for training and evaluation of DSPy programs.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="random demonstrations">
  <data key="d0">random demonstrations</data>
  <data key="d1">Tools</data>
  <data key="d2">Examples sampled randomly from the trainset to serve as demonstrations for program compilation.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="gsm8k_accuracy">
  <data key="d0">gsm8k_accuracy</data>
  <data key="d1">Variables</data>
  <data key="d2">A metric used to evaluate the accuracy of programs on the GSM8K dataset, a benchmark for math problems.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self-improve">
  <data key="d0">self-improve</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The process of enhancing program performance by iterative compilation and optimization, leading to better reasoning or problem-solving.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="nested bootstrap">
  <data key="d0">nested bootstrap</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A process where an already optimized bootstrap program is further used to bootstrap additional programs, creating a hierarchy of improvements.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ensembling">
  <data key="d0">ensembling</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Combining multiple candidate programs through majority voting to improve overall accuracy and robustness.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="reduce_fn=dspy.majority">
  <data key="d0">reduce_fn=dspy.majority</data>
  <data key="d1">Tools</data>
  <data key="d2">A function used to perform majority voting among multiple programs in an ensemble.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="humanCoT">
  <data key="d0">humanCoT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset variant that includes human reasoning chains to enhance program training and evaluation.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="GPT-3.5 and llama2-13b-chat">
  <data key="d0">GPT-3.5 and llama2-13b-chat</data>
  <data key="d1">Discipline</data>
  <data key="d2">Language Models (LMs)</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="programs">
  <data key="d0">programs</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code or instructions created within DSPy to perform specific tasks, such as question answering or information extraction, designed to be compiled into efficient AI systems.&lt;SEP&gt;Different DSPy modules and their compositions used to improve reasoning and accuracy in language models.&lt;SEP&gt;Specific code or instruction sets designed within DSPy to perform tasks like reasoning, retrieval, and answer generation, enabling systematic evaluation of AI models.&lt;SEP&gt;Specific code or instructions designed to perform particular tasks within the DSPy framework, used to evaluate and improve AI system performance.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="manual CoT">
  <data key="d0">manual CoT</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Manual Chain-of-Thought prompting used to improve model reasoning, compared with automated methods.&lt;SEP&gt;Manual chain-of-thought prompting used to improve model reasoning, compared with automated methods.</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self-consistency">
  <data key="d0">self-consistency</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that aggregates multiple answers or outputs from models to improve accuracy and robustness in reasoning tasks.&lt;SEP&gt;A technique that combines multiple outputs or answers from models to improve overall accuracy and robustness in reasoning tasks.&lt;SEP&gt;A technique that improves reasoning by sampling multiple reasoning paths and selecting the most consistent answer.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="llama2-34b">
  <data key="d0">llama2-34b</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Llama2-34b is a large language model variant evaluated for its performance in various tasks, with specific percentage scores indicating its capabilities.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="llama2-70b">
  <data key="d0">llama2-70b</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Llama2-70b is a larger variant of the Llama2 model, assessed for its performance, achieving a 56.8% score in the context described.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="program with the 13b variant">
  <data key="d0">program with the 13b variant</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A specific program utilizing the 13-billion parameter variant of a model, which is competitive despite not employing human reasoning chains.&lt;SEP&gt;A specific program utilizing the 13-billion parameter variant of a model, which is competitive despite not using human reasoning chains.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhao et al. (2023b)">
  <data key="d0">Zhao et al. (2023b)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">A study reporting 80.8% accuracy for Chain-of-Thought (CoT) prompting with GPT-3.5-turbo in April 2023.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="OpenAI (2023)">
  <data key="d0">OpenAI (2023)</data>
  <data key="d1">Research Studies</data>
  <data key="d2">OpenAI's report indicating GPT-3.5 scores 57.1% and GPT-4 scores 92%, noting GPT-4's pre-training on a subset of GSM8K.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="GSM8K">
  <data key="d0">GSM8K</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A benchmark dataset for evaluating reasoning and mathematical problem-solving capabilities of language models.&lt;SEP&gt;A dataset used for evaluating language models' reasoning capabilities, especially in mathematical and complex question answering tasks.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="HotPotQA">
  <data key="d0">HotPotQA</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset and benchmark for multi-hop question answering, used to evaluate the reasoning and retrieval capabilities of AI models.&lt;SEP&gt;A dataset and benchmark for multi-hop question answering, used to evaluate the reasoning, retrieval, and accuracy of AI models.&lt;SEP&gt;A dataset designed for multi-hop question answering in open-domain full wiki setting, used to evaluate multi-step reasoning abilities.&lt;SEP&gt;A dataset for multi-hop question answering in open-domain full wiki setting, used to evaluate multi-step reasoning abilities.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="search index of Wikipedia 2017 abstracts">
  <data key="d0">search index of Wikipedia 2017 abstracts</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval tool used to access relevant Wikipedia passages to support question answering tasks.&lt;SEP&gt;A search index utilized for retrieving relevant information from Wikipedia abstracts to facilitate question answering.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ColBERTv2">
  <data key="d0">ColBERTv2</data>
  <data key="d1">Tools</data>
  <data key="d2">A retrieval model used to efficiently fetch relevant documents from Wikipedia abstracts for multi-hop reasoning.&lt;SEP&gt;A retriever model used to conduct search queries over Wikipedia abstracts, facilitating information retrieval in the case study.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="BasicMultiHop">
  <data key="d0">BasicMultiHop</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A custom multi-hop program simulating information flow with multiple retrieval and reasoning steps, designed to improve multi-hop question answering.&lt;SEP&gt;A custom multi-hop question answering program that simulates information flow through iterative retrieval and reasoning steps to improve accuracy.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="BootstrapFewShotWithRandomSearch">
  <data key="d0">BootstrapFewShotWithRandomSearch</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A bootstrapping technique combining few-shot prompting with random search strategies to improve model performance.&lt;SEP&gt;A technique for bootstrapping models with few-shot prompting and random search to improve performance.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="answer EM">
  <data key="d0">answer EM</data>
  <data key="d1">Results</data>
  <data key="d2">Answer exact match (EM) is a strict metric used to evaluate the correctness of generated answers in question answering tasks.&lt;SEP&gt;Answer exact match (EM) is a strict metric used to evaluate the precision of AI models in producing correct answers in QA tasks.&lt;SEP&gt;Exact Match score used to evaluate the accuracy of model answers in question answering tasks, with improvements noted through various methods.&lt;SEP&gt;The Exact Match score used to quantify the accuracy of model-generated answers in question answering tasks.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Llama2-34b">
  <data key="d0">Llama2-34b</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A large language model variant evaluated for its performance in tasks, with specific accuracy percentages indicating its reasoning and generation capabilities.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Llama2-70b">
  <data key="d0">Llama2-70b</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A larger variant of the Llama2 model, assessed for performance, achieving 56.8% in the context described.</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="expert human reasoning">
  <data key="d0">expert human reasoning</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Expert human reasoning involves applying human intelligence, judgment, and problem-solving skills, often adapted for specific tasks such as reasoning in AI systems.&lt;SEP&gt;Expert human reasoning refers to the cognitive process of applying human intelligence, judgment, and problem-solving skills, often adapted for specific tasks such as retrieval or reasoning in AI systems.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Yao et al. (2022)">
  <data key="d0">Yao et al. (2022)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A scholarly publication from 2022 that discusses expert human reasoning and its adaptation to retrieval settings, serving as a foundational reference for the current work.&lt;SEP&gt;A scholarly work from 2022 that discusses expert human reasoning and its adaptation to retrieval settings, serving as a foundational reference for the current study.&lt;SEP&gt;Yao et al. (2022) is a referenced study or model that introduces or evaluates ReAct methodology in AI systems.&lt;SEP&gt;Yao et al. (2022) is a research study that introduces or evaluates the ReAct framework, providing insights into its methodology and effectiveness.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="llama2-13b-chat">
  <data key="d0">llama2-13b-chat</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large language model with 13 billion parameters, evaluated for its competitive performance in AI tasks, especially in finetuning and question answering scenarios.&lt;SEP&gt;A large language model with 13 billion parameters, evaluated for its competitive performance in AI tasks, particularly in finetuning and question answering.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="compiler">
  <data key="d0">compiler</data>
  <data key="d1">Tools</data>
  <data key="d2">A software component that translates high-level DSPy program descriptions into executable models, used to evaluate the finetuning capacity and performance of AI models.&lt;SEP&gt;A software component that translates high-level program descriptions into executable models, used here to evaluate finetuning capacity and performance of AI models.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="multihop t5">
  <data key="d0">multihop t5</data>
  <data key="d1">Tools</data>
  <data key="d2">A specific program based on T5-Large (770M parameters) designed for multi-hop reasoning tasks, used to assess model reasoning ability and answer accuracy.&lt;SEP&gt;A specific program based on T5-Large architecture, designed for multi-hop reasoning tasks, used to assess model performance in complex question answering.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="answer exact match (Ans)">
  <data key="d0">answer exact match (Ans)</data>
  <data key="d1">Results</data>
  <data key="d2">A metric indicating the percentage of answers that exactly match the correct answer in question answering tasks.&lt;SEP&gt;A metric indicating the percentage of answers that exactly match the correct answer, used to evaluate QA performance.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="passage accuracy (Psg)">
  <data key="d0">passage accuracy (Psg)</data>
  <data key="d1">Results</data>
  <data key="d2">A metric measuring the correctness of retrieved passages or supporting evidence in QA tasks.&lt;SEP&gt;A metric measuring the correctness of retrieved passages or supporting evidence in question answering evaluations.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="pair-retrieval accuracy (Psg)">
  <data key="d0">pair-retrieval accuracy (Psg)</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A metric assessing the accuracy of retrieving relevant passage pairs in multi-hop retrieval tasks.&lt;SEP&gt;A metric assessing the model's ability to correctly retrieve relevant passage pairs in multi-hop retrieval tasks.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="text-davinci-002">
  <data key="d0">text-davinci-002</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A GPT-3 model variant used as a baseline or comparison point in question answering performance evaluations.&lt;SEP&gt;A GPT-3 variant used as a baseline in evaluation studies for question answering and reasoning tasks.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="tool">
  <data key="d0">tool</data>
  <data key="d1">Tools</data>
  <data key="d2">An external or internal component that aids AI models in tasks such as search, reasoning, or data retrieval, exemplified by Wikipedia API integration.&lt;SEP&gt;An external or internal component used within DSPy pipelines to assist in tasks like search, reasoning, or data retrieval, exemplified by Wikipedia API integration.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self-knowledge">
  <data key="d0">self-knowledge</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The ability of models to internally generate, verify, or reason about information without external input, enhancing reasoning and accuracy.&lt;SEP&gt;The ability of models to internally generate, verify, or reason about information without external input, improving reasoning accuracy.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="code-davinci-002">
  <data key="d0">code-davinci-002</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A GPT-3 model variant optimized for code generation and reasoning, used in pipeline question answering tasks.&lt;SEP&gt;A GPT-3 model variant optimized for code generation and reasoning, used within DSPy pipelines for complex reasoning tasks.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="pipeline">
  <data key="d0">pipeline</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A sequence of interconnected modules and components designed to perform complex tasks by chaining simpler units, central to DSPy’s architecture.&lt;SEP&gt;A sequence of processing steps or modules designed to perform complex tasks by chaining simpler components, central to DSPy’s architecture.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="programming model">
  <data key="d0">programming model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A conceptual framework defining how modules, signatures, and teleprompters interact within DSPy to build AI systems systematically.&lt;SEP&gt;A conceptual framework that defines how components like modules, signatures, and teleprompters interact within AI system design.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="teleprompters">
  <data key="d0">teleprompters</data>
  <data key="d1">Tools</data>
  <data key="d2">A mechanism or component that guides or influences the flow of information and reasoning within the pipeline, supporting systematic and reliable AI behavior.&lt;SEP&gt;Components that guide, influence, or optimize the execution flow of modules within DSPy pipelines, supporting reliable and systematic reasoning.&lt;SEP&gt;Modules acting as optimizers within text transformation graphs to enhance LM performance and reliability.&lt;SEP&gt;Specific optimizer modules used within the text transformation graphs to guide or enhance language model outputs.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2&lt;SEP&gt;chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="information extraction">
  <data key="d0">information extraction</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A task enabled by DSPy, involving retrieving structured data or facts from unstructured text, demonstrating the framework’s versatility.&lt;SEP&gt;A task enabled by DSPy, involving retrieving structured facts or data from unstructured text, demonstrating the framework’s versatility.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="synthetic data generation">
  <data key="d0">synthetic data generation</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The creation of artificial data for training or testing AI models, showcasing DSPy’s capability in low-resource scenarios.&lt;SEP&gt;The process of creating artificial data for training or testing AI models, especially useful in low-resource scenarios, showcasing DSPy’s applicability.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="low-resource">
  <data key="d0">low-resource</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Refers to settings with limited labeled data, where DSPy aims to improve model performance through efficient reasoning and data synthesis.&lt;SEP&gt;Refers to tasks or settings with limited labeled data, where DSPy aims to improve performance with small models.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="text transformation graphs">
  <data key="d0">text transformation graphs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A systematic representation of how text is processed and transformed within DSPy, allowing complex reasoning, data manipulation, and task composition.&lt;SEP&gt;A systematic representation of how text is processed and transformed within DSPy, enabling complex reasoning and data manipulation.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="systematic">
  <data key="d0">systematic</data>
  <data key="d1">Disciplines</data>
  <data key="d2">Indicates a structured, methodical approach in AI system design, emphasizing reliability, reproducibility, and clarity.&lt;SEP&gt;Indicating a structured, methodical approach in AI system design, emphasizing reliability and reproducibility.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="composable modules">
  <data key="d0">composable modules</data>
  <data key="d1">Tools</data>
  <data key="d2">Modules designed to be combined flexibly to build complex pipelines, central to DSPy’s architecture.&lt;SEP&gt;Modules designed to be combined flexibly to build complex, adaptable pipelines within DSPy, facilitating rapid experimentation.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="optimizers (teleprompters)">
  <data key="d0">optimizers (teleprompters)</data>
  <data key="d1">Tools</data>
  <data key="d2">Components that adjust or guide the execution of modules to improve accuracy and efficiency in AI pipelines.&lt;SEP&gt;Components that adjust, guide, or enhance the execution of modules in DSPy, improving accuracy, efficiency, and system reliability.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="space and scope">
  <data key="d0">space and scope</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Refers to the contextual and boundary considerations in AI system development, briefly mentioned in the discussion of future work.&lt;SEP&gt;Refers to the contextual, boundary, and resource considerations in AI system development, briefly noted as future work.</data>
  <data key="d3">chunk-ed0f2ed4617819fd44b380fe3e3951f0</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="building sophisticated text transformation graphs">
  <data key="d0">building sophisticated text transformation graphs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A framework for constructing modular systems that leverage language models (LMs) through composable modules and optimizers (teleprompters) to enhance systematicity and reliability.&lt;SEP&gt;A framework for constructing modular systems that utilize language models (LMs) in systematic and reliable ways, emphasizing composability and optimizer modules (teleprompters).</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="text transformation graph">
  <data key="d0">text transformation graph</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An abstraction representing the computational graph structure that integrates modules and optimizers for systematic text processing with language models.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Stanford Institute for Human-Centered Artificial Intelligence (HAI)">
  <data key="d0">Stanford Institute for Human-Centered Artificial Intelligence (HAI)</data>
  <data key="d1">Discipline</data>
  <data key="d2">A research institute supporting AI research, including this work, with funding and collaborative support.&lt;SEP&gt;A research institute supporting the development of human-centered AI, funding and facilitating related research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="IBM">
  <data key="d0">IBM</data>
  <data key="d1">Organization</data>
  <data key="d2">A corporate supporter and collaborator providing funding and resources to the research.&lt;SEP&gt;A founding member supporting this research through funding and collaboration.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Virtusa">
  <data key="d0">Virtusa</data>
  <data key="d1">Organization</data>
  <data key="d2">A corporate supporter involved in funding and collaboration.&lt;SEP&gt;A supporter of the research, involved in funding and collaboration.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Cigna Healthcare">
  <data key="d0">Cigna Healthcare</data>
  <data key="d1">Organization</data>
  <data key="d2">A supporter contributing to the research efforts.&lt;SEP&gt;A supporter of the research, contributing resources.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Stanford DAWN project">
  <data key="d0">Stanford DAWN project</data>
  <data key="d1">Discipline</data>
  <data key="d2">A collaborative research initiative supported by industry members like Facebook, Google, VMware, focusing on data and AI advancements.&lt;SEP&gt;A research initiative supported by industry members including Facebook, Google, and VMware, focusing on data and AI advancements.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Facebook">
  <data key="d0">Facebook</data>
  <data key="d1">Organization</data>
  <data key="d2">A industry supporter of the Stanford DAWN project and AI research.&lt;SEP&gt;An industry supporter of the Stanford DAWN project, contributing to AI research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Google">
  <data key="d0">Google</data>
  <data key="d1">Organization</data>
  <data key="d2">A supporter of the Stanford DAWN project, contributing to AI and data science research.&lt;SEP&gt;An industry supporter of the Stanford DAWN project, contributing to AI and data science research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="VMware">
  <data key="d0">VMware</data>
  <data key="d1">Organization</data>
  <data key="d2">A supporter of the Stanford DAWN project, involved in advancing AI research.&lt;SEP&gt;An industry supporter of the Stanford DAWN project, involved in AI research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="NSF (National Science Foundation)">
  <data key="d0">NSF (National Science Foundation)</data>
  <data key="d1">Discipline</data>
  <data key="d2">A funding agency supporting this research through the CAREER grant CNS-1651570.&lt;SEP&gt;A government funding agency supporting the research through grants like CAREER CNS-1651570.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Omar Khattab">
  <data key="d0">Omar Khattab</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on retrieval systems, published in 2021.&lt;SEP&gt;A researcher involved in retrieval efficiency and effectiveness, published in 2021.&lt;SEP&gt;A researcher supported by the Apple Scholars in AI/ML fellowship, contributing to AI/ML research.&lt;SEP&gt;An individual supported by the Apple Scholars in AI/ML fellowship, contributing to the research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2&lt;SEP&gt;chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Apple Scholars in AI/ML fellowship">
  <data key="d0">Apple Scholars in AI/ML fellowship</data>
  <data key="d1">Discipline</data>
  <data key="d2">A fellowship providing support for AI/ML research, notably to Omar Khattab.&lt;SEP&gt;A fellowship providing support to Omar Khattab for AI/ML research.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama">
  <data key="d0">Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a paper on Optuna, a hyperparameter optimization framework, advancing ML methodologies.&lt;SEP&gt;Authors of a paper on Optuna, a hyperparameter optimization framework, contributing to ML methodologies.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Optuna">
  <data key="d0">Optuna</data>
  <data key="d1">Tools</data>
  <data key="d2">A hyperparameter optimization framework designed to automate and improve model tuning.&lt;SEP&gt;A next-generation hyperparameter optimization framework designed to improve model tuning efficiency.&lt;SEP&gt;An automatic hyperparameter optimization framework used to find the best program configuration by minimizing or maximizing an objective function.&lt;SEP&gt;Optuna is an automatic hyperparameter optimization framework used to find the best program configuration by minimizing or maximizing an objective function.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2&lt;SEP&gt;chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, et al.">
  <data key="d0">Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a paper on Theano, a Python framework for fast computation of mathematical expressions, contributing to computational tools.&lt;SEP&gt;Authors of a paper on Theano, a Python framework for fast computation of mathematical expressions.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Theano">
  <data key="d0">Theano</data>
  <data key="d1">Tools</data>
  <data key="d2">A Python framework for fast mathematical computation, supporting deep learning and scientific computing.&lt;SEP&gt;A Python library enabling fast computation of mathematical expressions, supporting scientific and deep learning workflows.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, Yoshua Bengio">
  <data key="d0">James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, Yoshua Bengio</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of multiple papers on Theano, contributing to deep learning tools and frameworks.&lt;SEP&gt;Authors of papers on Theano, contributing to deep learning tools and frameworks.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Deep learning on GPUs with Python">
  <data key="d0">Deep learning on GPUs with Python</data>
  <data key="d1">Methodology</data>
  <data key="d2">A methodology for leveraging GPU acceleration in Python-based deep learning workflows.&lt;SEP&gt;A methodology for utilizing GPU acceleration in Python-based deep learning workflows.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="James Bergstra, Daniel Yamins, and David Cox">
  <data key="d0">James Bergstra, Daniel Yamins, and David Cox</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of work on hyperparameter optimization for vision architectures, enhancing ML model search techniques.&lt;SEP&gt;Authors of work on hyperparameter optimization in vision architectures, contributing to ML methodology.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Rishi Bommasani, Drew A Hudson, Ehsan Adeli, et al.">
  <data key="d0">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors discussing foundation models, their opportunities and risks, contributing to AI research discourse.&lt;SEP&gt;Authors discussing foundation models, their opportunities, and risks, contributing to AI safety and deployment debates.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.">
  <data key="d0">Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a paper on language models as few-shot learners, a foundational study in NLP.&lt;SEP&gt;Authors of a seminal paper on language models as few-shot learners, foundational in NLP.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Language models as few-shot learners">
  <data key="d0">Language models as few-shot learners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A paradigm demonstrating that large language models can perform tasks with minimal task-specific data, revolutionizing NLP.&lt;SEP&gt;A paradigm showing large language models can perform tasks with minimal examples, transforming NLP approaches.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Hwchase17/langchain">
  <data key="d0">Hwchase17/langchain</data>
  <data key="d1">Tools</data>
  <data key="d2">A software library for building applications with language models, supporting chaining, memory, and complex workflows.&lt;SEP&gt;A software library for building language model applications, supporting chaining and complex workflows.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes">
  <data key="d0">Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of a paper on reading Wikipedia for open-domain question answering, contributing to NLP methodologies.&lt;SEP&gt;Authors of a paper on reading Wikipedia to answer open-domain questions, advancing NLP QA systems.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Reading Wikipedia to answer open-domain questions">
  <data key="d0">Reading Wikipedia to answer open-domain questions</data>
  <data key="d1">Study Design</data>
  <data key="d2">A research approach involving large-scale reading comprehension to develop QA systems.&lt;SEP&gt;A research approach involving large-scale reading comprehension to develop open-domain QA systems.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Lingjiao Chen, Matei Zaharia, James Zou">
  <data key="d0">Lingjiao Chen, Matei Zaharia, James Zou</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of Frugalgpt, a framework for reducing costs and improving performance of large language models.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Frugalgpt">
  <data key="d0">Frugalgpt</data>
  <data key="d1">Methodology</data>
  <data key="d2">A cost-reduction and efficiency technique for deploying large language models effectively.&lt;SEP&gt;A technique for cost-effective deployment and performance enhancement of large language models.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen">
  <data key="d0">Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors of work on Program of thoughts prompting, a method to separate reasoning from computation in numerical tasks.&lt;SEP&gt;Authors of work on Program of thoughts prompting, aiming to disentangle computation from reasoning in numerical tasks.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Program of thoughts prompting">
  <data key="d0">Program of thoughts prompting</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A prompting approach that disentangles reasoning and computation to improve numerical reasoning performance.&lt;SEP&gt;A prompting method that separates reasoning processes from computational steps to improve numerical reasoning.&lt;SEP&gt;A prompting technique designed to disentangle computation from reasoning in numerical tasks, aiming to improve model understanding and performance.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a&lt;SEP&gt;chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul">
  <data key="d0">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul</data>
  <data key="d1">Researchers</data>
  <data key="d2">Authors contributing to advancements in large language models, methodology unspecified.&lt;SEP&gt;Authors contributing to large language model advancements, methodology unspecified.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="discussions and feedback">
  <data key="d0">discussions and feedback</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">Community interactions, including discussions among researchers, developers, and users, providing insights and validation for the framework.</data>
  <data key="d3">chunk-583d728170557abef78a4cab2b68abb2</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Wenhu Chen">
  <data key="d0">Wenhu Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Wenhu Chen is an author contributing to research on prompting techniques for numerical reasoning tasks.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Xueguang Ma">
  <data key="d0">Xueguang Ma</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xueguang Ma is a researcher involved in studies related to computational reasoning.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Xinyi Wang">
  <data key="d0">Xinyi Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Xinyi Wang participates in research on language modeling and reasoning.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="William W Cohen">
  <data key="d0">William W Cohen</data>
  <data key="d1">Researcher</data>
  <data key="d2">William W Cohen is an author associated with computational linguistics and reasoning tasks.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="disentangling computation from reasoning">
  <data key="d0">disentangling computation from reasoning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A conceptual framework that separates computational processes from reasoning processes within language models to enhance interpretability and effectiveness.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Aakanksha Chowdhery">
  <data key="d0">Aakanksha Chowdhery</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in scaling language models using pathways, contributing to advancements in large-scale language modeling.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Scaling language modeling with pathways">
  <data key="d0">Scaling language modeling with pathways</data>
  <data key="d1">Methodology</data>
  <data key="d2">A methodology for expanding language model capabilities through pathway scaling, enabling more complex tasks and larger models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Training verifiers to solve math word problems">
  <data key="d0">Training verifiers to solve math word problems</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique involving training models to verify solutions to math problems, aiming to improve problem-solving accuracy.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2110.14168">
  <data key="d0">arXiv preprint arXiv:2110.14168</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint presenting methods for training verifiers in mathematical problem solving.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Torch">
  <data key="d0">Torch</data>
  <data key="d1">Tool</data>
  <data key="d2">A modular machine learning software library developed to facilitate building and training machine learning models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ronan Collobert, Samy Bengio, Johnny Mariethoz">
  <data key="d0">Ronan Collobert, Samy Bengio, Johnny Mariethoz</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers involved in developing Torch, a flexible library for machine learning applications.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Language model cascades">
  <data key="d0">Language model cascades</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A methodology involving sequential or layered language models to improve performance on complex tasks through cascading processes.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2207.10342">
  <data key="d0">arXiv preprint arXiv:2207.10342</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint exploring the design and application of language model cascades.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Rarr: Researching and revising what language models say, using language models">
  <data key="d0">Rarr: Researching and revising what language models say, using language models</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique for improving language model outputs through iterative research and revision, leveraging models' own capabilities.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Proceedings of the 61st Annual Meeting of the ACL">
  <data key="d0">Proceedings of the 61st Annual Meeting of the ACL</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A collection of research papers and findings presented at the ACL conference, including studies on language model revision.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Pal: Program-aided language models">
  <data key="d0">Pal: Program-aided language models</data>
  <data key="d1">Core Concept</data>
  <data key="d2">A framework integrating programmatic reasoning with language models to enhance reasoning and problem-solving capabilities.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="International Conference on Machine Learning">
  <data key="d0">International Conference on Machine Learning</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A major conference where research on program-aided language models was presented.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Qingyan Guo">
  <data key="d0">Qingyan Guo</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher exploring the connection between large language models and evolutionary algorithms for prompt optimization.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Connecting large language models with evolutionary algorithms yields powerful prompt optimizers">
  <data key="d0">Connecting large language models with evolutionary algorithms yields powerful prompt optimizers</data>
  <data key="d1">Methodology</data>
  <data key="d2">A novel approach combining language models with evolutionary algorithms to optimize prompts efficiently.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2309.08532">
  <data key="d0">arXiv preprint arXiv:2309.08532</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint detailing the methodology for integrating language models with evolutionary algorithms for prompt optimization.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Realm: Retrieval-augmented language model pre-training">
  <data key="d0">Realm: Retrieval-augmented language model pre-training</data>
  <data key="d1">Methodology</data>
  <data key="d2">A pre-training approach that combines retrieval mechanisms with language modeling to enhance knowledge access and reasoning.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2002.08909">
  <data key="d0">arXiv preprint arXiv:2002.08909</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint describing the Realm framework for retrieval-augmented language model pre-training.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, Christopher Ré">
  <data key="d0">Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, Christopher Ré</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers studying natural language explanations and their use in training classifiers.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Training classifiers with natural language explanations">
  <data key="d0">Training classifiers with natural language explanations</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique where classifiers are trained using explanations in natural language to improve interpretability and performance.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Proceedings of the 56th Annual Meeting of the ACL">
  <data key="d0">Proceedings of the 56th Annual Meeting of the ACL</data>
  <data key="d1">Study Dataset</data>
  <data key="d2">A collection of research papers presenting findings on natural language explanations in classifiers.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu">
  <data key="d0">Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers working on enabling intelligent interactions between agents and large language models using reinforcement learning.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach">
  <data key="d0">Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach</data>
  <data key="d1">Methodology</data>
  <data key="d2">A reinforcement learning method designed to facilitate effective and intelligent agent-LLM interactions.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2306.03604">
  <data key="d0">arXiv preprint arXiv:2306.03604</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint detailing reinforcement learning strategies for agent-LLM interaction.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han">
  <data key="d0">Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers investigating the self-improvement capabilities of large language models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Large language models can self-improve">
  <data key="d0">Large language models can self-improve</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The idea that large language models possess the ability to enhance their own performance through self-refinement processes.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2210.11610">
  <data key="d0">arXiv preprint arXiv:2210.11610</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint exploring mechanisms and evidence for self-improvement in language models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave">
  <data key="d0">Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers working on few-shot learning and retrieval-augmented language models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Few-shot learning with retrieval augmented language models">
  <data key="d0">Few-shot learning with retrieval augmented language models</data>
  <data key="d1">Methodology</data>
  <data key="d2">A technique leveraging retrieval mechanisms to enable models to learn effectively from limited examples.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv preprint arXiv:2208.03299">
  <data key="d0">arXiv preprint arXiv:2208.03299</data>
  <data key="d1">Study Design</data>
  <data key="d2">A preprint presenting methods for few-shot learning with retrieval-augmented models.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al.">
  <data key="d0">Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al.</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers developing modular neuro-symbolic architectures combining language models, external knowledge, and reasoning.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning">
  <data key="d0">Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</data>
  <data key="d1">Methodology</data>
  <data key="d2">A neuro-symbolic system integrating neural components with external knowledge and symbolic reasoning to enhance AI reasoning capabilities.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Omar Khattab, Christopher Potts, Matei Zaharia">
  <data key="d0">Omar Khattab, Christopher Potts, Matei Zaharia</data>
  <data key="d1">Researchers</data>
  <data key="d2">Researchers focusing on multi-hop reasoning and relevance-guided supervision in retrieval systems.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval">
  <data key="d0">Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval</data>
  <data key="d1">Methodology</data>
  <data key="d2">A retrieval-based approach enabling large-scale multi-hop reasoning through condensed retrieval techniques.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Relevance-guided supervision for openqa with ColBERT">
  <data key="d0">Relevance-guided supervision for openqa with ColBERT</data>
  <data key="d1">Methodology</data>
  <data key="d2">A supervision technique for open-domain question answering systems using relevance feedback and ColBERT for retrieval.</data>
  <data key="d3">chunk-1154df731c16a689d34b73768a34217a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Robust Multi-Hop Reasoning at Scale">
  <data key="d0">Robust Multi-Hop Reasoning at Scale</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A concept focusing on scalable multi-hop reasoning capabilities in neural models, emphasizing efficiency and effectiveness in processing complex queries.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Condensed Retrieval">
  <data key="d0">Condensed Retrieval</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A retrieval technique that condenses information to improve efficiency and relevance in large-scale information retrieval tasks.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Relevance-guided Supervision for OpenQA with ColBERT">
  <data key="d0">Relevance-guided Supervision for OpenQA with ColBERT</data>
  <data key="d1">Methods</data>
  <data key="d2">A supervised learning approach using relevance signals to enhance open-domain question answering models, specifically with the ColBERT architecture.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Demonstrate-search-predict">
  <data key="d0">Demonstrate-search-predict</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An approach that combines retrieval and language models to handle knowledge-intensive NLP tasks by integrating demonstration, search, and prediction components.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Large Language Models as Zero-Shot Reasoners">
  <data key="d0">Large Language Models as Zero-Shot Reasoners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Theoretical framework suggesting large language models can perform reasoning tasks without explicit training on specific tasks, functioning effectively in zero-shot settings.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Internet-augmented Language Models through Few-Shot Prompting">
  <data key="d0">Internet-augmented Language Models through Few-Shot Prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that enhances language models by incorporating external internet data via few-shot prompts to improve open-domain question answering.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Retrieval-augmented Generation for Knowledge-Intensive NLP Tasks">
  <data key="d0">Retrieval-augmented Generation for Knowledge-Intensive NLP Tasks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A framework that combines retrieval mechanisms with generative models to improve performance on tasks requiring external knowledge.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LlamaIndex">
  <data key="d0">LlamaIndex</data>
  <data key="d1">Tools</data>
  <data key="d2">A software tool or library designed for building and managing large language model applications, facilitating retrieval and knowledge management.&lt;SEP&gt;LlamaIndex is a library for building applications with language models that includes prompt management, with similar challenges as LangChain, targeted for comparison with DSPy.&lt;SEP&gt;LlamaIndex is a tool or framework for document indexing and retrieval, facilitating access to relevant information in large datasets or corpora for AI applications.&lt;SEP&gt;LlamaIndex is a tool or framework for document retrieval and indexing, supporting information access in AI systems.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956&lt;SEP&gt;chunk-ebb9fff9b6ca895234d2fcb26b588fe4&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self-refine: Iterative Refinement with Self-Feedback">
  <data key="d0">Self-refine: Iterative Refinement with Self-Feedback</data>
  <data key="d1">Methodologies</data>
  <data key="d2">An iterative process where models refine their outputs through self-feedback loops to improve accuracy and coherence.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="The Natural Language Decathlon">
  <data key="d0">The Natural Language Decathlon</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A multitask learning benchmark that trains models across multiple NLP tasks framed as question answering to evaluate generalization.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Semantic Kernel">
  <data key="d0">Semantic Kernel</data>
  <data key="d1">Tools</data>
  <data key="d2">A Microsoft framework for integrating semantic understanding into applications, enabling semantic reasoning and knowledge integration.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Training Language Models to Follow Instructions with Human Feedback">
  <data key="d0">Training Language Models to Follow Instructions with Human Feedback</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A training approach that uses human feedback to teach language models to better follow user instructions and improve alignment.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Din-sql: Decomposed In-Context Learning of Text-to-SQL with Self-Correction">
  <data key="d0">Din-sql: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique for teaching models to generate SQL queries from text by decomposing the task and applying self-correction mechanisms.</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Associates, Inc.">
  <data key="d0">Associates, Inc.</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A company involved in research activities, providing proceedings and papers related to machine learning and natural language processing.&lt;SEP&gt;A company that produces research proceedings, publications, and papers related to neural information processing and machine learning.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mohammadreza Pourreza">
  <data key="d0">Mohammadreza Pourreza</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author contributing to research on text-to-SQL and self-correction in language models, published in 2023.&lt;SEP&gt;An author of a preprint paper on decomposed in-context learning of text-to-SQL with self-correction, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Davood Rafiei">
  <data key="d0">Davood Rafiei</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author of the paper on decomposed in-context learning of text-to-SQL with self-correction, published in 2023.&lt;SEP&gt;An author collaborating on research about text-to-SQL with self-correction, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ofir Press">
  <data key="d0">Ofir Press</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author involved in measuring and narrowing the compositionality gap in language models, published in 2022.&lt;SEP&gt;An author involved in research measuring and narrowing the compositionality gap in language models, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Muru Zhang">
  <data key="d0">Muru Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on language model compositionality, published in 2022.&lt;SEP&gt;A researcher contributing to studies on language model compositionality, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sewon Min">
  <data key="d0">Sewon Min</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to research on language models' compositionality, published in 2022.&lt;SEP&gt;A researcher involved in language model compositionality studies, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ludwig Schmidt">
  <data key="d0">Ludwig Schmidt</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author in language model compositionality research, published in 2022.&lt;SEP&gt;A researcher working on language models and their compositionality, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Noah A Smith">
  <data key="d0">Noah A Smith</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author in language models' compositionality studies, published in 2022.&lt;SEP&gt;An author contributing to language model research, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Reid Pryzant">
  <data key="d0">Reid Pryzant</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on automatic prompt optimization using gradient descent and beam search, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Dan Iter">
  <data key="d0">Dan Iter</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author in prompt optimization research, published in 2023.&lt;SEP&gt;A researcher involved in prompt optimization methods, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jerry Li">
  <data key="d0">Jerry Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in prompt optimization with gradient descent and beam search, published in 2023.&lt;SEP&gt;A researcher contributing to prompt optimization research, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Yin Tat Lee">
  <data key="d0">Yin Tat Lee</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on prompt optimization techniques, published in 2023.&lt;SEP&gt;An author working on prompt optimization techniques, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chenguang Zhu">
  <data key="d0">Chenguang Zhu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to prompt optimization research, published in 2023.&lt;SEP&gt;A researcher involved in prompt optimization research, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Michael Zeng">
  <data key="d0">Michael Zeng</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in prompt optimization research, published in 2023.&lt;SEP&gt;An author working on prompt optimization, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Peng Qi">
  <data key="d0">Peng Qi</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author focusing on answering complex open-domain questions through iterative query generation, published in 2019.&lt;SEP&gt;An author focusing on answering complex open-domain questions via iterative query generation, published in 2019.&lt;SEP&gt;An author focusing on retrieve, rerank, read, and iterate methods for question answering, published in 2020.&lt;SEP&gt;An author of a methodology involving retrieve, rerank, read, then iterate for question answering, published in 2020.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Xiaowen Lin">
  <data key="d0">Xiaowen Lin</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on iterative question answering, published in 2019.&lt;SEP&gt;A researcher involved in open-domain question answering, published in 2019.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Leo Mehr">
  <data key="d0">Leo Mehr</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to open-domain question answering research, published in 2019.&lt;SEP&gt;An author contributing to iterative query generation research, published in 2019.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zijian Wang">
  <data key="d0">Zijian Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on complex question answering methods, published in 2019.&lt;SEP&gt;A researcher working on question answering methods, published in 2019.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Haejun Lee">
  <data key="d0">Haejun Lee</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to the retrieve, rerank, read, and iterate approach for question answering, published in 2020.&lt;SEP&gt;A researcher involved in question answering and language models, published in 2020.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Oghenetegiri Sido">
  <data key="d0">Oghenetegiri Sido</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on iterative question answering methods, published in 2020.&lt;SEP&gt;A researcher contributing to open-domain question answering techniques, published in 2020.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Karthik Narasimhan">
  <data key="d0">Karthik Narasimhan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to language model pre-training research, published in 2018.&lt;SEP&gt;A researcher involved in language model pre-training, published in 2018.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Tim Salimans">
  <data key="d0">Tim Salimans</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on language understanding via pre-training, published in 2018.&lt;SEP&gt;A researcher contributing to language understanding advancements, published in 2018.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Alexander J Ratner">
  <data key="d0">Alexander J Ratner</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on data programming for creating large training datasets quickly, published in 2016.&lt;SEP&gt;An author working on data programming for creating large training sets, published in 2016.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Christopher M De Sa">
  <data key="d0">Christopher M De Sa</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in data programming for training data generation, published in 2016.&lt;SEP&gt;A researcher involved in data programming for training data generation, published in 2016.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sen Wu">
  <data key="d0">Sen Wu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on data programming techniques, published in 2016.&lt;SEP&gt;A researcher contributing to data programming approaches, published in 2016.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Daniel Selsam">
  <data key="d0">Daniel Selsam</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on large training sets via data programming, published in 2016.&lt;SEP&gt;An author working on data programming for large training set creation, published in 2016.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Christopher R ´e">
  <data key="d0">Christopher R ´e</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in data programming and neural information processing, published in 2016.&lt;SEP&gt;A researcher involved in data programming and neural information processing, published in 2016.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Keshav Santhanam">
  <data key="d0">Keshav Santhanam</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on effective retrieval methods, specifically ColBERTv2, published in 2021.&lt;SEP&gt;Author of ColBERTv2, a lightweight late interaction retrieval system, published in 2021.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jon Saad-Falcon">
  <data key="d0">Jon Saad-Falcon</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to retrieval system research, published in 2021.&lt;SEP&gt;A researcher working on retrieval techniques, published in 2021.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Christopher Potts">
  <data key="d0">Christopher Potts</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in retrieval and language understanding, published in 2021.&lt;SEP&gt;A researcher contributing to retrieval and language understanding, published in 2021.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Matei Zaharia">
  <data key="d0">Matei Zaharia</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on retrieval systems, published in 2021.&lt;SEP&gt;A researcher involved in retrieval systems, published in 2021.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Timo Schick">
  <data key="d0">Timo Schick</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author of Toolformer, a model that enables language models to teach themselves to use tools, published in 2023.&lt;SEP&gt;An author working on Toolformer, enabling language models to teach themselves to use tools, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jane Dwivedi-Yu">
  <data key="d0">Jane Dwivedi-Yu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on the Toolformer project, published in 2023.&lt;SEP&gt;A researcher involved in Toolformer project, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Roberto Dess ı">
  <data key="d0">Roberto Dess ı</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in tool-use in language models, published in 2023.&lt;SEP&gt;A researcher working on tool-use in language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Roberta Raileanu">
  <data key="d0">Roberta Raileanu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to tool-learning in language models, published in 2023.&lt;SEP&gt;A researcher contributing to tool-learning in language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Maria Lomeli">
  <data key="d0">Maria Lomeli</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in tool-use research, published in 2023.&lt;SEP&gt;A researcher involved in tool-use research, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nicola Cancedda">
  <data key="d0">Nicola Cancedda</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to tool-use in language models, published in 2023.&lt;SEP&gt;A researcher contributing to tool-use in language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Thomas Scialom">
  <data key="d0">Thomas Scialom</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in tool-learning for language models, published in 2023.&lt;SEP&gt;A researcher involved in tool-learning for language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhihong Shao">
  <data key="d0">Zhihong Shao</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on synthetic prompting and chain-of-thought demonstrations, published in 2023.&lt;SEP&gt;An author working on synthetic prompting, generating chain-of-thought demonstrations for large language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Yeyun Gong">
  <data key="d0">Yeyun Gong</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to synthetic prompting research, published in 2023.&lt;SEP&gt;A researcher involved in synthetic prompting for large language models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nan Duan">
  <data key="d0">Nan Duan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to chain-of-thought demonstrations, published in 2023.&lt;SEP&gt;A researcher involved in synthetic prompting and chain-of-thought methods, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Noah Shinn">
  <data key="d0">Noah Shinn</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on Reflexion, an autonomous agent with self-reflection and dynamic memory, published in 2023.&lt;SEP&gt;An author working on Reflexion, an autonomous agent with self-reflection, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Beck Labash">
  <data key="d0">Beck Labash</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in autonomous agent development, published in 2023.&lt;SEP&gt;A researcher involved in autonomous agent development, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ashwin Gopinath">
  <data key="d0">Ashwin Gopinath</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on autonomous agents with self-reflection, published in 2023.&lt;SEP&gt;A researcher contributing to autonomous agents with dynamic memory, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chenglei Si">
  <data key="d0">Chenglei Si</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on Prompting GPT-3 to be reliable through prompting techniques, published in 2022.&lt;SEP&gt;An author working on making GPT-3 reliable through prompting, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhe Gan">
  <data key="d0">Zhe Gan</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in prompting GPT-3, published in 2022.&lt;SEP&gt;A researcher involved in prompting GPT-3, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhengyuan Yang">
  <data key="d0">Zhengyuan Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on prompt reliability for GPT-3, published in 2022.&lt;SEP&gt;A researcher working on prompting techniques, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jianfeng Wang">
  <data key="d0">Jianfeng Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on prompting GPT-3, published in 2022.&lt;SEP&gt;A researcher involved in prompt research, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jordan Boyd-Graber">
  <data key="d0">Jordan Boyd-Graber</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author involved in prompt reliability research, published in 2022.&lt;SEP&gt;A researcher working on prompt reliability and language modeling, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Lijuan Wang">
  <data key="d0">Lijuan Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author contributing to prompt reliability, published in 2022.&lt;SEP&gt;A researcher contributing to prompt reliability, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhiqing Sun">
  <data key="d0">Zhiqing Sun</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author working on recitation-augmented language models, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Yiming Yang">
  <data key="d0">Yiming Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-author working on recitation-augmented language models, published in 2022.&lt;SEP&gt;A researcher contributing to recitation-augmented language models, published in 2022.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Seiya Tokui">
  <data key="d0">Seiya Tokui</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author of the deep learning framework Chainer, published in 2015.&lt;SEP&gt;An author working on Chainer, a deep learning framework, published in 2015.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Kenta Oono">
  <data key="d0">Kenta Oono</data>
  <data key="d1">Researcher</data>
  <data key="d2">A co-developer of Chainer, published in 2015.&lt;SEP&gt;A researcher involved in Chainer framework development, published in 2015.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Shohei Hido">
  <data key="d0">Shohei Hido</data>
  <data key="d1">Researcher</data>
  <data key="d2">A contributor to the Chainer framework, published in 2015.&lt;SEP&gt;A researcher contributing to Chainer, published in 2015.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Justin Clayton">
  <data key="d0">Justin Clayton</data>
  <data key="d1">Researcher</data>
  <data key="d2">A contributor to the Chainer framework, published in 2015.&lt;SEP&gt;A researcher working on deep learning frameworks, published in 2015.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Louis Martin">
  <data key="d0">Louis Martin</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in Llama 2 development, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Kevin Stone">
  <data key="d0">Kevin Stone</data>
  <data key="d1">Researcher</data>
  <data key="d2">A contributor to Llama 2 models, published in 2023.&lt;SEP&gt;A researcher contributing to Llama 2 models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Peter Albert">
  <data key="d0">Peter Albert</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on Llama 2, published in 2023.&lt;SEP&gt;Involved in Llama 2 development, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Yasmine Babaei">
  <data key="d0">Yasmine Babaei</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to Llama 2 models, published in 2023.&lt;SEP&gt;Contributor to Llama 2 models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nikolay Bashlykov">
  <data key="d0">Nikolay Bashlykov</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on Llama 2, published in 2023.&lt;SEP&gt;Researcher involved in Llama 2 development, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Soumya Batra">
  <data key="d0">Soumya Batra</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher involved in Llama 2, published in 2023.&lt;SEP&gt;Contributor to Llama 2, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prrajjwal Bhargava">
  <data key="d0">Prrajjwal Bhargava</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher contributing to Llama 2 models, published in 2023.&lt;SEP&gt;Researcher involved in Llama 2, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Shruti Bhosale">
  <data key="d0">Shruti Bhosale</data>
  <data key="d1">Researcher</data>
  <data key="d2">A researcher working on Llama 2, published in 2023.&lt;SEP&gt;Contributor to Llama 2 models, published in 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="et al.">
  <data key="d0">et al.</data>
  <data key="d1">Researcher</data>
  <data key="d2">Indicates multiple authors involved in Llama 2 publication, 2023.&lt;SEP&gt;Indicates multiple authors involved in the Llama 2 publication, 2023.</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2307.09288">
  <data key="d0">arXiv:2307.09288</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A preprint research paper documenting the development and features of Llama 2, published on arXiv in 2023.&lt;SEP&gt;A preprint research paper published on arXiv in 2023 documenting the development, architecture, and capabilities of Llama 2.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Harsh Trivedi">
  <data key="d0">Harsh Trivedi</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author involved in research on integrating retrieval with chain-of-thought reasoning to enhance knowledge-intensive question answering.&lt;SEP&gt;Harsh Trivedi is an author involved in research on retrieval and reasoning methods in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Niranjan Balasubramanian">
  <data key="d0">Niranjan Balasubramanian</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author contributing to studies on multi-step reasoning and retrieval techniques in language models.&lt;SEP&gt;Niranjan Balasubramanian is an author contributing to studies on knowledge-intensive question answering.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Tushar Khot">
  <data key="d0">Tushar Khot</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author focusing on multi-step question answering and reasoning in language models, especially involving retrieval techniques.&lt;SEP&gt;Tushar Khot is involved in research on multi-step reasoning in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ashish Sabharwal">
  <data key="d0">Ashish Sabharwal</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author involved in research on reasoning, language understanding, and multi-step question answering.&lt;SEP&gt;Ashish Sabharwal is a researcher focusing on reasoning and language model capabilities.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Interleaving retrieval with chain-of-thought reasoning">
  <data key="d0">Interleaving retrieval with chain-of-thought reasoning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A theoretical framework that combines information retrieval with step-by-step reasoning processes to improve complex question answering.&lt;SEP&gt;A theoretical framework that combines retrieval techniques with chain-of-thought reasoning to improve multi-step question answering.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Knowledge-intensive multi-step questions">
  <data key="d0">Knowledge-intensive multi-step questions</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Questions requiring extensive background knowledge and multiple reasoning steps, used to evaluate language model reasoning capabilities.&lt;SEP&gt;Questions that require extensive background knowledge and multiple reasoning steps to answer accurately.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Fei Wang">
  <data key="d0">Fei Wang</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An author specializing in foundational methods for neural network training, including backpropagation with callbacks for efficient differentiable programming.&lt;SEP&gt;Fei Wang's work involves backpropagation with callbacks, foundational for efficient differentiable programming in neural networks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Backpropagation with callbacks">
  <data key="d0">Backpropagation with callbacks</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A technique that enhances backpropagation efficiency and expressiveness for training neural networks.&lt;SEP&gt;A technique that enhances the backpropagation process by allowing callbacks, leading to more efficient and flexible neural network training.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Rationale-augmented ensembles">
  <data key="d0">Rationale-augmented ensembles</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Ensemble approaches that incorporate rationale explanations to improve reasoning and interpretability in language models.&lt;SEP&gt;Ensemble techniques in language models that incorporate rationale explanations to improve reasoning accuracy.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self-consistency">
  <data key="d0">Self-consistency</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A method to improve chain of thought reasoning by aggregating multiple reasoning paths for more reliable outputs.&lt;SEP&gt;A technique that improves chain-of-thought reasoning by aggregating multiple reasoning paths to produce more reliable outputs.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chain of thought prompting">
  <data key="d0">Chain of thought prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A prompting technique designed to elicit reasoning processes in large language models to improve complex task performance.&lt;SEP&gt;A prompting technique that elicits reasoning processes in large language models to enhance performance on complex tasks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Large language models as optimizers">
  <data key="d0">Large language models as optimizers</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Investigating whether large language models can be effectively used as optimization tools in various tasks.&lt;SEP&gt;Research exploring the use of large language models to perform optimization tasks, expanding their functional scope.&lt;SEP&gt;Using large language models to perform optimization tasks, expanding their functional roles beyond traditional NLP tasks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="HotpotQA">
  <data key="d0">HotpotQA</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A dataset created for multi-hop, explainable question answering, used to evaluate reasoning and explanation capabilities of models.&lt;SEP&gt;A dataset designed for diverse, explainable multi-hop question answering, facilitating research on reasoning and explanation in NLP.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="React">
  <data key="d0">React</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A framework that combines reasoning and acting in language models to improve decision-making and problem-solving capabilities.&lt;SEP&gt;A framework that synergizes reasoning and acting in language models to improve decision-making and problem-solving.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Meta-reasoning">
  <data key="d0">Meta-reasoning</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A higher-order reasoning process where models evaluate and improve their own reasoning chains to enhance accuracy and reliability.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Star">
  <data key="d0">Star</data>
  <data key="d1">Tools</data>
  <data key="d2">A reasoning framework that bootstraps reasoning processes within language models, enabling iterative and self-improving reasoning capabilities.&lt;SEP&gt;A reasoning system that bootstraps and iteratively improves reasoning processes within language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Answering questions by meta-reasoning">
  <data key="d0">Answering questions by meta-reasoning</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Applying meta-reasoning over multiple reasoning chains to improve the accuracy and explainability of question answering in language models.&lt;SEP&gt;Applying meta-reasoning techniques to improve question-answering performance in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2304.13007">
  <data key="d0">arXiv:2304.13007</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A preprint exploring meta-reasoning over multiple chains of thought to enhance question answering.&lt;SEP&gt;A preprint exploring meta-reasoning techniques over multiple chains of thought for improved question answering.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2203.14465">
  <data key="d0">arXiv:2203.14465</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A preprint introducing 'Star', a bootstrapping approach for reasoning in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2210.03493">
  <data key="d0">arXiv:2210.03493</data>
  <data key="d1">Tools</data>
  <data key="d2">Automatic chain of thought prompting method for large language models to facilitate reasoning tasks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2308.10144">
  <data key="d0">arXiv:2308.10144</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Research on experiential learning in large language model agents, emphasizing their ability to learn from experience.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:2305.14333">
  <data key="d0">arXiv:2305.14333</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Research on automatic model selection techniques for reasoning tasks using large language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Transformers: State-of-the-art natural language processing">
  <data key="d0">Transformers: State-of-the-art natural language processing</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A comprehensive overview of the transformer architecture, which underpins modern NLP models.&lt;SEP&gt;A comprehensive overview of transformer architecture, foundational for modern NLP models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="arXiv:1809.09600">
  <data key="d0">arXiv:1809.09600</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A dataset (HotpotQA) designed for multi-hop question answering to evaluate reasoning and explanation capabilities.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou">
  <data key="d0">Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou</data>
  <data key="d1">Researcher Group</data>
  <data key="d2">A group of researchers collaborating on studies related to language models, reasoning, and ensemble techniques.&lt;SEP&gt;A group of researchers contributing to multiple studies on language models, reasoning, and ensemble methods.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Meta-reasoning over multiple chains of thought">
  <data key="d0">Meta-reasoning over multiple chains of thought</data>
  <data key="d1">Research Question/Hypotheses</data>
  <data key="d2">Exploring how models can meta-reason over multiple reasoning paths to improve accuracy.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Automatic chain of thought prompting">
  <data key="d0">Automatic chain of thought prompting</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for automatically generating prompts that induce reasoning in large language models.&lt;SEP&gt;Techniques to automatically generate prompts that elicit reasoning in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Bhargava, Shruti Bhosale, et al.">
  <data key="d0">Bhargava, Shruti Bhosale, et al.</data>
  <data key="d1">Researcher Group</data>
  <data key="d2">A group of researchers including Shruti Bhosale Bhargava who authored a paper on Llama 2, contributing to foundational AI research.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Shunyu Yao">
  <data key="d0">Shunyu Yao</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author involved in developing React, a framework for integrating reasoning and acting in language models.&lt;SEP&gt;An author involved in developing React, a framework for reasoning and acting in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ori Yoran">
  <data key="d0">Ori Yoran</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author researching meta-reasoning techniques over multiple chains of thought to improve question answering.&lt;SEP&gt;An author researching meta-reasoning techniques, specifically over multiple chains of thought, to enhance question answering.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Eric Zelikman">
  <data key="d0">Eric Zelikman</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author involved in developing Star, a system for bootstrapping reasoning through iterative processes in language models.&lt;SEP&gt;An author involved in developing Star, a system for iterative reasoning in language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Andrew Zhao">
  <data key="d0">Andrew Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author introducing ExpeL, a framework where LLM agents learn through experiential interactions.&lt;SEP&gt;An author who proposes ExpeL, a framework for experiential learning in language model agents.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ExpeL">
  <data key="d0">ExpeL</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">A framework demonstrating that large language model agents can learn from experience, enhancing their capabilities.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Xu Zhao">
  <data key="d0">Xu Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author investigating automatic model selection techniques for reasoning in large language models.&lt;SEP&gt;An author studying methods for automatic model selection to improve reasoning tasks in large language models.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Automatic model selection">
  <data key="d0">Automatic model selection</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Techniques for selecting the most appropriate model configurations automatically for reasoning tasks.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhilin Yang">
  <data key="d0">Zhilin Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">An author associated with HotpotQA, a dataset for multi-hop question answering and explainability.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zhilin Yang, Peng Qi, Saizheng Zhang, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning">
  <data key="d0">Zhilin Yang, Peng Qi, Saizheng Zhang, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning</data>
  <data key="d1">Researcher Group</data>
  <data key="d2">A group of researchers who created HotpotQA, a dataset for multi-hop reasoning.</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Python Classes for Signatures">
  <data key="d0">Python Classes for Signatures</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Python classes are used to explicitly define signatures that specify instructions for transformations and describe the format or role of each field, facilitating control and clarity in signature design.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Signature">
  <data key="d0">Signature</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A Signature in this context is a structured representation of transformation instructions, enabling explicit and controlled data processing within the system.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="GenerateSearchQuery">
  <data key="d0">GenerateSearchQuery</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A Python class that extends dspy.Signature, used to generate search queries based on context and questions, illustrating explicit signature design for query generation.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.Signature">
  <data key="d0">dspy.Signature</data>
  <data key="d1">Tools</data>
  <data key="d2">A base class from the DSPy library used to define structured signatures for various data transformation and generation tasks.&lt;SEP&gt;A construct defining the input-output schema for models, which can be modified or prepended with output fields to guide behavior.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.InputField">
  <data key="d0">dspy.InputField</data>
  <data key="d1">Tools</data>
  <data key="d2">A class used to define input fields within a signature, specifying descriptions and expected data types or contents.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.OutputField">
  <data key="d0">dspy.OutputField</data>
  <data key="d1">Tools</data>
  <data key="d2">A class used to define output fields within a signature, specifying the data type and role of the output.&lt;SEP&gt;A component used to define output fields in model signatures, here used to prepend reasoning prompts for chain-of-thought processes.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.Predict">
  <data key="d0">dspy.Predict</data>
  <data key="d1">Tools</data>
  <data key="d2">A class or function that performs the prediction task, executing the model with a given signature and returning predictions.&lt;SEP&gt;A class used to specify prediction tasks based on signatures, mediating the execution of defined signatures with input data.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.SearchQuery">
  <data key="d0">dspy.SearchQuery</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A data type representing search queries generated by the signature system, used as output in the query generation process.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="LangChain">
  <data key="d0">LangChain</data>
  <data key="d1">Tools</data>
  <data key="d2">LangChain is a library focused on prompt engineering and chaining language model components, hypothesized to face challenges with prompt length and manual prompt crafting, which DSPy aims to address.&lt;SEP&gt;LangChain is a software framework or toolkit that facilitates the development and deployment of language models, especially supporting reasoning, tool integration, and chain-of-thought processes.&lt;SEP&gt;LangChain is a tool or framework used to develop language models and applications, supporting tasks like reasoning and retrieval.&lt;SEP&gt;LangChain supports the implementation of Zero-shot ReAct by providing infrastructure for reasoning and tool use.&lt;SEP&gt;LangChain supports the implementation of Zero-shot ReAct by providing infrastructure for reasoning, chaining, and tool invocation within AI systems.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956&lt;SEP&gt;chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Task-specific Prompts">
  <data key="d0">Task-specific Prompts</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Prompts designed for particular tasks such as math problems, QA, or database queries, often lengthy and complex in current frameworks.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompt Length">
  <data key="d0">Prompt Length</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable representing the number of words or characters in prompt templates, often exceeding 1000 characters in existing systems, highlighting challenges in prompt engineering.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Large Prompt Templates">
  <data key="d0">Large Prompt Templates</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Extensive prompt templates used in frameworks like LangChain, often exceeding thousands of characters, exemplifying prompt engineering complexity.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompt Size">
  <data key="d0">Prompt Size</data>
  <data key="d1">Variables</data>
  <data key="d2">The measurement (in words or characters) of prompt templates, often large, indicating the extent of prompt engineering challenges.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompt Engineering Challenges">
  <data key="d0">Prompt Engineering Challenges</data>
  <data key="d1">Limitations</data>
  <data key="d2">The difficulties associated with manually designing and managing large, complex prompts for language models, motivating the development of automated systems like DSPy.</data>
  <data key="d3">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Gao et al. (2023a)">
  <data key="d0">Gao et al. (2023a)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Gao et al. (2023a) is a referenced study or publication, likely involving research or evidence collection.&lt;SEP&gt;Gao et al. (2023a) is a research publication or study that presents experimental or analytical work related to the evaluation of models or methods.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Math word problems (PAL)">
  <data key="d0">Math word problems (PAL)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Math word problems (PAL) are a specific set of mathematical problems used as benchmarks or datasets for evaluating AI reasoning and problem-solving capabilities.&lt;SEP&gt;Math word problems (PAL) refer to mathematical problem sets used for evaluation or training in AI models.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Gao et al. (2023b)">
  <data key="d0">Gao et al. (2023b)</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Gao et al. (2023b) is a referenced research or publication related to the development or evaluation of mathematical problem solving.&lt;SEP&gt;Gao et al. (2023b) is a research publication or study that extends or complements Gao et al. (2023a), focusing on further evaluation or development of mathematical problem-solving approaches.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Zero-shot ReAct">
  <data key="d0">Zero-shot ReAct</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Zero-shot ReAct is an application of the ReAct framework that operates without task-specific training, enabling generalization to new tasks using reasoning and tool use.&lt;SEP&gt;Zero-shot ReAct is an approach that applies the ReAct framework without task-specific training, enabling zero-shot reasoning capabilities.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="QA with sources">
  <data key="d0">QA with sources</data>
  <data key="d1">Methodologies</data>
  <data key="d2">QA with sources is a methodology that involves question answering systems utilizing external sources or documents for accurate responses.&lt;SEP&gt;QA with sources is a methodology that involves question-answering systems leveraging external sources or documents to generate accurate and source-supported answers.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="IRS chatbot">
  <data key="d0">IRS chatbot</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">IRS chatbot is an AI-powered conversational agent designed to assist users with IRS-related inquiries, utilizing document retrieval and question-answering techniques.&lt;SEP&gt;IRS chatbot refers to an AI-based chatbot designed to assist with IRS-related inquiries, possibly using LlamaIndex for document retrieval.</data>
  <data key="d3">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Kelvin Hopkins">
  <data key="d0">Kelvin Hopkins</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Kelvin Hopkins is an individual accused of inappropriate physical contact and sexual harassment, with allegations dating back to 2017, leading to suspension by the Labour party pending investigation.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="2017">
  <data key="d0">2017</data>
  <data key="d1">Study Design</data>
  <data key="d2">The year when Kelvin Hopkins was accused of misconduct, serving as a temporal marker for the allegation.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Labour Party">
  <data key="d0">Labour Party</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The Labour Party is the political organization that suspended Kelvin Hopkins following allegations of misconduct.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Inappropriate Physical Contact">
  <data key="d0">Inappropriate Physical Contact</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A type of misconduct involving physical interaction deemed unacceptable, central to the allegations against Kelvin Hopkins.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sexual Harassment">
  <data key="d0">Sexual Harassment</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A form of misconduct involving unwanted sexual advances or behaviors, which Kelvin Hopkins was accused of.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ava Etemadzadeh">
  <data key="d0">Ava Etemadzadeh</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Ava Etemadzadeh is the Labour Party activist who was allegedly harassed by Kelvin Hopkins.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Suspension">
  <data key="d0">Suspension</data>
  <data key="d1">Results</data>
  <data key="d2">Kelvin Hopkins was suspended by the Labour Party as a consequence of the allegations and pending investigation.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Investigation">
  <data key="d0">Investigation</data>
  <data key="d1">Study Design</data>
  <data key="d2">The process undertaken by the Labour Party to examine the allegations against Kelvin Hopkins.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Allegations">
  <data key="d0">Allegations</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Claims made against Kelvin Hopkins for inappropriate physical contact and sexual harassment.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Physical Contact">
  <data key="d0">Physical Contact</data>
  <data key="d1">Variables</data>
  <data key="d2">Specific act of physical interaction alleged to be inappropriate and part of the misconduct.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Misconduct">
  <data key="d0">Misconduct</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Unacceptable behaviors including sexual harassment and inappropriate contact attributed to Kelvin Hopkins.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Disciplinary Action">
  <data key="d0">Disciplinary Action</data>
  <data key="d1">Results</data>
  <data key="d2">The suspension by the Labour Party constitutes disciplinary action following the allegations.</data>
  <data key="d3">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Leah">
  <data key="d0">Leah</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Leah is a person characterized by her possession of chocolates and her activity of eating chocolates, which is used to illustrate a basic arithmetic problem.&lt;SEP&gt;Leah is a person who possesses chocolates and is involved in a scenario illustrating basic arithmetic operations.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sister">
  <data key="d0">Sister</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Leah's sister is a person involved in the same scenario, sharing chocolates and contributing to the total count in the problem.&lt;SEP&gt;Leah's sister is a person sharing chocolates with Leah, contributing to the total quantity in the problem.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chocolates">
  <data key="d0">Chocolates</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Chocolates are consumable items used in the problem to quantify quantities and perform calculations.&lt;SEP&gt;Chocolates are tangible objects representing quantities in the problem, used for calculations.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Total Chocolates">
  <data key="d0">Total Chocolates</data>
  <data key="d1">Variables</data>
  <data key="d2">Sum of Leah's and her sister's chocolates before eating, representing the total initial quantity.&lt;SEP&gt;Total chocolates represent the sum of Leah's and her sister's chocolates before eating, serving as a variable in the calculation.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chocolates Eaten">
  <data key="d0">Chocolates Eaten</data>
  <data key="d1">Variables</data>
  <data key="d2">Chocolates eaten is a variable indicating the quantity consumed, affecting the remaining chocolates.&lt;SEP&gt;Number of chocolates consumed, affecting the remaining chocolates count.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chocolates Left">
  <data key="d0">Chocolates Left</data>
  <data key="d1">Results</data>
  <data key="d2">Chocolates left is the result of subtracting chocolates eaten from total chocolates, representing the remaining quantity after consumption.&lt;SEP&gt;Remaining chocolates after consumption, the outcome of subtracting chocolates eaten from total chocolates.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Parking Lot">
  <data key="d0">Parking Lot</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The parking lot is a location used in a problem about counting cars, illustrating addition.&lt;SEP&gt;The parking lot is a location used in a problem involving counting cars, illustrating a basic addition scenario.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Cars Initial">
  <data key="d0">Cars Initial</data>
  <data key="d1">Variables</data>
  <data key="d2">Initial number of cars in the parking lot, used as a starting point for counting.&lt;SEP&gt;Number of cars initially in the parking lot, serving as the starting point for counting.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Cars Arrived">
  <data key="d0">Cars Arrived</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of cars arriving later, contributing to the total count.&lt;SEP&gt;Number of cars arriving, contributing to the total count.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Total Cars">
  <data key="d0">Total Cars</data>
  <data key="d1">Results</data>
  <data key="d2">Total cars after arrival, representing the sum of initial cars and new arrivals.&lt;SEP&gt;Total number of cars after new cars arrive, calculated by addition.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trees in Grove">
  <data key="d0">Trees in Grove</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Trees in a grove are objects used to illustrate growth or planting problems, representing change over time.&lt;SEP&gt;Trees in the grove are objects used to frame a problem about planting and growth, illustrating change over time.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Initial Trees">
  <data key="d0">Initial Trees</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of trees initially in the grove, serving as the baseline for growth calculation.&lt;SEP&gt;Number of trees initially in the grove, serving as the starting point for growth calculation.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trees After">
  <data key="d0">Trees After</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of trees after planting, used to calculate the number of trees added.&lt;SEP&gt;Number of trees after planting, used to determine how many trees were added.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trees Added">
  <data key="d0">Trees Added</data>
  <data key="d1">Results</data>
  <data key="d2">Number of trees planted by workers, calculated as the difference between Trees After and Initial Trees.&lt;SEP&gt;Number of trees planted by workers, obtained by subtracting initial trees from total after planting.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Solution Function">
  <data key="d0">Solution Function</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The solution functions are procedural code snippets that perform calculations based on input variables to produce results.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Mathematical Operations">
  <data key="d0">Mathematical Operations</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Addition and subtraction are the primary mathematical techniques used to compute totals, differences, and remaining quantities.</data>
  <data key="d3">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Rebel Without a Cause">
  <data key="d0">Rebel Without a Cause</data>
  <data key="d1">Object of Study</data>
  <data key="d2">A 1955 American film that explores teenage angst and social issues, directed by Elia Kazan.&lt;SEP&gt;A 1955 film that is a significant work in American cinema, associated with the director's prominence.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Elia Kazan">
  <data key="d0">Elia Kazan</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">An influential American film and theatre director, producer, screenwriter, and actor known for his contributions to cinema and theatre.&lt;SEP&gt;An influential American film and theatre director, producer, screenwriter, and actor, known for directing Rebel Without a Cause.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Arthur’s Magazine">
  <data key="d0">Arthur’s Magazine</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A literary magazine published in Philadelphia from 1844 to 1846, representing 19th-century American literary culture.&lt;SEP&gt;An American literary periodical published in Philadelphia from 1844 to 1846, representing 19th-century American literary culture.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="First for Women">
  <data key="d0">First for Women</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A women's magazine published by Bauer Media Group in the USA, started in 1989, focusing on women's interests and lifestyle.&lt;SEP&gt;A women's magazine published by Bauer Media Group in the USA, started in 1989, focusing on women's interests.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Pavel Urysohn">
  <data key="d0">Pavel Urysohn</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A Soviet mathematician renowned for his contributions to dimension theory and topology.&lt;SEP&gt;A Soviet mathematician renowned for his work in dimension theory, contributing significantly to topology.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Leonid Levin">
  <data key="d0">Leonid Levin</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A Soviet-American mathematician and computer scientist known for his work in computational complexity and algorithms.&lt;SEP&gt;A Soviet-American mathematician and computer scientist known for work in computational complexity and algorithms.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="1911">
  <data key="d0">1911</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Refers to the date 1911, marking a specific point in time relevant to the context.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="June 16, 1979">
  <data key="d0">June 16, 1979</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">Refers to the date June 16, 1979, marking a specific point in time relevant to the context.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="American film director">
  <data key="d0">American film director</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A person who directs films in America, known for their artistic and technical leadership in filmmaking.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Screenwriter">
  <data key="d0">Screenwriter</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A person who writes scripts for films, responsible for crafting dialogue and story structure.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Actor">
  <data key="d0">Actor</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A person who performs roles in films, theater, or television, bringing characters to life.</data>
  <data key="d3">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Ukrainian People">
  <data key="d0">Ukrainian People</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The Ukrainian People are characterized by fearlessness, courage, and determination, inspiring others around the world.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Groups of Citizens">
  <data key="d0">Groups of Citizens</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Thirty-two groups of citizens, including students, retirees, teachers, and soldiers, actively blocking tanks with their bodies in defense of Ukraine.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="COVID-19">
  <data key="d0">COVID-19</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">COVID-19 is a highly infectious disease that has caused significant loss of life and societal disruption, prompting calls for societal reset and unity.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Loss of Life due to COVID-19">
  <data key="d0">Loss of Life due to COVID-19</data>
  <data key="d1">Results</data>
  <data key="d2">Significant loss of life and time spent apart, highlighting the pandemic's severe impact.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Societal Reset">
  <data key="d0">Societal Reset</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Encouraging societal reflection and unity by reframing COVID-19 as a shared challenge rather than a partisan divide.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Fellow Americans">
  <data key="d0">Fellow Americans</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The collective identity of Americans, emphasizing unity and shared purpose.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="U.S. Police Officers (Wilbert Mora and Jason Rivera)">
  <data key="d0">U.S. Police Officers (Wilbert Mora and Jason Rivera)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Two officers responding to a 9-1-1 call, killed in the line of duty, representing sacrifice and community safety.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Funerals of Officers Mora and Rivera">
  <data key="d0">Funerals of Officers Mora and Rivera</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">Events marking the mourning and honoring of fallen officers, illustrating community grief and respect.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trust and Safety">
  <data key="d0">Trust and Safety</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Efforts to restore trust and safety in communities through honoring sacrifices and reinforcing police-community relations.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Russian Invasion of Ukraine">
  <data key="d0">Russian Invasion of Ukraine</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A foreign invasion by Russia targeting Ukraine, causing global geopolitical and economic consequences.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Russian Dictator">
  <data key="d0">Russian Dictator</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A leader of Russia responsible for initiating invasion, with significant international impact and sanctions.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Russian Sanctions">
  <data key="d0">Russian Sanctions</data>
  <data key="d1">Tools</data>
  <data key="d2">Economic sanctions imposed to target Russia's economy, aiming to exert pressure and influence its actions.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Oil Reserves">
  <data key="d0">Oil Reserves</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The United States and allied countries' strategic petroleum reserves used to stabilize gas prices and support energy security.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="United States">
  <data key="d0">United States</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The U.S. government coordinating energy reserves and sanctions in response to geopolitical events.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Global Oil Market">
  <data key="d0">Global Oil Market</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Market dynamics influenced by reserve releases, supply constraints, and demand.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Gas Prices">
  <data key="d0">Gas Prices</data>
  <data key="d1">Variables</data>
  <data key="d2">Fuel prices affected by geopolitical tensions, reserve releases, and market conditions.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sanctions and Energy Policy">
  <data key="d0">Sanctions and Energy Policy</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Use of sanctions and strategic reserves as tools to influence international energy markets and geopolitical stability.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ARPA-H (Advanced Research Projects Agency for Health)">
  <data key="d0">ARPA-H (Advanced Research Projects Agency for Health)</data>
  <data key="d1">Tools</data>
  <data key="d2">A research agency modeled after DARPA, focused on driving breakthroughs in health technology including cancer, Alzheimer’s, and diabetes.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DARPA">
  <data key="d0">DARPA</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Defense Advanced Research Projects Agency, a U.S. Department of Defense agency responsible for innovative technological breakthroughs, inspiring ARPA-H.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Research in Health Breakthroughs">
  <data key="d0">Research in Health Breakthroughs</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Focused on innovation and technological development to address major health issues like cancer, Alzheimer’s, and diabetes.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="American Democracy">
  <data key="d0">American Democracy</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The United States' democratic system characterized by debate, freedom, liberty, and resilience through history.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Historical Challenges">
  <data key="d0">Historical Challenges</data>
  <data key="d1">Results</data>
  <data key="d2">Fighting totalitarianism, terror, and expanding liberty, shaping the nation's character and future.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Nation’s Character and Future">
  <data key="d0">Nation’s Character and Future</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">The current moment of responsibility and resolve, shaping the future based on past sacrifices and achievements.</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Pay attention to use only the column names you can see in the tables below">
  <data key="d0">Pay attention to use only the column names you can see in the tables below</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Instruction emphasizing careful selection of column names based on provided table schemas.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Pay attention to which column is in which table">
  <data key="d0">Pay attention to which column is in which table</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Instruction highlighting the importance of understanding table schemas and column placements.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Use today() function to get the current date">
  <data key="d0">Use today() function to get the current date</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Instruction to utilize the today() function for date-related queries involving 'today'.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ORDER BY clause should always be after WHERE clause">
  <data key="d0">ORDER BY clause should always be after WHERE clause</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Guideline indicating proper SQL syntax order, specifically placing ORDER BY after WHERE.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DO NOT add semicolon to the end of SQL">
  <data key="d0">DO NOT add semicolon to the end of SQL</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Instruction to omit semicolons at the end of SQL statements in this context.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Use the following format:">
  <data key="d0">Use the following format:</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Directive to adhere to a specified output format for entities and relationships.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Example format:">
  <data key="d0">Example format:</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Providing an example structure for expected output formatting.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Let’s begin:">
  <data key="d0">Let’s begin:</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Instruction indicating the start of the task.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="{table info}">
  <data key="d0">{table info}</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Placeholder indicating where table schema details are provided.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Question: {input}">
  <data key="d0">Question: {input}</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Placeholder for the user's question or activity prompt.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="SQLQuery:">
  <data key="d0">SQLQuery:</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Label indicating where the generated SQL query should be provided.</data>
  <data key="d3">chunk-9cee6a97737c75b57f27605ea82e4163</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ParameterLM">
  <data key="d0">ParameterLM</data>
  <data key="d1">Methodology</data>
  <data key="d2">A language model parameter used as part of a computational process, initialized with default settings and involved in generating text completions.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ParameterDemonstrations">
  <data key="d0">ParameterDemonstrations</data>
  <data key="d1">Methodology</data>
  <data key="d2">A collection of demonstrations used to guide the language model's behavior during training or inference, initialized as empty in this context.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="forward">
  <data key="d0">forward</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that executes the process of generating predictions given input data, involving selecting the appropriate language model, signature, and demonstrations, and producing a prediction object.&lt;SEP&gt;A method that forwards inputs through the chain of thought module, executing the prediction with reasoning incorporated.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="get_the_right_lm">
  <data key="d0">get_the_right_lm</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that retrieves the appropriate language model based on provided parameters, ensuring the correct model is used for generation.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="get_the_right_signature">
  <data key="d0">get_the_right_signature</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that retrieves the correct signature (input-output schema) for the model, based on context or parameters.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="get_the_right_demonstrations">
  <data key="d0">get_the_right_demonstrations</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that selects or constructs demonstrations appropriate for the current inference context.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prediction">
  <data key="d0">Prediction</data>
  <data key="d1">Results</data>
  <data key="d2">An object representing the output generated by the language model, constructed from completions and associated with a specific signature.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.ParameterLM">
  <data key="d0">dspy.ParameterLM</data>
  <data key="d1">Tools</data>
  <data key="d2">A tool or class used to define and manage language models within the framework, allowing for default or customized models.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.ParameterDemonstrations">
  <data key="d0">dspy.ParameterDemonstrations</data>
  <data key="d1">Tools</data>
  <data key="d2">A tool or class used to manage demonstrations that guide model behavior during inference.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.settings.trace">
  <data key="d0">dspy.settings.trace</data>
  <data key="d1">Tools</data>
  <data key="d2">A global setting that records internal traces during model execution, enabling extraction of internal call information.&lt;SEP&gt;A global setting that records the trace of internal model calls during execution, useful for debugging or analysis.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.setting.context">
  <data key="d0">dspy.setting.context</data>
  <data key="d1">Tools</data>
  <data key="d2">A context manager that temporarily modifies settings such as enabling trace collection during model execution.&lt;SEP&gt;A context manager to enable temporary settings, such as trace collection, during execution.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trace">
  <data key="d0">Trace</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">A record of internal calls and outputs during model execution, used for debugging or understanding model behavior.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="__init__">
  <data key="d0">__init__</data>
  <data key="d1">Methodology</data>
  <data key="d2">Initialization method for the ChainOfThought class, which modifies the model signature to include a reasoning output field and sets up a sub-module for prediction.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="SimplifiedBootstrapFewShot">
  <data key="d0">SimplifiedBootstrapFewShot</data>
  <data key="d1">Methodology</data>
  <data key="d2">A class implementing a bootstrap few-shot learning technique, which compiles a training program by collecting traces from a teacher model on training examples based on a metric.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="predicted_traces">
  <data key="d0">predicted_traces</data>
  <data key="d1">Spatiotemporal Information</data>
  <data key="d2">A record of internal model calls and outputs during a single prediction, used to extract demonstrations for training.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self.metric">
  <data key="d0">self.metric</data>
  <data key="d1">Variables</data>
  <data key="d2">A function or object used to evaluate whether a prediction meets a certain criterion, guiding the collection of demonstrations.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="dspy.Example">
  <data key="d0">dspy.Example</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A data structure representing an input-output pair used to add demonstrations to the training program.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="SimplifiedBootstrapFewShotWithRandomSearch">
  <data key="d0">SimplifiedBootstrapFewShotWithRandomSearch</data>
  <data key="d1">Methodology</data>
  <data key="d2">An extension of bootstrap few-shot learning that performs multiple trials with random shuffling to identify the best set of demonstrations based on evaluation metrics.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="shuffle">
  <data key="d0">shuffle</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that randomly permutes the training set with a specified seed to generate different training configurations for evaluation.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="evaluate_program">
  <data key="d0">evaluate_program</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that assesses the quality of a program (model with demonstrations) against a validation set using a specified metric, returning a score.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="max(candidates, key=lambda x: x[0])">
  <data key="d0">max(candidates, key=lambda x: x[0])</data>
  <data key="d1">Results</data>
  <data key="d2">A process that selects the best candidate program from multiple trials based on evaluation scores.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="SimplifiedBootstrapFewShotWithOptuna">
  <data key="d0">SimplifiedBootstrapFewShotWithOptuna</data>
  <data key="d1">Methodology</data>
  <data key="d2">A hyperparameter optimization approach that uses the Optuna framework to tune demonstrations and select the best performing program over multiple trials.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="objective">
  <data key="d0">objective</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function defining the optimization goal for Optuna, which involves creating candidate programs, tuning demonstrations, and evaluating their performance.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="trial.suggest_int">
  <data key="d0">trial.suggest_int</data>
  <data key="d1">Tools</data>
  <data key="d2">An Optuna method that proposes an integer value within a specified range for hyperparameter tuning, used here to select demonstration indices.</data>
  <data key="d3">chunk-06f4652e3f59fe7bb73c99f5346f2b82</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Demonstrations">
  <data key="d0">Demonstrations</data>
  <data key="d1">Study Design</data>
  <data key="d2">Demonstrations refer to sample tasks or examples used to illustrate or evaluate a program, often utilized in machine learning for training or testing models.&lt;SEP&gt;Sample tasks or examples used to illustrate or evaluate a program, often in machine learning, serving as training or testing data.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Predictor1">
  <data key="d0">Predictor1</data>
  <data key="d1">Variables</data>
  <data key="d2">A predictor component or feature in the program, used to make predictions, possibly a model parameter or input feature.&lt;SEP&gt;Predictor1 is one of the predictors in the program, likely a feature or model component used to make predictions.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Predictor2">
  <data key="d0">Predictor2</data>
  <data key="d1">Variables</data>
  <data key="d2">An alternative or complementary predictor component in the program, used alongside Predictor1 for evaluation.&lt;SEP&gt;Predictor2 is another predictor in the program, serving as an alternative or complementary feature or module.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Demos">
  <data key="d0">Demos</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Demos are the set of demonstrations or example tasks used within the program for evaluation or training.&lt;SEP&gt;The set of demonstrations or example tasks used within the program for evaluation or training purposes.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Program">
  <data key="d0">Program</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The computational model or algorithm being evaluated, optimized, or trained in the process.&lt;SEP&gt;The program refers to the computational or machine learning model being evaluated or optimized.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Score">
  <data key="d0">Score</data>
  <data key="d1">Results</data>
  <data key="d2">A numerical performance metric obtained by evaluating the program on a dataset, used to compare or select models.&lt;SEP&gt;Score is a quantitative measure obtained by evaluating the program on a dataset, used to determine its performance.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Objective Function">
  <data key="d0">Objective Function</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The function defining the goal of optimization, such as maximizing score, guiding the training or tuning process.&lt;SEP&gt;The objective function defines the goal of optimization, such as maximizing score, guiding the training or tuning process.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="BootstrapFewshot">
  <data key="d0">BootstrapFewshot</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology that generates a large number of potential demonstrations by leveraging bootstrap sampling, facilitating few-shot learning.&lt;SEP&gt;BootstrapFewshot is a methodology to generate a large number of potential demonstrations by leveraging bootstrap sampling techniques for few-shot learning.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Training Set">
  <data key="d0">Training Set</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The dataset used to train or fine-tune the model, containing labeled examples for learning.&lt;SEP&gt;The training set is the dataset used to train or fine-tune the model, containing examples for learning.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Validation Set">
  <data key="d0">Validation Set</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A dataset used to evaluate the model’s performance during tuning, separate from training data.&lt;SEP&gt;The validation set is used to evaluate the model's performance during tuning, separate from the training data.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Best Program">
  <data key="d0">Best Program</data>
  <data key="d1">Results</data>
  <data key="d2">The best program is the configuration identified through optimization that yields the highest score or desired performance metric.&lt;SEP&gt;The configuration of the program identified through optimization that yields the highest performance score.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Validation Data">
  <data key="d0">Validation Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The data used to validate the program’s performance during tuning, separate from training data.</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="context">
  <data key="d0">context</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The 'context' field refers to the background information or setting provided to frame the questions and search queries in the document.&lt;SEP&gt;The 'context' provides background information and setting for the questions and prompts, framing the scope of the inquiry.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="question">
  <data key="d0">question</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The 'question' field is a specific inquiry or hypothesis that guides the search or investigation based on the provided context.&lt;SEP&gt;The 'question' is a specific inquiry designed to guide the search or investigation based on the context, aiming to find targeted information.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="search query">
  <data key="d0">search query</data>
  <data key="d1">Tools</data>
  <data key="d2">The 'search query' is a formulated string used to retrieve relevant data or information related to the question and context.&lt;SEP&gt;The 'search query' is a formulated string used to retrieve relevant information or data related to the question and context.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Reasoning">
  <data key="d0">Reasoning</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The 'Reasoning' describes the logical, step-by-step process used to derive the search query from the context and question, ensuring systematic reasoning.&lt;SEP&gt;The 'Reasoning' section involves a step-by-step logical process used to derive the search query from the context and question.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Twilight (novel series)">
  <data key="d0">Twilight (novel series)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A series of four vampire-themed fantasy romance novels by Stephenie Meyer, used as an example in context for literature and publication date analysis.&lt;SEP&gt;A series of vampire-themed fantasy romance novels by Stephenie Meyer, used here as an example in the context.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Harper Connelly Mysteries">
  <data key="d0">Harper Connelly Mysteries</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A series of fantasy mystery novels by Charlaine Harris, cited as part of the context for information extraction.&lt;SEP&gt;A series of fantasy mystery novels by Charlaine Harris, referenced as an example of literary series relevant to the context.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="The Dark Heroine">
  <data key="d0">The Dark Heroine</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A series of vampire-themed fantasy romance novels by Abigail Gibbs, included as contextual background for publication analysis.&lt;SEP&gt;A series of vampire-themed fantasy romance novels by Abigail Gibbs, included as contextual background.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="The Victorians">
  <data key="d0">The Victorians</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A British documentary series about Victorian art and culture, serving as background context for historical and cultural references.&lt;SEP&gt;A documentary series about Victorian art and culture, used as an example context in the document.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="The Caxtons">
  <data key="d0">The Caxtons</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">An 1849 Victorian novel by Edward Bulwer-Lytton, cited as part of historical literary context.&lt;SEP&gt;An 1849 Victorian novel by Edward Bulwer-Lytton, referenced as part of historical context.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Victorian (comics)">
  <data key="d0">Victorian (comics)</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A comic book series published starting in 1999, exemplifying Victorian-themed media in contemporary culture.&lt;SEP&gt;A comic book series published starting in 1999, exemplifying Victorian-themed media in the context.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Jeremy Paxman">
  <data key="d0">Jeremy Paxman</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A British broadcaster and journalist, whose birth year is relevant to the context about Victorian art and culture.&lt;SEP&gt;A British journalist and broadcaster, whose birth year is relevant to the context about Victorian era and cultural history.</data>
  <data key="d3">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="DSP Y Program">
  <data key="d0">DSP Y Program</data>
  <data key="d3">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d2">DSP Y-optimized pipelines significantly outperform standard few-shot prompting on GPT-3.5 models in accuracy and efficiency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Empirical Findings">
  <data key="d0">Empirical Findings</data>
  <data key="d3">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d2">The paper reports empirical results demonstrating DSPy's ability to build high-quality LM systems without manual prompt engineering.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="hand-written prompts">
  <data key="d0">hand-written prompts</data>
  <data key="d3">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d2">DSPy aims to replace hand-crafted prompts with modular components, reducing reliance on artful prompt construction.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="bootstrap">
  <data key="d0">bootstrap</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d2">Bootstrap techniques generate and refine demonstration chains to self-improve program accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="self-coherence">
  <data key="d0">self-coherence</data>
  <data key="d3">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d2">Self-coherence involves sampling multiple reasoning paths and selecting the most consistent answer to improve reasoning accuracy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="GPT-3.5-turbo">
  <data key="d0">GPT-3.5-turbo</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d2">Zhao et al. (2023b) reports high accuracy (80.8%) for GPT-3.5-turbo using Chain-of-Thought prompting, establishing a benchmark for reasoning performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="question answering">
  <data key="d0">question answering</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d2">The BasicMultiHop program simulates multi-hop reasoning by retrieving passages iteratively and generating answers, aiming to improve performance on complex questions."|&gt;"multi-hop reasoning, information flow</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="multihop program">
  <data key="d0">multihop program</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d2">This bootstrapping technique enhances the multihop program's performance by combining few-shot prompting with random search strategies."|&gt;"training, performance enhancement</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="multihop question answering">
  <data key="d0">multihop question answering</data>
  <data key="d3">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d2">Answer EM scores are used to evaluate the effectiveness of the multihop programs, with higher scores indicating better accuracy."|&gt;"evaluation, accuracy</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="WebGPT: Browser-assisted Question-Answering with Human Feedback">
  <data key="d0">WebGPT: Browser-assisted Question-Answering with Human Feedback</data>
  <data key="d3">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d2">WebGPT employs browser assistance and human feedback to improve retrieval and answer quality in open-domain QA systems."|</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="research source">
  <data key="d0">research source</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Provides proceedings and papers related to neural information processing and machine learning research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Din-sql">
  <data key="d0">Din-sql</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of the paper on decomposed in-context learning of text-to-SQL with self-correction, 2023.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Measuring and narrowing the compositionality gap">
  <data key="d0">Measuring and narrowing the compositionality gap</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author involved in research on language model compositionality, 2022.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Automatic prompt optimization">
  <data key="d0">Automatic prompt optimization</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author working on prompt optimization with gradient descent and beam search, 2023.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Answering complex open-domain questions">
  <data key="d0">Answering complex open-domain questions</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of research on iterative query generation for answering complex questions, 2019.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Retrieve, rerank, read, then iterate">
  <data key="d0">Retrieve, rerank, read, then iterate</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of a methodology for answering questions through iterative retrieval and reading, 2020.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Improving language understanding">
  <data key="d0">Improving language understanding</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of work on generative pre-training to enhance language comprehension, 2018.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Data programming">
  <data key="d0">Data programming</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of work on creating large training sets quickly via data programming, 2016.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Synthetic Prompting">
  <data key="d0">Synthetic Prompting</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author working on generating chain-of-thought demonstrations for large language models, 2023.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Reflexion">
  <data key="d0">Reflexion</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author working on autonomous agent with self-reflection and dynamic memory, 2023.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Prompting GPT-3">
  <data key="d0">Prompting GPT-3</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author working on making GPT-3 reliable through prompting techniques, 2022.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Recitation-Augmented Language Models">
  <data key="d0">Recitation-Augmented Language Models</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author working on recitation-augmented models, 2022.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chainer">
  <data key="d0">Chainer</data>
  <data key="d3">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d2">Author of the deep learning framework Chainer, 2015.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Chengrun Yang">
  <data key="d0">Chengrun Yang</data>
  <data key="d3">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d2">Chengrun Yang's recent work explores using large language models as optimization tools.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Police Officers Mora and Rivera">
  <data key="d0">Police Officers Mora and Rivera</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d2">Their sacrifice and the subsequent funerals symbolize community commitment to trust and safety.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Russian Invasion">
  <data key="d0">Russian Invasion</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d2">Sanctions and strategic reserve releases are tools used by the U.S. and allies to respond to the invasion and influence Russia's economy and energy markets.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Sanctions and Oil Reserves">
  <data key="d0">Sanctions and Oil Reserves</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d2">Sanctions and strategic reserve releases are tools used by the U.S. and allies to respond to the invasion and influence Russia's economy and energy markets.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="ARPA-H">
  <data key="d0">ARPA-H</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d2">ARPA-H aims to drive technological innovations in health, inspired by DARPA's success in technological breakthroughs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Health Breakthroughs">
  <data key="d0">Health Breakthroughs</data>
  <data key="d3">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d2">ARPA-H aims to drive technological innovations in health, inspired by DARPA's success in technological breakthroughs.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Trial">
  <data key="d0">Trial</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d2">The trial stores the current program configuration as a user attribute for comparison during optimization.&lt;SEP&gt;The trial stores the program as a user attribute for comparison and selection during optimization.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="Self">
  <data key="d0">Self</data>
  <data key="d3">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d2">The training set is assigned to the object’s attribute for use in model training.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</node>
<node id="HPC (High Performance Computing)">
  <data key="d0">HPC (High Performance Computing)</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">HPC refers to the use of supercomputers and parallel processing techniques to solve complex computational problems efficiently.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-specific Models">
  <data key="d0">HPC-specific Models</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Models tailored for HPC focus on generating and optimizing parallel code, addressing the unique challenges of high-performance computing environments.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-INSTRUCT Dataset">
  <data key="d0">HPC-INSTRUCT Dataset</data>
  <data key="d1">Study Designs</data>
  <data key="d2">HPC-INSTRUCT is a synthetic high-quality dataset created to map existing parallel code samples to instruct-answer pairs, used for training and evaluating HPC-specific code models.&lt;SEP&gt;HPC-INSTRUCT is a synthetic high-quality dataset created to map parallel code samples to instruct-answer pairs, used for training and evaluating code LLMs.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2">
  <data key="d0">HPC-Coder-V2</data>
  <data key="d1">Results</data>
  <data key="d2">A fine-tuned code large language model (LLM) designed for parallel code generation, considered the most capable open-source model for this task.&lt;SEP&gt;A specialized language model designed for code generation in high-performance computing (HPC), evaluated across various problem types and execution models to assess its performance and capabilities.&lt;SEP&gt;A specialized large language model fine-tuned for HPC code generation, demonstrating high throughput and accuracy in parallel code tasks.&lt;SEP&gt;A specific large language model fine-tuned for high-performance computing tasks, demonstrating high throughput and accuracy in parallel code generation.&lt;SEP&gt;HPC-Coder-V2 is a fine-tuned open-source code LLM that achieves the best performance for parallel code generation, nearing GPT-4 levels.&lt;SEP&gt;HPC-Coder-V2 is a fine-tuned open-source code language model that achieves the best performance for parallel code generation, nearing GPT-4 levels.&lt;SEP&gt;HPC-Coder-V2 is a specialized language model designed for code generation in high-performance computing (HPC), evaluated across various problem types and execution models.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726&lt;SEP&gt;chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="StarCoder2">
  <data key="d0">StarCoder2</data>
  <data key="d1">Results</data>
  <data key="d2">StarCoder2 is a large language model (LLM) with parameter sizes of 1.3B, 7B, and 15B, pre-trained on a large corpus of mostly code data from The Stack V2, used for code generation tasks.&lt;SEP&gt;StarCoder2 is a large language model with sizes 1.3B, 7B, and 15B, trained on code data from The Stack V2, used for code generation tasks.&lt;SEP&gt;StarCoder2 models trained on The Stack v2 dataset perform similarly or worse than models trained on less data, indicating data quality issues.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3&lt;SEP&gt;chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="The Stack v2 Dataset">
  <data key="d0">The Stack v2 Dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">A large dataset containing permissively licensed code and related data used for training code LLMs.&lt;SEP&gt;A large dataset containing permissively licensed code and related data, used for training code LLMs.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Performance Evaluation">
  <data key="d0">Model Performance Evaluation</data>
  <data key="d1">Analytical Techniques</data>
  <data key="d2">Assessing models' ability to generate parallel code and comparing results to state-of-the-art benchmarks like GPT-4.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="High-Quality Parallel Code Data">
  <data key="d0">High-Quality Parallel Code Data</data>
  <data key="d1">Variables</data>
  <data key="d2">High-quality parallel code data refers to well-annotated, representative datasets of parallel code used to train models effectively.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Challenges in HPC Code Generation">
  <data key="d0">Challenges in HPC Code Generation</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The study investigates the hurdles preventing effective HPC code generation by LLMs and how fine-tuning impacts performance.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Hurdles in Generating Parallel Code">
  <data key="d0">Hurdles in Generating Parallel Code</data>
  <data key="d1">Limitations</data>
  <data key="d2">Challenges include lack of high-quality parallel code data and intrinsic difficulty of parallel code generation for LLMs.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic Data Generation Methodology">
  <data key="d0">Synthetic Data Generation Methodology</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The approach used to create HPC-INSTRUCT involves mapping existing parallel code samples to instruct-answer pairs to generate training data.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact on Scientific Computing">
  <data key="d0">Impact on Scientific Computing</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Improving HPC-specific code generation tools can significantly enhance scientific research productivity and discovery speed.</data>
  <data key="d3">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-I NSTRUCT">
  <data key="d0">HPC-I NSTRUCT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large synthetic dataset of high-quality parallel code instruction data used for training and evaluating code language models.&lt;SEP&gt;A newly introduced synthetic dataset composed of high-performance computing instruction data, generated from open-source parallel code and synthetic data methods, used for training and evaluating HPC LLMs.&lt;SEP&gt;A newly introduced synthetic dataset for training and evaluating HPC code LLMs, generated using models and open-source parallel code.&lt;SEP&gt;HPC-I NSTRUCT is a dataset comprising synthetic and semi-synthetic code instruction samples used for training and fine-tuning large language models in high-performance computing contexts.&lt;SEP&gt;HPC-I NSTRUCT is a dataset containing synthetic and semi-synthetic code instruction samples used for training and fine-tuning language models.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-tuning Base Model">
  <data key="d0">Fine-tuning Base Model</data>
  <data key="d1">Variables</data>
  <data key="d2">The choice of initial pre-trained model used as the foundation for fine-tuning, affecting the model's ability to learn parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data Representation">
  <data key="d0">Data Representation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The way in which data is structured and presented to influence the learning process of models, particularly in training neural networks for parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Parameters">
  <data key="d0">Training Parameters</data>
  <data key="d1">Variables</data>
  <data key="d2">Settings and configurations used during the training of models, including learning rate, batch size, and epochs, which impact model performance.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact on Models">
  <data key="d0">Impact on Models</data>
  <data key="d1">Results</data>
  <data key="d2">The effect of data representation and training parameters on the ability of models to learn and generate parallel code effectively.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Next Generation HPC AI Developer Tools">
  <data key="d0">Next Generation HPC AI Developer Tools</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Advanced tools leveraging improved models to facilitate development of parallel algorithms and optimize high-performance computing workflows.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic Dataset HPC-I NSTRUCT">
  <data key="d0">Synthetic Dataset HPC-I NSTRUCT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A large, high-quality set of artificially generated parallel code instructions used for training and evaluation of code models.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-tuning a code LLM">
  <data key="d0">Fine-tuning a code LLM</data>
  <data key="d1">Methodologies</data>
  <data key="d2">The process of adapting a pre-trained language model specifically for the task of parallel code generation by training on specialized datasets.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Open-source code LLM">
  <data key="d0">Open-source code LLM</data>
  <data key="d1">Tools</data>
  <data key="d2">A publicly available language model designed for code generation, which can be fine-tuned for specific tasks like parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Research Questions">
  <data key="d0">Research Questions</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Questions exploring how different factors such as model choice, data quality, and model size influence the performance of code LLMs in parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Instruction Masking">
  <data key="d0">Instruction Masking</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A fine-tuning technique where parts of instructions are masked to assess impact on model performance in code generation tasks.&lt;SEP&gt;A fine-tuning technique where parts of instructions are masked to assess impact on model performance in code generation tasks; found to have little impact in this study.&lt;SEP&gt;A fine-tuning technique where parts of the instruction tokens are masked to prevent the model from learning undesirable patterns, enhancing response quality.&lt;SEP&gt;A training technique where instructions are hidden or masked to prevent the model from learning undesirable patterns, aiming to improve response quality and generalization.&lt;SEP&gt;Instruction masking is a technique used during training to hide or omit the instruction part of input data, aiming to prevent the model from learning undesirable patterns and to focus learning on responses.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic Data">
  <data key="d0">Synthetic Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Artificial data generated from language models, used to train and evaluate HPC instruction datasets.&lt;SEP&gt;Artificially generated data created using models like LLMs to supplement limited real datasets, especially for training HPC code generation models.&lt;SEP&gt;Artificially generated data from LLMs and open-source code, used to train and evaluate models on parallel code tasks.&lt;SEP&gt;Artificially generated data used to train and evaluate models, especially for specialized tasks like parallel code generation.&lt;SEP&gt;Artificially generated datasets used to fine-tune models. Data sources include Llama3-70B, DBRX, Mixtral, Gemini, which influence the quality and performance of the models.&lt;SEP&gt;Artificially generated training data produced by different models (e.g., Gemini-Pro, DBRX, Llama-3-70B, Mixtral-8x7B), used to study the impact of data quality on model fine-tuning.&lt;SEP&gt;Artificially generated training data used to fine-tune models, with variations depending on the source model (e.g., Llama3-70B, DBRX).&lt;SEP&gt;Synthetic data refers to artificially generated code samples designed to augment training datasets, improve model performance, and address data scarcity for HPC-related code generation tasks.&lt;SEP&gt;Synthetic data refers to artificially generated code samples used to augment training datasets for fine-tuning code LLMs, especially to address data scarcity and improve model performance in HPC contexts.&lt;SEP&gt;Synthetic data refers to artificially generated training data, often produced by models like Gemini-Pro, DBRX, Llama-3-70B, and Mixtral-8x7B, used to augment or replace real data in training.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19&lt;SEP&gt;chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Study of Data and Fine-tuning Parameters">
  <data key="d0">Study of Data and Fine-tuning Parameters</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Systematic investigation into how variations in data quality, quantity, and training configurations affect model performance in parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Data Quality">
  <data key="d0">Training Data Quality</data>
  <data key="d1">Variables</data>
  <data key="d2">The accuracy, relevance, and diversity of training datasets, which impact the model's ability to generate high-quality parallel code.&lt;SEP&gt;The quality and characteristics of data used for fine-tuning, which significantly impact the performance of code LLMs in parallel code generation.&lt;SEP&gt;The quality, relevance, and comprehensiveness of the data used for fine-tuning significantly influence the performance of code LLMs in parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Evaluation of Fine-tuning Impact">
  <data key="d0">Evaluation of Fine-tuning Impact</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Assessment of how different fine-tuning strategies and data qualities influence model performance in parallel code generation.</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code LLMs">
  <data key="d0">Code LLMs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Code Language Models (LLMs) are AI models trained to understand, generate, and manipulate programming code, often used to assist in code generation, translation, optimization, and parallelization tasks.&lt;SEP&gt;Code Language Models (LLMs) are artificial intelligence models trained to understand, generate, and manipulate programming code, used to improve code synthesis, translation, optimization, and parallelization in high-performance computing.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-INSTRUCT">
  <data key="d0">HPC-INSTRUCT</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A newly introduced dataset comprising synthetic data generated from LLMs and open-source parallel code, used for fine-tuning and evaluating HPC code models.&lt;SEP&gt;A newly introduced dataset comprising synthetic data generated from language models and open-source parallel code, designed for fine-tuning and evaluating HPC code LLMs in parallel code generation.&lt;SEP&gt;HPC-INSTRUCT is a large synthetic dataset of parallel code instruction-response pairs created to fine-tune HPC-capable code LLMs, comprising approximately 120,000 pairs generated from open-source code snippets and multiple LLMs.&lt;SEP&gt;HPC-INSTRUCT is a large synthetic dataset of parallel code instruction-response pairs created to fine-tune HPC-capable code LLMs, comprising roughly 120,000 pairs generated from open-source code snippets and multiple LLMs.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Ablation Studies">
  <data key="d0">Ablation Studies</data>
  <data key="d1">Study Designs</data>
  <data key="d2">Ablation studies are experimental designs used to analyze the impact of different training configurations and data partitions on model performance, particularly in code generation tasks.&lt;SEP&gt;Ablation studies are experimental investigations that systematically vary specific components or axes of a model to understand their individual contributions to overall performance, particularly in the context of model fine-tuning.&lt;SEP&gt;Ablation studies are systematic experiments that vary specific axes or components of models, such as data amount, data quality, or model size, to understand their individual contributions to performance, especially in model fine-tuning.&lt;SEP&gt;Ablation studies systematically assess how variations in data quality, model size, prompt construction, and other parameters affect the performance of code LLMs in generating parallel code, guiding best practices.&lt;SEP&gt;Ablation studies systematically evaluate the impact of different data, model, and prompt parameters on the ability of code LLMs to generate parallel code, helping identify best practices.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="ParEval Benchmark">
  <data key="d0">ParEval Benchmark</data>
  <data key="d1">Evidence Types</data>
  <data key="d2">A standardized evaluation benchmark used to compare the performance of different HPC code LLMs on parallel code generation tasks.&lt;SEP&gt;A standardized evaluation framework used to compare the performance of different HPC code LLMs in parallel code generation tasks.&lt;SEP&gt;A standardized evaluation suite for assessing the code generation performance of language models across multiple problem types and datasets.&lt;SEP&gt;A standardized suite of tests for evaluating the code generation capabilities of language models across different problem types and settings.&lt;SEP&gt;ParEval is a benchmark dataset used to evaluate the performance of fine-tuned code LLMs on real parallel code generation tasks, providing a standard for comparison.&lt;SEP&gt;ParEval is a benchmark dataset used to evaluate the performance of fine-tuned code LLMs on real-world parallel code generation tasks, providing a standard metric for comparison.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad&lt;SEP&gt;chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data Representation and Quality">
  <data key="d0">Data Representation and Quality</data>
  <data key="d1">Variables</data>
  <data key="d2">Data representation and quality impact how well a code LLM learns to generate accurate and efficient parallel code, affecting overall model performance.&lt;SEP&gt;Data representation and quality impact the ability of code LLMs to learn accurate code mappings and generate high-quality parallel code, affecting overall performance.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Seed Code Snippets">
  <data key="d0">Seed Code Snippets</data>
  <data key="d1">Tools</data>
  <data key="d2">Seed code snippets are small, representative pieces of open-source parallel code used as prompts to generate diverse synthetic data samples across programming languages and paradigms.&lt;SEP&gt;Seed code snippets are small, representative pieces of open-source parallel code used as prompts to generate diverse synthetic data samples in various programming languages and paradigms.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel Programming Languages">
  <data key="d0">Parallel Programming Languages</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Languages such as C, Fortran, CUDA, Chapel, and OpenCL are studied for their use in high-performance computing and parallel code development.&lt;SEP&gt;Languages such as C, Fortran, CUDA, Chapel, and OpenCL are studied for their use in high-performance parallel computing and serve as target languages for code translation and parallelization tasks.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Open-source Codebases">
  <data key="d0">Open-source Codebases</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Open-source codebases like The Stack V2 provide seed snippets and real code samples in HPC languages, forming the basis for synthetic data generation and model training.&lt;SEP&gt;Open-source codebases such as The Stack V2 provide real code snippets and seed examples in HPC languages like C, Fortran, CUDA, etc., serving as sources for synthetic data generation and model training.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code Samples">
  <data key="d0">Code Samples</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Code samples are specific pieces of code used as input prompts or training data, including seed snippets and generated synthetic code for training and evaluation.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Diversity of Synthetic Data">
  <data key="d0">Diversity of Synthetic Data</data>
  <data key="d1">Variables</data>
  <data key="d2">Diversity of synthetic data, achieved through multiple LLMs and varied prompts, enhances the robustness and generalization of fine-tuned models in parallel code generation.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Evaluation Metrics">
  <data key="d0">Model Evaluation Metrics</data>
  <data key="d1">Results</data>
  <data key="d2">Evaluation metrics such as accuracy, code correctness, and performance on benchmarks like ParEval quantify the success of fine-tuned models in generating high-quality parallel code.</data>
  <data key="d3">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="generation model">
  <data key="d0">generation model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A generation model is an AI system designed to produce human-like text or code outputs, often trained on large datasets to learn language or code patterns.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Pre-trained Model">
  <data key="d0">Pre-trained Model</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A pre-trained model is a neural network trained on large, general datasets prior to fine-tuning for specific tasks, such as code generation.&lt;SEP&gt;A pre-trained model is an AI model trained on large general datasets before fine-tuning on specific tasks, such as code generation in this context.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="DeepSeek-Coder">
  <data key="d0">DeepSeek-Coder</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">DeepSeek-Coder is a family of state-of-the-art code modeling language models with variants of 1.3b, 6.7b, and 16b parameters, based on llama and mixture-of-experts architectures, trained on code and natural language datasets.&lt;SEP&gt;DeepSeek-Coder is a family of state-of-the-art code modeling language models, including variants with 1.3 billion, 6.7 billion, and 16 billion parameters, trained primarily on code data with some natural language, utilizing architectures such as llama and mixture-of-experts (MOE).&lt;SEP&gt;DeepSeek-Coder is a large language model designed for code understanding, code generation, and programming tasks.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic HPC Code Data">
  <data key="d0">Synthetic HPC Code Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Synthetic HPC code data are artificially generated code samples designed to mimic real HPC code, used to train and evaluate code models.&lt;SEP&gt;Synthetic HPC code data refers to artificially generated code samples used to train and evaluate code language models, enhancing their ability to generalize and perform in HPC contexts.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="AxoNN Framework">
  <data key="d0">AxoNN Framework</data>
  <data key="d1">Tools</data>
  <data key="d2">AxoNN is a parallel deep learning framework built on PyTorch, used to facilitate the distributed training and fine-tuning of large models across multiple GPUs.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Context Window">
  <data key="d0">Context Window</data>
  <data key="d1">Variables</data>
  <data key="d2">The context window length indicates the number of tokens the model considers at a time, impacting memory and performance.&lt;SEP&gt;The context window specifies the number of tokens the model considers at once during training or inference, impacting memory usage and performance.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code Benchmarks">
  <data key="d0">Code Benchmarks</data>
  <data key="d1">Results</data>
  <data key="d2">Code benchmarks are standardized tests used to evaluate and compare the performance of different code models across various coding tasks.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Semi-synthetic Data">
  <data key="d0">Semi-synthetic Data</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Semi-synthetic code data is artificially generated data that mimics real code, used to improve model generalization during training.&lt;SEP&gt;Semi-synthetic data is artificially created data that mimics real code, used to enhance model generalization during training.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance Hyperparameters">
  <data key="d0">Performance Hyperparameters</data>
  <data key="d1">Variables</data>
  <data key="d2">Performance hyperparameters include settings like batch size, sequence length, and optimizer parameters that are tuned to optimize training outcomes.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="generation">
  <data key="d0">generation</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Generation refers to the process by which models produce new data, such as text or code, based on learned patterns from training data.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Architecture">
  <data key="d0">Architecture</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">The architecture of a model defines its structural design, such as llama or mixture-of-experts (MOE), impacting its scalability, efficiency, and capacity.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code Benchmark">
  <data key="d0">Code Benchmark</data>
  <data key="d1">Results</data>
  <data key="d2">Code benchmarks are standardized tests used to evaluate the performance of code models across various coding tasks, measuring accuracy, efficiency, and generalization.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Batch Size">
  <data key="d0">Batch Size</data>
  <data key="d1">Variables</data>
  <data key="d2">Batch size is a hyperparameter that determines the number of samples processed before updating the model weights during training.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Sequence Length">
  <data key="d0">Sequence Length</data>
  <data key="d1">Variables</data>
  <data key="d2">Sequence length specifies the number of tokens the model processes at once, affecting memory usage and training stability.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Optimizer">
  <data key="d0">Optimizer</data>
  <data key="d1">Variables</data>
  <data key="d2">The optimizer, such as AdamW, is a hyperparameter that determines how model weights are updated during training based on loss gradients.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Epochs">
  <data key="d0">Training Epochs</data>
  <data key="d1">Variables</data>
  <data key="d2">Training epochs refer to the number of complete passes through the training dataset, impacting model learning and convergence.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel Deep Learning Framework">
  <data key="d0">Parallel Deep Learning Framework</data>
  <data key="d1">Tools</data>
  <data key="d2">AxoNN is a framework that enables parallelized training of large models across multiple GPUs, facilitating efficient fine-tuning.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Hyperparameters">
  <data key="d0">Training Hyperparameters</data>
  <data key="d1">Variables</data>
  <data key="d2">Hyperparameters like batch size, sequence length, and learning rate are tuned to optimize training efficacy and model quality.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Loss">
  <data key="d0">Training Loss</data>
  <data key="d1">Results</data>
  <data key="d2">Training loss quantifies how well the model fits the training data, guiding hyperparameter tuning and model improvements.</data>
  <data key="d3">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Choice of Base Model">
  <data key="d0">Choice of Base Model</data>
  <data key="d1">Core Concept</data>
  <data key="d2">The selection between different foundational language models (e.g., Deepseek-Coder 1.3B and 6.7B) influences the effectiveness and characteristics of subsequent fine-tuning processes.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Instruction Masking Impact">
  <data key="d0">Instruction Masking Impact</data>
  <data key="d1">Variables</data>
  <data key="d2">The impact of instruction masking on model learning, balancing noise reduction against potential loss of informative signals from instructions, is a key variable in training effectiveness.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Amount of Parallel Code Data">
  <data key="d0">Amount of Parallel Code Data</data>
  <data key="d1">Variables</data>
  <data key="d2">The quantity of parallel code samples used in fine-tuning, such as MPI code samples, affects the model's ability to generate specific types of code, with performance potentially plateauing beyond certain data volumes.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Quality of Data">
  <data key="d0">Quality of Data</data>
  <data key="d1">Variables</data>
  <data key="d2">The quality of synthetic or real training data influences the final performance of the fine-tuned model, with higher-quality data expected to improve output accuracy and relevance.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Base Model">
  <data key="d0">Base Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The foundational language models, such as Deepseek-Coder 1.3B and 6.7B, used as the starting point for fine-tuning, influencing the adaptation and effectiveness of subsequent training.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="MPI Code Samples">
  <data key="d0">MPI Code Samples</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Parallel programming code samples containing MPI-specific substrings (e.g., 'mpi.h', 'MPI Init') used to evaluate the impact of data quantity on model performance in generating MPI code.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data Quantity">
  <data key="d0">Data Quantity</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount of data, such as MPI code samples, used in fine-tuning, which affects model performance and potential performance plateau.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data Quality">
  <data key="d0">Data Quality</data>
  <data key="d1">Variables</data>
  <data key="d2">Attributes of training data that impact model performance, emphasizing the importance of high-quality data over simply increasing data quantity in HPC code LLM training.&lt;SEP&gt;Attributes of training data that influence model performance; emphasizes that high data quality is more critical than data quantity for effective HPC code LLM training.&lt;SEP&gt;Refers to the attributes of training datasets that affect how well models learn, with higher quality data leading to better code generation performance.&lt;SEP&gt;Refers to the attributes of training datasets that affect model learning, with higher quality leading to better code generation performance.&lt;SEP&gt;The inherent quality of training data, whether real or synthetic, influencing the effectiveness of fine-tuned models.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Trade-off Between Data and Quality">
  <data key="d0">Trade-off Between Data and Quality</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The hypothesis that increasing data volume may have diminishing returns and that data quality becomes more crucial for improving model performance.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic Data Generation">
  <data key="d0">Synthetic Data Generation</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Method of creating artificial data using models like LLMs to augment training datasets, especially useful when real data is limited or costly to obtain.&lt;SEP&gt;The process of creating artificial data using various models (Gemini-Pro, DBRX, Llama-3-70B, Mixtral-8x7B) to augment training datasets.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance Plateau">
  <data key="d0">Performance Plateau</data>
  <data key="d1">Results</data>
  <data key="d2">A point where increasing data quantity no longer significantly improves model performance, indicating diminishing returns.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Trade-off Analysis">
  <data key="d0">Trade-off Analysis</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The analysis of the balance between data quantity and data quality to optimize model fine-tuning efficiency and effectiveness.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact of Synthetic Data Quality">
  <data key="d0">Impact of Synthetic Data Quality</data>
  <data key="d1">Results</data>
  <data key="d2">The study of how the quality of synthetic data, as generated by different models, impacts the final performance of fine-tuned models.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact of Data Amount">
  <data key="d0">Impact of Data Amount</data>
  <data key="d1">Results</data>
  <data key="d2">The investigation into how varying the amount of training data affects model performance and whether performance improves monotonically.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact of Model Size">
  <data key="d0">Impact of Model Size</data>
  <data key="d1">Results</data>
  <data key="d2">Assessment of how different model sizes influence the fine-tuning outcome and overall performance.</data>
  <data key="d3">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="RX">
  <data key="d0">RX</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">RX refers to a dataset or a model version used in the context of fine-tuning language models, possibly representing a specific model or dataset identifier.&lt;SEP&gt;RX refers to a dataset or model version used in the context of fine-tuning language models, possibly representing a specific dataset or experimental setup.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Llama-3-70B">
  <data key="d0">Llama-3-70B</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Llama-3-70B is a large language model with 70 billion parameters, used for fine-tuning and performance evaluation.&lt;SEP&gt;Llama-3-70B is a large language model with 70 billion parameters, used for fine-tuning experiments and performance evaluation.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Mixtral-8x7B">
  <data key="d0">Mixtral-8x7B</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Mixtral-8x7B is a large language model with 8x7 billion parameters, utilized alongside other models for comparison in fine-tuning and performance studies.&lt;SEP&gt;Mixtral-8x7B is another large language model with 8x7 billion parameters, used in comparative model studies.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="data">
  <data key="d0">data</data>
  <data key="d1">Variables</data>
  <data key="d2">A 2D array containing data points to be processed in the metric computation and summation.&lt;SEP&gt;The data encompasses the datasets used for fine-tuning models, including the HPC-I NSTRUCT dataset, which influences model performance and evaluation outcomes.&lt;SEP&gt;The data refers to the dataset used for fine-tuning the models, including the HPC-I NSTRUCT dataset and others, impacting model performance.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="parallell code generation">
  <data key="d0">parallell code generation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Parallel code generation is a specific task where models generate code that can run concurrently across multiple processors or nodes, tested using benchmarks like ParEval.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Magicoder">
  <data key="d0">Magicoder</data>
  <data key="d1">The Magicoder model</data>
  <data key="d2">Magicoder is a 6.7B parameter model fine-tuned on synthetic open-source code data, evaluated for code generation efficacy.&lt;SEP&gt;Magicoder is a 6.7B parameter model that is a fine-tuning of DeepseekCoder-6.7B, trained on synthetic open-source code data, aimed at improving code generation performance.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8&lt;SEP&gt;chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="resources">
  <data key="d0">resources</data>
  <data key="d1">Variables</data>
  <data key="d2">Resources refer to hardware like GPUs (e.g., H100) and memory capacity, which influence model throughput and feasibility of deployment.</data>
  <data key="d3">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Gemini-1.5-flash">
  <data key="d0">Gemini-1.5-flash</data>
  <data key="d1">The Gemini-1.5-flash model</data>
  <data key="d2">Gemini-1.5-flash is a commercial model accessible via API from Google, used for code-related tasks in a commercial setting.&lt;SEP&gt;Gemini-1.5-flash is a commercial model accessible via API from Google, used in practical applications of code generation.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel Code Generation Performance">
  <data key="d0">Parallel Code Generation Performance</data>
  <data key="d1">Results</data>
  <data key="d2">This refers to the effectiveness of different models in generating code in parallel, measured via metrics like Pass@1, across various training configurations and datasets.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Choice of Base Model and Instruction Masking">
  <data key="d0">Choice of Base Model and Instruction Masking</data>
  <data key="d1">Research Question</data>
  <data key="d2">This research question investigates how the selection of the base model and the use of instruction masking during fine-tuning affect the code generation performance of models.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact of Fine-Tuning Data Quantity and Quality">
  <data key="d0">Impact of Fine-Tuning Data Quantity and Quality</data>
  <data key="d1">Research Question</data>
  <data key="d2">This question explores how varying the amount and quality of parallel code training data influences the performance of code language models on specific parallel code generation tasks.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="MPI Code Generation">
  <data key="d0">MPI Code Generation</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">MPI code generation refers to the ability of models to generate Message Passing Interface (MPI) code, a parallel programming model, used here to evaluate the impact of training data size on model performance.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Data for MPI">
  <data key="d0">Training Data for MPI</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount of MPI training data (number of samples) used to fine-tune models, ranging from 0k to 12k samples, to assess its effect on MPI code generation performance.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Data Quantity">
  <data key="d0">Training Data Quantity</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of MPI training samples (0k, 2k, 4k, 6k, 8k, 10k, 12k) used to fine-tune models, assessing its impact on MPI code generation performance.</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel Execution Model">
  <data key="d0">Parallel Execution Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A framework for executing tasks simultaneously to improve computational efficiency, relevant to code generation and performance.&lt;SEP&gt;A framework or paradigm in computing that allows multiple processes or threads to execute simultaneously, which is relevant to code generation models that produce parallel code.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="SMALLER Models">
  <data key="d0">SMALLER Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Refers to lightweight language models (e.g., 1.3B parameters) whose performance is sensitive to training data size and prone to overfitting at higher data volumes.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Larger Models">
  <data key="d0">Larger Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Refers to more complex language models (e.g., 6.7B, 16B parameters) that exhibit diminishing returns in performance gains with increased data, and are more robust to overfitting.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="MPI Fine-Tuning Data">
  <data key="d0">MPI Fine-Tuning Data</data>
  <data key="d1">Variables</data>
  <data key="d2">Data specifically used to adapt models for MPI code generation tasks, with variations depending on data source quality and size, affecting model performance.&lt;SEP&gt;Data used specifically to adapt models for MPI (Message Passing Interface) code generation tasks, influencing model performance.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="MPI Code Generation Performance">
  <data key="d0">MPI Code Generation Performance</data>
  <data key="d1">Results</data>
  <data key="d2">The effectiveness of models in generating MPI code, measured by Pass@1, varies with data quality, model size, and amount of training data.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance Metrics (Pass@1)">
  <data key="d0">Performance Metrics (Pass@1)</data>
  <data key="d1">Results</data>
  <data key="d2">Quantitative measure indicating the percentage of correct first-attempt code generations, used to compare models.&lt;SEP&gt;Quantitative measure of a model's ability to correctly generate code on the first attempt, used to compare different models and data sources.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Trade-offs">
  <data key="d0">Trade-offs</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The balance between model size, training data quality, and performance gains, crucial for designing practical code LLMs.&lt;SEP&gt;The balance between model size, training data quality, and performance gains, essential for designing practical and efficient code LLMs.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Small Models">
  <data key="d0">Small Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Refers to lightweight language models (e.g., 1.3B parameters) whose performance is sensitive to training data size, often prone to overfitting with excessive data.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Large Models">
  <data key="d0">Large Models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Refers to larger language models (e.g., 6.7B, 16B parameters) that tend to show diminishing returns in performance improvements as model size increases.</data>
  <data key="d3">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="ParEval benchmark">
  <data key="d0">ParEval benchmark</data>
  <data key="d1">Study Design</data>
  <data key="d2">A benchmark suite used to evaluate the performance of code language models across different problem types and parallel execution models in HPC.&lt;SEP&gt;The ParEval benchmark suite is used to evaluate the performance of code LLMs across different problem types and execution models.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code generation performance">
  <data key="d0">Code generation performance</data>
  <data key="d1">Results</data>
  <data key="d2">The measure of how accurately and efficiently models produce code for different problem types and execution models, including correctness, speed, and resource usage.&lt;SEP&gt;The performance of HPC-Coder-V2 and other models is measured in terms of code correctness, speed, and memory efficiency across problem types and execution models.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model sizes">
  <data key="d0">Model sizes</data>
  <data key="d1">Variables</data>
  <data key="d2">Different sizes of HPC-Coder-V2 (1.3B, 6.7B, 16B) are compared to analyze how size impacts performance, speed, and resource requirements.&lt;SEP&gt;Different sizes of HPC-Coder-V2 models (1.3B, 6.7B, 16B) are compared to assess trade-offs between performance, speed, and resource requirements.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel execution models">
  <data key="d0">Parallel execution models</data>
  <data key="d1">Methods</data>
  <data key="d2">Models are evaluated on serial, OpenMP, CUDA, HIP, Kokkos, MPI, and MPI+OpenMP execution models to analyze their effectiveness in generating parallel code.&lt;SEP&gt;Various parallel programming models such as serial, OpenMP, CUDA, HIP, Kokkos, MPI, and MPI+OpenMP, used to evaluate model performance.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-tuning strategies">
  <data key="d0">Fine-tuning strategies</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Data and training strategies employed to enhance the models' ability to generate high-quality parallel code efficiently, balancing performance and resource consumption.&lt;SEP&gt;The study discusses data and fine-tuning strategies that improve the models' ability to generate high-quality parallel code without sacrificing memory and throughput.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model performance comparison">
  <data key="d0">Model performance comparison</data>
  <data key="d1">Results</data>
  <data key="d2">HPC-Coder-V2 models outperform some larger models in speed and memory efficiency, particularly in parallel code generation, despite some models like Magicoder-6.7B performing better in serial code generation.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Trade-offs in model design">
  <data key="d0">Trade-offs in model design</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">The results highlight that with high-quality fine-tuning data, smaller models can achieve competitive performance in HPC code generation, making them more practical for developers.&lt;SEP&gt;The study highlights how smaller, fine-tuned models can outperform larger models in certain aspects, offering practical advantages for developers in HPC environments.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Related work on code LLMs for HPC">
  <data key="d0">Related work on code LLMs for HPC</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Literature discussing the development, fine-tuning, and application of large language models tailored for high-performance computing and parallel programming tasks.&lt;SEP&gt;The literature discusses the development and fine-tuning of code LLMs specifically tailored for high-performance computing and parallel programming tasks.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Comparison with other models">
  <data key="d0">Comparison with other models</data>
  <data key="d1">Results</data>
  <data key="d2">HPC-Coder-V2 models demonstrate competitive performance, outperforming some larger models like Phind-V2-34B and Magicoder-6.7B in parallel code generation, despite being smaller.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Speed and memory efficiency">
  <data key="d0">Speed and memory efficiency</data>
  <data key="d1">Variables</data>
  <data key="d2">Metrics assessing how fast models generate code and how much memory they consume, crucial for practical deployment.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model performance trade-offs">
  <data key="d0">Model performance trade-offs</data>
  <data key="d1">Applications/Implications</data>
  <data key="d2">Findings suggest smaller models can achieve high performance with lower resource requirements, making them more accessible for real-world HPC use.</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Code LLMs for HPC">
  <data key="d0">Code LLMs for HPC</data>
  <data key="d1">Discipline</data>
  <data key="d2">Research area focused on developing and applying large language models tailored for high-performance computing and parallel code synthesis.&lt;SEP&gt;Research area focusing on developing and applying large language models tailored for high-performance computing and parallel code generation tasks.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-tuning Specialized Code LLMs">
  <data key="d0">Fine-tuning Specialized Code LLMs</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">Approaches to adapt language models for specific domains or languages by leveraging synthetic data, addressing data quality and quantity challenges.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Throughput">
  <data key="d0">Throughput</data>
  <data key="d1">Variables</data>
  <data key="d2">Measurement of the number of tokens generated per second during code synthesis, reflecting model efficiency and speed.&lt;SEP&gt;Measurement of tokens generated per second during code synthesis, reflecting model efficiency and speed.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Pass@1">
  <data key="d0">Pass@1</data>
  <data key="d1">Results</data>
  <data key="d2">A performance metric indicating the percentage of times the model's top prediction is correct, used to evaluate code generation accuracy.&lt;SEP&gt;Pass@1 is a performance metric indicating the probability that the correct answer is among the top one predictions made by the model, used to evaluate model accuracy.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Memory Requirements">
  <data key="d0">Model Memory Requirements</data>
  <data key="d1">Variables</data>
  <data key="d2">The amount of RAM or VRAM needed for a model to run effectively, relevant when comparing models like HPC-Coder-V2 and StarCoder2.&lt;SEP&gt;The amount of computational memory (RAM/VRAM) needed for models to operate, relevant for comparing different models' efficiency.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Performance Comparison">
  <data key="d0">Model Performance Comparison</data>
  <data key="d1">Results</data>
  <data key="d2">Analysis comparing different models based on throughput, memory, and correctness, showing that smaller models can perform as well or better than larger ones.&lt;SEP&gt;Analysis comparing different models based on throughput, memory, and correctness, showing that smaller models can perform comparably to larger ones.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Fine-tuning Impact">
  <data key="d0">Model Fine-tuning Impact</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">Investigates how individual choices in data, model architecture, and prompt configuration influence the effectiveness of HPC code LLMs.</data>
  <data key="d3">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="7B Model">
  <data key="d0">7B Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A smaller language model with 7 billion parameters that achieves performance comparable to larger models while generating tokens faster, emphasizing efficiency in HPC code generation.&lt;SEP&gt;A smaller model with 7 billion parameters that achieves performance comparable to larger models while generating tokens faster.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="34B Model">
  <data key="d0">34B Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">A large language model with 34 billion parameters used as a benchmark for performance comparison in high-performance computing tasks.&lt;SEP&gt;A larger language model with 34 billion parameters used as a performance benchmark in the study.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Base Models">
  <data key="d0">Base Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Pre-trained language models used as the foundation for further fine-tuning, which in this study are compared to instruct variants.&lt;SEP&gt;Pre-trained language models used as the foundation for further fine-tuning, with the study suggesting they perform better for parallel code generation than instruct variants.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Instruct Variants">
  <data key="d0">Instruct Variants</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Versions of models fine-tuned with instruction data, compared against base models for effectiveness in parallel code tasks.&lt;SEP&gt;Versions of models that are fine-tuned with instruction-based data to improve task-specific performance.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2-1.3B">
  <data key="d0">HPC-Coder-V2-1.3B</data>
  <data key="d1">Tools</data>
  <data key="d2">A state-of-the-art HPC code LLM with 1.3 billion parameters, fine-tuned and evaluated for parallel code generation capabilities.&lt;SEP&gt;A state-of-the-art HPC code LLM with 1.3 billion parameters, fine-tuned and evaluated in the study for parallel code generation.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2-6.7B">
  <data key="d0">HPC-Coder-V2-6.7B</data>
  <data key="d1">Tools</data>
  <data key="d2">A medium-sized HPC code LLM with 6.7 billion parameters, assessed for its performance and efficiency in parallel code tasks.&lt;SEP&gt;A medium-sized HPC code LLM with 6.7 billion parameters, fine-tuned and evaluated for performance in parallel code tasks.&lt;SEP&gt;HPC-Coder-V2-6.7B is a language model evaluated on various problem types and execution models, with performance metrics such as pass@1 scores.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2-16B">
  <data key="d0">HPC-Coder-V2-16B</data>
  <data key="d1">Tools</data>
  <data key="d2">A large HPC code LLM with 16 billion parameters, assessed for its capabilities in parallel code generation.&lt;SEP&gt;A large HPC code LLM with 16 billion parameters, evaluated for its ability to generate parallel code efficiently.&lt;SEP&gt;A specific size variant of HPC-Coder-V2 with 16 billion parameters, showing high pass@1 scores in code generation tasks.&lt;SEP&gt;A variant of HPC-Coder-V2 with 16 billion parameters, showing high performance in code generation correctness.&lt;SEP&gt;HPC-Coder-V2-16B is a language model assessed across problem types and execution models, with performance scores indicating its capabilities.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479&lt;SEP&gt;chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Open-source Models">
  <data key="d0">Open-source Models</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Models that are publicly available and serve as benchmarks or comparison points in the study for performance, speed, and memory usage.&lt;SEP&gt;Models that are publicly available and used as benchmarks or comparison points in the study for performance and speed.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Speed and Memory Efficiency">
  <data key="d0">Speed and Memory Efficiency</data>
  <data key="d1">Results</data>
  <data key="d2">The fine-tuned HPC models run faster and use less memory compared to similar models, enhancing practical usability.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Future HPC Developers">
  <data key="d0">Future HPC Developers</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">HPC developers and researchers who will benefit from the improved models and insights into code generation for HPC applications.&lt;SEP&gt;Target users who will benefit from the improved models and insights for developing HPC applications.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Research Insights">
  <data key="d0">Research Insights</data>
  <data key="d1">Results</data>
  <data key="d2">Key findings about the impact of instruction masking, data quality, model size, and training strategies on the performance of HPC code LLMs.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Speed">
  <data key="d0">Speed</data>
  <data key="d1">Results</data>
  <data key="d2">The fine-tuned models operate faster than comparable models, making them more practical for HPC applications.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Memory Usage">
  <data key="d0">Memory Usage</data>
  <data key="d1">Results</data>
  <data key="d2">The models use less memory than similar models with comparable or even superior parallel code generation capabilities.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Insights">
  <data key="d0">Insights</data>
  <data key="d1">Results</data>
  <data key="d2">Key findings about the minimal impact of instruction masking, the importance of data quality, the effects of model size, and the benefits of fine-tuning strategies on model performance.</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="J. Bitton">
  <data key="d0">J. Bitton</data>
  <data key="d1">Researcher</data>
  <data key="d2">J. Bitton is an author contributing to studies on foundation models and code intelligence.&lt;SEP&gt;J. Bitton is an author involved in studies on foundation models for code and AI systems related to programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="M. Bhatt">
  <data key="d0">M. Bhatt</data>
  <data key="d1">Researcher</data>
  <data key="d2">M. Bhatt is a researcher contributing to research on large language models and code modeling techniques.&lt;SEP&gt;M. Bhatt is an author involved in research on large language models and code modeling.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. C. Ferrer">
  <data key="d0">C. C. Ferrer</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. C. Ferrer is a researcher contributing to foundational AI models for code.&lt;SEP&gt;C. C. Ferrer is a researcher working on foundation models for code and AI applications in programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="A. Grattafiori">
  <data key="d0">A. Grattafiori</data>
  <data key="d1">Researcher</data>
  <data key="d2">A. Grattafiori is involved in research on foundation models for code and AI systems.&lt;SEP&gt;A. Grattafiori is involved in research related to AI models for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="W. Xiong">
  <data key="d0">W. Xiong</data>
  <data key="d1">Researcher</data>
  <data key="d2">W. Xiong is a researcher contributing to large language models and code intelligence research.&lt;SEP&gt;W. Xiong is a researcher working on foundation models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="A. D ´efossez">
  <data key="d0">A. D ´efossez</data>
  <data key="d1">Researcher</data>
  <data key="d2">A. D ´efossez is a researcher contributing to foundational AI models for code.&lt;SEP&gt;A. D ´efossez is a researcher involved in foundation models for code and AI systems.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="J. Copet">
  <data key="d0">J. Copet</data>
  <data key="d1">Researcher</data>
  <data key="d2">J. Copet is a researcher working on large language models, foundation models, and code intelligence.&lt;SEP&gt;J. Copet is involved in research on large language models and code intelligence.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="F. Azhar">
  <data key="d0">F. Azhar</data>
  <data key="d1">Researcher</data>
  <data key="d2">F. Azhar is a researcher contributing to foundation models and large language models for code.&lt;SEP&gt;F. Azhar is a researcher working on AI models related to code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="H. Touvron">
  <data key="d0">H. Touvron</data>
  <data key="d1">Researcher</data>
  <data key="d2">H. Touvron is a researcher associated with open foundation and chat models, notably Llama 2.&lt;SEP&gt;H. Touvron is a researcher associated with open foundation and fine-tuned chat models, notably Llama 2.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="L. Martin">
  <data key="d0">L. Martin</data>
  <data key="d1">Researcher</data>
  <data key="d2">L. Martin is involved in research on foundation models for AI.&lt;SEP&gt;L. Martin is involved in research on foundation models, large language models, and AI systems for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="N. Usunier">
  <data key="d0">N. Usunier</data>
  <data key="d1">Researcher</data>
  <data key="d2">N. Usunier is a researcher contributing to large language model research.&lt;SEP&gt;N. Usunier is a researcher working on large language models, knowledge distillation, and foundation models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="T. Scialom">
  <data key="d0">T. Scialom</data>
  <data key="d1">Researcher</data>
  <data key="d2">T. Scialom is a researcher contributing to large language models, foundation models, and AI for code.&lt;SEP&gt;T. Scialom is a researcher working on language models and code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="G. Synnaeve">
  <data key="d0">G. Synnaeve</data>
  <data key="d1">Researcher</data>
  <data key="d2">G. Synnaeve is a researcher involved in foundation models for code and large language models.&lt;SEP&gt;G. Synnaeve is a researcher involved in foundational AI models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="OpenAI System Card">
  <data key="d0">OpenAI System Card</data>
  <data key="d1">Document</data>
  <data key="d2">A detailed system card describing GPT-4o, including its architecture, safety, and capabilities.&lt;SEP&gt;A system card document describing GPT-4o, detailing its architecture, capabilities, and safety measures.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="GPT-4o System">
  <data key="d0">GPT-4o System</data>
  <data key="d1">AI System</data>
  <data key="d2">GPT-4o is an advanced large language model developed by OpenAI, with documentation outlining its features, safety measures, and system details.&lt;SEP&gt;GPT-4o is an advanced large language model developed by OpenAI, with documentation outlining its features.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="X. Xu">
  <data key="d0">X. Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">X. Xu is a researcher conducting surveys on knowledge distillation of large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="M. Li">
  <data key="d0">M. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">M. Li is a researcher involved in large language model knowledge distillation studies.&lt;SEP&gt;M. Li is a researcher involved in studying knowledge distillation techniques for large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. Tao">
  <data key="d0">C. Tao</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. Tao is a researcher working on knowledge transfer and distillation in large language models.&lt;SEP&gt;C. Tao is a researcher working on large language models and knowledge transfer methods.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="T. Shen">
  <data key="d0">T. Shen</data>
  <data key="d1">Researcher</data>
  <data key="d2">T. Shen is a researcher contributing to knowledge distillation and large language model research.&lt;SEP&gt;T. Shen is involved in research on large language models and distillation techniques.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="R. Cheng">
  <data key="d0">R. Cheng</data>
  <data key="d1">Researcher</data>
  <data key="d2">R. Cheng is a researcher contributing to large language model research.&lt;SEP&gt;R. Cheng is involved in research on large language models and their training methodologies.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="J. Li">
  <data key="d0">J. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">J. Li is a researcher contributing to code models, large language models, and AI for programming.&lt;SEP&gt;J. Li is a researcher working on large language models and code intelligence.&lt;SEP&gt;J. Li is a researcher working on large language models and code understanding.&lt;SEP&gt;J. Li is a researcher working on large language models, knowledge transfer, and code modeling.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. Xu">
  <data key="d0">C. Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. Xu is a researcher contributing to foundation models, large language models, and knowledge distillation.&lt;SEP&gt;C. Xu is a researcher involved in large language model research and knowledge distillation.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="D. Tao">
  <data key="d0">D. Tao</data>
  <data key="d1">Researcher</data>
  <data key="d2">D. Tao is a researcher contributing to large language models and their training methodologies.&lt;SEP&gt;D. Tao is a researcher involved in large language model research and knowledge transfer techniques.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="T. Zhou">
  <data key="d0">T. Zhou</data>
  <data key="d1">Researcher</data>
  <data key="d2">T. Zhou is a researcher working on large language models and knowledge distillation methods.&lt;SEP&gt;T. Zhou is involved in research on large language models and knowledge transfer.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Knowledge Distillation Survey">
  <data key="d0">Knowledge Distillation Survey</data>
  <data key="d1">Study</data>
  <data key="d2">A comprehensive survey reviewing methods for knowledge distillation in large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="D. Guo">
  <data key="d0">D. Guo</data>
  <data key="d1">Researcher</data>
  <data key="d2">D. Guo is a researcher working on code intelligence and large language models.&lt;SEP&gt;D. Guo is a researcher working on code intelligence, large language models, and AI applications in programming.&lt;SEP&gt;D. Guo is a researcher working on code models and AI applications.&lt;SEP&gt;D. Guo is a researcher working on code models, AI for code, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Q. Zhu">
  <data key="d0">Q. Zhu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Q. Zhu is a researcher contributing to code models, AI for code, and large language model applications.&lt;SEP&gt;Q. Zhu is a researcher involved in code intelligence and large language model development.&lt;SEP&gt;Q. Zhu is a researcher involved in code intelligence, large language models, and AI for programming.&lt;SEP&gt;Q. Zhu is involved in research on code models and large language model applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="D. Yang">
  <data key="d0">D. Yang</data>
  <data key="d1">Researcher</data>
  <data key="d2">D. Yang is a researcher contributing to code intelligence and large language model research.&lt;SEP&gt;D. Yang is involved in research on code intelligence, large language models, and programming AI.&lt;SEP&gt;D. Yang is involved in research on code understanding, large language models, and AI for programming.&lt;SEP&gt;D. Yang is involved in research on large language models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Z. Xie">
  <data key="d0">Z. Xie</data>
  <data key="d1">Researcher</data>
  <data key="d2">Z. Xie is a researcher working on code models and AI applications in programming.&lt;SEP&gt;Z. Xie is a researcher working on code models and AI for programming.&lt;SEP&gt;Z. Xie is a researcher working on code models, AI applications, and large language models.&lt;SEP&gt;Z. Xie is a researcher working on code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="K. Dong">
  <data key="d0">K. Dong</data>
  <data key="d1">Researcher</data>
  <data key="d2">K. Dong is a researcher contributing to large language models, code understanding, and AI for programming.&lt;SEP&gt;K. Dong is involved in research on large language models and code understanding.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="W. Zhang">
  <data key="d0">W. Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">W. Zhang is a researcher involved in AI models for code and large language models.&lt;SEP&gt;W. Zhang is a researcher working on code intelligence models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="G. Chen">
  <data key="d0">G. Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">G. Chen is a researcher contributing to AI models for code.&lt;SEP&gt;G. Chen is a researcher working on code intelligence, large language models, and AI programming tools.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="X. Bi">
  <data key="d0">X. Bi</data>
  <data key="d1">Researcher</data>
  <data key="d2">X. Bi is a researcher contributing to code intelligence models.&lt;SEP&gt;X. Bi is a researcher contributing to code models, AI for code, and large language models.&lt;SEP&gt;X. Bi is a researcher contributing to code models, AI for programming, and large language models.&lt;SEP&gt;X. Bi is involved in research on large language models and code applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. Wu">
  <data key="d0">Y. Wu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. Wu is a researcher involved in code AI, large language models, and programming applications.&lt;SEP&gt;Y. Wu is a researcher involved in code intelligence research.&lt;SEP&gt;Y. Wu is a researcher working on code intelligence and language models.&lt;SEP&gt;Y. Wu is a researcher working on code intelligence, large language models, and programming AI.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. K. Li">
  <data key="d0">Y. K. Li</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. K. Li is a researcher involved in large language model research.&lt;SEP&gt;Y. K. Li is a researcher involved in large language models and code understanding.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="F. Luo">
  <data key="d0">F. Luo</data>
  <data key="d1">Researcher</data>
  <data key="d2">F. Luo is a researcher contributing to code models, AI applications, and large language models.&lt;SEP&gt;F. Luo is a researcher contributing to large language models, code understanding, and AI for programming.&lt;SEP&gt;F. Luo is a researcher working on code models and AI applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. Xiong">
  <data key="d0">Y. Xiong</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. Xiong is a researcher contributing to large language models for code.&lt;SEP&gt;Y. Xiong is a researcher working on large language models, code intelligence, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="W. Liang">
  <data key="d0">W. Liang</data>
  <data key="d1">Researcher</data>
  <data key="d2">W. Liang is a researcher contributing to large language models for code.&lt;SEP&gt;W. Liang is a researcher involved in AI models for code and large language models.&lt;SEP&gt;W. Liang is a researcher working on large language models, code understanding, and AI for programming.&lt;SEP&gt;W. Liang is an author contributing to studies on code intelligence and language models.&lt;SEP&gt;W. Liang is involved in research on code intelligence and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Deepseek-coder">
  <data key="d0">Deepseek-coder</data>
  <data key="d1">Tools</data>
  <data key="d2">A 2024 code intelligence system by Guo et al., combining large language models with programming environments to enhance code understanding and generation.&lt;SEP&gt;Deepseek-coder is a large language model designed for code intelligence and programming tasks.&lt;SEP&gt;Deepseek-coder is a large language model designed for code intelligence, focusing on meeting the needs of programming and AI code generation.&lt;SEP&gt;Deepseek-coder is a large language model focused on code intelligence, particularly in the context of programming and AI-driven code generation.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210&lt;SEP&gt;chunk-9605dfcbeda0fc2efdc5d6c829baafeb&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Z. Shao">
  <data key="d0">Z. Shao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Z. Shao is a researcher contributing to code intelligence models.&lt;SEP&gt;Z. Shao is a researcher contributing to code intelligence, large language models, and programming AI.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="R. Xu">
  <data key="d0">R. Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">R. Xu is a researcher contributing to code AI models.&lt;SEP&gt;R. Xu is a researcher contributing to code intelligence and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="H. Gao">
  <data key="d0">H. Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">H. Gao is a researcher contributing to code AI models.&lt;SEP&gt;H. Gao is a researcher contributing to code models and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="S. Ma">
  <data key="d0">S. Ma</data>
  <data key="d1">Researcher</data>
  <data key="d2">S. Ma is a researcher involved in code intelligence and large language model research.&lt;SEP&gt;S. Ma is a researcher working on code intelligence, large language models, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="W. Zeng">
  <data key="d0">W. Zeng</data>
  <data key="d1">Researcher</data>
  <data key="d2">W. Zeng is a researcher involved in large language models and code understanding.&lt;SEP&gt;W. Zeng is a researcher working on code models and AI applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Z. Gu">
  <data key="d0">Z. Gu</data>
  <data key="d1">Researcher</data>
  <data key="d2">Z. Gu is a researcher involved in large language models for code.&lt;SEP&gt;Z. Gu is a researcher working on large language models, code understanding, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="H. Xu">
  <data key="d0">H. Xu</data>
  <data key="d1">Researcher</data>
  <data key="d2">H. Xu is a researcher involved in code models, AI applications, and large language models.&lt;SEP&gt;H. Xu is a researcher working on code models and AI applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="D. Dai">
  <data key="d0">D. Dai</data>
  <data key="d1">Researcher</data>
  <data key="d2">D. Dai is a researcher contributing to code intelligence and large language models.&lt;SEP&gt;D. Dai is a researcher working on code intelligence, large language models, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="L. Zhang">
  <data key="d0">L. Zhang</data>
  <data key="d1">Researcher</data>
  <data key="d2">L. Zhang is a researcher involved in code models, AI applications, and large language models.&lt;SEP&gt;L. Zhang is a researcher working on code models and AI applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. Piao">
  <data key="d0">Y. Piao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. Piao is a researcher contributing to code intelligence models.&lt;SEP&gt;Y. Piao is a researcher working on code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Z. Gou">
  <data key="d0">Z. Gou</data>
  <data key="d1">Researcher</data>
  <data key="d2">Z. Gou is a researcher contributing to large language models, code understanding, and AI for programming.&lt;SEP&gt;Z. Gou is involved in research on large language models and code applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Z. Hao">
  <data key="d0">Z. Hao</data>
  <data key="d1">Researcher</data>
  <data key="d2">Z. Hao is a researcher contributing to large language models for code.&lt;SEP&gt;Z. Hao is a researcher involved in large language models, code understanding, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="B. Wang">
  <data key="d0">B. Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">B. Wang is involved in research on code intelligence and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="J. Song">
  <data key="d0">J. Song</data>
  <data key="d1">Researcher</data>
  <data key="d2">J. Song is a researcher working on code AI models.&lt;SEP&gt;J. Song is a researcher working on code intelligence, large language models, and programming AI.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="D. Chen">
  <data key="d0">D. Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">D. Chen is a researcher contributing to code intelligence models.&lt;SEP&gt;D. Chen is a researcher involved in code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="X. Xie">
  <data key="d0">X. Xie</data>
  <data key="d1">Researcher</data>
  <data key="d2">X. Xie is a researcher involved in code models and AI applications.&lt;SEP&gt;X. Xie is a researcher working on code models, AI applications, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="K. Guan">
  <data key="d0">K. Guan</data>
  <data key="d1">Researcher</data>
  <data key="d2">K. Guan is a researcher contributing to large language models, code understanding, and AI for programming.&lt;SEP&gt;K. Guan is a researcher working on large language models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. You">
  <data key="d0">Y. You</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. You is a researcher contributing to code intelligence research.&lt;SEP&gt;Y. You is a researcher involved in code intelligence, large language models, and programming AI.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="A. Liu">
  <data key="d0">A. Liu</data>
  <data key="d1">Researcher</data>
  <data key="d2">A. Liu is a researcher working on large language models, code understanding, and AI for programming.&lt;SEP&gt;A. Liu is involved in research on large language models and code applications.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Q. Du">
  <data key="d0">Q. Du</data>
  <data key="d1">Researcher</data>
  <data key="d2">Q. Du is a researcher contributing to code models, AI for programming, and large language models.&lt;SEP&gt;Q. Du is a researcher working on code models and AI.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="W. Gao">
  <data key="d0">W. Gao</data>
  <data key="d1">Researcher</data>
  <data key="d2">W. Gao is a researcher contributing to large language models for code.&lt;SEP&gt;W. Gao is a researcher involved in large language models, code understanding, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="X. Lu">
  <data key="d0">X. Lu</data>
  <data key="d1">Researcher</data>
  <data key="d2">X. Lu is a researcher involved in code intelligence and language models.&lt;SEP&gt;X. Lu is a researcher working on code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Q. Chen">
  <data key="d0">Q. Chen</data>
  <data key="d1">Researcher</data>
  <data key="d2">Q. Chen is a researcher contributing to large language models, code understanding, and AI for programming.&lt;SEP&gt;Q. Chen is a researcher working on large language models for code.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Y. Wang">
  <data key="d0">Y. Wang</data>
  <data key="d1">Researcher</data>
  <data key="d2">Y. Wang is a researcher contributing to code AI and large language models.&lt;SEP&gt;Y. Wang is a researcher involved in code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. Deng">
  <data key="d0">C. Deng</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. Deng is a researcher involved in code models and AI applications.&lt;SEP&gt;C. Deng is a researcher working on large language models, code understanding, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. Zhao">
  <data key="d0">C. Zhao</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. Zhao is a researcher contributing to code intelligence models.&lt;SEP&gt;C. Zhao is a researcher working on large language models, code understanding, and AI for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="C. Ruan">
  <data key="d0">C. Ruan</data>
  <data key="d1">Researcher</data>
  <data key="d2">C. Ruan is a researcher involved in code understanding, large language models, and programming AI.&lt;SEP&gt;C. Ruan is a researcher involved in large language models for programming.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Knowledge Distillation of Large Language Models">
  <data key="d0">Knowledge Distillation of Large Language Models</data>
  <data key="d1">Study</data>
  <data key="d2">A survey reviewing methods and techniques for knowledge distillation applied to large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="BigCode">
  <data key="d0">BigCode</data>
  <data key="d1">Research Project</data>
  <data key="d2">BigCode is a research initiative focused on large code models, benchmarks, and AI for code understanding.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="B. WANG">
  <data key="d0">B. WANG</data>
  <data key="d1">Researcher</data>
  <data key="d2">B. WANG is a researcher contributing to code models, AI for programming, and large language models.</data>
  <data key="d3">chunk-354c52be46cd0307688197f61d891210</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Pytorch">
  <data key="d0">Pytorch</data>
  <data key="d1">Tools</data>
  <data key="d2">An imperative style, high-performance deep learning library developed by Chilamkurthy et al. in 2019, used for building and training neural networks.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fixing Weight Decay Regularization in Adam">
  <data key="d0">Fixing Weight Decay Regularization in Adam</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A research paper by Loshchilov and Hutter proposing a method to improve the Adam optimizer's regularization by addressing weight decay issues.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Phind-codellama-34b-v2">
  <data key="d0">Phind-codellama-34b-v2</data>
  <data key="d1">Tools</data>
  <data key="d2">A specific large language model available on Hugging Face, designed for code-related tasks, released in 2023 by Phind.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Gemini">
  <data key="d0">Gemini</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A family of highly capable multimodal models introduced in 2023, integrating multiple data modalities for advanced AI applications.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Language Models Are Few-Shot Learners">
  <data key="d0">Language Models Are Few-Shot Learners</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A foundational paper by Brown et al. (2020) demonstrating that large language models can perform tasks with minimal examples, highlighting few-shot learning capabilities.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Gpt-4 Technical Report">
  <data key="d0">Gpt-4 Technical Report</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A comprehensive technical report by OpenAI (2023) detailing the architecture, training, and capabilities of GPT-4, a state-of-the-art language model.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Mpirigen">
  <data key="d0">Mpirigen</data>
  <data key="d1">Tools</data>
  <data key="d2">A tool for MPI code generation using domain-specific language models, presented by Schneider et al. in 2024, facilitating automated code creation for parallel computing.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data Race Detection Using Large Language Models">
  <data key="d0">Data Race Detection Using Large Language Models</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">A study by Chen et al. (2023) exploring how large language models can be applied to identify data races in parallel programs.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Scope Is All You Need">
  <data key="d0">Scope Is All You Need</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">A 2023 approach by Kadosh et al. to transform large language models for high-performance computing (HPC) code, emphasizing the importance of scope in model adaptation.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Modeling Parallel Programs Using Large Language Models">
  <data key="d0">Modeling Parallel Programs Using Large Language Models</data>
  <data key="d1">Study Designs</data>
  <data key="d2">A 2024 study by Nichols et al. proposing methods to model parallel computing programs with large language models to improve performance and understanding.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance-Aligned LLMs for Generating Fast Code">
  <data key="d0">Performance-Aligned LLMs for Generating Fast Code</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A 2024 approach by Nichols et al. focusing on aligning large language models with performance metrics to generate optimized code.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Chathpc">
  <data key="d0">Chathpc</data>
  <data key="d1">Tools</data>
  <data key="d2">A system described in 2025 by Yin et al. that empowers high-performance computing (HPC) users with large language models for various tasks.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Lassi">
  <data key="d0">Lassi</data>
  <data key="d1">Tools</data>
  <data key="d2">A 2024 pipeline developed by Dearing et al., utilizing large language models for automated, self-correcting translation of parallel scientific codes.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Ompgpt">
  <data key="d0">Ompgpt</data>
  <data key="d1">Tools</data>
  <data key="d2">A generative pre-trained transformer model for OpenMP (Open Multi-Processing) introduced by Chen et al. in 2024, designed to assist in parallel programming.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Biocoder">
  <data key="d0">Biocoder</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">A benchmark dataset introduced for bioinformatics code generation, evaluating models' ability to generate accurate bioinformatics code based on context.&lt;SEP&gt;Biocoder is a benchmark dataset aimed at evaluating code generation in bioinformatics, emphasizing contextual pragmatic knowledge.&lt;SEP&gt;Biocoder is a benchmark dataset designed for evaluating bioinformatics code generation, emphasizing contextual pragmatic knowledge in bioinformatics.</data>
  <data key="d3">chunk-9605dfcbeda0fc2efdc5d6c829baafeb&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Verilogeval">
  <data key="d0">Verilogeval</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Verilogeval is an evaluation framework for large language models generating Verilog code, assessing their performance in hardware description language tasks.&lt;SEP&gt;Verilogeval is an evaluation framework for large language models generating Verilog hardware description language code.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Knowledge transfer from high-resource to low-resource programming languages">
  <data key="d0">Knowledge transfer from high-resource to low-resource programming languages</data>
  <data key="d1">Theories/Models</data>
  <data key="d2">This concept involves transferring knowledge learned from high-resource programming languages to improve code generation in low-resource languages for code language models (LLMs).&lt;SEP&gt;This theory involves transferring knowledge from resource-rich programming languages to resource-scarce languages to improve code generation performance of language models.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Reproducibility">
  <data key="d0">Reproducibility</data>
  <data key="d1">Methodologies</data>
  <data key="d2">Reproducibility is the practice of sharing datasets, scripts, and models publicly to enable validation and replication of research results.&lt;SEP&gt;Reproducibility refers to the practice of making scripts, datasets, and models available publicly to enable replication and validation of research results.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-I NSTRUCT dataset">
  <data key="d0">HPC-I NSTRUCT dataset</data>
  <data key="d1">Study Populations/Dataset</data>
  <data key="d2">The HPC-I NSTRUCT dataset contains problem statements and solutions used for evaluating code generation models, accessible via Hugging Face.&lt;SEP&gt;The HPC-I NSTRUCT dataset is a collection of problem statements and solutions used for evaluating code generation models, available on Hugging Face.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2 models">
  <data key="d0">HPC-Coder-V2 models</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">HPC-Coder-V2 models are large language models designed for code generation tasks, with various sizes and performance metrics evaluated in the study.&lt;SEP&gt;HPC-Coder-V2 models are large language models designed for code generation, with various sizes and performance metrics evaluated in the study.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="StarCoder2-3B">
  <data key="d0">StarCoder2-3B</data>
  <data key="d1">Results</data>
  <data key="d2">A smaller model with 3 billion parameters, demonstrating lower performance compared to larger models.&lt;SEP&gt;A smaller model with 3 billion parameters, with comparatively lower performance in code correctness.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Problem Statement in code optimization">
  <data key="d0">Problem Statement in code optimization</data>
  <data key="d1">Research Questions/Hypotheses</data>
  <data key="d2">The core research question involves how to parallelize a sequential data aggregation component to improve performance in high-performance computing applications.&lt;SEP&gt;The problem investigates how to parallelize a sequential code component for aggregating metrics in high-performance computing to reduce bottlenecks.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallelization using OpenMP">
  <data key="d0">Parallelization using OpenMP</data>
  <data key="d1">Methodologies</data>
  <data key="d2">A methodology that involves modifying existing sequential code by applying OpenMP directives to enable parallel execution on multiple CPU cores.&lt;SEP&gt;A technique to modify sequential code by adding OpenMP directives to enable parallel execution across multiple CPU cores.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Luo, Y">
  <data key="d0">Luo, Y</data>
  <data key="d1">Authors</data>
  <data key="d2">Luo, Y is an author involved in research on large language models and code intelligence, contributing to academic publications.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Xiong, W">
  <data key="d0">Xiong, W</data>
  <data key="d1">Authors</data>
  <data key="d2">Xiong, W is an author collaborating on research related to code generation and artificial intelligence.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC application">
  <data key="d0">HPC application</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The HPC application refers to high-performance computing systems that process large datasets and require optimized code components.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data array data&lt;sub&gt;i,j&lt;/sub&gt;">
  <data key="d0">Data array data&lt;sub&gt;i,j&lt;/sub&gt;</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">The data array contains the dataset points to be processed for statistical metrics in the HPC application.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="compute_metric">
  <data key="d0">compute_metric</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function that performs complex calculations on individual data points, remaining unchanged during parallelization.&lt;SEP&gt;A function used to evaluate or measure data points within the dataset, remaining unchanged in the parallelization process.&lt;SEP&gt;The compute_metric function is a computational tool used to calculate a metric from a data point, remaining unchanged during parallelization.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c&lt;SEP&gt;chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="aggregation process">
  <data key="d0">aggregation process</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">The process of summing metric values over a dataset, which is currently sequential and is the target for parallelization.</data>
  <data key="d3">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="aggregate_metrics">
  <data key="d0">aggregate_metrics</data>
  <data key="d1">Methodology</data>
  <data key="d2">A function designed to process a dataset, compute metrics for each element, and sum these metrics, which is parallelized using OpenMP.&lt;SEP&gt;A function that sums computed metrics across a dataset, which is parallelized using OpenMP directives to improve performance.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="sum">
  <data key="d0">sum</data>
  <data key="d1">Variables</data>
  <data key="d2">A variable that accumulates the total sum of metrics computed across all data points in the dataset.&lt;SEP&gt;An accumulator variable that holds the total sum of all computed metrics across the dataset.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="for(intj = 0; j &lt; cols; j++) { sum += compute_metric(data[i][j]); }">
  <data key="d0">for(intj = 0; j &lt; cols; j++) { sum += compute_metric(data[i][j]); }</data>
  <data key="d1">Methodology</data>
  <data key="d2">A code snippet demonstrating a loop that iterates over dataset columns to compute and accumulate metrics for each data point.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="printf(\&quot;Sum of metrics: %d \\n\&quot;, sum);">
  <data key="d0">printf(\"Sum of metrics: %d \\n\", sum);</data>
  <data key="d1">Tools</data>
  <data key="d2">A function call used to output the final accumulated sum of metrics to the console.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="rows">
  <data key="d0">rows</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of rows in the dataset, used to control iteration over the dataset.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="cols">
  <data key="d0">cols</data>
  <data key="d1">Variables</data>
  <data key="d2">Number of columns in the dataset, used to control inner loop iteration.</data>
  <data key="d3">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Phind-V2-34B">
  <data key="d0">Phind-V2-34B</data>
  <data key="d1">Objects of Study</data>
  <data key="d2">Phind-V2-34B is a language model tested on multiple problem types and parallel execution models, with pass@1 scores provided as performance metrics.</data>
  <data key="d3">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Serial">
  <data key="d0">Serial</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Serial refers to a computational problem or execution model where tasks are processed sequentially without parallelization.</data>
  <data key="d3">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Execution Model">
  <data key="d0">Execution Model</data>
  <data key="d1">Core Concepts</data>
  <data key="d2">Execution models define how computational tasks are carried out, whether sequentially or in parallel, influencing the performance and scalability of models.</data>
  <data key="d3">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Synthetic Dataset">
  <data key="d0">Synthetic Dataset</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The synthetic dataset is used to train and fine-tune HPC-Coder-V2 to improve its parallel code generation performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Data and Fine-tuning Parameters Study">
  <data key="d0">Data and Fine-tuning Parameters Study</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The study investigates how variations in data and fine-tuning parameters affect the model's ability to generate parallel code effectively.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance Impact">
  <data key="d0">Performance Impact</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The study investigates how variations in data and fine-tuning parameters affect the model's ability to generate parallel code effectively.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-tuning HPC-Coder-V2">
  <data key="d0">Fine-tuning HPC-Coder-V2</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The synthetic dataset is used to fine-tune the HPC-Coder-V2 model to improve its parallel code generation capabilities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Study Impact">
  <data key="d0">Study Impact</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The research questions guide the investigation into how different factors influence model performance in parallel code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model Choice">
  <data key="d0">Model Choice</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">The choice of base model affects the model's capacity to learn from synthetic data and generate parallel code effectively.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Learning Ability">
  <data key="d0">Learning Ability</data>
  <data key="d3">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d2">Larger models tend to have greater capacity to learn complex patterns from synthetic data, enhancing parallel code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Comparison with Other Models">
  <data key="d0">Comparison with Other Models</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">StarCoder2 is compared with Magicoder, Phind-V2, Gemini-1.5-flash, GPT-3.5, and GPT-4 to evaluate relative performance in code generation tasks.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Training Data and Fine-Tuning">
  <data key="d0">Training Data and Fine-Tuning</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">Magicoder is a fine-tuned model based on DeepseekCoder-6.7B, trained on synthetic data generated from open-source code, aimed at enhancing code generation capabilities.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Commercial Model">
  <data key="d0">Commercial Model</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">Gemini-1.5-flash is an API-accessible commercial model from Google, used in practical applications of code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact Analysis">
  <data key="d0">Impact Analysis</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">Ablation studies analyze how different training configurations and data amounts impact model performance in code generation.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Parallel Code Generation Results">
  <data key="d0">Parallel Code Generation Results</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">Results show how models perform in parallel code generation tasks, measured by Pass@1, with variations based on training configurations.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Impact of Training Data Quantity">
  <data key="d0">Impact of Training Data Quantity</data>
  <data key="d3">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d2">Increasing the amount of MPI training data improves the performance of smaller models on MPI code generation, with diminishing returns for larger models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder-V2 and other models">
  <data key="d0">HPC-Coder-V2 and other models</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d2">The performance metrics compare how well different models generate correct, efficient parallel code across problem types and execution models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Performance and resource requirements">
  <data key="d0">Performance and resource requirements</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d2">Different sizes of HPC-Coder-V2 models show trade-offs, with larger models generally performing better but requiring more memory and time.&lt;SEP&gt;Larger models tend to perform better but require more memory and computational resources; smaller models like 1.3B can outperform larger ones in speed and efficiency.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Model evaluation">
  <data key="d0">Model evaluation</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d2">Models perform differently depending on the execution model, with serial and OpenMP codes being generated more accurately than MPI codes, indicating varying effectiveness across parallel paradigms.&lt;SEP&gt;Models perform differently depending on the execution model, with serial and OpenMP codes being generated more accurately than MPI codes.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Literature">
  <data key="d0">Literature</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d2">The section discusses prior research on code LLMs for HPC, including development, fine-tuning, and application in high-performance and parallel computing contexts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Related work">
  <data key="d0">Related work</data>
  <data key="d3">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d2">The section discusses prior research on code LLMs for HPC, including development, fine-tuning, and application in high-performance and parallel computing contexts.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="Fine-Tuning Dataset">
  <data key="d0">Fine-Tuning Dataset</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d2">HPC-INSTRUCT is used as the primary dataset for fine-tuning the HPC models, impacting their ability to generate parallel code.&lt;SEP&gt;HPC-INSTRUCT is used to fine-tune the HPC code models, impacting their ability to generate parallel code.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<node id="HPC-Coder Models">
  <data key="d0">HPC-Coder Models</data>
  <data key="d3">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d2">The HPC-Coder models (V2-1.3B, V2-6.7B, V2-16B) outperform other open-source models in generating parallel code, with faster speed and lower memory use.&lt;SEP&gt;The fine-tuned HPC-Coder models outperform other open-source models in generating parallel code and are faster and more memory-efficient.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</node>
<edge source="Codex" target="Python code-writing capabilities">
  <data key="d5">20.0</data>
  <data key="d6">Codex is designed to generate Python code from natural language prompts, demonstrating its core function in code synthesis.&lt;SEP&gt;Codex's primary function is to generate Python code based on natural language prompts, demonstrating its core purpose.</data>
  <data key="d7">model function, code synthesis</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="HumanEval">
  <data key="d5">18.0</data>
  <data key="d6">Codex's performance is evaluated on the HumanEval dataset, measuring its ability to generate functionally correct code from docstrings.&lt;SEP&gt;The model's performance is evaluated on the HumanEval dataset, measuring its ability to generate functionally correct code from docstrings.</data>
  <data key="d7">evaluation, performance measurement</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="GPT-3">
  <data key="d5">34.0</data>
  <data key="d6">Codex is a fine-tuned, specialized version of GPT-3 designed explicitly for code generation tasks, leveraging the base model's capabilities.&lt;SEP&gt;Codex is a specialized, fine-tuned version of GPT-3 designed specifically for code generation tasks.&lt;SEP&gt;Codex outperforms GPT-3 in code generation tasks, solving 28.8% versus 0% on HumanEval, indicating its improved capabilities.</data>
  <data key="d7">model comparison, performance&lt;SEP&gt;model specialization, fine-tuning</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b&lt;SEP&gt;chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Repeated Sampling">
  <data key="d5">18.0</data>
  <data key="d6">Repeated sampling is an effective strategy for Codex to produce correct solutions, significantly increasing success rates.&lt;SEP&gt;Repeated sampling significantly increases the likelihood of generating correct code, achieving a 70.2% success rate with 100 samples per problem.</data>
  <data key="d7">method effectiveness, code generation strategy&lt;SEP&gt;method effectiveness, code synthesis</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Limitations">
  <data key="d5">16.0</data>
  <data key="d6">The model has difficulties with long chains of operations in docstrings and variable binding, indicating areas for future improvement.&lt;SEP&gt;The model has limitations with long chains of operations in docstrings and variable binding, which impacts its performance.</data>
  <data key="d7">model limitations, areas for improvement&lt;SEP&gt;model limitations, performance issues</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="GPT-J">
  <data key="d5">7.0</data>
  <data key="d6">Codex outperforms GPT-J in code generation, solving 28.8% versus 11.4%, demonstrating superior performance.</data>
  <data key="d7">model comparison, performance</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Program Synthesis">
  <data key="d5">9.0</data>
  <data key="d6">Program synthesis is the primary activity enabled by Codex, involving generating functional code from natural language descriptions.</data>
  <data key="d7">core activity, natural language to code</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Fine-Tuning">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuning GPT-3 on code datasets results in Codex, a model specialized in code generation and problem solving.</data>
  <data key="d7">model adaptation, domain specialization</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Model training">
  <data key="d5">8.0</data>
  <data key="d6">The training process of Codex involves optimizing the model on code data using specific hyperparameters and tokenization strategies.</data>
  <data key="d7">training process, optimization</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="pass@k">
  <data key="d5">9.0</data>
  <data key="d6">Codex's performance is measured using pass@k metrics, which evaluate the likelihood of at least one correct sample among the top k outputs.</data>
  <data key="d7">performance measurement, evaluation metrics</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Threat Actors">
  <data key="d5">16.0</data>
  <data key="d6">Threat actors may misuse Codex to generate malicious code, facilitate supply chain attacks, or discover vulnerabilities, leveraging its capabilities and limitations.&lt;SEP&gt;Threat actors may misuse Codex to produce malicious code, conduct vulnerability discovery, or facilitate supply chain attacks, due to its capabilities and limitations.</data>
  <data key="d7">malicious use, cybersecurity threats</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Malicious Code">
  <data key="d5">14.0</data>
  <data key="d6">Codex can generate code components that, when assembled, may facilitate malware development or exploit vulnerabilities.&lt;SEP&gt;Codex can produce code snippets that, when integrated, could enable malware or exploit vulnerabilities in software systems.</data>
  <data key="d7">code generation, cybersecurity risks&lt;SEP&gt;code generation, security risks</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Vulnerability Discovery">
  <data key="d5">12.0</data>
  <data key="d6">Codex's ability to identify vulnerabilities is limited compared to specialized static analysis tools, but future improvements could enhance its effectiveness.&lt;SEP&gt;While current capabilities are limited, future improvements could enhance Codex's ability to identify security vulnerabilities, impacting cybersecurity practices.</data>
  <data key="d7">security testing, model capability&lt;SEP&gt;security testing, model improvement</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Supply Chain Attack">
  <data key="d5">18.0</data>
  <data key="d6">Codex's suggestions of malicious or typosquatted packages pose a risk to software supply chains, especially if exploited intentionally.&lt;SEP&gt;Codex's tendency to suggest malicious or typosquatted packages can be exploited for supply chain attacks, threatening software integrity.</data>
  <data key="d7">software security, attack vectors</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Phishing Pretext">
  <data key="d5">10.0</data>
  <data key="d6">Codex models trained on source code do not currently offer advantages in generating phishing content, limiting their misuse in social engineering.&lt;SEP&gt;Codex models trained on source code do not offer advantages in generating phishing content over traditional language models, limiting their misuse in this area.</data>
  <data key="d7">social engineering, code-based attack&lt;SEP&gt;social engineering, code-based phishing</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="security risks">
  <data key="d5">18.0</data>
  <data key="d6">Codex sometimes produces cryptographic configurations that are insecure, such as short RSA keys or ECB mode AES, which can lead to vulnerabilities.&lt;SEP&gt;The potential for Codex to produce insecure configurations poses security risks to software systems.</data>
  <data key="d7">insecurity, AI limitations&lt;SEP&gt;security vulnerabilities, AI limitations</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="programmers and engineers">
  <data key="d5">8.0</data>
  <data key="d6">Impacts</data>
  <data key="d7">Codex impacts programmers by potentially increasing productivity but also introducing risks if insecure code is generated.</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="economic implications">
  <data key="d5">8.0</data>
  <data key="d6">Codex's ability to generate code faster may reduce software production costs and alter labor market dynamics.</data>
  <data key="d7">cost reduction, labor market impact</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="machine learning packages">
  <data key="d5">8.0</data>
  <data key="d6">Codex suggests machine learning packages like TensorFlow and PyTorch, influencing user choices and market entrenchment.</data>
  <data key="d7">recommendation influence, market entrenchment</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="prompt engineering">
  <data key="d5">7.0</data>
  <data key="d6">Prompt engineering guides how users interact with Codex, affecting the quality and relevance of its code suggestions.</data>
  <data key="d7">user interaction, output quality</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex" target="Machine learning packages">
  <data key="d5">8.0</data>
  <data key="d6">Codex suggests popular machine learning libraries like TensorFlow and PyTorch, affecting their adoption and market share.</data>
  <data key="d7">recommendation influence, market dynamics</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-3" target="Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.">
  <data key="d5">10.0</data>
  <data key="d6">GPT-3 is a large language model capable of few-shot learning, significantly impacting NLP research and applications.</data>
  <data key="d7">application, core concept</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-3" target="Financial Sentiment">
  <data key="d5">14.0</data>
  <data key="d6">GPT-3 is used to analyze and manipulate financial sentiment, demonstrating its application in financial NLP.&lt;SEP&gt;GPT-3 is used to analyze, generate, and manipulate financial sentiment, demonstrating its application in financial NLP tasks.</data>
  <data key="d7">application, sentiment analysis</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="GPT-3" target="OpenAI Codex">
  <data key="d5">16.0</data>
  <data key="d6">OpenAI Codex is a descendant of GPT-3, built upon its architecture and capabilities to focus on code generation."|&gt;"Model-Lineage&lt;SEP&gt;OpenAI Codex is a descendant of GPT-3, built upon its architecture and trained on code data to specialize in code generation tasks."|&gt;"Model-Lineage</data>
  <data key="d7">8</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GPT-3" target="Minds and Machines">
  <data key="d5">8.0</data>
  <data key="d6">Floridi and Chiriatti's paper explores GPT-3's nature, scope, limits, and societal consequences, linking the model to philosophical and technological discussions."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GPT-3" target="Floridi">
  <data key="d5">8.0</data>
  <data key="d6">Floridi and Chiriatti's paper discusses GPT-3's nature, scope, limits, and societal consequences, linking philosophical analysis to AI technology."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GPT-J" target="Model Performance Metrics">
  <data key="d5">16.0</data>
  <data key="d6">GPT-J's capabilities are assessed similarly, providing a basis for comparison with Codex models."|&lt;SEP&gt;GPT-J's capabilities are assessed similarly, providing a benchmark for larger open-source models."|</data>
  <data key="d7">benchmarking, evaluation&lt;SEP&gt;evaluation, benchmarking</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Limitations" target="Model Performance">
  <data key="d5">18.0</data>
  <data key="d6">Limitations highlight areas where models fail or underperform, such as not solving all problems or requiring heuristic sample selection.&lt;SEP&gt;Limitations such as incomplete problem solving and biases restrict the real-world applicability of models."|&gt;"performance constraints, challenges</data>
  <data key="d7">9&lt;SEP&gt;performance constraints, challenges</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Limitations" target="Broader Impacts">
  <data key="d5">7.0</data>
  <data key="d6">Limitations of current models highlight the need for further research to address biases, safety, and reliability issues."|&gt;"challenges, future directions</data>
  <data key="d7">7</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Limitations" target="Results">
  <data key="d5">26.0</data>
  <data key="d6">Limitations constrain the interpretation and scope of results.&lt;SEP&gt;Limitations impact the interpretation and applicability of the results.&lt;SEP&gt;The study identifies limitations such as lower performance for less mature models and the need for prompt keyword enhancements."|</data>
  <data key="d7">limitations, performance&lt;SEP&gt;study constraints, generalizability issues&lt;SEP&gt;weaknesses, constraints</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17&lt;SEP&gt;chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Limitations" target="Seamless Integration">
  <data key="d5">14.0</data>
  <data key="d6">Challenges in integrating external knowledge include managing conflicts and incomplete information."|&lt;SEP&gt;Challenges in integrating external knowledge include managing conflicts, incomplete data, and rejection options."|</data>
  <data key="d7">integration challenges, information conflicts</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Limitations" target="Scalability and Adaptability">
  <data key="d5">16.0</data>
  <data key="d6">Scaling knowledge management and updating models as knowledge bases grow remains a significant challenge."|</data>
  <data key="d7">scalability, knowledge management</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Limitations" target="Limitations of LLMs">
  <data key="d5">12.0</data>
  <data key="d6">Current constraints hinder the effectiveness of LLMs in highly specialized or variable result scenarios."|</data>
  <data key="d7">model limitations, variability</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Limitations" target="Study">
  <data key="d5">8.0</data>
  <data key="d6">The study identifies current limitations of LLMs in generating complex, correct, and efficient parallel code, indicating areas for future improvement.</data>
  <data key="d7">model limitations, research gaps</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Limitations" target="training datasets">
  <data key="d5">5.0</data>
  <data key="d6">Some models' training datasets are proprietary, limiting transparency and reproducibility."|</data>
  <data key="d7">data transparency</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Limitations" target="Limitations">
  <data key="d5">7.0</data>
  <data key="d6">The evaluation discusses limitations such as low efficiency and varying model performance.</data>
  <data key="d7">study limitations, performance constraints</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Limitations" target="limitations">
  <data key="d5">7.0</data>
  <data key="d6">The evaluation discusses limitations such as low efficiency, performance constraints, and variability in model effectiveness across tasks and models."|&gt;"study limitations, performance constraints</data>
  <data key="d7">7</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Limitations" target="Evaluation">
  <data key="d5">6.0</data>
  <data key="d6">The study's limitations include current AI performance constraints, especially on less mature models, and the need for further prompt optimization."|</data>
  <data key="d7">limitations, future work</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Limitations" target="Reproducibility and Replicability">
  <data key="d5">12.0</data>
  <data key="d6">Highlights the difficulties in reproducing research results in rapidly evolving AI environments, impacting scientific validation.</data>
  <data key="d7">research challenge, reproducibility</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Limitations" target="Reproducibility and Repl icability">
  <data key="d5">12.0</data>
  <data key="d6">Highlights the difficulties in reproducing research results in rapidly evolving AI environments, impacting scientific validation.</data>
  <data key="d7">research challenge, reproducibility</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Limitations" target="Dynamic Control-Flow">
  <data key="d5">7.0</data>
  <data key="d6">Dynamic control-flow in kernels like bfs and streamcluster limits optimization due to dependency analysis challenges."|&gt;"limitation of optimization</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Limitations" target="Large Search Space">
  <data key="d5">7.0</data>
  <data key="d6">Large search spaces in applications like cfd, nw, and pathfinder hinder optimization due to scheduling complexity."|&gt;"limitation of optimization</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Limitations" target="Memory Overhead">
  <data key="d5">6.0</data>
  <data key="d6">High memory usage in kernels such as myocyte restricts scalability and performance."|&gt;"limitation of hardware or software</data>
  <data key="d7">6</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Program Synthesis" target="Probabilistic Context-Free Grammar">
  <data key="d5">8.0</data>
  <data key="d6">PCFGs are classical formal models used to generate syntactic structures of programs, facilitating probabilistic program generation.</data>
  <data key="d7">formal grammar, program generation</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Program Synthesis" target="AST">
  <data key="d5">7.0</data>
  <data key="d6">ASTs are used to represent program syntax structures, and models like Maddison &amp; Tarlow (2014) improve synthesis by conditioning on AST-related features.</data>
  <data key="d7">program representation, syntax modeling</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Program Synthesis" target="Core Concepts">
  <data key="d5">8.0</data>
  <data key="d6">Program synthesis involves generating code from specifications, with classical methods using PCFGs and neural approaches utilizing ASTs and language models."|&lt;"program generation, natural language</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Program Synthesis" target="PSB2">
  <data key="d5">8.0</data>
  <data key="d6">Helmuth and Kelly's benchmark suite is used to evaluate program synthesis algorithms, serving as a standard for comparison."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Code Generation" target="Unit Tests">
  <data key="d5">16.0</data>
  <data key="d6">Code generated by models is evaluated for correctness by passing predefined unit tests, a key performance indicator."|&gt;"evaluation method, correctness&lt;SEP&gt;Code generation models are evaluated by generating code and verifying correctness through passing unit tests.</data>
  <data key="d7">8&lt;SEP&gt;evaluation method, correctness</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Generation" target="Training Data">
  <data key="d5">12.0</data>
  <data key="d6">The training data influences the code generated by Codex, but instances of exact copying are rare, and generated code mostly reflects learned patterns.&lt;SEP&gt;Training data influences the code generated by Codex, with most outputs being novel rather than copied, due to the model's predictive nature.</data>
  <data key="d7">data influence, originality</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Generation" target="User Control">
  <data key="d5">14.0</data>
  <data key="d6">Users can edit and accept generated code, making the process similar to auto-completion, which influences safety and usability.&lt;SEP&gt;Users have the ability to edit and accept generated code, making the process akin to auto-completion and reducing risks of unintended outputs.</data>
  <data key="d7">user interaction, safety</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Generation" target="Results">
  <data key="d5">15.0</data>
  <data key="d6">Code generation tests the model's ability to produce correct and meaningful HPC code.&lt;SEP&gt;Generated code rarely matches training snippets exactly, indicating pattern learning rather than copying.</data>
  <data key="d7">accuracy, correctness&lt;SEP&gt;output originality, copying</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368&lt;SEP&gt;chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Code Generation" target="Predictive Weightings">
  <data key="d5">6.0</data>
  <data key="d6">Predictive weightings determine whether generated code is similar to training data or novel, affecting legal and ethical considerations.</data>
  <data key="d7">model parameters, originality</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Generation" target="Language Models">
  <data key="d5">18.0</data>
  <data key="d6">Language models trained on code data can be applied to generate or complete source code, facilitating automation and efficiency in programming.&lt;SEP&gt;Language models trained on code datasets can be used to generate or complete source code, facilitating automation and reducing manual effort.</data>
  <data key="d7">application, automation&lt;SEP&gt;automation, programming</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Code Generation" target="LLMs (Large Language Models)">
  <data key="d5">2.0</data>
  <data key="d6">Large Language Models generate code outputs based on prompts, which are then evaluated for correctness and efficiency.</data>
  <data key="d7">AI code synthesis, model evaluation</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Code Generation" target="AMT">
  <data key="d5">16.0</data>
  <data key="d6">The AMT serves as the basis for generating optimized C++ code, incorporating techniques like function-inlining and loop-unrolling.</data>
  <data key="d7">code optimization, performance enhancement</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generation" target="Hardware Language">
  <data key="d5">14.0</data>
  <data key="d6">The hardware description in JSON guides the code generation process to produce hardware-aware optimized code.</data>
  <data key="d7">hardware mapping, code optimization</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generation" target="Function-inlining/Loop-unrolling">
  <data key="d5">16.0</data>
  <data key="d6">Code generation incorporates function-inlining and loop-unrolling techniques to optimize performance of the generated code.</data>
  <data key="d7">performance optimization, code efficiency</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generation" target="Inlined Function Call">
  <data key="d5">16.0</data>
  <data key="d6">Inlining functions involves replacing calls with their code, which is part of the code generation and optimization process.</data>
  <data key="d7">performance optimization, code transformation</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generation" target="Loop Unrolling">
  <data key="d5">14.0</data>
  <data key="d6">Loop unrolling is a technique applied during code generation to optimize performance by expanding loops.</data>
  <data key="d7">optimization, performance enhancement</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generation" target="Large Language Models">
  <data key="d5">9.0</data>
  <data key="d6">LLMs are employed to automate and improve code creation, especially in specialized domains like HPC.</data>
  <data key="d7">application</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Code Generation" target="Prompt Templates">
  <data key="d5">16.0</data>
  <data key="d6">Prompt templates guide the model to generate specific code outputs, such as code translation or problem creation.&lt;SEP&gt;Prompt templates guide the model to produce specific code outputs, such as translation or problem creation.</data>
  <data key="d7">prompt design, output control</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="Model Performance">
  <data key="d5">17.0</data>
  <data key="d6">Metrics like pass@k and BLEU scores are used to evaluate and compare model effectiveness."|&lt;SEP&gt;Metrics such as pass@k and compile success rates quantify the effectiveness of models in code generation tasks.</data>
  <data key="d7">evaluation, comparison&lt;SEP&gt;performance measurement, evaluation</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5&lt;SEP&gt;chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="Application Domains">
  <data key="d5">7.0</data>
  <data key="d6">Evaluation metrics are used to measure the success of domain-specific LLMs in their respective fields."|</data>
  <data key="d7">performance assessment, benchmarks</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="StarCoderBase">
  <data key="d5">7.0</data>
  <data key="d6">StarCoderBase's performance is assessed using metrics like pass@k, which measure correctness over multiple attempts."|</data>
  <data key="d7">performance evaluation</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="Phind-CodeLlama-V2">
  <data key="d5">7.0</data>
  <data key="d6">Phind-CodeLlama-V2's code generation capabilities are evaluated using pass@k and other performance metrics."|</data>
  <data key="d7">performance assessment</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="GPT-3.5 and GPT-4">
  <data key="d5">7.0</data>
  <data key="d6">GPT-3.5 and GPT-4 are evaluated with pass@k metrics to determine their correctness in code generation tasks."|</data>
  <data key="d7">performance measurement</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="Variables">
  <data key="d5">6.0</data>
  <data key="d6">pass@k, speedup n@k, and efficiency n@k are metrics used to quantify correctness and performance of code generated by language models."|</data>
  <data key="d7">performance quantification</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Evaluation Metrics" target="model performance">
  <data key="d5">9.0</data>
  <data key="d6">Metrics like pass@k, speedup n@k, and efficiency n@k are used to evaluate the correctness and efficiency of models' generated code."|</data>
  <data key="d7">performance assessment</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Safety, Security, Economics" target="Broader impacts">
  <data key="d5">8.0</data>
  <data key="d6">Discussion of potential societal impacts of deploying powerful code generation models like Codex, including safety and economic implications.</data>
  <data key="d7">impact analysis, societal considerations</data>
  <data key="d8">chunk-8fe9cc07243d88be8beafa4c1840509b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Large Language Models" target="Performance Modeling">
  <data key="d5">18.0</data>
  <data key="d6">LLMs are utilized to predict execution times and performance variations of scientific codes, enabling automated performance analysis.&lt;SEP&gt;They are used to predict performance metrics and analyze performance impacts of code changes.</data>
  <data key="d7">performance prediction, analysis&lt;SEP&gt;performance prediction, automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Large Language Models" target="Modeling Parallel Programs">
  <data key="d5">17.0</data>
  <data key="d6">Large language models are utilized to understand, generate, and optimize parallel programs in HPC environments.&lt;SEP&gt;The models are used to understand and automate tasks related to parallel programs in HPC, including code generation and performance modeling.</data>
  <data key="d7">parallel program analysis, automation&lt;SEP&gt;parallel program modeling, automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Large Language Models" target="Domain Specialization">
  <data key="d5">32.0</data>
  <data key="d6">Domain specialization enhances the disruptive potential of large language models by tailoring their capabilities to specific fields.&lt;SEP&gt;Domain specialization techniques are developed to adapt LLMs for specific domains, enhancing their effectiveness by addressing data heterogeneity, knowledge complexity, and domain constraints.&lt;SEP&gt;Domain specialization techniques are developed to make LLMs more effective and disruptive within specific application domains by addressing data heterogeneity, domain knowledge, objectives, and constraints.&lt;SEP&gt;Specializing large language models to specific domains enhances their disruptive potential by tailoring their capabilities to particular fields.</data>
  <data key="d7">application enhancement, domain adaptation&lt;SEP&gt;disruption, specialization</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="Domain Objectives">
  <data key="d5">7.0</data>
  <data key="d6">Domain objectives guide the customization of LLMs to meet specific goals and tasks within a domain."|&gt;"goal-oriented adaptation</data>
  <data key="d7">7</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="Constraints in Domain Applications">
  <data key="d5">6.0</data>
  <data key="d6">Constraints such as social norms and ethical standards influence how LLMs are adapted and deployed in specific domains."|&gt;"ethical compliance, operational constraints</data>
  <data key="d7">6</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="Domain-Specific Tools">
  <data key="d5">18.0</data>
  <data key="d6">LLMs call domain tools to augment their capabilities, combining general intelligence with specialized functionalities.&lt;SEP&gt;LLMs call domain tools to augment their capabilities, combining language understanding with specialized functionalities.</data>
  <data key="d7">tool augmentation, hybrid intelligence</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="LLM-adapters">
  <data key="d5">16.0</data>
  <data key="d6">LLM-adapters framework supports large language models like LLaMA, GPT-J, enabling domain-specific adaptation with multiple adapter components.</data>
  <data key="d7">scalability, extensibility</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="Biomedical Information Access">
  <data key="d5">18.0</data>
  <data key="d6">Augmenting LLMs with domain tools enhances access to biomedical information, improving retrieval and understanding.&lt;SEP&gt;Augmenting LLMs with domain-specific tools enhances their ability to access and retrieve biomedical information.</data>
  <data key="d7">domain tools, information access&lt;SEP&gt;domain tools, information retrieval</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models" target="Code Completion">
  <data key="d5">16.0</data>
  <data key="d6">Large language models are used to generate code snippets, including parallel code, based on prompts, demonstrating their ability to assist in software development.&lt;SEP&gt;Large language models are used to generate code snippets, including parallel code, based on prompts, demonstrating their potential to assist developers.</data>
  <data key="d7">application, AI-assisted coding</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Large Language Models" target="Research Questions/Hypotheses">
  <data key="d5">18.0</data>
  <data key="d6">LLMs are investigated for their ability to generate programming exercises, explanations, and code, aiming to enhance coding education and automation.&lt;SEP&gt;LLMs are studied for their ability to generate code, explanations, and educational content, aiming to automate and enhance programming tasks.</data>
  <data key="d7">AI in Education</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="HumanEval Dataset" target="Model Performance">
  <data key="d5">14.0</data>
  <data key="d6">The HumanEval dataset provides a benchmark for assessing the success rates of models like Codex in generating correct code."|&gt;"benchmarking, evaluation&lt;SEP&gt;The HumanEval dataset provides a benchmark for measuring the performance of models like Codex in generating correct code.</data>
  <data key="d7">7&lt;SEP&gt;benchmarking, evaluation</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Unit Tests" target="Methodologies">
  <data key="d5">15.0</data>
  <data key="d6">Unit tests are used to filter solutions and determine if models produce correct code outputs."|&gt;"testing procedures, correctness verification&lt;SEP&gt;Unit tests serve as the basis for filtering solutions and determining model success in code generation tasks."|&gt;"testing procedures, correctness verification</data>
  <data key="d7">7&lt;SEP&gt;8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Unit Tests" target="Problems">
  <data key="d5">26.0</data>
  <data key="d6">Problems are created by extracting test cases from unit tests and incorrect solutions, enabling automated evaluation of code correctness.&lt;SEP&gt;Problems are validated using unit tests to ensure correctness and filter out ambiguous or non-deterministic tasks.&lt;SEP&gt;Unit tests are used to validate problems and filter out ambiguous or non-deterministic tasks during problem curation.</data>
  <data key="d7">problem creation, test case extraction&lt;SEP&gt;validation, quality assurance&lt;SEP&gt;validation, quality control</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Pass@k Metric" target="Model Performance">
  <data key="d5">20.0</data>
  <data key="d6">The pass@k metric quantifies the likelihood that at least one of the top k generated samples passes all unit tests, serving as a performance measure."|&gt;"evaluation metric, success rate&lt;SEP&gt;The pass@k metric quantifies the likelihood that at least one of the top k generated samples passes all unit tests, used to evaluate model effectiveness.</data>
  <data key="d7">10&lt;SEP&gt;performance measurement, evaluation</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Functional Correctness" target="Results">
  <data key="d5">18.0</data>
  <data key="d6">A primary evaluation metric for code models, measuring whether generated code functions correctly, with improvements observed with more sampling."|&lt;"evaluation metric, code quality&lt;SEP&gt;Functional correctness is used as a key evaluation metric, with models showing improved performance with more sampling and better training.</data>
  <data key="d7">9&lt;SEP&gt;evaluation metric, code quality</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Fine-tuning" target="Model Performance">
  <data key="d5">16.0</data>
  <data key="d6">Fine-tuning Codex on correct functions improves its success rate in code synthesis tasks.&lt;SEP&gt;Fine-tuning Codex on correctly implemented functions improves its success rate in generating correct code."|&gt;"training improvement, performance enhancement</data>
  <data key="d7">8&lt;SEP&gt;training improvement, performance enhancement</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Fine-tuning" target="Domain Specialization of LLMs">
  <data key="d5">20.0</data>
  <data key="d6">Model fine-tuning updates the internal parameters of an LLM with domain-specific data, requiring full access."|&gt;"internal modification, training data</data>
  <data key="d7">10</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Fine-tuning" target="White Box">
  <data key="d5">16.0</data>
  <data key="d6">White box models allow full access, necessary for internal model modifications and fine-tuning."|&gt;"full access, parameters</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Fine-tuning" target="Application Domains">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning is applied within specific domains to enhance LLM performance for particular tasks."|</data>
  <data key="d7">domain-specific training, customization</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Performance" target="Model Size">
  <data key="d5">7.0</data>
  <data key="d6">Larger models like GPT-12B generally show higher success rates in code synthesis tasks compared to smaller models."|&gt;"model capability, size effect</data>
  <data key="d7">7</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance" target="Model Sampling">
  <data key="d5">18.0</data>
  <data key="d6"> Generating multiple samples per problem and selecting the best increases the chance of passing all tests, improving overall success."|&gt;"sampling strategy, success probability&lt;SEP&gt;Generating multiple samples and selecting the best improves the likelihood of passing unit tests, thus boosting success rates."|&gt;"sampling, success rate</data>
  <data key="d7">9</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance" target="Heuristic Selection">
  <data key="d5">8.0</data>
  <data key="d6">Using heuristics like highest mean log-probability helps in selecting the best code sample for deployment, impacting success rates."|&gt;"heuristic method, sample selection</data>
  <data key="d7">8</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance" target="Model Fine-tuning Data">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning on correctly implemented functions enhances the model's ability to produce accurate code."|&gt;"training data, performance boost</data>
  <data key="d7">8</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance" target="Results">
  <data key="d5">16.0</data>
  <data key="d6">Model performance results are based on metrics like pass@1, speedup, and efficiency, indicating effectiveness.&lt;SEP&gt;The outcomes of evaluations, including success rates and error types, for different models."|&gt;"performance outcomes, evaluation results</data>
  <data key="d7">7&lt;SEP&gt;performance metrics, effectiveness</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance" target="Fine-Tuning">
  <data key="d5">29.0</data>
  <data key="d6">Fine-tuning improves perplexity but can cause catastrophic forgetting, reducing downstream task performance.&lt;SEP&gt;Fine-tuning on HPC data enhances models' ability to generate HPC-specific code but may cause catastrophic forgetting.</data>
  <data key="d7">catastrophic forgetting, model optimization&lt;SEP&gt;domain adaptation, performance trade-offs</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Parallelism in HPC Code">
  <data key="d5">16.0</data>
  <data key="d6">Models trained on HPC code aim to generate code with OpenMP and MPI parallelism, with performance varying across models.</data>
  <data key="d7">specialized training, HPC features</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Syntactic Correctness">
  <data key="d5">18.0</data>
  <data key="d6">The percentage of syntactically correct code samples reflects models' ability to generate valid code.</data>
  <data key="d7">code validity, syntax</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Evaluation Tasks">
  <data key="d5">8.0</data>
  <data key="d6">Model performance is assessed based on their ability to generate correct, syntactically valid, and HPC-specific code.</data>
  <data key="d7">performance assessment, code quality</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Evaluation Figures">
  <data key="d5">7.0</data>
  <data key="d6">Figures 4-7 visualize the performance metrics of models across different evaluation criteria.</data>
  <data key="d7">visualization, performance comparison</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Training Iterations">
  <data key="d5">7.0</data>
  <data key="d6">Number of training iterations impacts the degree of overfitting and performance trends during fine-tuning.</data>
  <data key="d7">training process, overfitting</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Model Capacity">
  <data key="d5">8.0</data>
  <data key="d6">Larger models generally perform better in code generation tasks, but are limited by training data and capacity.</data>
  <data key="d7">model size, performance</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Pre-Training">
  <data key="d5">8.0</data>
  <data key="d6">Pre-training influences initial capabilities, with domain-specific fine-tuning further refining performance.</data>
  <data key="d7">initial training, performance</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance" target="Graph Problems">
  <data key="d5">8.0</data>
  <data key="d6">Some larger LLMs like StarCoder-Base and CodeLlama perform better on graph problems compared to smaller models."|</data>
  <data key="d7">model capability, problem type</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Model Performance" target="RAG (Retrieval-Augmented Generation)">
  <data key="d5">16.0</data>
  <data key="d6">The performance of RAG models varies with the number of retrieved documents, affecting metrics like Rouge-L and Bleu-1.</data>
  <data key="d7">model efficacy, retrieval impact&lt;SEP&gt;performance, metrics, retrieval</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Model Performance" target="Fine-tuning">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuning on high-quality datasets like HPC-INSTRUCT improves models' ability to generate accurate parallel code.</data>
  <data key="d7">training, performance improvement</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Training Parameters">
  <data key="d5">7.0</data>
  <data key="d6">Training settings such as learning rate and batch size significantly affect the model's ability to learn and generate parallel code.</data>
  <data key="d7">training impact, model performance</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Instruction Masking">
  <data key="d5">26.0</data>
  <data key="d6">Instruction masking impacts the learning process by filtering out instructions, which can affect the model's ability to follow complex prompts or learn from instruction data.&lt;SEP&gt;Masking instructions during fine-tuning has little to no impact on the models' ability to generate parallel code, suggesting other factors are more influential.&lt;SEP&gt;Masking instructions during fine-tuning has little to no impact on the models' ability to generate parallel code.&lt;SEP&gt;Using instruction masking during fine-tuning improves the model's ability to generate relevant parallel code responses.</data>
  <data key="d7">fine-tuning technique, response quality&lt;SEP&gt;technique impact, performance&lt;SEP&gt;training technique, learning effectiveness</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Synthetic Data">
  <data key="d5">18.0</data>
  <data key="d6">Models trained on higher-quality synthetic data (e.g., Llama-generated) outperform those trained on lower-quality data (e.g., DBRX).&lt;SEP&gt;The quality and quantity of synthetic code data directly influence the effectiveness of model training and subsequent code generation performance.</data>
  <data key="d7">data quality, model performance&lt;SEP&gt;data quality, training effectiveness</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Evaluation of Fine-tuning Impact">
  <data key="d5">8.0</data>
  <data key="d6">Different fine-tuning strategies and data qualities impact how well the models generate parallel code.</data>
  <data key="d7">fine-tuning impact, performance evaluation</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Ablation Studies">
  <data key="d5">14.0</data>
  <data key="d6">Ablation studies analyze how variations in data quality, model size, prompt design, and other parameters impact the ability of code LLMs to generate accurate parallel code.&lt;SEP&gt;Ablation studies analyze how variations in data, model size, and prompt construction affect the ability of code LLMs to generate parallel code.</data>
  <data key="d7">model analysis, parameter impact</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Diversity of Synthetic Data">
  <data key="d5">8.0</data>
  <data key="d6">High diversity in synthetic data improves the robustness and generalization of fine-tuned code LLMs in parallel code generation.</data>
  <data key="d7">data diversity, model robustness</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Prompt Construction">
  <data key="d5">7.0</data>
  <data key="d6">Effective prompt construction influences the quality and diversity of generated code, impacting model training and evaluation outcomes.</data>
  <data key="d7">prompt engineering, data quality</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Model Selection">
  <data key="d5">8.0</data>
  <data key="d6">Choosing a pre-trained model trained on code tasks affects the success of fine-tuning and the quality of generated parallel code.</data>
  <data key="d7">pre-trained models, training effectiveness</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Hyperparameters">
  <data key="d5">16.0</data>
  <data key="d6">Adjusting hyperparameters like batch size and sequence length affects the efficiency and effectiveness of model training.&lt;SEP&gt;Adjusting hyperparameters like batch size, sequence length, and optimizer settings influences training efficiency and model quality.</data>
  <data key="d7">training optimization, performance</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Training Setup">
  <data key="d5">7.0</data>
  <data key="d6">The training setup, including hardware and hyperparameters, impacts the resulting model's performance and efficiency.</data>
  <data key="d7">training environment, model quality</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Amount of Parallel Code Data">
  <data key="d5">9.0</data>
  <data key="d6">The quantity of code samples used in fine-tuning influences the model's ability to generate specific code types; more data generally improves performance until a plateau.</data>
  <data key="d7">data quantity, performance trend</data>
  <data key="d8">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Quality of Data">
  <data key="d5">10.0</data>
  <data key="d6">Higher quality synthetic data, generated by different models, can lead to better fine-tuned model performance, indicating the importance of data quality in training outcomes.</data>
  <data key="d7">data quality, model accuracy</data>
  <data key="d8">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Phind-V2">
  <data key="d5">6.0</data>
  <data key="d6">Phind-V2 was the best model on the BigCode leaderboard at release, fine-tuned on proprietary data, used for high-performance code modeling.</data>
  <data key="d7">performance benchmark, dataset</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Parallel Code Generation Results">
  <data key="d5">10.0</data>
  <data key="d6">Results show how models perform in parallel code generation tasks, measured by Pass@1, with variations based on training configurations.</data>
  <data key="d7">performance measurement, results</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Impact of Training Data Quantity">
  <data key="d5">11.0</data>
  <data key="d6">Increasing the amount of MPI training data improves the performance of smaller models on MPI code generation, with diminishing returns for larger models.</data>
  <data key="d7">training data, performance impact</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Training Data">
  <data key="d5">9.0</data>
  <data key="d6">Training data quality and quantity directly influence the performance of code LLMs, with higher quality and optimal amounts leading to better results.</data>
  <data key="d7">data quality, model training</data>
  <data key="d8">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="ParEval Benchmark">
  <data key="d5">8.0</data>
  <data key="d6">The benchmark provides a standardized way to evaluate and compare the code generation capabilities of different models.</data>
  <data key="d7">evaluation, performance comparison</data>
  <data key="d8">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Performance Metrics">
  <data key="d5">10.0</data>
  <data key="d6">Pass@1 scores quantify the success rate of models in generating correct code on first attempt, enabling performance assessment.</data>
  <data key="d7">evaluation metric, code accuracy</data>
  <data key="d8">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Model Fine-Tuning">
  <data key="d5">16.0</data>
  <data key="d6">Fine-tuning on HPC-INSTRUCT improves the models' ability to generate parallel code effectively.&lt;SEP&gt;Fine-tuning with datasets like HPC-INSTRUCT improves the models' ability to generate parallel code.</data>
  <data key="d7">training impact, code generation</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Base Models">
  <data key="d5">14.0</data>
  <data key="d6">Fine-tuning base models yields better parallel code generation capabilities than fine-tuning instruct variants, indicating the importance of starting from pre-trained models.&lt;SEP&gt;Fine-tuning base models yields better parallel code generation capabilities than fine-tuning instruct variants.</data>
  <data key="d7">training strategy, model capability</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Performance" target="Training Data Quality">
  <data key="d5">18.0</data>
  <data key="d6">Higher quality parallel code data significantly enhances the performance of the models in generating accurate parallel code.&lt;SEP&gt;Higher quality parallel code data significantly improves model performance.</data>
  <data key="d7">data quality, model accuracy</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Evaluation Framework" target="Models">
  <data key="d5">8.0</data>
  <data key="d6">The evaluation framework provides standardized metrics and datasets to systematically compare model performance in code generation."|&gt;"benchmarking, assessment</data>
  <data key="d7">8</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evaluation Framework" target="Results">
  <data key="d5">8.0</data>
  <data key="d6">The procedures and metrics used to assess and compare model performance."|&gt;"evaluation process, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evaluation Framework" target="Specifications">
  <data key="d5">18.0</data>
  <data key="d6">The evaluation framework centers on specifications as objects of study, which are used to measure the capabilities of code synthesis models."|&gt;"core_concept, objects of study&lt;SEP&gt;The framework centers on specifications as objects of study, used to measure the capabilities of code synthesis models."|&gt;"core_concept, objects of study</data>
  <data key="d7">9</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sandbox Environment" target="Models">
  <data key="d5">8.0</data>
  <data key="d6">A sandbox environment is used to safely execute and test generated code, ensuring correctness without security risks."|&gt;"safe testing, execution environment</data>
  <data key="d7">8</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Size" target="Scaling Behavior">
  <data key="d5">9.0</data>
  <data key="d6">Larger models tend to perform better, with performance scaling smoothly as size increases, often following a sigmoid trend."|</data>
  <data key="d7">scaling, performance</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Size" target="Model Performance Scaling">
  <data key="d5">9.0</data>
  <data key="d6">Performance improves with increased model size, demonstrating a predictable scaling pattern."|</data>
  <data key="d7">scaling, performance</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Size" target="Model">
  <data key="d5">8.0</data>
  <data key="d6">Larger models like GPT-J-6B are compared to smaller models like Codex-85M to assess performance differences."|</data>
  <data key="d7">model comparison, capacity</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Size" target="Model Efficiency">
  <data key="d5">6.0</data>
  <data key="d6">Codex-S is more parameter-efficient than Codex, indicating better performance relative to size.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Size" target="Performance">
  <data key="d5">40.0</data>
  <data key="d6">Increasing model size from 1.3B to 6.7B significantly improves performance; further increases yield diminishing returns.&lt;SEP&gt;Larger models tend to have greater capacity and potentially better performance after fine-tuning, but the relationship may also involve diminishing returns or increased computational costs.&lt;SEP&gt;Larger models tend to perform better on structured dense problems but have limitations on complex or unstructured problems."|&lt;SEP&gt;Moving from small to medium-sized HPC code models results in significant performance improvements; further size increases show diminishing returns.&lt;SEP&gt;Moving from small to medium-sized models results in significant improvements, while larger models have diminishing returns.</data>
  <data key="d7">model capability, problem type&lt;SEP&gt;model capacity, performance impact&lt;SEP&gt;model capacity, performance scaling&lt;SEP&gt;model scaling, performance gains</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49&lt;SEP&gt;chunk-8ea08005bc62e40cce7faf8139912fa4&lt;SEP&gt;chunk-cdd832b6a605bd8b46aec4e0615ea5df&lt;SEP&gt;chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Fine-tuning Base Model">
  <data key="d5">6.0</data>
  <data key="d6">The choice of initial base model influences the model's capacity and ability to learn from synthetic data, affecting overall performance.</data>
  <data key="d7">model selection, capacity</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Performance Impact">
  <data key="d5">9.0</data>
  <data key="d6">Larger models generally have a greater capacity to learn from synthetic data, potentially leading to better parallel code generation performance.</data>
  <data key="d7">model capacity, learning ability</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Learning Ability">
  <data key="d5">8.0</data>
  <data key="d6">Larger models tend to have greater capacity to learn complex patterns from synthetic data, enhancing parallel code generation.</data>
  <data key="d7">model capacity, learning ability</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Synthetic Data">
  <data key="d5">6.0</data>
  <data key="d6">Different base models produce synthetic data of varying quality, which in turn affects the performance of fine-tuned models.</data>
  <data key="d7">data source, data quality</data>
  <data key="d8">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Knowledge Distillation">
  <data key="d5">7.0</data>
  <data key="d6">Larger models approach the performance of the teacher model, with performance gains tapering as size increases due to knowledge distillation limits.</data>
  <data key="d7">model capacity, knowledge transfer</data>
  <data key="d8">chunk-4a6314582e8689e73a9ecc2fc735a9ad</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Size" target="Model Performance Comparison">
  <data key="d5">16.0</data>
  <data key="d6">Smaller models like HPC-Coder-1.3B can match or outperform larger models like 34B in throughput and correctness, indicating efficiency.&lt;SEP&gt;Smaller models like HPC-Coder-V2 can achieve high throughput and accuracy similar to larger models, indicating efficiency in model design."|</data>
  <data key="d7">8&lt;SEP&gt;efficiency, performance</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Algorithmic Tasks" target="Objects of Study">
  <data key="d5">7.0</data>
  <data key="d6">The programming problems in HumanEval serve as objects of study for evaluating models' ability to solve language comprehension and algorithmic challenges."|&gt;"benchmark problems, assessment</data>
  <data key="d7">7</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Broader Impacts" target="Applications/Implications">
  <data key="d5">8.0</data>
  <data key="d6">Developing advanced code synthesis models influences software automation, raises ethical considerations, and impacts industry practices."|&gt;"impact, societal implications</data>
  <data key="d7">8</data>
  <data key="d8">chunk-fa78213378585b3030dac86aee61a21d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="test-driven development" target="unit tests">
  <data key="d5">16.0</data>
  <data key="d6">Test-driven development requires converting software requirements into unit tests before implementation, and success is defined by passing these tests.</data>
  <data key="d7">development methodology, testing</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="unit tests" target="Evaluation">
  <data key="d5">8.0</data>
  <data key="d6">Unit tests are used to assess whether generated code solutions meet correctness criteria, serving as an evaluation benchmark.</data>
  <data key="d7">assessment, correctness verification</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k metric" target="Kulal et al. (2019)">
  <data key="d5">18.0</data>
  <data key="d6">Kulal et al. (2019) evaluate functional correctness using the pass@k metric, which estimates the probability that generated code samples pass unit tests.</data>
  <data key="d7">evaluation metric, research study</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="functional correctness" target="exact match">
  <data key="d5">21.0</data>
  <data key="d6">Using exact match as a metric may not fully capture functional correctness, as code can be correct without syntactic similarity.&lt;SEP&gt;Using exact match as an evaluation metric can be misleading, as it does not necessarily reflect functional correctness of code.</data>
  <data key="d7">evaluation, correctness</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Large Language Models trained on code" target="code generation tools">
  <data key="d5">9.0</data>
  <data key="d6">Large language models like Codex are designed to generate code, automate tasks, and influence programming workflows.</data>
  <data key="d7">automation, programming workflows</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="HumanEval dataset" target="programming problems">
  <data key="d5">20.0</data>
  <data key="d6">The HumanEval dataset contains hand-written programming problems used to evaluate models' functional correctness, including problem descriptions and unit tests.&lt;SEP&gt;The HumanEval dataset contains hand-written programming problems used to evaluate the functional correctness of code-generating models.</data>
  <data key="d7">evaluation dataset, problem set</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="sandbox environment" target="secure code execution">
  <data key="d5">14.0</data>
  <data key="d6">A sandbox environment was developed to safely run untrusted programs against unit tests, preventing malicious activity and resource compromise.</data>
  <data key="d7">security, safety</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="sandbox environment" target="security">
  <data key="d5">7.0</data>
  <data key="d6">Sandboxing enhances security by isolating code execution and preventing malicious activities.</data>
  <data key="d7">security, isolation</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="gVisor container runtime" target="security boundary">
  <data key="d5">16.0</data>
  <data key="d6">gVisor provides a security boundary by emulating host resources, protecting the host from malicious containers during code execution.</data>
  <data key="d7">security technology, container runtime</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Kubernetes" target="cloud infrastructure">
  <data key="d5">12.0</data>
  <data key="d6">Kubernetes is used as part of the infrastructure for training and deploying models, addressing limitations of cloud environments.</data>
  <data key="d7">cloud computing, infrastructure</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model evaluation" target="results">
  <data key="d5">9.0</data>
  <data key="d6">The evaluation process assesses the performance and correctness of code generation models like Codex using datasets and metrics.</data>
  <data key="d7">assessment, performance</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="code generation" target="economic and labor market implications">
  <data key="d5">14.0</data>
  <data key="d6">Automating coding tasks can impact employment and economic productivity, with potential security considerations.&lt;SEP&gt;Automating coding tasks impacts employment and productivity, with security considerations influencing economic outcomes.</data>
  <data key="d7">economic impact, automation</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model performance" target="training data">
  <data key="d5">7.0</data>
  <data key="d6">The size and quality of training data, such as code repositories, influence the effectiveness of models like Codex.</data>
  <data key="d7">data influence, training</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model performance" target="model fine-tuning">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning models on code datasets improves their ability to generate correct and functional code.</data>
  <data key="d7">training process, performance enhancement</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model performance" target="results">
  <data key="d5">9.0</data>
  <data key="d6">The performance metrics, such as pass@k, quantify how well the models generate correct code solutions.</data>
  <data key="d7">evaluation, metrics</data>
  <data key="d8">chunk-f37ccebb4e61f6d64cd2096830d27b04</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model performance" target="code generation tasks">
  <data key="d5">8.0</data>
  <data key="d6">HPC-Coder outperforms other models in HPC-specific code generation tasks, passing at a 53% higher rate."|</data>
  <data key="d7">task performance, benchmarking</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="model performance" target="labeling OpenMP pragmas">
  <data key="d5">9.0</data>
  <data key="d6">The model's ability to label OpenMP pragmas with 97% accuracy enhances parallel code analysis."|</data>
  <data key="d7">accuracy, parallel programming</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="model performance" target="Fine-tuning">
  <data key="d5">7.0</data>
  <data key="d6">Fine-tuning on specific datasets enhances models' abilities in targeted tasks such as code generation."|</data>
  <data key="d7">training methodology</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="model performance" target="LLMs">
  <data key="d5">8.0</data>
  <data key="d6">The performance of LLMs in generating correct parallel code is assessed through pass@1 scores and correctness metrics.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="model performance" target="Results">
  <data key="d5">9.0</data>
  <data key="d6">Model performance results, based on pass@1, speedup, and efficiency, indicate the effectiveness of models in code generation and translation tasks."|&gt;"performance metrics, effectiveness</data>
  <data key="d7">9</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="training data" target="insecure code">
  <data key="d5">18.0</data>
  <data key="d6">Training data containing insecure code patterns can lead models to generate insecure code in outputs.&lt;SEP&gt;Training data containing insecure code patterns influences the model to produce insecure outputs.</data>
  <data key="d7">training bias, security vulnerabilities</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="training data" target="cryptographic vulnerabilities">
  <data key="d5">9.0</data>
  <data key="d6">Insecure cryptographic code in training data can lead models to generate similar insecure configurations.</data>
  <data key="d7">training bias, security vulnerabilities</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="training data" target="Biases in model outputs">
  <data key="d5">9.0</data>
  <data key="d6">Biases present in training data may be reflected in Codex's suggestions, impacting the diversity and accuracy of recommendations.</data>
  <data key="d7">data bias, model bias</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="training data" target="correct implementation">
  <data key="d5">7.0</data>
  <data key="d6">Training LLMs with correct code examples improves their ability to generate correct parallel code."|</data>
  <data key="d7">training, accuracy</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="training data" target="performance">
  <data key="d5">8.0</data>
  <data key="d6">The datasets used for training, including synthetic and real code data, directly impact the models' ability to generate correct and efficient code.</data>
  <data key="d7">training, model efficacy</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-Tuning" target="Model">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning involves retraining models on curated, high-quality datasets or with human feedback to enhance output quality and alignment.</data>
  <data key="d7">training process, performance enhancement</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Data Collection" target="Training Dataset">
  <data key="d5">8.0</data>
  <data key="d6">Data collection from GitHub repositories provides the raw data for training and fine-tuning Codex.</data>
  <data key="d7">data sourcing, dataset creation</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Pass@k measures the likelihood that at least one generated sample passes unit tests, reflecting the model's problem-solving ability.</data>
  <data key="d7">performance metric, evaluation method</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Performance Metric">
  <data key="d5">16.0</data>
  <data key="d6">pass@k measures the percentage of test cases passed within top k solutions, evaluating model accuracy.&lt;SEP&gt;pass@k measures the success rate of passing test cases within top k solutions, serving as an evaluation metric.</data>
  <data key="d7">evaluation, performance&lt;SEP&gt;performance evaluation, accuracy</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Temperature">
  <data key="d5">8.0</data>
  <data key="d6">Optimal temperature settings vary for different pass@k metrics, with higher temperatures often needed for broader sample diversity.</data>
  <data key="d7">parameter tuning, model behavior</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Optimal Temperature">
  <data key="d5">8.0</data>
  <data key="d6">Different optimal temperatures are used for evaluating pass@1 and pass@100, reflecting model sampling behavior.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Variable">
  <data key="d5">16.0</data>
  <data key="d6">A metric indicating the probability that at least one of k generated samples passes unit tests, used to assess code generation quality."|&gt;"variable&lt;SEP&gt;A success probability metric for code samples, indicating if at least one passes."|&gt;"variable</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@k" target="Models">
  <data key="d5">16.0</data>
  <data key="d6">pass@k is used to evaluate the likelihood that models produce correct answers within multiple attempts, revealing their upper performance limits."|"&lt;performance evaluation, multiple attempts&lt;SEP&gt;pass@k measures the probability that models generate correct code within k attempts, providing a comprehensive performance evaluation.</data>
  <data key="d7">8&lt;SEP&gt;performance evaluation, multiple attempts</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="pass@k" target="performance metrics">
  <data key="d5">8.0</data>
  <data key="d6">pass@k provides insight into how increasing attempts affects the likelihood of generating correct code, indicating upper performance limits.</data>
  <data key="d7">performance analysis, multiple attempts</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="pass@k" target="Results">
  <data key="d5">8.0</data>
  <data key="d6">Increasing k improves the chances of correct code generation, but performance tends to plateau indicating an upper limit."|"&lt;performance trend, multiple attempts</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="pass@k" target="performance">
  <data key="d5">20.0</data>
  <data key="d6">pass@k measures the probability of generating at least one correct solution within k attempts, serving as a key efficacy metric.&lt;SEP&gt;pass@k quantifies the probability of generating at least one correct solution within k attempts, serving as a key performance indicator.</data>
  <data key="d7">accuracy, probabilistic evaluation&lt;SEP&gt;accuracy, probabilistic measure</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="power law scaling" target="Model size and performance">
  <data key="d5">8.0</data>
  <data key="d6">Performance metrics such as test loss and pass@k scale according to a power law with model size, indicating predictable improvements.</data>
  <data key="d7">scaling law, model capacity</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="nucleus sampling" target="Tools">
  <data key="d5">8.0</data>
  <data key="d6">Nucleus sampling is used during code generation to produce diverse outputs by sampling tokens based on a top p threshold.</data>
  <data key="d7">sampling technique, diversity enhancement</data>
  <data key="d8">chunk-fd545f17fc1dff68e5c09ae892fd670e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model size" target="training distribution">
  <data key="d5">6.0</data>
  <data key="d6">The size of the model affects its capacity, which in turn influences its susceptibility to biases and risks of misalignment.</data>
  <data key="d7">model capacity, risk</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Language Modeling" target="Sampling Strategies">
  <data key="d5">8.0</data>
  <data key="d6">Sampling strategies are techniques used within language modeling to select the best or most probable sample outputs.</data>
  <data key="d7">methodology, sampling, evaluation</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sampling Strategies" target="Token Log Probability">
  <data key="d5">15.0</data>
  <data key="d6">Sampling strategies often utilize token log probabilities to rank or select samples, aiming to improve model output quality."|&lt;SEP&gt;Sampling strategies utilize token log probabilities to rank generated samples, aiming to improve the quality of selected outputs."|</data>
  <data key="d7">evaluation, ranking</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance Metrics" target="Codex-12B">
  <data key="d5">18.0</data>
  <data key="d6">Codex-12B's performance is evaluated using metrics like pass@k and BLEU scores, demonstrating its effectiveness in code generation tasks."|&lt;SEP&gt;The performance of Codex-12B is evaluated using pass@k and BLEU scores to measure its effectiveness in code generation."|</data>
  <data key="d7">performance evaluation</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance Metrics" target="GPT-Neo">
  <data key="d5">16.0</data>
  <data key="d6">GPT-Neo's performance on coding benchmarks is compared to Codex-12B, illustrating strengths and limitations."|&lt;SEP&gt;GPT-Neo's performance on coding benchmarks is compared to Codex-12B, showing relative strengths and weaknesses."|</data>
  <data key="d7">benchmarking&lt;SEP&gt;benchmarking, comparison</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Performance Metrics" target="Training and Validation">
  <data key="d5">8.0</data>
  <data key="d6">Metrics like perplexity and token accuracy are used to evaluate the effectiveness of the fine-tuned models.</data>
  <data key="d7">model evaluation, performance assessment</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Performance Metrics" target="Evaluation">
  <data key="d5">12.0</data>
  <data key="d6">Metrics such as accuracy, factuality percentage, and diversity scores quantify the models' effectiveness in generating correct and diverse responses.</data>
  <data key="d7">evaluation metrics, model comparison</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Codex-12B" target="performance">
  <data key="d5">20.0</data>
  <data key="d6">Codex-12B's pass rate declines as the number of chained components in the docstring increases, demonstrating performance degradation with complexity.&lt;SEP&gt;Codex-12B's pass rate decreases as the number of chained components in the synthetic docstring increases, demonstrating performance decline with complexity.</data>
  <data key="d7">performance degradation, complexity&lt;SEP&gt;performance, complexity</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-Neo" target="Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S">
  <data key="d5">9.0</data>
  <data key="d6">Introduction of GPT-Neo, a large-scale autoregressive language model utilizing mesh-tensorflow, highlighting tools for NLP.</data>
  <data key="d7">application, tools</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-Neo" target="HPC Dataset">
  <data key="d5">16.0</data>
  <data key="d6">GPT-Neo is fine-tuned on HPC data, impacting its code generation capabilities.</data>
  <data key="d7">fine-tuning, domain-specific training</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="APPS Dataset" target="Study Population/Dataset">
  <data key="d5">9.0</data>
  <data key="d6">The APPS dataset provides a set of coding problems and unit tests for evaluating language models' coding abilities."|</data>
  <data key="d7">dataset, evaluation</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="BLEU Score" target="Evidence Types">
  <data key="d5">7.0</data>
  <data key="d6">BLEU scores are used as evidence to gauge the similarity and potential correctness of generated solutions."|</data>
  <data key="d7">evaluation, similarity</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Temperature" target="Sampling Parameter">
  <data key="d5">14.0</data>
  <data key="d6">Adjusting temperature influences the diversity of model outputs, affecting the likelihood of passing test cases.&lt;SEP&gt;Temperature influences the diversity of model outputs, affecting the likelihood of passing test cases.</data>
  <data key="d7">sampling diversity, model behavior&lt;SEP&gt;sampling, diversity</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Oracle" target="Evaluation">
  <data key="d5">7.0</data>
  <data key="d6">The oracle provides an upper bound for performance, representing the best possible outcome based on prior knowledge."|</data>
  <data key="d7">benchmarking, ideal performance</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Synthesis Tasks" target="Evaluation">
  <data key="d5">8.0</data>
  <data key="d6">Models are tested on code synthesis tasks involving full programs, measuring their ability to generate correct solutions."|</data>
  <data key="d7">task performance</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Datasets" target="Model">
  <data key="d5">16.0</data>
  <data key="d6">Models like GPT-Neo and GPT-J are trained on datasets such as The Pile, which include code and text sources."|&lt;SEP&gt;Training datasets like The Pile influence model capabilities by providing diverse sources of text and code."|</data>
  <data key="d7">training data, model capacity&lt;SEP&gt;training, data source</data>
  <data key="d8">chunk-08628624fca4ace77b10c93f87dd3bd5</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Variance Reduction Measure" target="Strict Accuracy">
  <data key="d5">18.0</data>
  <data key="d6">Using variance reduction techniques to emphasize strict accuracy improves the reliability and consistency of model evaluation."|&gt;"measurement accuracy, evaluation reliability&lt;SEP&gt;Using variance reduction to focus evaluation on strict accuracy improves reliability of performance measurement.</data>
  <data key="d7">9&lt;SEP&gt;measurement accuracy, evaluation reliability</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evaluating Large Language Models Trained on Code" target="Results">
  <data key="d5">16.0</data>
  <data key="d6">The study assesses different models' pass@k metrics, comparing their performance on code generation tasks.&lt;SEP&gt;The study measures and compares the performance of various models on code generation benchmarks."|&gt;"model benchmarking, performance assessment</data>
  <data key="d7">8&lt;SEP&gt;model performance, code evaluation</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evaluating Large Language Models Trained on Code" target="Study">
  <data key="d5">16.0</data>
  <data key="d6">A research effort to benchmark and assess the performance of large language models on code-related tasks."|&lt;SEP&gt;A study examining the challenges of benchmarking inductive program synthesis methods, emphasizing evaluation difficulties.</data>
  <data key="d7">8&lt;SEP&gt;benchmarking, program synthesis evaluation</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-J pass@1" target="Models">
  <data key="d5">7.0</data>
  <data key="d6">GPT-J's performance is quantitatively compared to other models like Codex variants, showing its effectiveness."|&gt;"model comparison, performance benchmarking</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT-J pass@1" target="Results">
  <data key="d5">7.0</data>
  <data key="d6">GPT-J's pass@1 rate is compared to other models, indicating its relative effectiveness in code tasks."|&gt;"performance comparison, model effectiveness</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="PASS @k" target="Metrics">
  <data key="d5">6.0</data>
  <data key="d6">Pass@k metrics are used to evaluate the success rate of models in passing code tests at various thresholds."|&gt;"performance metrics, evaluation standards</data>
  <data key="d7">6</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="PASS @k" target="Results">
  <data key="d5">8.0</data>
  <data key="d6">The metrics used to evaluate the success rate of models at passing code tests at different thresholds."|&gt;"evaluation metrics, success rates</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Evaluation Metrics" target="Tools">
  <data key="d5">16.0</data>
  <data key="d6">Metrics such as pass@k and filtered pass@k are used to quantify the correctness of generated code solutions."|&gt;"assessment tools, correctness measurement&lt;SEP&gt;Metrics such as pass@k, filtered pass@k, and time-out solutions are used to quantify code correctness and efficiency."|&gt;"assessment tools, performance metrics</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Supervised Fine-Tuning" target="Methodologies">
  <data key="d5">13.0</data>
  <data key="d6">Fine-tuning models on curated datasets of correct solutions enhances their performance and reliability."|&gt;"training enhancement, model improvement&lt;SEP&gt;Fine-tuning models with curated datasets enhances their ability to generate correct and efficient code."|&gt;"training method, performance improvement</data>
  <data key="d7">6&lt;SEP&gt;7</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems from Competitive Programming" target="Objects of Study">
  <data key="d5">14.0</data>
  <data key="d6">These problems are used as data sources for supervised fine-tuning, providing well-structured examples and test cases."|&gt;"training data, dataset sources&lt;SEP&gt;These problems serve as curated datasets for supervised fine-tuning, providing structured problem statements and test cases."|&gt;"dataset source, training data</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems from Continuous Integration" target="Objects of Study">
  <data key="d5">16.0</data>
  <data key="d6">Real-world code problems from open source projects used to adapt models to practical coding environments."|&gt;"dataset source, real-world data&lt;SEP&gt;Real-world function data collected during automated tests from open source projects are used to adapt models to practical coding environments."|&gt;"real-world data, model adaptation</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Study Populations/Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The datasets from competitive programming sites and open source projects used to train and fine-tune models."|&gt;"training dataset, source data</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Legal Considerations">
  <data key="d5">8.0</data>
  <data key="d6">Training on internet data such as GitHub repositories raises legal issues related to fair use and copyright.</data>
  <data key="d7">legal, copyright</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Legal Implications">
  <data key="d5">8.0</data>
  <data key="d6">Training on internet sources like GitHub raises legal questions about fair use and copyright.</data>
  <data key="d7">legal issues, data source</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Adversarial Inputs">
  <data key="d5">3.0</data>
  <data key="d6">Adversarial inputs can manipulate Codex's outputs, especially if the training data contains biases or vulnerabilities, increasing misuse potential.</data>
  <data key="d7">adversarial attack, training data</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Model Capabilities">
  <data key="d5">2.0</data>
  <data key="d6">The data used for training and fine-tuning directly impacts Codex's capabilities, biases, and potential for misuse.</data>
  <data key="d7">training data, model performance</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Training Data" target="Open-source Code Data">
  <data key="d5">8.0</data>
  <data key="d6">Open-source code repositories provide the source data for training and fine-tuning LLMs for HPC tasks.</data>
  <data key="d7">training data, data sources</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Training Data" target="Model Capacity">
  <data key="d5">8.0</data>
  <data key="d6">The quality and scope of training data, including HPC code, affect the capacity of models to learn domain-specific skills.</data>
  <data key="d7">training data, model capacity</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Training Data" target="LLM">
  <data key="d5">8.0</data>
  <data key="d6">The training data provides the foundational knowledge for LLMs, enabling their ability to perform tasks via prompting without additional training.</data>
  <data key="d7">knowledge base, model training</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Data" target="Semi-synthetic Data">
  <data key="d5">7.0</data>
  <data key="d6">Semi-synthetic data complements synthetic data, providing additional training samples to improve model generalization.</data>
  <data key="d7">training data, data augmentation</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Training Data" target="Model Evaluation">
  <data key="d5">8.0</data>
  <data key="d6">The training datasets are used to evaluate the model's ability to generate accurate and generalizable code.</data>
  <data key="d7">dataset, performance assessment</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Benchmarking" target="Study Design">
  <data key="d5">7.0</data>
  <data key="d6">Designs the comparative evaluation of models using standardized metrics."|&gt;"study design, comparative analysis</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Benchmarking" target="Hecbench">
  <data key="d5">8.0</data>
  <data key="d6">Jin's Hecbench suite is used to evaluate HPC hardware/software performance."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Fine-tuning Approaches" target="Methodologies">
  <data key="d5">8.0</data>
  <data key="d6">Supervised fine-tuning on curated datasets improves model accuracy and generalization."|&gt;"training strategy, model improvement</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Test Cases" target="Tools">
  <data key="d5">8.0</data>
  <data key="d6">Input/output examples used for validation and dataset creation."|&gt;"validation data, testing tools</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Test Cases" target="Problems">
  <data key="d5">8.0</data>
  <data key="d6">Problems are created by extracting test cases from code, incorrect solutions, and problem statements to automate evaluation.</data>
  <data key="d7">test case generation, problem creation</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Automated Testing" target="Methodologies">
  <data key="d5">8.0</data>
  <data key="d6">Automated execution of code solutions against test cases to verify correctness and measure performance."|&gt;"testing methodology, correctness verification</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b15d5bb768f451028654d71370cc6879</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems" target="Open Source Projects">
  <data key="d5">14.0</data>
  <data key="d6">Problems are curated from open source projects, utilizing their codebases and CI workflows for problem generation.&lt;SEP&gt;Problems are sourced from open source repositories, utilizing their code and CI workflows for problem curating.</data>
  <data key="d7">source of problems, open source code</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems" target="Tracing Methodology">
  <data key="d5">12.0</data>
  <data key="d6">Tracing methodology collects data during function execution to generate problems and improve dataset quality.&lt;SEP&gt;Tracing methodology is used to generate data for problems by monitoring function calls during execution.</data>
  <data key="d7">data collection, problem creation&lt;SEP&gt;data collection, problem generation</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems" target="Filtering Problems">
  <data key="d5">14.0</data>
  <data key="d6">Filtering applies criteria to remove problematic or low-quality problems, ensuring dataset integrity.&lt;SEP&gt;Filtering ensures only high-quality, deterministic problems are used for training or evaluation.</data>
  <data key="d7">quality control, dataset curation&lt;SEP&gt;quality control, problem selection</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Problems" target="Study Designs">
  <data key="d5">16.0</data>
  <data key="d6">Problems are structured tasks designed to evaluate core functionalities of algorithms or models, often with variations to test robustness and understanding.&lt;SEP&gt;Problems are structured tasks designed to evaluate core functionalities of models or algorithms, often with variations to test understanding.</data>
  <data key="d7">evaluation, testing</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Continuous Integration" target="Tracing Methodology">
  <data key="d5">18.0</data>
  <data key="d6">CI employs tracing methodologies like sys.setprofile to monitor function calls and collect input-output data during testing.&lt;SEP&gt;CI workflows employ tracing methodologies, such as sys.setprofile, to collect function inputs and outputs during testing.</data>
  <data key="d7">data collection, code analysis&lt;SEP&gt;test data collection, code analysis</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GitHub Repos" target="CI Frameworks">
  <data key="d5">12.0</data>
  <data key="d6">GitHub repositories often use CI frameworks like Travis and Tox to automate build and test processes.&lt;SEP&gt;GitHub repositories often use CI tools such as Travis and Tox to automate build, test, and deployment processes.</data>
  <data key="d7">automation, CI tools&lt;SEP&gt;tool usage, automation</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="prompt" target="generated code">
  <data key="d5">7.0</data>
  <data key="d6">The prompt guides the model to produce the generated code, influencing its structure and content.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Problem Statements" target="Solution Function">
  <data key="d5">8.0</data>
  <data key="d6">The problem statements define the questions that the solution functions address through calculations.</data>
  <data key="d7">guidance</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Model Evaluation" target="Results">
  <data key="d5">10.0</data>
  <data key="d6">Model performance is assessed using metrics like pass@k and validation loss to determine effectiveness.</data>
  <data key="d7">performance metrics, evaluation</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Evaluation" target="Performance Data">
  <data key="d5">8.0</data>
  <data key="d6">Performance data is used to evaluate the accuracy of the models' predictions and analysis.</data>
  <data key="d7">model validation, evaluation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Evaluation" target="ParEval Benchmark">
  <data key="d5">18.0</data>
  <data key="d6">Fine-tuned models are evaluated against the ParEval benchmark to assess their performance in generating real parallel code tasks.&lt;SEP&gt;Fine-tuned models are evaluated against the ParEval benchmark to determine their effectiveness in generating real-world parallel code.</data>
  <data key="d7">performance assessment, benchmark testing</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Evaluation" target="Model">
  <data key="d5">8.0</data>
  <data key="d6">Evaluation assesses how well a model generates code based on benchmarks and validation datasets.</data>
  <data key="d7">performance metrics, validation</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Training Dataset" target="Fine-tuning">
  <data key="d5">6.0</data>
  <data key="d6">The dataset comprising code and docstrings is used to fine-tune the model for specific tasks.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Fine-tuning" target="Validation Loss">
  <data key="d5">7.0</data>
  <data key="d6">Fine-tuning Codex aims to optimize validation loss to improve generalization performance.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Fine-tuning" target="Training on HPC Code">
  <data key="d5">20.0</data>
  <data key="d6">Training on HPC source code datasets is used to fine-tune pre-trained models for HPC-specific code generation tasks.&lt;SEP&gt;Training on HPC source code datasets is used to fine-tune pre-trained models, making them more effective for HPC-specific code tasks.</data>
  <data key="d7">specialization, domain adaptation&lt;SEP&gt;specialization, model adaptation</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Fine-tuning" target="Downstream Tasks">
  <data key="d5">22.0</data>
  <data key="d6">Fine-tuning improves model performance on specific downstream tasks such as code labeling or generation.&lt;SEP&gt;Fine-tuning improves model performance on specific downstream tasks such as code labeling, generation, or prediction.</data>
  <data key="d7">performance enhancement, task adaptation&lt;SEP&gt;performance improvement, task specificity</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Fine-tuning" target="HPC Tasks">
  <data key="d5">18.0</data>
  <data key="d6">Fine-tuning a language model on HPC code improves its ability to generate correct and performant code.&lt;SEP&gt;Fine-tuning enhances the model's ability to generate correct and performant HPC code from limited data.</data>
  <data key="d7">training process, domain adaptation</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Fine-tuning" target="Augmentation">
  <data key="d5">16.0</data>
  <data key="d6">Fine-tuning involves training the model’s internal parameters with domain-specific data to deepen specialization."|&lt;SEP&gt;Fine-tuning is an augmentation technique where the model's internal parameters are updated using domain-specific data."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-tuning" target="LLMs">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning adapts LLMs to specific domains like Earth science, finance, or law, enhancing task performance.</data>
  <data key="d7">model adaptation, domain specialization</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-tuning" target="Robustness and Efficiency">
  <data key="d5">14.0</data>
  <data key="d6">Fine-tuning improves the robustness and efficiency of pre-trained models through regularized optimization techniques.&lt;SEP&gt;Fine-tuning through regularized optimization improves the robustness and efficiency of pre-trained models.</data>
  <data key="d7">optimization, model performance</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-tuning" target="Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, Warren J Gross">
  <data key="d5">5.0</data>
  <data key="d6">Their 2022 preprint focuses on efficient fine-tuning of compressed language models.</data>
  <data key="d7">model tuning, compression</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-tuning" target="Danilo Vucetic">
  <data key="d5">5.0</data>
  <data key="d6">Their 2022 preprint addresses efficient fine-tuning of compressed language models.</data>
  <data key="d7">model tuning, compression</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-tuning" target="HPC-INSTRUCT Dataset">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuning on the HPC-INSTRUCT dataset improves models’ ability to generate high-quality parallel code.</data>
  <data key="d7">methodology</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="Synthetic Data">
  <data key="d5">16.0</data>
  <data key="d6">Synthetic data is used to fine-tune pre-trained code LLMs, enhancing their ability to generate parallel code for HPC tasks.&lt;SEP&gt;Synthetic data is used to fine-tune pre-trained code LLMs, enhancing their capability to generate parallel code for HPC applications.</data>
  <data key="d7">training data, model improvement</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="Pre-trained Model">
  <data key="d5">16.0</data>
  <data key="d6">Pre-trained models are further trained on specific datasets to adapt them for specialized tasks like HPC code generation.&lt;SEP&gt;Pre-trained models are further trained on specific datasets, like HPC-I NSTRUCT, to improve their performance on HPC code tasks.</data>
  <data key="d7">training process, adaptation</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="Synthetic HPC Code Data">
  <data key="d5">16.0</data>
  <data key="d6">Synthetic HPC code data is used to fine-tune models to improve their ability to generate HPC-related code.&lt;SEP&gt;Synthetic HPC code data is used to fine-tune models, enhancing their ability to generate HPC-related code.</data>
  <data key="d7">training data, model improvement</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="AxoNN Framework">
  <data key="d5">7.0</data>
  <data key="d6">AxoNN facilitates the parallel fine-tuning of large models across multiple GPUs, enabling efficient training.</data>
  <data key="d7">training infrastructure, parallelization</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="Code LLMs for HPC">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning improves the models' ability to generate high-quality parallel code by adapting them to domain-specific data and tasks."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fine-tuning" target="HPC-Coder-V2">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuning enhances the HPC-Coder model's ability to generate high-quality parallel code by adapting it to HPC instruction data.</data>
  <data key="d7">improvement, adaptation</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Validation Loss" target="Tokens">
  <data key="d5">6.0</data>
  <data key="d6">Validation loss is computed based on the tokens processed during training to monitor model performance.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="pass@1" target="Models">
  <data key="d5">20.0</data>
  <data key="d6">pass@1 scores evaluate the correctness of code generated or translated by models, serving as a key performance metric for accuracy and effectiveness in the tasks."|&gt;"accuracy, correctness&lt;SEP&gt;pass@1 scores evaluate the correctness of code generated or translated by models, serving as a performance metric.</data>
  <data key="d7">10&lt;SEP&gt;accuracy, correctness</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Codex-S" target="performance">
  <data key="d5">8.0</data>
  <data key="d6">Codex-S demonstrates improved pass@k performance over Codex, indicating better sample quality and diversity.</data>
  <data key="d7">model comparison, performance improvement</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex-S" target="Performance">
  <data key="d5">8.0</data>
  <data key="d6">Codex-S outperforms Codex in pass@k metrics, demonstrating improved efficiency and accuracy.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sample Selection Heuristics" target="performance">
  <data key="d5">9.0</data>
  <data key="d6">Heuristics like mean log probability ranking improve the quality of selected code samples compared to random selection.</data>
  <data key="d7">heuristics, sample quality</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex-D" target="docstring generation">
  <data key="d5">8.0</data>
  <data key="d6">Codex-D is trained to generate descriptive docstrings from code, aiming to describe the function's purpose and behavior.</data>
  <data key="d7">model purpose, safety application</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Performance Comparison" target="HPC-Coder-V2">
  <data key="d5">9.0</data>
  <data key="d6">HPC-Coder-V2 demonstrates comparable or superior performance to larger models like 34B models while maintaining efficiency."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Back-Translation Objective" target="Ranking Strategies">
  <data key="d5">7.0</data>
  <data key="d6">Back-translation is used as an alternative ranking method for selecting the best generated sample, but it underperforms mean log probability ranking.</data>
  <data key="d7">ranking method, model evaluation</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sample Ranking" target="Generated Samples">
  <data key="d5">7.0</data>
  <data key="d6">Sample ranking methods select the best samples based on log probabilities or back-translation scores.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sample Ranking" target="Back-Translation">
  <data key="d5">7.0</data>
  <data key="d6">Back-translation is used as a scoring method to evaluate the quality of generated samples for ranking purposes.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Safety Reasons" target="Generated Docstrings">
  <data key="d5">7.0</data>
  <data key="d6">Accurate docstrings can enhance code safety, documentation, and understanding.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Figure 7" target="Models and Prompts">
  <data key="d5">16.0</data>
  <data key="d6">Figure 7 shows the efficiency@1 for different models across MPI, OpenMP, and Kokkos prompts, illustrating comparative performance and resource utilization across models and models' effectiveness in different contexts."|&gt;"performance comparison, resource utilization&lt;SEP&gt;Figure 7 shows the efficiency@1 for different models across MPI, OpenMP, and Kokkos prompts, illustrating comparative performance.</data>
  <data key="d7">8&lt;SEP&gt;performance comparison, efficiency evaluation</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance" target="synthetic problems dataset">
  <data key="d5">17.0</data>
  <data key="d6">The dataset shows performance drops exponentially as the number of basic building blocks increases, indicating difficulty in handling complex instructions.&lt;SEP&gt;The synthetic dataset shows that as the number of basic building blocks increases, Codex's accuracy decreases exponentially.</data>
  <data key="d7">complexity, performance</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance" target="behavior degradation">
  <data key="d5">18.0</data>
  <data key="d6">Codex's ability to generate correct code diminishes exponentially with increasing docstring length and chaining of operations.&lt;SEP&gt;Codex's ability to generate correct code diminishes with increased docstring length and chaining of operations.</data>
  <data key="d7">performance, complexity</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance" target="Results">
  <data key="d5">15.0</data>
  <data key="d6">Performance metrics indicate that Codex's success rate declines with increased complexity and length of code instructions.&lt;SEP&gt;The effectiveness of various compilation strategies is measured by improvements in model accuracy.</data>
  <data key="d7">evaluation, model performance&lt;SEP&gt;performance, metrics</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020&lt;SEP&gt;chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf&lt;SEP&gt;DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="performance" target="GPT">
  <data key="d5">29.0</data>
  <data key="d6">Fine-tuning GPT on code data enhances its ability to generate functionally correct code from natural language descriptions.&lt;SEP&gt;Fine-tuning GPT on code improves its ability to generate functionally correct code bodies from natural language.</data>
  <data key="d7">model, performance</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance" target="dataset of human-written problems">
  <data key="d5">31.0</data>
  <data key="d6">Model performance improves when training data resembles evaluation problems, especially with multiple sample generations.&lt;SEP&gt;Performance is evaluated on a dataset of problems, with improvements seen when training data matches evaluation distribution.</data>
  <data key="d7">evaluation, data</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance" target="reverse task of producing docstrings from code bodies">
  <data key="d5">33.0</data>
  <data key="d6">Models trained on this reverse task demonstrate similar performance profiles, indicating bidirectional learning capabilities.&lt;SEP&gt;Models trained on this reverse task show similar performance profiles, indicating effective bidirectional learning.</data>
  <data key="d7">model, evaluation</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance" target="parallel code">
  <data key="d5">8.0</data>
  <data key="d6">The runtime performance and scalability of generated parallel code are key measures of LLM effectiveness."|</data>
  <data key="d7">evaluation, performance</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance" target="Llama-3-70B">
  <data key="d5">16.0</data>
  <data key="d6">Llama-3-70B's size and training data impact its performance in code generation tasks, as evaluated through benchmarks.&lt;SEP&gt;Llama-3-70B's size and training data impact its performance in code generation, as evaluated through benchmarks.</data>
  <data key="d7">model size, dataset impact&lt;SEP&gt;model size, performance</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="Mixtral-8x7B">
  <data key="d5">14.0</data>
  <data key="d6">Mixtral-8x7B is compared to larger models like Llama-3-70B to assess performance trade-offs related to size and resources.&lt;SEP&gt;Mixtral-8x7B is used to compare performance against larger models like Llama-3-70B in code generation tasks.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="fine-tuning">
  <data key="d5">16.0</data>
  <data key="d6">Fine-tuning models on datasets like HPC-I NSTRUCT aims to enhance their accuracy and resource efficiency in parallel code generation.&lt;SEP&gt;Fine-tuning models on datasets like HPC-I NSTRUCT aims to improve their accuracy and efficiency in parallel code generation.</data>
  <data key="d7">training, efficacy&lt;SEP&gt;training, model efficacy</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="parallell code generation">
  <data key="d5">18.0</data>
  <data key="d6">The evaluation demonstrates how different models perform in generating correct parallel code across diverse problem types and execution models.&lt;SEP&gt;The evaluation of models on parallel code generation tasks demonstrates their capability to generate correct and efficient code across different problem types and execution models.</data>
  <data key="d7">task performance, correctness</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="ParEval">
  <data key="d5">18.0</data>
  <data key="d6">ParEval provides the benchmark problems, execution models, and correctness testing used to evaluate model capabilities in code generation.&lt;SEP&gt;ParEval provides the metrics and problems used to evaluate the models' code correctness and diversity across tasks.</data>
  <data key="d7">benchmarking, evaluation&lt;SEP&gt;benchmarking, evaluation metrics</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="models">
  <data key="d5">16.0</data>
  <data key="d6">Models like StarCoder2, Magicoder, and Phind-V2 are compared based on their size, training data, and pass@k scores to assess their relative effectiveness in code generation.&lt;SEP&gt;Models like StarCoder2, Magicoder, and Phind-V2 are compared based on their size, training data, and pass@k scores to assess their relative efficacy in code generation.</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance" target="resources">
  <data key="d5">7.0</data>
  <data key="d6">Hardware resources such as GPUs and memory influence the throughput and feasibility of deploying models for code generation tasks.</data>
  <data key="d7">computing resources, deployment</data>
  <data key="d8">chunk-1ee2903e9d138e70e07c1af3cf2fa9a3</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="hazards of code generation" target="Broader impacts">
  <data key="d5">7.0</data>
  <data key="d6">The analysis highlights safety risks and societal impacts from deploying Codex, including potential misuse and safety hazards.</data>
  <data key="d7">safety, societal impact</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="hazards of code generation" target="Applications/Implications">
  <data key="d5">8.0</data>
  <data key="d6">Potential safety hazards include generating incorrect code, misaligned outputs, and misuse, which are critical for deployment considerations.</data>
  <data key="d7">safety, hazards</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="hazard analysis" target="Broader impacts">
  <data key="d5">6.0</data>
  <data key="d6">The hazard analysis aims to identify risks that could cause harm from using Codex, informing responsible deployment.</data>
  <data key="d7">risk assessment, safety</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="overfitting" target="Results">
  <data key="d5">7.0</data>
  <data key="d6">The back-translation ranking heuristic appears to overfit quickly, reducing its effectiveness in generalization.</data>
  <data key="d7">overfitting, heuristic</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="training dataset" target="Objects of Study">
  <data key="d5">8.0</data>
  <data key="d6">The extensive dataset of Python code from GitHub is used to train Codex, impacting its capabilities and limitations.</data>
  <data key="d7">training data, model</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="metrics for code evaluation" target="Methodologies">
  <data key="d5">6.0</data>
  <data key="d6">Qualitative metrics were developed to evaluate Codex's code generation capabilities, particularly in complex scenarios.</data>
  <data key="d7">evaluation, metrics</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="performance metrics" target="generated code">
  <data key="d5">18.0</data>
  <data key="d6">Performance metrics evaluate how well the generated code performs in terms of correctness and efficiency.&lt;SEP&gt;Performance metrics evaluate the efficiency and correctness of the generated code.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance metrics" target="parEval">
  <data key="d5">9.0</data>
  <data key="d6">ParEval introduces metrics like pass@1, efficiency@1, and speedup@1 to quantify code correctness, efficiency, and scalability."|</data>
  <data key="d7">metrics, evaluation</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance metrics" target="parallel code">
  <data key="d5">9.0</data>
  <data key="d6">Metrics like pass@1, efficiency@1, and speedup@1 are used to evaluate the quality of generated parallel code."|</data>
  <data key="d7">evaluation, quality</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance metrics" target="ensemble methods">
  <data key="d5">16.0</data>
  <data key="d6">Ensemble strategies are used to improve accuracy by combining multiple module outputs, as shown in the experimental results.</data>
  <data key="d7">ensemble learning, accuracy boost</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="performance metrics" target="Variables">
  <data key="d5">8.0</data>
  <data key="d6">Metrics such as accuracy and efficiency used to evaluate the success of modular pipelines and compilation strategies."|</data>
  <data key="d7">evaluation criteria</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="system-level operation" target="Core Concepts">
  <data key="d5">7.0</data>
  <data key="d6">Codex's limited ability to understand and generate system-level code impacts its overall performance and safety.</data>
  <data key="d7">system-level, performance</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="societal impacts" target="Applications/Implications">
  <data key="d5">7.0</data>
  <data key="d6">The societal impacts encompass both benefits like education and onboarding, and risks like safety hazards and misuse.</data>
  <data key="d7">societal, impacts</data>
  <data key="d8">chunk-d25502af8737c59d857916af40cc4020</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="impact analysis" target="Models">
  <data key="d5">8.0</data>
  <data key="d6">Impact analysis is conducted on models to evaluate their safety features, risks, and overall societal impact.</data>
  <data key="d7">evaluation, safety assessment</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="over-reliance" target="human oversight">
  <data key="d5">7.0</data>
  <data key="d6">Over-reliance on models increases the need for human oversight to prevent unsafe outputs and ensure correct task execution.</data>
  <data key="d7">risk management, safety protocols</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="misalignment" target="model capabilities">
  <data key="d5">9.0</data>
  <data key="d6">Misalignment occurs when capable models produce outputs that do not match user intentions, highlighting the gap between capability and helpfulness.</data>
  <data key="d7">alignment, safety</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="bias and representation" target="training distribution">
  <data key="d5">8.0</data>
  <data key="d6">Bias and harmful stereotypes in model outputs are influenced by the biases present in the training data.</data>
  <data key="d7">training data, bias</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="human oversight" target="UI designs">
  <data key="d5">7.0</data>
  <data key="d6">Effective UI designs can enhance human oversight by making safety features and warnings more prominent, reducing reliance on automated outputs alone.</data>
  <data key="d7">user interface, safety</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="UI designs" target="Human oversight">
  <data key="d5">7.0</data>
  <data key="d6">Effective UI designs can enhance user vigilance and oversight, reducing reliance on automated outputs and improving safety.</data>
  <data key="d7">UI, oversight</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="empirical investigation" target="model capabilities">
  <data key="d5">8.0</data>
  <data key="d6">Empirical investigations help determine the actual safety performance and limitations of models in real-world scenarios.</data>
  <data key="d7">research, safety testing</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model capabilities" target="interest of potential attackers">
  <data key="d5">14.0</data>
  <data key="d6">Enhanced model capabilities may attract increased malicious interest, aiming to exploit vulnerabilities.&lt;SEP&gt;Greater model capabilities may attract increased interest from attackers seeking to exploit vulnerabilities.</data>
  <data key="d7">capability exploitation&lt;SEP&gt;capability, security risk</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Impact of code generation systems" target="Models">
  <data key="d5">9.0</data>
  <data key="d6">The impact of code generation systems encompasses their effects on safety, efficiency, and societal implications, including risks and benefits.</data>
  <data key="d7">system impact, societal effects</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Impact of code generation systems" target="Alignment issues">
  <data key="d5">8.0</data>
  <data key="d6">Misalignment between model outputs and user intentions can lead to unsafe or unhelpful code, affecting overall system safety.</data>
  <data key="d7">misalignment, safety</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Impact of code generation systems" target="Safety risks">
  <data key="d5">10.0</data>
  <data key="d6">Safety risks are inherent in the use of these models and require mitigation strategies to prevent harm.</data>
  <data key="d7">risk management, safety</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Impact of code generation systems" target="Future research directions">
  <data key="d5">7.0</data>
  <data key="d6">Future research aims to address safety, bias, and misalignment challenges as models advance.</data>
  <data key="d7">research, future directions</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Safety risks" target="Risk mitigation strategies">
  <data key="d5">9.0</data>
  <data key="d6">Mitigation strategies aim to reduce safety risks such as insecure code, bias, and over-reliance.</data>
  <data key="d7">risk reduction, safety protocols</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Risk mitigation strategies" target="Over-reliance on models">
  <data key="d5">8.0</data>
  <data key="d6">Over-reliance increases the importance of risk mitigation strategies like human oversight and documentation to ensure safety.</data>
  <data key="d7">dependence, safety measures</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias and harmful stereotypes" target="Training data biases">
  <data key="d5">8.0</data>
  <data key="d6">Biases in training data can lead models to generate stereotypical or harmful outputs, affecting safety and fairness.</data>
  <data key="d7">data bias, fairness</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model size" target="Model capabilities">
  <data key="d5">7.0</data>
  <data key="d6">Larger models tend to have greater capabilities but also pose increased risks of misalignment and bias.</data>
  <data key="d7">model capacity, risk</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Empirical investigation" target="Model capabilities">
  <data key="d5">8.0</data>
  <data key="d6">Empirical studies help assess actual model performance, safety, and bias in real-world use.</data>
  <data key="d7">evaluation, safety testing</data>
  <data key="d8">chunk-eec372249525c58f53cd0e9c24744ffb</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias and Representation Issues" target="Code Generation Models">
  <data key="d5">18.0</data>
  <data key="d6">Biases in models reflect societal stereotypes present in training data, leading to safety and ethical concerns.&lt;SEP&gt;Biases in models reflect societal stereotypes present in training data, raising safety and ethical concerns.</data>
  <data key="d7">bias reflection, societal stereotypes</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias and Representation Issues" target="Filtration and Moderation">
  <data key="d5">16.0</data>
  <data key="d6">Filtration and moderation techniques aim to mitigate biases and safety risks in generated code.&lt;SEP&gt;Methods like filtering and moderation aim to mitigate biases and safety risks in generated code.</data>
  <data key="d7">risk mitigation, safety interventions</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias and Representation Issues" target="Model Training Data">
  <data key="d5">8.0</data>
  <data key="d6">Training data may contain biased or sensitive information, influencing model outputs and security.</data>
  <data key="d7">training data influence, bias, security</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias and Representation Issues" target="Training Data Sources">
  <data key="d5">8.0</data>
  <data key="d6">Training datasets may contain biased or sensitive information influencing model outputs and security.</data>
  <data key="d7">training data bias, security risk</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Natural Language Processing" target="Prompt-based Distribution Alignment">
  <data key="d5">16.0</data>
  <data key="d6">The distribution alignment method is applied within NLP to improve domain generalization in text classification.&lt;SEP&gt;The method is applied to NLP tasks to improve domain generalization in text classification.</data>
  <data key="d7">application, domain adaptation</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Natural Language Processing" target="Disciplines">
  <data key="d5">16.0</data>
  <data key="d6">Natural Language Processing is the discipline focused on computational understanding and generation of human language."|&lt;SEP&gt;Natural Language Processing is the discipline studying how to computationally process and generate human language."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Natural Language Processing" target="Advances">
  <data key="d5">7.0</data>
  <data key="d6">Hirschberg and Manning's work contributed to progress in NLP techniques and understanding."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Natural Language Processing" target="Hirschberg, Manning">
  <data key="d5">7.0</data>
  <data key="d6">They contributed to advances in NLP, improving understanding, algorithms, and applications in language processing."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Code Generation Models" target="Security Implications">
  <data key="d5">9.0</data>
  <data key="d6">Generated code may contain vulnerabilities or be misused for cybercrime, requiring review and safeguards.</data>
  <data key="d7">security risks, misuse prevention</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Generation Models" target="Safety Risks">
  <data key="d5">9.0</data>
  <data key="d6">Generated code may contain vulnerabilities or unsafe elements, necessitating review and safeguards.</data>
  <data key="d7">security, safety</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Malware Development" target="Code Vulnerabilities">
  <data key="d5">14.0</data>
  <data key="d6">AI-generated insecure code could be exploited for malicious purposes, impacting cybersecurity.&lt;SEP&gt;AI-generated insecure or malicious code could be exploited for cyberattacks, impacting cybersecurity.</data>
  <data key="d7">security vulnerabilities, malicious exploitation&lt;SEP&gt;security vulnerabilities, malicious use</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Training Data" target="Synthetic Data">
  <data key="d5">12.0</data>
  <data key="d6">Synthetic data from LLMs and open-source code are utilized to train and evaluate models.&lt;SEP&gt;Synthetic data generated from LLMs and open-source code forms the basis of training and evaluation datasets for the models.</data>
  <data key="d7">training data, dataset quality</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Environmental Impacts" target="Compute Resources">
  <data key="d5">16.0</data>
  <data key="d6">Training and inference of large models consume significant energy, affecting environmental sustainability.</data>
  <data key="d7">energy consumption, environmental footprint&lt;SEP&gt;energy use, environmental footprint</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Environmental Impacts" target="Energy Consumption">
  <data key="d5">8.0</data>
  <data key="d6">High energy consumption during training and inference contributes to environmental impacts, including carbon emissions.</data>
  <data key="d7">energy consumption, environmental impact</data>
  <data key="d8">chunk-96b489f22f60397ff887486ccf77f457</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Compute Consumption" target="Azure Platform">
  <data key="d5">7.0</data>
  <data key="d6">Azure's use of renewable energy and carbon credits helps mitigate the environmental impact of compute resources used for training models like Codex.</data>
  <data key="d7">environmental sustainability</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Azure Platform" target="Carbon Credits">
  <data key="d5">7.0</data>
  <data key="d6">Azure purchases carbon credits to offset emissions from compute operations, supporting environmental sustainability.</data>
  <data key="d7">sustainability, environmental impact</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Risk Mitigation" target="Models like Codex">
  <data key="d5">16.0</data>
  <data key="d6">Implementation of safety strategies&lt;SEP&gt;Risk mitigation strategies</data>
  <data key="d7">Applying documentation, filtering, review, and restrictions helps mitigate harms like insecure or offensive code outputs.&lt;SEP&gt;Implementing documentation, filtering, review, and restrictions helps reduce harms like offensive or insecure code outputs.</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Learning" target="Methodologies">
  <data key="d5">9.0</data>
  <data key="d6">Neural program learning approaches, such as program induction and synthesis, enable models to learn and generate code or execute algorithms directly from data.</data>
  <data key="d7">learning paradigms, algorithm execution</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Learning" target="Algorithms">
  <data key="d5">9.0</data>
  <data key="d6">Neural program learning encompasses methods like program induction, synthesis, and interpreters, enabling models to generate or execute code directly.</data>
  <data key="d7">learning methods, algorithms</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Responsible and Safe AI" target="Disciplinary Approach">
  <data key="d5">8.0</data>
  <data key="d6">Ongoing engagement with policymakers, ethical standards, and safety practices to ensure responsible AI deployment.</data>
  <data key="d7">ethics, policy</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Interpreter" target="Universal Transformer">
  <data key="d5">6.0</data>
  <data key="d6">The Universal Transformer architecture can be used as a basis for interpreters that process code sequences with recurrence, as discussed by Dehghani et al. (2019).</data>
  <data key="d7">model architecture, sequence processing</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Interpreter" target="Recurrent Neural Program Interpreter">
  <data key="d5">7.0</data>
  <data key="d6">Recurrent neural program interpreters are related to neural program interpretation models, such as those by Reed &amp; de Freitas (2016), Shin et al. (2018), and Pierrot et al. (2021).</data>
  <data key="d7">program interpretation, neural modeling</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Differentiable Neural Computer" target="Utskever">
  <data key="d5">6.0</data>
  <data key="d6">Utskever is associated with neural networks and deep learning, possibly contributing to or utilizing the Differentiable Neural Computer architecture in research or applications.</data>
  <data key="d7">neural networks, deep learning</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Deep Learning Resurgence" target="Theories/Models">
  <data key="d5">8.0</data>
  <data key="d6">Led to advances in neural networks that underpin program induction and code generation capabilities.</data>
  <data key="d7">technological progress, neural networks</data>
  <data key="d8">chunk-a17c97bd5849d0ccac2f6a28eab01a8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Universal Transformer" target="Dehghani et al. (2019)">
  <data key="d5">7.0</data>
  <data key="d6">Dehghani et al. (2019) introduced the Universal Transformer, which applies recurrence to transformer models, relevant for sequence modeling and program understanding.</data>
  <data key="d7">sequence modeling, transformer</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Maddison &amp; Tarlow (2014)" target="Research Studies">
  <data key="d5">7.0</data>
  <data key="d6">Improved program synthesis by learning a state vector to condition child node expansion in syntax trees."|&lt;"syntax modeling, neural conditioning</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Allamanis et al. (2015)" target="Research Studies">
  <data key="d5">7.0</data>
  <data key="d6">Applied syntax-based models to text-to-code retrieval, improving code search and understanding."|&lt;"text-to-code, retrieval</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Yin &amp; Neubig (2017)" target="Research Studies">
  <data key="d5">7.0</data>
  <data key="d6">Utilized neural models for text-conditional code generation, advancing natural language to code translation."|&lt;"neural code generation, NLP</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code2seq (Alon et al., 2018)" target="Tools">
  <data key="d5">8.0</data>
  <data key="d6">Leverages ASTs for translating code snippets into natural language descriptions, aiding in code comprehension."|&lt;"code-to-text, ASTs</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tufano et al. (2020)" target="Research Studies">
  <data key="d5">14.0</data>
  <data key="d6">Applied transformer models to generate unit tests, outperforming some commercial tools in code testing."|&lt;"unit test generation, model performance&lt;SEP&gt;Applied transformers to generate unit tests, outperforming some commercial tools in code testing.</data>
  <data key="d7">7&lt;SEP&gt;unit test generation, model performance</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Aye et al. (2021)" target="Research Studies">
  <data key="d5">14.0</data>
  <data key="d6">Built an auto-complete tool at Facebook, improving code completion by training on user accepted completions.&lt;SEP&gt;Developed an auto-complete system at Facebook, improving code completion by training on user accepted completions."|&lt;"auto-complete, user data</data>
  <data key="d7">7&lt;SEP&gt;auto-complete, user data</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Static or Dynamic Code Analysis" target="Methodologies">
  <data key="d5">6.0</data>
  <data key="d6">Traditional bug detection approaches involving code execution and analysis to locate and fix issues."|&lt;"bug detection, code debugging</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Genetic Programming" target="Methodologies">
  <data key="d5">6.0</data>
  <data key="d6">Evolutionary algorithms used to evolve code solutions and assist in debugging or synthesis tasks."|&lt;"evolutionary algorithms, program synthesis</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Genetic Programming" target="Objects of Study">
  <data key="d5">16.0</data>
  <data key="d6">Genetic programming is used as a benchmark for evaluating AI-based code synthesis tools like Copilot, focusing on program synthesis effectiveness.&lt;SEP&gt;Genetic programming serves as a benchmark for evaluating the quality and efficiency of AI-powered code synthesis tools like Copilot.</data>
  <data key="d7">Program Synthesis</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Early Datasets" target="Objects of Study">
  <data key="d5">7.0</data>
  <data key="d6">Datasets like FlashFill and Hearthstone serve as benchmarks for neural program synthesis, focusing on specific tasks or games.</data>
  <data key="d7">benchmark datasets, domain-specific</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Korel &amp; Rilling, 1997" target="learned association rules">
  <data key="d5">6.0</data>
  <data key="d6">Korel &amp; Rilling, 1997 provide foundational concepts on learned association rules relevant to debugging and AI analysis.</data>
  <data key="d7">reference, foundational</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="learned association rules" target="test suite">
  <data key="d5">13.0</data>
  <data key="d6">Learned association rules are used to evaluate and identify problems within test suites during debugging processes.&lt;SEP&gt;Learned association rules are used to evaluate code correctness and identify problems within test suites.</data>
  <data key="d7">evaluation, debugging</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="genetic programming" target="test suite">
  <data key="d5">15.0</data>
  <data key="d6">Genetic programming approaches depend on test suites to assess the correctness of generated or evolved code.&lt;SEP&gt;Genetic programming approaches rely on test suites to assess the correctness of generated code.</data>
  <data key="d7">program evaluation, debugging</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="recent works (Tufano et al., 2019; Drain et al., 2021)" target="neural machine translation">
  <data key="d5">17.0</data>
  <data key="d6">Recent studies treat bug fixing as neural machine translation, linking AI models to program repair methodologies.&lt;SEP&gt;Recent works consider bug fixing as neural machine translation, linking AI approaches with program repair.</data>
  <data key="d7">AI, bug fixing&lt;SEP&gt;AI, program repair</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="neural machine translation" target="Neural machine translation is used to translate buggy code into correct code, aiming to automate bug fixing.">
  <data key="d5">9.0</data>
  <data key="d6">AI, program repair</data>
  <data key="d7">9</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="neural machine translation" target="Neural machine translation models are used to translate buggy code into corrected code, facilitating automated bug fixing.">
  <data key="d5">10.0</data>
  <data key="d6">AI, bug fixing</data>
  <data key="d7">10</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="weak test suites" target="limitations">
  <data key="d5">23.0</data>
  <data key="d6">Weak test suites often fail to detect bugs or verify functional correctness, limiting the effectiveness of automated evaluation.&lt;SEP&gt;Weak test suites often fail to detect bugs or verify functional correctness, posing challenges for evaluation.</data>
  <data key="d7">testing, evaluation</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="human developers" target="test suites">
  <data key="d5">25.0</data>
  <data key="d6">Human developers create targeted test suites, but these may not be sufficient for evaluating AI-generated code.&lt;SEP&gt;Human developers write targeted test suites, but these may not be sufficient for automated evaluation of AI-generated code.</data>
  <data key="d7">practices, evaluation</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="large language models" target="training on code from GitHub">
  <data key="d5">27.0</data>
  <data key="d6">Models are trained on code repositories like GitHub to learn coding patterns and improve code generation.&lt;SEP&gt;Models are trained on large code repositories like GitHub to learn coding patterns and improve code generation.</data>
  <data key="d7">training data, AI</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="GPT" target="Autoregressive Language Models">
  <data key="d5">16.0</data>
  <data key="d6">GPT is an autoregressive language model generating contextually relevant content based on previous tokens."|&gt;"model architecture, text generation&lt;SEP&gt;GPT is an autoregressive language model that generates contextually relevant content based on previous tokens."|&gt;"model architecture, text generation</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="broader impacts of code generating models" target="model limitations">
  <data key="d5">35.0</data>
  <data key="d6">Discussion highlights that current models have significant room for improvement, affecting their broader application.&lt;SEP&gt;Discussion highlights the significant room for improvement in current models, affecting their broader societal and practical impacts.</data>
  <data key="d7">impacts, limitations</data>
  <data key="d8">chunk-66bca7f3ed0b2cecb27fd3dceca5cb83</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="code2seq" target="2019.Alon, U., Brody, S., Levy, O., and Yahav, E">
  <data key="d5">8.0</data>
  <data key="d6">The publication introduces code2seq, a method for generating sequences from structured code representations, which is applied to code understanding and generation tasks.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Learning autocompletion" target="Aye, G. A., Kim, S., and Li, H">
  <data key="d5">7.0</data>
  <data key="d6">The study explores models that learn to perform autocompletion based on real-world datasets, contributing to software engineering practices.</data>
  <data key="d7">research, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="wav2vec 2.0" target="Baevski, A., Zhou, H., Mohamed, A., and Auli, M">
  <data key="d5">8.0</data>
  <data key="d6">wav2vec 2.0 is a framework for self-supervised learning of speech representations that advances speech processing technologies.</data>
  <data key="d7">application, core concept</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Deepcoder" target="Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D">
  <data key="d5">9.0</data>
  <data key="d6">Deepcoder is a neural system that learns to write programs, representing a methodology for program synthesis.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Beit" target="Bao, H., Dong, L., and Wei, F">
  <data key="d5">8.0</data>
  <data key="d6">Beit is a pre-trained image transformer model that applies BERT-style masked image modeling for vision tasks.</data>
  <data key="d7">application, core concept</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Christiano, P." target="AI Alignment Forum">
  <data key="d5">7.0</data>
  <data key="d6">Christiano's work on clarifying AI alignment is discussed on the forum, indicating a relationship of discourse and clarification.</data>
  <data key="d7">discussion, clarification</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Clarkson, M. R., Finkbeiner" target="Nondeterministic computation">
  <data key="d5">6.0</data>
  <data key="d6">Educational material explaining nondeterministic computation concepts, relevant to theoretical computer science.</data>
  <data key="d7">study design, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in NLP" target="Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H">
  <data key="d5">10.0</data>
  <data key="d6">The survey critically examines biases in NLP models and their societal impacts.</data>
  <data key="d7">content, discipline</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="S. Radford, A., and Sutskever, I." target="Generating long sequences with sparse transformers">
  <data key="d5">9.0</data>
  <data key="d6">Authored a paper on sequence generation using sparse transformers, establishing a foundational approach.</data>
  <data key="d7">authorship, research contribution</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Clarkson, M. R., Finkbeiner, B., Koleini, M., Micinski, K. K., Rabe, M. N., and Sánchez, C." target="Temporal logics for hyperproperties">
  <data key="d5">8.0</data>
  <data key="d6">This work introduces formal methods relevant to security and trust, connected to the study of hyperproperties.</data>
  <data key="d7">formal methods, security</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Clement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N." target="Pymt5">
  <data key="d5">8.0</data>
  <data key="d6">The team developed Pymt5, a transformer-based system for multi-mode translation of language and code.</data>
  <data key="d7">development, NLP, code translation</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Crawford, K." target="Bias in AI">
  <data key="d5">9.0</data>
  <data key="d6">Crawford's work addresses societal biases in AI, highlighting issues in technology and ethics.</data>
  <data key="d7">bias, societal impact</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Crawford, K." target="Atlas of AI">
  <data key="d5">8.0</data>
  <data key="d6">Crawford authored the book exploring AI's power, politics, and environmental costs.</data>
  <data key="d7">authorship, societal analysis</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Dai, A. M. and Le, Q. V." target="Semi-supervised sequence learning">
  <data key="d5">8.0</data>
  <data key="d6">They developed methods for training neural sequence models with limited labeled data, enhancing learning efficiency.</data>
  <data key="d7">methodology, neural networks</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., Parikh, D., and Batra, D." target="Visual dialog">
  <data key="d5">8.0</data>
  <data key="d6">This team created a system for AI to understand and generate dialogue about visual content, integrating vision and language.</data>
  <data key="d7">application, multimodal AI</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Davis, B." target="Application security">
  <data key="d5">8.0</data>
  <data key="d6">Davis's work on automated software diversity aims to improve application security by generating diverse program variants.</data>
  <data key="d7">security, software diversity</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Łukasz Kaiser" target="Universal transformers">
  <data key="d5">9.0</data>
  <data key="d6">They proposed the Universal Transformer architecture, extending the standard transformer for better flexibility.</data>
  <data key="d7">model architecture, NLP</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A., and Kohli, P." target="BERT">
  <data key="d5">9.0</data>
  <data key="d6">This team developed BERT, a foundational deep bidirectional transformer model for language understanding.</data>
  <data key="d7">model, NLP</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="BERT" target="Ming-Wei Chang">
  <data key="d5">8.0</data>
  <data key="d6">Chang contributed to BERT's development, impacting language understanding tasks."|&lt;SEP&gt;Chang contributed to BERT's development, impacting question answering capabilities."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BERT" target="Kenton Lee">
  <data key="d5">8.0</data>
  <data key="d6">Lee's work on BERT enhances bidirectional language modeling for question answering."|&lt;SEP&gt;Lee's work on BERT enhances deep bidirectional language models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BERT" target="Kristina Toutanova">
  <data key="d5">8.0</data>
  <data key="d6">Toutanova's research supports BERT's application in language understanding."|&lt;SEP&gt;Toutanova's research supports BERT's application in question answering and NLP tasks."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BERT" target="Jacob Devlin">
  <data key="d5">4.0</data>
  <data key="d6">Devlin's BERT model is foundational for language understanding and question answering tasks."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I." target="Jukebox">
  <data key="d5">8.0</data>
  <data key="d6">They created Jukebox, a model capable of generating music, advancing AI in creative domains.</data>
  <data key="d7">generative model, music</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Drain, D., Wu, C., Svyatkovskiy, A., and Sundaresan, N." target="Generating bug-fixes">
  <data key="d5">8.0</data>
  <data key="d6">This research applies pretrained transformers to automatically generate bug fixes, improving software maintenance.</data>
  <data key="d7">software engineering, automation</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Eghbal, N." target="Open source software">
  <data key="d5">8.0</data>
  <data key="d6">Eghbal's work discusses the practices and maintenance of open source projects, highlighting community and sustainability.</data>
  <data key="d7">software development, open source</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al." target="CodeBERT">
  <data key="d5">9.0</data>
  <data key="d6">They developed CodeBERT, a model for understanding code and natural language, facilitating programming tasks.</data>
  <data key="d7">model, code understanding</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="CodeBERT" target="Tools">
  <data key="d5">8.0</data>
  <data key="d6">CodeBERT trained on paired docstrings and code enhances code search and understanding tasks.</data>
  <data key="d7">code understanding, search</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Frey, C. B." target="Technology trap">
  <data key="d5">8.0</data>
  <data key="d6">Frey discusses how technological advancements can lead societies into traps, influencing policy and development paths.</data>
  <data key="d7">theory, societal impact</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C." target="The Pile">
  <data key="d5">9.0</data>
  <data key="d6">They compiled The Pile, an extensive dataset of 800GB of diverse text sources for training large language models.</data>
  <data key="d7">dataset, NLP</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Goldblum, M., Tsipras, D., Xie, C., Chen, X., Schwarzschild, A., Song, D., Madry, A., Li, B., and Goldstein, T." target="Data poisoning and defenses">
  <data key="d5">9.0</data>
  <data key="d6">Their research focuses on security threats like poisoning and backdoors in datasets, and strategies to defend against them.</data>
  <data key="d7">security, data integrity</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Goues, C. L., Dewey-Vogt, M., Forrest, S., and Weimer, W." target="Automated program repair">
  <data key="d5">8.0</data>
  <data key="d6">They conducted a systematic study demonstrating effective automatic bug fixing, fixing 55 bugs at low cost.</data>
  <data key="d7">software engineering, automation</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Graves, A." target="Generating sequences with RNNs">
  <data key="d5">8.0</data>
  <data key="d6">Graves pioneered sequence generation techniques using recurrent neural networks, foundational for many NLP applications.</data>
  <data key="d7">sequence modeling, neural networks</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Graves, A., Wayne, G., and Danihelka, I." target="Neural Turing Machines">
  <data key="d5">9.0</data>
  <data key="d6">They introduced Neural Turing Machines, integrating external memory with neural networks for complex tasks.</data>
  <data key="d7">model architecture, AI</data>
  <data key="d8">chunk-d82fbac5a45159b990b5c96afdf81c21</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Information Processing Systems" target="gvisor">
  <data key="d5">6.0</data>
  <data key="d6">The gvisor sandboxed container runtime was discussed in the context of advancements presented at the Neural Information Processing Systems conference in 2019.</data>
  <data key="d7">conference, technology</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="gvisor" target="Lacasse">
  <data key="d5">10.0</data>
  <data key="d6">Lacasse's 2018 work on open-sourcing gvisor relates to container security and sandboxing technologies.&lt;SEP&gt;Lacasse's work in 2018 on open-sourcing gvisor relates to container security and runtime environments.</data>
  <data key="d7">software release, security&lt;SEP&gt;software, security</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Unsupervised translation of programming languages" target="Lachaux et al.">
  <data key="d5">16.0</data>
  <data key="d6">The study by Lachaux et al. (2020) explores the application of unsupervised learning techniques to translate programming languages.&lt;SEP&gt;Their research explores unsupervised machine learning approaches to translate programming languages, indicating a focus on AI and code understanding.</data>
  <data key="d7">research focus, machine learning&lt;SEP&gt;research, AI</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Risk Matrix" target="Leveson">
  <data key="d5">14.0</data>
  <data key="d6">Leveson (2019) discusses improvements to the standard risk matrix to better assess and manage risks.&lt;SEP&gt;Leveson (2019) proposes improvements to the risk matrix to better assess safety and operational risks.</data>
  <data key="d7">risk assessment, safety standards&lt;SEP&gt;risk management, safety standards</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Empirical Software Engineering" target="Li, Ko, and Begel">
  <data key="d5">16.0</data>
  <data key="d6">Their 2020 study identifies traits that distinguish great software engineers, contributing to the discipline of empirical software engineering.&lt;SEP&gt;Their research investigates the characteristics that distinguish great software engineers within the discipline of empirical software engineering.</data>
  <data key="d7">professional skills, software engineering&lt;SEP&gt;software, skills</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Latent predictor networks" target="Ling et al.">
  <data key="d5">18.0</data>
  <data key="d6">Ling et al. (2016) introduce latent predictor networks as a methodology for code generation tasks, leveraging deep learning techniques.&lt;SEP&gt;Ling et al. (2016) introduced latent predictor networks as a methodology for code generation leveraging deep learning techniques.</data>
  <data key="d7">machine learning, code&lt;SEP&gt;machine learning, code generation</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Roberta" target="Liu et al.">
  <data key="d5">20.0</data>
  <data key="d6">Liu et al. (2019) developed Roberta, a pretraining approach to improve natural language understanding models.&lt;SEP&gt;Liu et al. (2019) developed Roberta, an improved pretraining approach for NLP models, enhancing language understanding.</data>
  <data key="d7">NLP, pretraining&lt;SEP&gt;pretraining, NLP</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="VILBERT" target="Lu et al.">
  <data key="d5">18.0</data>
  <data key="d6">Lu et al. (2019) proposed VILBERT, a model for vision-and-language understanding through pretraining.&lt;SEP&gt;Lu et al. (2019) proposed VILBERT, a multimodal model for vision-and-language understanding via pretraining.</data>
  <data key="d7">multimodal learning, pretraining&lt;SEP&gt;multimodal, pretraining</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codexglue" target="Lu et al.">
  <data key="d5">16.0</data>
  <data key="d6">Lu et al. (2021) created Codexglue, a benchmark dataset for code understanding and generation tasks.</data>
  <data key="d7">datasets, code&lt;SEP&gt;datasets, machine learning</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Automatic program synthesis" target="Manna and Waldinger">
  <data key="d5">12.0</data>
  <data key="d6">Their 1971 work explores early concepts and approaches toward automatic program synthesis.&lt;SEP&gt;Their work from 1971 explores early concepts and approaches toward automatic synthesis of programs.</data>
  <data key="d7">program synthesis, automation</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Cryptography" target="Menezes, van Oorschot, and Vanstone">
  <data key="d5">14.0</data>
  <data key="d6">Their book covers principles and applications of cryptography for secure communication.&lt;SEP&gt;Their book provides comprehensive coverage of applied cryptography principles and methods.</data>
  <data key="d7">security, encryption</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Distributed word representations" target="Mikolov et al.">
  <data key="d5">7.0</data>
  <data key="d6">Mikolov et al. (2013) developed distributed word and phrase representations capturing semantic and syntactic properties.</data>
  <data key="d7">word embeddings, NLP</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Open source software supply chain attacks" target="Ohm et al.">
  <data key="d5">16.0</data>
  <data key="d6">They review and analyze supply chain attacks affecting open source software.&lt;SEP&gt;They review security vulnerabilities and attack techniques affecting open source software supply chains.</data>
  <data key="d7">cybersecurity, software supply chain&lt;SEP&gt;security, cybersecurity</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Intellectual property protection for AI" target="O’Keefe et al.">
  <data key="d5">14.0</data>
  <data key="d6">O’Keefe et al. (2019) discuss legal frameworks and policies for protecting AI innovations.&lt;SEP&gt;O’Keefe et al. (2019) discuss policies and legal considerations for protecting AI innovations.</data>
  <data key="d7">IP law, AI&lt;SEP&gt;IP law, innovation</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Pretraining of vision-and-language models" target="Lu et al.">
  <data key="d5">9.0</data>
  <data key="d6">Lu et al. (2019) describe training techniques for models like VILBERT to jointly understand visual and linguistic data.</data>
  <data key="d7">multimodal models, pretraining</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Wallach et al.">
  <data key="d5">6.0</data>
  <data key="d6">The volume edited by Wallach et al. (2019) includes research on neural systems, methodologies, and applications in AI.</data>
  <data key="d7">conference, publication</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="O*NET" target="Software developers">
  <data key="d5">6.0</data>
  <data key="d6">O*NET classifies and describes the roles, skills, and job requirements of software developers, as documented in 2021.</data>
  <data key="d7">occupational data, job roles</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="O*NET" target="Study Populations/Dataset">
  <data key="d5">6.0</data>
  <data key="d6">O*NET 15-1252.00 provides detailed occupational data and role descriptions for software developers.</data>
  <data key="d7">occupational data, roles</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="O*NET" target="Software Developers">
  <data key="d5">8.0</data>
  <data key="d6">O*NET provides occupational data that helps define and categorize software developers, including skills and tasks involved.</data>
  <data key="d7">occupational classification, career information</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="O*NET" target="Objects of Study">
  <data key="d5">8.0</data>
  <data key="d6">O*NET categorizes and provides detailed data on software developers, including skills, tasks, and employment statistics."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="O*NET" target="Tools">
  <data key="d5">7.0</data>
  <data key="d6">O*NET serves as a tool providing occupational data relevant for understanding the role of software developers."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Wavenet" target="Raw Audio">
  <data key="d5">7.0</data>
  <data key="d6">Wavenet models generate or analyze raw audio waveforms, enabling applications in speech synthesis and audio generation.</data>
  <data key="d7">audio modeling, generative modeling</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Wavenet" target="Theories/Models">
  <data key="d5">8.0</data>
  <data key="d6">Wavenet is a neural network model used for generating and analyzing raw audio, with applications in speech synthesis."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Contrastive Predictive Coding" target="Representation Learning">
  <data key="d5">8.0</data>
  <data key="d6">This methodology is used within neural network training to learn useful data representations by predicting future data points.</data>
  <data key="d7">representation learning, predictive modeling</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Pantridge, E., Helmuth, T., McPhee, N. F., and Spector, L." target="Research">
  <data key="d5">9.0</data>
  <data key="d6">Authors analyzing the environmental impact of training large neural networks, focusing on carbon emissions."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Synthesis" target="Methodologies">
  <data key="d5">16.0</data>
  <data key="d6">Techniques combining neural networks and search algorithms to automatically generate and understand programs."|&lt;SEP&gt;Techniques that combine neural networks with search algorithms to generate and interpret programs."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Synthesis" target="Poisoning Vulnerabilities">
  <data key="d5">15.0</data>
  <data key="d6">Identifying security vulnerabilities in neural code completion relates to neural program synthesis by highlighting potential risks and security challenges in automated code generation systems."|&lt;SEP&gt;The study of poisoning vulnerabilities in neural code completion systems relates to neural program synthesis by highlighting security challenges in automated code generation."|</data>
  <data key="d7">security, neural code</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Neural Program Synthesis" target="Sequence to Sequence Learning">
  <data key="d5">17.0</data>
  <data key="d6">Sequence to sequence models are foundational for neural program synthesis tasks, translating instructions into code or other sequences."|&lt;SEP&gt;Sequence to sequence models underpin neural program synthesis, enabling translation of natural language or instructions into code or other sequences."|</data>
  <data key="d7">model architecture, code generation&lt;SEP&gt;model architecture, code translation</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Planning" target="Methodologies">
  <data key="d5">7.0</data>
  <data key="d6">A structured approach used in neural program learning to guide program generation through search and planning."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="The economic impacts of inadequate infrastructure for software testing" target="Study">
  <data key="d5">16.0</data>
  <data key="d6">A study examining how poor infrastructure affects software testing efficiency and economic outcomes."|&lt;SEP&gt;Analyzes how poor infrastructure hampers software testing efficiency and economic productivity."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Python Software Foundation and JetBrains" target="Organizations">
  <data key="d5">7.0</data>
  <data key="d6">Organizations conducting surveys to gather data on Python developers' practices and trends."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Python Developers Survey 2020" target="Evidence Types">
  <data key="d5">16.0</data>
  <data key="d6">Provides empirical data on Python developers' demographics, tools, and practices."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Qi, Z., Long, F., Achour, S., and Rinard, M." target="Research">
  <data key="d5">16.0</data>
  <data key="d6">Authors analyzing generate-and-validate patch systems, focusing on their plausibility and correctness."|&lt;SEP&gt;Authors analyzing the plausibility and correctness of generate-and-validate patches, improving software repair techniques."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Generate-and-Validate Patch Generation Systems" target="Methodologies">
  <data key="d5">16.0</data>
  <data key="d6">Automated code repair systems that generate patches and validate correctness to fix software bugs."|&lt;SEP&gt;Automated systems that generate patches and validate their correctness to repair software bugs."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Radford, A., Narasimhan, K., Salimans, T., Sutskever, I." target="Research">
  <data key="d5">9.0</data>
  <data key="d6">Authors developing large-scale language models through generative pre-training, significantly advancing NLP."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al." target="Research">
  <data key="d5">18.0</data>
  <data key="d6">Authors working on transfer learning and multi-modal models like CLIP for vision and language."|&lt;SEP&gt;Authors working on transfer learning and multi-modal models like CLIP, combining visual and textual data."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Reed, S. and de Freitas, N." target="Research">
  <data key="d5">16.0</data>
  <data key="d6">Authors proposing neural interpreters that interpret and execute programs, advancing AI's ability to understand code."|&lt;SEP&gt;Authors proposing neural interpreters that learn to interpret and execute programs from data."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S." target="Research">
  <data key="d5">16.0</data>
  <data key="d6">Authors introducing CodeBLEU, an automatic metric for evaluating code generation quality."|&lt;SEP&gt;Authors proposing CodeBLEU, an automatic metric for evaluating code generation and synthesis."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al." target="Research">
  <data key="d5">18.0</data>
  <data key="d6">Authors showing how large-scale unsupervised learning of protein sequences reveals biological structures and functions."|&lt;SEP&gt;Authors studying how large-scale unsupervised learning of protein sequences reveals biological structures and functions."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Sourcefinder" target="Malware Source Code">
  <data key="d5">18.0</data>
  <data key="d6">Sourcefinder is used to locate and analyze malware source code within repositories, establishing a direct link between the tool and the objects of study."|&lt;SEP&gt;Sourcefinder is used to locate malware source code in repositories, directly related to analyzing malicious code objects."|</data>
  <data key="d7">malware analysis, code localization&lt;SEP&gt;tool, malware analysis</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Representation Learning with Contrastive Predictive Coding" target="Theories/Models">
  <data key="d5">8.0</data>
  <data key="d6">This technique is a form of representation learning used within neural network training to improve data representations."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Carbon Emissions from Large Neural Network Training" target="Results">
  <data key="d5">8.0</data>
  <data key="d6">Training large neural models significantly increases carbon footprint, raising environmental sustainability issues."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Deep Contextualized Word Representations" target="Theories/Models">
  <data key="d5">16.0</data>
  <data key="d6">A technique for capturing nuanced word meanings in context, improving NLP task performance."|&lt;SEP&gt;Advanced word embedding models that capture context-dependent meanings for NLP tasks."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Planning in Neural Program Learning" target="Methodologies">
  <data key="d5">7.0</data>
  <data key="d6">Structured search and planning methods used to guide neural models in program generation."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Python Software Foundation" target="Organizations">
  <data key="d5">7.0</data>
  <data key="d6">An organization supporting Python language development and community activities."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="JetBrains" target="Organizations">
  <data key="d5">7.0</data>
  <data key="d6">A company providing developer tools and conducting surveys on programming practices."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Biological Structure and Function" target="Scaling Unsupervised Learning">
  <data key="d5">16.0</data>
  <data key="d6">Scaling unsupervised learning to large protein datasets enables the discovery of biological structures and functions, indicating a relationship between data scale and biological understanding."|&lt;SEP&gt;Scaling unsupervised learning to large protein datasets helps reveal biological structures and functions, indicating a relationship between data scale and biological insight.</data>
  <data key="d7">data scale, biological insight&lt;SEP&gt;scale, biological discovery</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="End-to-End Memory Networks" target="Question Answering Tasks">
  <data key="d5">6.0</data>
  <data key="d6">End-to-end memory networks are designed to improve reasoning over sequences, making them suitable for tasks like fact extraction and verification.</data>
  <data key="d7">reasoning, neural architectures</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Women’s Participation in Open Source Software" target="Open Source">
  <data key="d5">12.0</data>
  <data key="d6">The survey examines women's participation in open source projects, relating gender participation to open source development."|&lt;SEP&gt;The survey investigates women's roles and participation patterns in open source projects, relating gender diversity to community development."|</data>
  <data key="d7">diversity, community participation</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Attention is All You Need" target="Transformers">
  <data key="d5">20.0</data>
  <data key="d6">The Transformer architecture introduced in the paper forms the basis for many modern models used in code generation, natural language processing, and sequence modeling."|&lt;SEP&gt;The Transformer architecture introduced in the paper underpins many modern neural models, including those for code generation and language understanding."|</data>
  <data key="d7">model innovation, attention mechanism</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Attention is All You Need" target="A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin">
  <data key="d5">20.0</data>
  <data key="d6">This foundational paper introduces the Transformer architecture, underpinning many modern neural network models for language and code."|&gt;"theoretical foundation, model architecture&lt;SEP&gt;This foundational paper introduces the Transformer architecture, which underpins many modern NLP and code models."|&gt;"theoretical foundation, model architecture</data>
  <data key="d7">10</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="GPT-J-6B" target="Natural Language to Code">
  <data key="d5">18.0</data>
  <data key="d6">GPT-J-6B is employed for code generation and understanding, linking large language models with applications in translating natural language prompts into code."|&lt;SEP&gt;GPT-J-6B is used for code generation and understanding, linking large language models with applications in natural language to code translation."|</data>
  <data key="d7">language model, code application&lt;SEP&gt;language model, code generation</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Xu, F. F., Vasilescu, B., and Neubig, G." target="Research Paper">
  <data key="d5">12.0</data>
  <data key="d6">Authors of a paper discussing code generation from natural language, focusing on promise and challenges."|&gt;"authorship&lt;SEP&gt;Authors of a paper discussing code generation from natural language, highlighting its promise and challenges."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Yin, P. and Neubig, G." target="Research Paper">
  <data key="d5">12.0</data>
  <data key="d6">Authors of a study presenting a syntactic neural model for general-purpose code generation."|&gt;"authorship&lt;SEP&gt;Authors presenting a neural model that uses syntax to improve code generation across tasks."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Zaremba, W. and Sutskever, I." target="Research Paper">
  <data key="d5">12.0</data>
  <data key="d6">Authors exploring neural methods for learning to execute code."|&gt;"authorship&lt;SEP&gt;Authors of a paper focused on learning to execute code, exploring methods for neural execution models."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y." target="Research Paper">
  <data key="d5">12.0</data>
  <data key="d6">Authors of a paper introducing Merlot, a multimodal neural script knowledge model."|&gt;"authorship&lt;SEP&gt;Authors of a study on Merlot, a multimodal neural script knowledge model."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S." target="Research Paper">
  <data key="d5">12.0</data>
  <data key="d6">Authors of a paper on improving few-shot performance of language models through calibration."|&gt;"authorship&lt;SEP&gt;Authors proposing calibration techniques to improve few-shot learning in language models."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Ziegler, A." target="Researcher">
  <data key="d5">12.0</data>
  <data key="d6">Author of a report examining rote learning in GitHub Copilot suggestions."|&gt;"authorship&lt;SEP&gt;Author of a report on rote learning tendencies in GitHub Copilot suggestions."|&gt;"authorship</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Estimating pass@k" target="Methodology">
  <data key="d5">16.0</data>
  <data key="d6">An unbiased estimator for evaluating the success probability of code generation models."|&gt;"methodology&lt;SEP&gt;An unbiased statistical estimator used to evaluate the success probability of code generation models, ensuring fair comparison."|&gt;"methodology</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="n" target="Variable">
  <data key="d5">16.0</data>
  <data key="d6">Number of code samples evaluated in the pass@k metric."|&gt;"variable&lt;SEP&gt;Number of samples drawn in the pass@k estimation process."|&gt;"variable</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="n" target="range">
  <data key="d5">7.0</data>
  <data key="d6">n defines the upper limit for the range of numbers evaluated in the function, setting the scope of analysis.</data>
  <data key="d7">range definition, scope</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="c" target="Variable">
  <data key="d5">16.0</data>
  <data key="d6">Count of correct code samples passing tests among total samples."|&gt;"variable&lt;SEP&gt;Number of correct samples passing unit tests among the total samples."|&gt;"variable</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="c, the number of correct samples that pass the unit tests, is distributed Binom(n; p)" target="Variable">
  <data key="d5">8.0</data>
  <data key="d6">Statistical distribution describing the number of successful samples in the evaluation of code generation models."|&gt;"distribution</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="c, the number of correct samples that pass the unit tests, is distributed Binom(n; p)" target="Distribution">
  <data key="d5">8.0</data>
  <data key="d6">Statistical distribution modeling the number of successful samples."|&gt;"distribution</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Figure 13" target="Figure">
  <data key="d5">16.0</data>
  <data key="d6">A figure comparing bias and variance of estimators for pass@k, illustrating estimator performance."|&gt;"illustration&lt;SEP&gt;Graph comparing bias and variance of pass@k estimators."|&gt;"illustration</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Random Problems and Solutions from Codex-12B" target="Dataset/Task">
  <data key="d5">16.0</data>
  <data key="d6">A set of randomly selected problems with generated solutions from Codex-12B for evaluating code generation models."|&gt;"dataset&lt;SEP&gt;A set of randomly selected programming problems with solutions generated by Codex-12B for evaluation."|&gt;"dataset</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="def words_string(s):" target="Function">
  <data key="d5">16.0</data>
  <data key="d6">A function designed to split a string of words into an array of individual words, handling commas and spaces."|&gt;"function&lt;SEP&gt;A function to split strings into words, handling spaces and commas."|&gt;"function</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="is_prime(n)" target="Function">
  <data key="d5">16.0</data>
  <data key="d6">A function to check if a number is prime."|&gt;"function&lt;SEP&gt;A function to determine if a given number n is prime, returning true or false."|&gt;"function</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3ac45c735d3bc9849e74dd1a2c194b20</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="abcde" target="Vowels Count">
  <data key="d5">7.0</data>
  <data key="d6">The count of vowels in 'abcde' is 2, with 'e' and 'a' counted, illustrating standard vowel counting without 'Y' at the end.</data>
  <data key="d7">vowels in standard position, vowel counting</data>
  <data key="d8">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="ACEDY" target="Vowels Count">
  <data key="d5">16.0</data>
  <data key="d6">The count of vowels in 'ACEDY' is 3, including 'Y' because it is at the end of the word, demonstrating the rule for 'Y' as a vowel.</data>
  <data key="d7">vowels at word end, vowel counting</data>
  <data key="d8">chunk-2218fd96f193ea8c303a15bc41a21f5b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="b_digit" target="result">
  <data key="d5">16.0</data>
  <data key="d6">b_digit is used in the calculation of result, contributing to the final value based on digit multiplication and positional significance.&lt;SEP&gt;b_digit is used in the calculation of result, contributing to the final value based on digit multiplication and positional value.</data>
  <data key="d7">calculation, digit contribution</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="result" target="a_digit">
  <data key="d5">7.0</data>
  <data key="d6">a_digit influences the calculation of result, affecting the overall output based on its value.</data>
  <data key="d7">digit influence, calculation</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="result" target="power">
  <data key="d5">6.0</data>
  <data key="d6">power determines the positional significance of digits in the calculation of result.</data>
  <data key="d7">positional value, calculation</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="ifb" target="return 0 - result">
  <data key="d5">7.0</data>
  <data key="d6">The condition ifb &lt; 0 triggers the return of the negated result, indicating flow control based on the value of b.</data>
  <data key="d7">control flow, condition</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="number" target="product">
  <data key="d5">6.0</data>
  <data key="d6">number is the product of a and b, which is used in further calculations or as an input for other operations.</data>
  <data key="d7">multiplication, input</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="string" target="digit-wise operation">
  <data key="d5">7.0</data>
  <data key="d6">The string representation of number allows digit-by-digit processing for summing digits or other manipulations.</data>
  <data key="d7">string processing, digits</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="string" target="anti_shuffle">
  <data key="d5">14.0</data>
  <data key="d6">The anti_shuffle function processes strings to produce ordered versions based on ASCII values, applied word-wise.</data>
  <data key="d7">method, string processing</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="total" target="digit sum">
  <data key="d5">6.0</data>
  <data key="d6">total accumulates the sum of individual digits in the string, used in a separate function to analyze digit properties.</data>
  <data key="d7">digit sum, analysis</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="i" target="digit">
  <data key="d5">6.0</data>
  <data key="d6">i iterates over each character (digit) in the string, enabling digit-wise operations.</data>
  <data key="d7">iteration, digit processing</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code Synthesis and Generation" target="Specifications">
  <data key="d5">16.0</data>
  <data key="d6">Specifications serve as benchmarks to evaluate the correctness and expressivity of generated code."|&gt;"evaluation, benchmarks&lt;SEP&gt;Specifications serve as benchmarks to evaluate the correctness, expressivity, and complexity of generated code."|&gt;"evaluation, benchmarks</data>
  <data key="d7">8</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Specifications" target="Attributes for Measuring Specifications">
  <data key="d5">14.0</data>
  <data key="d6">Attributes such as reasoning over abstractions and variable dependencies are used to assess the complexity and expressivity of specifications."|&gt;"measurement, attributes&lt;SEP&gt;Attributes such as reasoning over abstractions, variable dependencies, and inter-procedural reasoning are used to assess the specifications' complexity and expressivity."|&gt;"measurement, attributes</data>
  <data key="d7">7</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Specifications" target="High-level and Low-level Specifications">
  <data key="d5">6.0</data>
  <data key="d6">The distinction between high-level and low-level specifications impacts the difficulty of synthesis and the degree of ambiguity."|&gt;"abstraction levels, complexity</data>
  <data key="d7">6</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Variable Interdependencies" target="Temporal Reasoning">
  <data key="d5">14.0</data>
  <data key="d6">Both involve reasoning about program states and dependencies over time, crucial for understanding and evaluating synthesis capabilities."|&gt;"reasoning, program states&lt;SEP&gt;Understanding variable dependencies over time is crucial for reasoning about program safety and liveness."|&gt;"reasoning, program states</data>
  <data key="d7">7</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Variable Interdependencies" target="High-level Specifications">
  <data key="d5">6.0</data>
  <data key="d6">High-level specifications require implicit derivation of lower-level details, involving complex interdependencies."|&gt;"abstraction, complexity</data>
  <data key="d7">6</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Variable Interdependencies" target="Low-level Specifications">
  <data key="d5">6.0</data>
  <data key="d6">Low-level specifications are explicitly defined, with well-declared constraints, simplifying interdependencies."|&gt;"clarity, complexity</data>
  <data key="d7">6</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Concurrency and Parallelism" target="Properties">
  <data key="d5">13.0</data>
  <data key="d6">Concurrency properties such as mutual exclusion and fairness are essential for correct parallel code synthesis."|&gt;"correctness, properties&lt;SEP&gt;Concurrency properties such as mutual exclusion, fairness, and synchronization are essential for correct parallel code synthesis."|&gt;"correctness, properties</data>
  <data key="d7">6&lt;SEP&gt;7</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Nondeterminism" target="Hyperproperties">
  <data key="d5">5.0</data>
  <data key="d6">Hyperproperties include nondeterminism as a property, relating to the behavior of algorithms in different executions."|&gt;"property, algorithm behavior</data>
  <data key="d7">5</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Nondeterminism" target="Algorithms">
  <data key="d5">7.0</data>
  <data key="d6">Nondeterminism describes algorithms that can produce different outputs for the same input across different runs, relevant for evaluating algorithm behavior."|&gt;"algorithm property, behavior</data>
  <data key="d7">7</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Nondeterminism" target="Outputs">
  <data key="d5">16.0</data>
  <data key="d6">Outputs are subject to nondeterminism, meaning they can vary across different executions even with the same inputs, especially in probabilistic systems or ML algorithms."|&lt;"system variability, probabilistic behavior</data>
  <data key="d7">8</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Nondeterminism" target="Random Number Generator">
  <data key="d5">14.0</data>
  <data key="d6">A common example illustrating nondeterminism, as it produces different results in different runs, serving as a simple model for nondeterministic processes."|&lt;"algorithm example, variability</data>
  <data key="d7">7</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Nondeterminism" target="ML Algorithms">
  <data key="d5">16.0</data>
  <data key="d6">ML algorithms can exhibit nondeterministic behavior, especially during training or inference, making outcomes variable."|&lt;"model behavior, variability</data>
  <data key="d7">8</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Noninterference" target="Hyperproperties">
  <data key="d5">8.0</data>
  <data key="d6">Noninterference is a hyperproperty ensuring that high-security inputs do not influence low-security outputs, maintaining confidentiality."|&gt;"security, information flow</data>
  <data key="d7">8</data>
  <data key="d8">chunk-38f5b1edf9dde338a2640f132d99659a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Low-Security Users" target="Outputs">
  <data key="d5">14.0</data>
  <data key="d6">Outputs are observed for low-security users, and their consistency indicates system security boundaries.</data>
  <data key="d7">security boundary, output consistency</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="High-Security Users" target="Outputs">
  <data key="d5">12.0</data>
  <data key="d6">Inputs from high-security users can influence the system, but do not alter observed outputs for low-security users, implying a security boundary."|&lt;"security boundary, input influence</data>
  <data key="d7">6</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Specification-Independent Coding Practices" target="Code and Parameterized Reuse">
  <data key="d5">14.0</data>
  <data key="d6">Code reuse and parameterization are key practices that support specification-independent coding, enabling flexible and adaptable program development."|&lt;"coding practices, system design</data>
  <data key="d7">7</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Specification-Independent Coding Practices" target="Well-Defined">
  <data key="d5">14.0</data>
  <data key="d6">Well-defined attributes ensure clarity and consistency in coding practices, supporting specification independence."|&lt;"system clarity, coding standards</data>
  <data key="d7">7</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code and Parameterized Reuse" target="Automatic Determination of Program Architecture">
  <data key="d5">12.0</data>
  <data key="d6">Systems can infer or optimize program structures automatically, reducing manual specification and enhancing reuse."|&lt;"automation, program design</data>
  <data key="d7">6</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Code and Parameterized Reuse" target="Wide Range of Programming Constructs">
  <data key="d5">12.0</data>
  <data key="d6">A broad set of programming constructs facilitates reuse and flexibility in coding practices."|&lt;"programming flexibility, system capability</data>
  <data key="d7">6</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Increasingly Higher Level Specifications" target="Code Generation Algorithms">
  <data key="d5">16.0</data>
  <data key="d6">Higher-level specifications are intended to be inferred by code generation algorithms, reducing explicit coding requirements."|&lt;"automation, abstraction</data>
  <data key="d7">8</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Alignment Problems" target="Evaluation of Alignment">
  <data key="d5">18.0</data>
  <data key="d6">Evaluations aim to identify and understand issues like misalignment, bias, or security flaws in models."|&lt;"model assessment, security, bias</data>
  <data key="d7">9</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Capability" target="Pre-Training">
  <data key="d5">7.0</data>
  <data key="d6">Pre-training on large datasets influences models' initial ability to generate code, which is then refined through fine-tuning.</data>
  <data key="d7">initial training, model ability</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Detecting Problems with Models" target="Evaluation of Alignment">
  <data key="d5">16.0</data>
  <data key="d6">Evaluations are used to detect problems such as misalignment, security vulnerabilities, or bias in models."|&lt;"model testing, quality assurance</data>
  <data key="d7">8</data>
  <data key="d8">chunk-50450e84744379a5c4f059c7d4f84019</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="alignment evaluations" target="model">
  <data key="d5">10.0</data>
  <data key="d6">The evaluation assesses the model's ability to produce correct, bug-free code and follow instructions, indicating its level of alignment.</data>
  <data key="d7">model assessment, performance</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="alignment evaluations" target="dataset">
  <data key="d5">8.0</data>
  <data key="d6">The dataset provides problems and solutions used to test the model's ability to generate accurate code and detect bugs.</data>
  <data key="d7">data source, evaluation basis</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model" target="human feedback">
  <data key="d5">9.0</data>
  <data key="d6">Human feedback guides the fine-tuning process to improve model alignment and correctness.</data>
  <data key="d7">training data, model improvement</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model" target="fine-tuning">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning on high-quality datasets aims to enhance the model's ability to generate bug-free, aligned code.</data>
  <data key="d7">training process, model optimization</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model" target="RLHF">
  <data key="d5">9.0</data>
  <data key="d6">Reinforcement Learning from Human Feedback is applied to improve the model's alignment with human preferences and correctness.</data>
  <data key="d7">training methodology, alignment improvement</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model" target="transparency tools">
  <data key="d5">6.0</data>
  <data key="d6">Transparency tools help evaluate whether the model's outputs are aligned with desired standards by providing insights into its behavior.</data>
  <data key="d7">evaluation aids, interpretability</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="model" target="pass@1 score">
  <data key="d5">9.0</data>
  <data key="d6">The pass@1 score measures the likelihood that the top generated output is correct, indicating model accuracy.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="dataset" target="formal analysis">
  <data key="d5">7.0</data>
  <data key="d6">Formal analysis is used to filter or label datasets based on code quality, impacting the training and evaluation of models.</data>
  <data key="d7">dataset curation, quality assurance</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="fine-tuning" target="performance of LLMs">
  <data key="d5">7.0</data>
  <data key="d6">Fine-tuning improves the models' ability to generate accurate parallel code.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Alignment Evaluations" target="Model">
  <data key="d5">10.0</data>
  <data key="d6">The evaluations measure the model's ability to produce correct, bug-free code and follow instructions, indicating its alignment.</data>
  <data key="d7">performance, correctness</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Fine-tuned GPT-Neo">
  <data key="d5">20.0</data>
  <data key="d6">GPT-Neo models are fine-tuned on curated problems and reference solutions to improve code generation accuracy.&lt;SEP&gt;GPT-Neo models are fine-tuned using curated problems and reference solutions to improve code generation performance.</data>
  <data key="d7">model training, performance enhancement&lt;SEP&gt;model training, performance improvement</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Datasets">
  <data key="d5">9.0</data>
  <data key="d6">Datasets like HumanEval are used to evaluate the model's performance, accuracy, and alignment.</data>
  <data key="d7">evaluation basis, training data</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Human Feedback">
  <data key="d5">9.0</data>
  <data key="d6">Human feedback is used to guide the model's training and fine-tuning processes to improve alignment and reduce bugs.</data>
  <data key="d7">training guidance, model improvement</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="RLHF">
  <data key="d5">9.0</data>
  <data key="d6">Reinforcement Learning from Human Feedback is applied to optimize the model based on human-provided reward signals, improving alignment.</data>
  <data key="d7">training method, alignment</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Metrics for Alignment">
  <data key="d5">8.0</data>
  <data key="d6">Metrics are used to quantitatively evaluate how well the model's outputs match desired qualities like correctness and helpfulness.</data>
  <data key="d7">evaluation criteria, performance measurement</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Transparency Tools">
  <data key="d5">7.0</data>
  <data key="d6">Transparency tools help interpret and evaluate the model's behavior, assisting in assessing its alignment.</data>
  <data key="d7">evaluation aid, interpretability</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Subtle Bugs">
  <data key="d5">8.0</data>
  <data key="d6">The model's ability to detect or avoid subtle bugs is a key aspect of alignment evaluation.</data>
  <data key="d7">performance metric, bug detection</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Pre-training Dataset">
  <data key="d5">7.0</data>
  <data key="d6">Curating the pre-training dataset to remove buggy code can improve the model's performance and alignment.</data>
  <data key="d7">dataset curation, training quality</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Automated Testing and Formal Verification Tools">
  <data key="d5">7.0</data>
  <data key="d6">These tools assist in evaluating and filtering code outputs, guiding dataset preparation and model assessment.</data>
  <data key="d7">quality assurance, evaluation support</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Curated Datasets">
  <data key="d5">8.0</data>
  <data key="d6">Training on curated, high-quality datasets aims to improve the model's ability to generate correct and safe code.</data>
  <data key="d7">training data, output quality</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model" target="Metrics of Code Quality">
  <data key="d5">7.0</data>
  <data key="d6">Metrics guide the filtering of datasets and assessment of model outputs to ensure high standards.</data>
  <data key="d7">evaluation metrics, training standards</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Datasets" target="CodeSearchNet">
  <data key="d5">8.0</data>
  <data key="d6">CodeSearchNet provides a large corpus of code for training and evaluating code understanding and synthesis models across multiple languages.</data>
  <data key="d7">large corpus, multi-language</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Datasets" target="CodeXGLUE">
  <data key="d5">7.0</data>
  <data key="d6">CodeXGLUE aggregates multiple benchmarks and uses metrics like CodeBLEU to evaluate code generation quality.</data>
  <data key="d7">benchmark suite, evaluation metrics</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Datasets" target="APPS Benchmark">
  <data key="d5">8.0</data>
  <data key="d6">APPS measures functional correctness of code solutions from competitive programming problems, serving as a standard benchmark.</data>
  <data key="d7">benchmark, competitive programming</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Datasets" target="Formal Analysis">
  <data key="d5">8.0</data>
  <data key="d6">Formal analysis techniques are applied to datasets to assess code quality and filter out buggy code.</data>
  <data key="d7">quality control, dataset filtering</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Downstream Tasks Evaluation" target="Evaluation Datasets">
  <data key="d5">7.0</data>
  <data key="d6">Datasets like HumanEval are used to evaluate the model's performance on practical coding tasks after alignment efforts.</data>
  <data key="d7">performance assessment, practical application</data>
  <data key="d8">chunk-acc067e535432960edbc682652b521e0</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Capabilities" target="Pre-training and Fine-tuning Processes">
  <data key="d5">4.0</data>
  <data key="d6">The training procedures influence Codex's susceptibility to adversarial inputs and its ability to generate malicious or insecure code.</data>
  <data key="d7">training methodology, security</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Capabilities" target="GPT-3.5">
  <data key="d5">4.0</data>
  <data key="d6">GPT-3.5 is a state-of-the-art model from OpenAI, accessible via API, used for advanced language and code tasks.</data>
  <data key="d7">model capability, API access</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Capabilities" target="GPT-4">
  <data key="d5">3.0</data>
  <data key="d6">GPT-4 is OpenAI's latest high-performance model, used for complex language understanding and code generation.</data>
  <data key="d7">advanced capabilities, API</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Methodologies" target="PCFG">
  <data key="d5">7.0</data>
  <data key="d6">Probabilistic Context-Free Grammar (PCFG) is a classical formalism used to generate syntactic program structures in synthesis."|&lt;"formal grammar, syntax</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Agrawal et al. (1995); Korel &amp; Rilling (1997)">
  <data key="d5">6.0</data>
  <data key="d6">Static or dynamic code analysis techniques used to locate bugs, verify correctness, and fix faulty code."|&lt;"bug detection, code debugging</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Jeffrey et al. (2009)">
  <data key="d5">6.0</data>
  <data key="d6">Learned association rules for code analysis, aiding in bug detection and code fixing."|&lt;"association rules, bug fixing</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Goues et al. (2012)">
  <data key="d5">6.0</data>
  <data key="d6">Genetic programming approaches for code synthesis and debugging, evolving code solutions."|&lt;"genetic algorithms, program evolution</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Tools">
  <data key="d5">6.0</data>
  <data key="d6">Methodologies include tools that facilitate systematic data collection and analysis.</data>
  <data key="d7">procedures, instruments</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Study Designs">
  <data key="d5">24.0</data>
  <data key="d6">Methodologies encompass various study designs that structure research activities.&lt;SEP&gt;Methodologies guide the choice of study designs to ensure appropriate data collection and analysis.</data>
  <data key="d7">research framework, study planning&lt;SEP&gt;research planning, procedural guidance</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Methodologies" target="Model Editing">
  <data key="d5">16.0</data>
  <data key="d6">Model editing techniques are used to modify neural networks without retraining from scratch."|&lt;SEP&gt;Model editing techniques enable post-training modifications to neural networks, including locating, editing, and updating parameters."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Methodologies" target="Memory-based Model Editing">
  <data key="d5">14.0</data>
  <data key="d6">Memory-based approaches utilize stored information to update models efficiently."|&lt;SEP&gt;Memory-based model editing uses stored information to efficiently update or correct model knowledge."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Methodologies" target="Raise a Child in Large Language Model">
  <data key="d5">14.0</data>
  <data key="d6">A fine-tuning approach aimed at improving generalization and effectiveness of large language models.</data>
  <data key="d7">fine-tuning, generalization</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Methodologies" target="SEQZERO">
  <data key="d5">14.0</data>
  <data key="d6">SEQZERO enhances semantic parsing with sequential prompts and zero-shot learning.</data>
  <data key="d7">semantic parsing, zero-shot learning</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Methodologies" target="ParEval">
  <data key="d5">10.0</data>
  <data key="d6">ParEval is a benchmark framework designed to systematically evaluate LLMs' ability to generate parallel code across diverse problem types and execution models.</data>
  <data key="d7">benchmark design, evaluation methodology</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="sumOfMinimumElements">
  <data key="d5">10.0</data>
  <data key="d6">This function serves as an example task to assess the model's ability to generate correct code across different execution models.</data>
  <data key="d7">code example, task assessment</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="Disciplines">
  <data key="d5">12.0</data>
  <data key="d6">Disciplines influence the selection and development of research methodologies.</data>
  <data key="d7">interdisciplinary approaches, methodological frameworks</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="Code Translation">
  <data key="d5">8.0</data>
  <data key="d6">Code translation is evaluated as a methodology to assess model scalability and translation accuracy.</data>
  <data key="d7">method evaluation, performance testing</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="Parallel Code Generation">
  <data key="d5">8.0</data>
  <data key="d6">Parallel code generation is a methodology used to automate the creation of parallel programs, assessed for correctness and efficiency.</data>
  <data key="d7">automation, correctness</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="code translation">
  <data key="d5">8.0</data>
  <data key="d6">Code translation is a methodology to convert code from one execution model to another, evaluating the translation accuracy and impact on performance."|&gt;"method evaluation, scalability</data>
  <data key="d7">8</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="parallel code generation">
  <data key="d5">8.0</data>
  <data key="d6">Parallel code generation is a methodology involving automated creation of parallel programs, assessed for correctness, efficiency, and scalability."|&gt;"automation, correctness</data>
  <data key="d7">8</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Methodologies" target="Kokkos">
  <data key="d5">14.0</data>
  <data key="d6">Kokkos is a high-level abstraction aiming to provide portable performance across hardware architectures, often used in HPC applications.</data>
  <data key="d7">performance portability, high-level abstraction</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Methodologies" target="Thrust">
  <data key="d5">14.0</data>
  <data key="d6">Thrust offers high-level parallel algorithms for GPU programming, facilitating code that is portable and expressive in HPC contexts.</data>
  <data key="d7">parallel algorithms, GPU programming</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Methodologies" target="SyCL">
  <data key="d5">16.0</data>
  <data key="d6">SyCL provides a high-level programming interface for heterogeneous systems, allowing code portability and performance optimization across diverse hardware.</data>
  <data key="d7">heterogeneous computing, code portability</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Methodologies" target="Support for shared, distributed memory, and offloading models">
  <data key="d5">9.0</data>
  <data key="d6">Supporting multiple programming models allows flexible deployment and efficient use of heterogeneous hardware."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Methodologies" target="Assignment of work to execution units">
  <data key="d5">8.0</data>
  <data key="d6">Mapping tasks to specific hardware units based on system architecture and workload characteristics."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Methodologies" target="test-time bootstrapping">
  <data key="d5">7.0</data>
  <data key="d6">A proposed technique for dynamically assessing model performance during testing by resampling or reinitializing components to improve robustness."|</data>
  <data key="d7">dynamic evaluation, robustness</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Methodologies" target="automatic backtracking-like logic">
  <data key="d5">7.0</data>
  <data key="d6">An automated reasoning process allowing models to revisit previous steps or decisions, improving accuracy and handling complex tasks."|</data>
  <data key="d7">automated reasoning, error correction</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Methodologies" target="self-coherence">
  <data key="d5">6.0</data>
  <data key="d6">Self-coherence involves sampling multiple reasoning paths and selecting the most consistent answer to improve reasoning accuracy.</data>
  <data key="d7">reasoning accuracy, sampling</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Study Designs" target="Core Concepts">
  <data key="d5">6.0</data>
  <data key="d6">Study designs are based on core concepts that guide the structure of research.</data>
  <data key="d7">research framework, foundational principles</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Study Designs" target="Results">
  <data key="d5">7.0</data>
  <data key="d6">Results are derived from applying specific study designs to collect and analyze data.</data>
  <data key="d7">data collection, analysis outcomes</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Study Designs" target="Deployment scenarios">
  <data key="d5">8.0</data>
  <data key="d6">Studying various deployment scenarios helps understand impacts on productivity, code quality, and economic outcomes.</data>
  <data key="d7">impact analysis, study design</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Study Designs" target="Hammond Pearce et al. 2022 Study">
  <data key="d5">18.0</data>
  <data key="d6">This study evaluates security vulnerabilities in GitHub Copilot's code contributions, highlighting security risks of AI-generated code.&lt;SEP&gt;This study evaluates the security vulnerabilities in GitHub Copilot's code contributions, highlighting implications for software security.</data>
  <data key="d7">Security Analysis</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Carbon Emissions and Large Neural Network Training">
  <data key="d5">8.0</data>
  <data key="d6">Training large neural networks significantly contributes to carbon emissions, raising sustainability concerns."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="tuple">
  <data key="d5">7.0</data>
  <data key="d6">The output tuple contains counts of even and odd palindromes, summarizing the analysis within the range.</data>
  <data key="d7">summary, counts</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="Objects of Study">
  <data key="d5">16.0</data>
  <data key="d6">Results are derived from analyzing objects of study, providing insights into their characteristics or behaviors.&lt;SEP&gt;Results are obtained by analyzing objects of study using various techniques.</data>
  <data key="d7">data analysis, research findings</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="Variables">
  <data key="d5">12.0</data>
  <data key="d6">Results depend on variables being measured and their interactions within the study.&lt;SEP&gt;Results often depend on the variables being studied and their interactions.</data>
  <data key="d7">data, effect measurement&lt;SEP&gt;data, effects, relationships</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="Study Populations/Dataset">
  <data key="d5">22.0</data>
  <data key="d6">Results are based on data from study populations or datasets.&lt;SEP&gt;The dataset or population under study provides the basis for deriving results.</data>
  <data key="d7">data sources, analysis&lt;SEP&gt;data-driven conclusions, sample analysis</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="Applications/Implications">
  <data key="d5">23.0</data>
  <data key="d6">Research results inform practical applications and implications in relevant fields.&lt;SEP&gt;Results inform practical applications and implications of research.</data>
  <data key="d7">real-world impact, usage&lt;SEP&gt;technology transfer, policy development</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Results" target="OpenMP Pragma Labeling">
  <data key="d5">8.0</data>
  <data key="d6">This task assesses how well the model can identify and label OpenMP parallelization directives in code.</data>
  <data key="d7">parsing, annotation</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Results" target="Relative Performance Prediction">
  <data key="d5">9.0</data>
  <data key="d6">The model's performance in predicting code performance demonstrates its language comprehension capabilities.</data>
  <data key="d7">prediction, comprehension</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Results" target="Performance Evaluation Metrics">
  <data key="d5">10.0</data>
  <data key="d6">Novel metrics are used to measure the quality and effectiveness of generated code, revealing strengths and weaknesses of models.</data>
  <data key="d7">performance measurement, code quality</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Evaluation of Language Models">
  <data key="d5">10.0</data>
  <data key="d6">The performance metrics and analysis reveal the strengths and limitations of each model in generating accurate and efficient parallel code.</data>
  <data key="d7">performance results, model capabilities</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Code Performance">
  <data key="d5">10.0</data>
  <data key="d6">The newly introduced metrics quantify the effectiveness of generated code, demonstrating how well models handle complex parallel programming tasks.</data>
  <data key="d7">performance measurement, code quality</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Complexity of Parallel Code">
  <data key="d5">8.0</data>
  <data key="d6">The study finds that models often struggle with complex algorithms requiring reasoning about data distribution, synchronization, and optimization.</data>
  <data key="d7">model limitations, complexity challenges</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Limitations of Current Models">
  <data key="d5">8.0</data>
  <data key="d6">Current models have significant limitations in reliably generating correct, optimized, and complex parallel code, indicating the need for further advancements.</data>
  <data key="d7">model shortcomings, research gaps</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="MBPP">
  <data key="d5">7.0</data>
  <data key="d6">MBPP results are based on performance metrics obtained from benchmark evaluations.</data>
  <data key="d7">benchmarking, code generation performance</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Speedupmax@k">
  <data key="d5">8.0</data>
  <data key="d6">Speedupmax@k estimates the maximum achievable speedup across all resource configurations, indicating the peak performance potential of generated code.</data>
  <data key="d7">peak performance, optimization</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Models">
  <data key="d5">23.0</data>
  <data key="d6">Larger and more advanced models like GPT-4 generally outperform smaller models across various parallel programming tasks."|"&lt;model size, accuracy&lt;SEP&gt;Models like RAG outperform previous state-of-the-art models on multiple datasets, indicating progress in retrieval and generation."|&lt;SEP&gt;Models like RAG outperform previous state-of-the-art models on various datasets, indicating advancements in retrieval and generation capabilities.</data>
  <data key="d7">7&lt;SEP&gt;performance improvement, benchmarking</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Results" target="Evidence Types">
  <data key="d5">14.0</data>
  <data key="d6">Results are derived from specific evidence types, such as quantitative data or qualitative observations.</data>
  <data key="d7">data support, empirical evidence</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Research Questions/Hypotheses">
  <data key="d5">16.0</data>
  <data key="d6">Results directly address the research questions or test the hypotheses posed at the study's outset.</data>
  <data key="d7">outcome measurement, hypothesis testing</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Inefficiency of Parallel Code">
  <data key="d5">8.0</data>
  <data key="d6">The study finds the generated parallel code to be generally inefficient, with low resource utilization.</data>
  <data key="d7">performance limitation, resource utilization</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="inefficiency of parallel code">
  <data key="d5">8.0</data>
  <data key="d6">The study finds that the parallel code generated by models is generally inefficient, with low resource utilization despite correctness, highlighting the need for improved efficiency."|&gt;"performance limitation, resource utilization</data>
  <data key="d7">8</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Results" target="Proficiency Metric">
  <data key="d5">12.0</data>
  <data key="d6">The proficiency metric assesses the quality of AI-generated code suggestions, providing a quantitative basis for comparing different models and prompts.&lt;SEP&gt;The proficiency metric assesses the quality of AI-generated code suggestions, providing a quantitative measure to compare across prompts and models."|</data>
  <data key="d7">assessment, comparison&lt;SEP&gt;evaluation, comparison</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Language-supported Programming Models">
  <data key="d5">7.0</data>
  <data key="d6">The effectiveness of AI-generated kernels varies across programming languages and models, with some models showing higher maturity and better outputs.</data>
  <data key="d7">performance, language dependence</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Language Support">
  <data key="d5">7.0</data>
  <data key="d6">The effectiveness of AI-generated kernels varies depending on the programming language and model maturity; C++ and CUDA perform well, HIP less so; Julia models perform acceptably."|</data>
  <data key="d7">performance, language dependence</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="&lt;study&gt;">
  <data key="d5">8.0</data>
  <data key="d6">The results summarize the accuracy levels achieved across different kernels and models."|&gt;"findings, evaluation</data>
  <data key="d7">8</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Results">
  <data key="d5">9.0</data>
  <data key="d6">Results refer to the outcomes of experiments or evaluations, such as code quality metrics, performance measurements, and accuracy assessments presented in the study."|&gt;"results evaluation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Code quality">
  <data key="d5">8.0</data>
  <data key="d6">Code quality refers to the correctness, efficiency, and readability of generated code, assessed through metrics and expert evaluation."|&gt;"assessment metrics</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Code generation">
  <data key="d5">8.0</data>
  <data key="d6">Code generation is the process of automatically producing source code using AI tools like Copilot, evaluated based on accuracy, completeness, and performance."|&gt;"automated coding</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Code Complexity">
  <data key="d5">14.0</data>
  <data key="d6">Increased code complexity makes obtaining acceptable results more challenging, highlighting the need for better tools and methodologies.&lt;SEP&gt;Increased code complexity makes obtaining acceptable results more challenging.</data>
  <data key="d7">difficulty, computational challenge&lt;SEP&gt;difficulty, programming challenges</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Results" target="Evaluation">
  <data key="d5">6.0</data>
  <data key="d6">The RAG models' superior performance demonstrates the effectiveness of combining parametric and non-parametric memory in NLP tasks.</data>
  <data key="d7">performance, benchmarking</data>
  <data key="d8">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Results" target="RAG">
  <data key="d5">18.0</data>
  <data key="d6">The RAG models achieved state-of-the-art results on open-domain QA benchmarks, demonstrating their effectiveness.</data>
  <data key="d7">performance</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Results" target="Factuality">
  <data key="d5">16.0</data>
  <data key="d6">Improved factual grounding in responses leads to higher perceived accuracy and trustworthiness.</data>
  <data key="d7">quality</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Results" target="Performance Analysis">
  <data key="d5">16.0</data>
  <data key="d6">Performance analysis provides data on runtime improvements, speedups, and efficiency."|&gt;"evaluation&lt;SEP&gt;Performance analysis provides quantitative data on runtime, speedup, and efficiency gains."|&gt;"evaluation</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Results" target="Compiler Optimizations">
  <data key="d5">14.0</data>
  <data key="d6">Compiler optimizations contribute to observed performance speedups, as in lavaMD."|&gt;"performance enhancement&lt;SEP&gt;Compiler optimizations, such as those from Intel OneAPI, contribute to observed speedups, especially in lavaMD."|&gt;"performance enhancement</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Results" target="GPT-3.5 and llama2-13b-chat">
  <data key="d5">16.0</data>
  <data key="d6">These models' performances are compared to previous benchmarks, showing significant improvements with DSPy compilation strategies.</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Results" target="manual CoT">
  <data key="d5">14.0</data>
  <data key="d6">Manual chain-of-thought prompting is used as a benchmark, with DSPy approaches aiming to match or surpass it.</data>
  <data key="d7">benchmarking, reasoning chains</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Results" target="programs">
  <data key="d5">7.0</data>
  <data key="d6">Different DSPy modules and their compositions lead to improved reasoning accuracy across models.</data>
  <data key="d7">performance improvement, module composition</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Results" target="Model Performance Comparison">
  <data key="d5">9.0</data>
  <data key="d6">Experimental results demonstrate that the HPC-Coder-V2 models perform as well or better than larger models in parallel code generation.</data>
  <data key="d7">performance, efficiency</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Variables" target="Controllable Working Memory">
  <data key="d5">7.0</data>
  <data key="d6">Controlling the working memory in large language models affects reasoning, context retention, and overall performance.</data>
  <data key="d7">memory control, reasoning</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Variables" target="Factual Associations">
  <data key="d5">16.0</data>
  <data key="d6">Factual associations are the relationships between data points and facts stored within language models."|&lt;SEP&gt;Factual associations are the relationships between stored data and facts within language models."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Variables" target="Execution Models">
  <data key="d5">8.0</data>
  <data key="d6">Different execution models provide varied paradigms for parallel programming, essential for testing LLMs' adaptability.</data>
  <data key="d7">programming paradigms, testing diversity</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Serial, OpenMP, MPI, CUDA, Kokkos">
  <data key="d5">8.0</data>
  <data key="d6">These are the source and target programming paradigms involved in translation tasks to evaluate model capabilities.</data>
  <data key="d7">programming paradigms, translation tasks</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Runtime of Sample j">
  <data key="d5">8.0</data>
  <data key="d6">The runtime of each sample is used to compute the speedup and analyze the distribution of performance across samples.</data>
  <data key="d7">performance measurement, statistical analysis</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Runtime of Sequential Baseline for prompt p">
  <data key="d5">8.0</data>
  <data key="d6">Serves as the reference for calculating speedup by comparing sample runtimes to the baseline.</data>
  <data key="d7">benchmarking, performance comparison</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Number of Samples N">
  <data key="d5">8.0</data>
  <data key="d6">The total number of samples generated per prompt, influencing the distribution of runtimes and the accuracy of speedup estimates.</data>
  <data key="d7">sample size, statistical robustness</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Number of Resources n">
  <data key="d5">8.0</data>
  <data key="d6">The hardware resources used during evaluation, directly impacting the speedup and efficiency metrics.</data>
  <data key="d7">hardware configuration, performance measurement</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Set of Resource Counts procs">
  <data key="d5">8.0</data>
  <data key="d6">The collection of different resource levels over which maximum speedup is evaluated, indicating scalability and resource efficiency.</data>
  <data key="d7">scalability, resource utilization</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Objects of Study">
  <data key="d5">12.0</data>
  <data key="d6">Variables are attributes or factors that describe or influence the objects of study.</data>
  <data key="d7">measurement, description</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Variables" target="Prompt">
  <data key="d5">8.0</data>
  <data key="d6">In AI code generation, prompts are input instructions or queries that guide the model to produce specific code outputs, with language and syntax adapted to the community."|&gt;"input instructions</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Variables" target="Community size">
  <data key="d5">8.0</data>
  <data key="d6">Community size influences the performance of high-level abstractions; larger communities tend to have more optimized and well-supported tools like Kokkos, Thrust, and SyCL."|&gt;"support and optimization</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="Ling et al. (2016)">
  <data key="d5">14.0</data>
  <data key="d6">Developed latent predictor networks that generate code for specific tasks, such as Magic: The Gathering cards, using character-level models.&lt;SEP&gt;Developed latent predictor networks that generate code for tasks like Magic: The Gathering, using character-level models with latent modes."|&lt;"code generation, latent modes</data>
  <data key="d7">7&lt;SEP&gt;code generation, latent modes</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="DeepCoder">
  <data key="d5">8.0</data>
  <data key="d6">DeepCoder predicts functions in source code to guide program search, improving synthesis efficiency.</data>
  <data key="d7">function prediction, program search</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="PyMT5">
  <data key="d5">8.0</data>
  <data key="d6">PyMT5 translates between different code languages and natural language, enabling multilingual code understanding.</data>
  <data key="d7">multilingual translation, code understanding</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="TransCoder">
  <data key="d5">8.0</data>
  <data key="d6">TransCoder translates code between languages, emphasizing functional correctness over traditional metrics like BLEU.</data>
  <data key="d7">code translation, language translation</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="ContraCode">
  <data key="d5">8.0</data>
  <data key="d6">ContraCode uses contrastive learning over the space of correct programs to improve model robustness and inference.</data>
  <data key="d7">contrastive learning, program robustness</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="RobustFill">
  <data key="d5">8.0</data>
  <data key="d6">RobustFill synthesizes multiple candidate programs via beam search to find programs consistent with input-output examples.</data>
  <data key="d7">program synthesis, beam search</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Balog et al. (2017)">
  <data key="d5">8.0</data>
  <data key="d6">DeepCoder trains neural models to predict functions in source code, guiding program synthesis."|&lt;"function prediction, program search</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Feng et al. (2020)">
  <data key="d5">8.0</data>
  <data key="d6">CodeBERT, trained on paired docstrings and code, achieves strong performance in code search and understanding."|&lt;"code understanding, search</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Clement et al. (2020)">
  <data key="d5">8.0</data>
  <data key="d6">PyMT5, a multilingual transformer model trained with T5 objectives, translates between code and natural language."|&lt;"multilingual translation, code understanding</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Lachaux et al. (2020)">
  <data key="d5">8.0</data>
  <data key="d6">TransCoder translates code between languages, emphasizing functional correctness over BLEU scores."|&lt;"code translation, unsupervised learning</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Jain et al. (2020)">
  <data key="d5">8.0</data>
  <data key="d6">ContraCode uses contrastive learning to leverage the space of correct programs, improving robustness and inference."|&lt;"contrastive learning, program robustness</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Devlin et al. (2017)">
  <data key="d5">8.0</data>
  <data key="d6">RobustFill synthesizes multiple program samples via beam search to find programs consistent with input-output examples."|&lt;"program synthesis, beam search</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Analytical Techniques">
  <data key="d5">28.0</data>
  <data key="d6">Tools are used to implement analytical techniques during data analysis.&lt;SEP&gt;Tools are used to implement various analytical techniques during data processing.&lt;SEP&gt;Tools are used to implement analytical techniques for data interpretation.</data>
  <data key="d7">data analysis, computational methods&lt;SEP&gt;instrumentation, data analysis methods&lt;SEP&gt;instrumentation, methods</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe&lt;SEP&gt;chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Tools" target="Retrieval augmentation">
  <data key="d5">9.0</data>
  <data key="d6">Retrieval augmentation involves fetching external domain knowledge to supplement model responses, enhancing depth and accuracy."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="Executable Commands">
  <data key="d5">20.0</data>
  <data key="d6">Commands are generated by LLMs to invoke domain tools, enabling precise task execution.&lt;SEP&gt;Commands generated by LLMs are used to invoke external domain tools, enabling precise task execution.</data>
  <data key="d7">command execution, tool invocation&lt;SEP&gt;command invocation, tool calling</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="APIs">
  <data key="d5">18.0</data>
  <data key="d6">APIs provide structured access to domain functionalities, which LLMs can call to perform specific actions.&lt;SEP&gt;APIs provide structured access to domain functionalities, which LLMs invoke to perform specific operations or retrieve data.</data>
  <data key="d7">domain access, automation</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="Result">
  <data key="d5">6.0</data>
  <data key="d6">Results are outputs produced after executing commands or using tools, which are then used for further analysis or decision-making.</data>
  <data key="d7">output, data</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="Communicative Agents">
  <data key="d5">8.0</data>
  <data key="d6">AI agents designed for social and informational interactions, exploring societal implications of large-scale language models.</data>
  <data key="d7">societal impact, interaction</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="Adamix">
  <data key="d5">14.0</data>
  <data key="d6">Adamix is a tuning method that uses a mixture of adapters for efficient large model adaptation.</data>
  <data key="d7">parameter efficiency, model tuning</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="IDPG">
  <data key="d5">14.0</data>
  <data key="d6">IDPG is a prompt generation method tailored to create instance-dependent prompts for models.</data>
  <data key="d7">prompt engineering, adaptability</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tools" target="TOKOMPILER">
  <data key="d5">8.0</data>
  <data key="d6">TOKOMPILER is a tokenizer tailored for HPC code, enhancing LLMs' ability to process and generate HPC-specific code.</data>
  <data key="d7">tool development, HPC processing</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="Prompts for Parallel Code Generation">
  <data key="d5">9.0</data>
  <data key="d6">These prompts serve as concrete test cases to evaluate LLMs' proficiency in generating parallel code for various computational problems.</data>
  <data key="d7">testing, code generation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="Prompts">
  <data key="d5">18.0</data>
  <data key="d6">Prompts are instructions provided to LLMs to generate code or responses, serving as the basis for performance evaluation.&lt;SEP&gt;Prompts serve as instructions for LLMs to generate code or responses, essential in code generation evaluation.</data>
  <data key="d7">instruction, evaluation</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="C++">
  <data key="d5">14.0</data>
  <data key="d6">C++ is used as a programming language in prompts to generate code for various parallel programming models, supporting object-oriented and generic programming.&lt;SEP&gt;C++ is used as a programming language in prompts to generate code for various parallel programming models.</data>
  <data key="d7">programming language, code generation</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="CUDA">
  <data key="d5">22.0</data>
  <data key="d6">CUDA is a GPU programming platform used in prompts to generate high-performance code for NVIDIA GPUs.&lt;SEP&gt;CUDA is a GPU programming platform used in prompts to generate parallel code for NVIDIA GPUs.&lt;SEP&gt;CUDA provides GPU acceleration for the generated code, used in performance evaluation.</data>
  <data key="d7">6&lt;SEP&gt;GPU programming, parallel code</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30&lt;SEP&gt;chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="HIP">
  <data key="d5">14.0</data>
  <data key="d6">HIP supports GPU programming for AMD hardware, enabling code generation across different architectures.&lt;SEP&gt;HIP supports GPU programming for AMD hardware, enabling portability and performance across different GPU architectures.</data>
  <data key="d7">GPU programming, cross-platform</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="Kokkos">
  <data key="d5">16.0</data>
  <data key="d6">Kokkos provides a performance-portable programming interface, allowing code to run efficiently on CPUs and GPUs across architectures.&lt;SEP&gt;Kokkos provides a portable interface for parallel programming, allowing code to run efficiently on multiple architectures.</data>
  <data key="d7">performance portability, parallel programming</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="CodeLlama">
  <data key="d5">9.0</data>
  <data key="d6">CodeLlama models are fine-tuned variants of Llama 2, optimized for code generation with support for various sizes and infilling capabilities.</data>
  <data key="d7">code generation, model variants</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="StarCoderBase">
  <data key="d5">9.0</data>
  <data key="d6">StarCoderBase is a large code-focused language model supporting infilling and multiple programming languages, trained on extensive datasets.</data>
  <data key="d7">code modeling, natural language and code dataset</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="AMD MI50 GPU">
  <data key="d5">6.0</data>
  <data key="d6">The AMD MI50 GPU is used to run parallel kernels, contributing to performance testing.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="NVIDIA A100 GPU">
  <data key="d5">6.0</data>
  <data key="d6">The NVIDIA A100 GPU is used for high-performance kernel execution during evaluation.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tools" target="GitHub Copilot">
  <data key="d5">8.0</data>
  <data key="d6">GitHub Copilot, powered by OpenAI Codex, is used as a tool to generate and suggest code implementations for HPC numerical kernels."|</data>
  <data key="d7">tool, AI assistance</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="Copilot">
  <data key="d5">16.0</data>
  <data key="d6">Copilot generates code snippets that assist in implementing HPC and scientific computing tasks across various languages.&lt;SEP&gt;Copilot generates code snippets that assist in implementing HPC and scientific computing tasks across various programming languages.</data>
  <data key="d7">AI code generation, scientific computing</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="numpy">
  <data key="d5">16.0</data>
  <data key="d6">NumPy provides core numerical operations in Python, essential for scientific computing and data analysis.</data>
  <data key="d7">numerical computing, data analysis</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="cuPy">
  <data key="d5">16.0</data>
  <data key="d6">cuPy enables GPU-accelerated array computations in Python, offering compatibility with NumPy for high-performance tasks.</data>
  <data key="d7">GPU acceleration, array computations</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="pyCUDA">
  <data key="d5">16.0</data>
  <data key="d6">pyCUDA allows direct CUDA programming from Python, facilitating custom GPU kernel development.</data>
  <data key="d7">GPU kernel development, CUDA APIs</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="Numba">
  <data key="d5">14.0</data>
  <data key="d6">Numba accelerates Python code via JIT compilation, supporting GPU code generation but with certain hardware limitations.</data>
  <data key="d7">JIT compilation, GPU support</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="JuliaGPU/AMDGPU.jl">
  <data key="d5">14.0</data>
  <data key="d6">JuliaGPU/AMDGPU.jl enables GPU programming in Julia for AMD and other GPUs, supporting high-performance parallel computations.&lt;SEP&gt;JuliaGPU/AMDGPU.jl enables GPU programming in Julia, facilitating high-performance computations on AMD and other GPUs.</data>
  <data key="d7">GPU Programming</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Tools" target="Source-to-Source Compiler">
  <data key="d5">16.0</data>
  <data key="d6">The PPL acts as a source-to-source compiler, transforming high-level DSL code into optimized C++ code compatible with modern compilers."|&gt;"code transformation, portability&lt;SEP&gt;The PPLtool acts as a source-to-source compiler, transforming code to improve portability and optimize performance across hardware."|&gt;"code transformation, portability</data>
  <data key="d7">8</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="C++ Source Code">
  <data key="d5">14.0</data>
  <data key="d6">The output code generated by the PPL toolchain is compatible with C++17 compilers, facilitating broad deployment."|&gt;"compatibility, code generation&lt;SEP&gt;The output of the PPL source-to-source compiler is compatible with C++17 compilers, enabling broad applicability."|&gt;"compatibility, code generation</data>
  <data key="d7">7</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="DSL">
  <data key="d5">16.0</data>
  <data key="d6">The custom DSL used in PPL allows for targeted optimizations beyond syntactical translation, focusing on application-specific tuning."|&gt;"domain-specific optimization, language design&lt;SEP&gt;The custom DSL used in PPL enables application-specific optimizations, allowing targeted code transformations beyond syntactic translation."|&gt;"domain-specific optimization, language design</data>
  <data key="d7">8</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="OpenSBLI">
  <data key="d5">14.0</data>
  <data key="d6">OpenSBLI generates fluid dynamics code optimized for heterogeneous architectures, exemplifying application-specific code generation."|&gt;"application domain, code generation</data>
  <data key="d7">7</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Kempf et al.'s Code Generator">
  <data key="d5">14.0</data>
  <data key="d6">This generator produces code for the discontinuous Galerkin method, enabling vectorization and high-performance numerical computations."|&gt;"numerical methods, code optimization&lt;SEP&gt;This generator produces code for the discontinuous Galerkin method, supporting vectorization and high-performance numerical computation."|&gt;"numerical methods, code optimization</data>
  <data key="d7">7</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Lift">
  <data key="d5">14.0</data>
  <data key="d6">Lift utilizes OpenCL patterns to generate GPU code, supporting heterogeneous hardware and parallelism."|&gt;"GPU code, parallel patterns</data>
  <data key="d7">7</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="YASK">
  <data key="d5">12.0</data>
  <data key="d6">YASK focuses on generating optimized stencil kernels within a DSL framework, targeting performance on various architectures."|&gt;"stencil kernels, performance</data>
  <data key="d7">6</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="SDFGs and the DaCe Tool">
  <data key="d5">18.0</data>
  <data key="d6">SDFGs and DaCe apply rule-based, semi-automatic optimizations to hotspots in computations, aiming to improve overall performance and flexibility."|&gt;"optimization, automation&lt;SEP&gt;SDFGs and DaCe apply rule-based, semi-automatic optimizations to hotspots in computations, aiming to improve performance."|&gt;"optimization, automation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Shared memory parallelism">
  <data key="d5">8.0</data>
  <data key="d6">Shared memory parallelism is supported through POSIX threads, with thread pinning to cores to optimize workload distribution.</data>
  <data key="d7">parallel programming, resource management</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Shared memory">
  <data key="d5">8.0</data>
  <data key="d6">Shared memory enables fast data sharing and synchronization within a device, supporting efficient parallel execution."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Intra-device synchronization">
  <data key="d5">8.0</data>
  <data key="d6">Synchronization within a device, such as a GPU or CPU, to coordinate parallel tasks."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Intra-node synchronization">
  <data key="d5">9.0</data>
  <data key="d6">Synchronization across accelerators and host within a node, managing data exchange and task coordination."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Inter-device synchronization">
  <data key="d5">8.0</data>
  <data key="d6">Synchronization between distributed devices like GPUs, often involving data transfer protocols."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Pinning of workload">
  <data key="d5">7.0</data>
  <data key="d6">Pinning workload to cores or processing units helps optimize resource utilization and reduce overhead."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Parallel Patterns">
  <data key="d5">14.0</data>
  <data key="d6">Parallel patterns abstract parallelism, reducing boilerplate and improving productivity."|</data>
  <data key="d7">Design,Maintainability</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="Next Turn">
  <data key="d5">12.0</data>
  <data key="d6">Next Turn is a platform or suite of tools supporting parallel algorithm development, deployment, and management."|&lt;SEP&gt;Next Turn provides tools or platforms supporting parallel algorithm development and execution."|</data>
  <data key="d7">tools, parallel computing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tools" target="LM Pipelines">
  <data key="d5">8.0</data>
  <data key="d6">Tools such as retrieval models, multimodal models, APIs, and calculators are integrated into language model pipelines to extend their capabilities and perform complex tasks.</data>
  <data key="d7">tool integration, pipeline extension</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Analytical Techniques" target="Evaluation Techniques">
  <data key="d5">9.0</data>
  <data key="d6">Automated evaluation techniques are used to assess correctness and performance across the generated code outputs.</data>
  <data key="d7">performance assessment, correctness verification</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Discipines">
  <data key="d5">7.0</data>
  <data key="d6">Core concepts underpin various disciplines, providing foundational knowledge.</data>
  <data key="d7">fundamental ideas, discipline principles</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Disciplines">
  <data key="d5">6.0</data>
  <data key="d6">Core concepts underpin disciplines, providing foundational knowledge.</data>
  <data key="d7">fundamental ideas, knowledge base</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Domain Tool Augmentation">
  <data key="d5">14.0</data>
  <data key="d6">The use of specialized tools enhances domain-specific task performance but may require strict input formats."|</data>
  <data key="d7">specialized software, domain-specific tasks</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Core Concepts" target="HPCCoder">
  <data key="d5">9.0</data>
  <data key="d6">HPCCoder is a model fine-tuned for HPC code generation, labeling, and performance prediction, serving as a specialized tool in HPC code development.</data>
  <data key="d7">specialization, code generation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Problem Types">
  <data key="d5">9.0</data>
  <data key="d6">Problem types categorize computational tasks, guiding the design of prompts to test specific code generation capabilities.</data>
  <data key="d7">problem categorization, testing scope</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Efficiency n@k">
  <data key="d5">8.0</data>
  <data key="d6">Efficiency n@k measures how effectively generated code utilizes hardware resources, calculated as the ratio of speedup to the number of resources, reflecting resource utilization efficiency.</data>
  <data key="d7">resource utilization, performance efficiency</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Theories/Models">
  <data key="d5">18.0</data>
  <data key="d6">Core Concepts serve as foundational ideas that underpin the development of theories and models in the field.</data>
  <data key="d7">foundational knowledge, theoretical framework</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Core Concepts" target="Fortran">
  <data key="d5">16.0</data>
  <data key="d6">Fortran is central to scientific computing, with legacy codes that are still relevant and support for parallel models like OpenMP and OpenACC.</data>
  <data key="d7">legacy scientific computing, parallel support</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Core Concepts" target="Python">
  <data key="d5">18.0</data>
  <data key="d6">Python's popularity in AI and scientific research makes it a key language, with libraries like NumPy, cuPy, and pyCUDA enabling GPU-accelerated computations.</data>
  <data key="d7">AI, scientific computing, GPU acceleration</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Core Concepts" target="Parallel programming models">
  <data key="d5">8.0</data>
  <data key="d6">Parallel programming models are frameworks and abstractions, such as OpenMP, CUDA, Kokkos, Thrust, and SyCL, that facilitate writing parallel code for various hardware architectures."|&gt;"frameworks for parallelism</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Core Concepts" target="Knowledge-intensive tasks">
  <data key="d5">18.0</data>
  <data key="d6">These tasks depend on retrieving and reasoning over external knowledge sources like Wikipedia."|&lt;SEP&gt;These tasks rely on external knowledge sources like Wikipedia to perform question answering, generation, and fact verification."|</data>
  <data key="d7">task dependence</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Core Concepts" target="Ukrainian People">
  <data key="d5">7.0</data>
  <data key="d6">The Ukrainian People's fearlessness and determination inspire global admiration and support.</data>
  <data key="d7">inspiration, national resilience</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Core Concepts" target="Knowledge transfer from high-resource to low-resource programming languages">
  <data key="d5">14.0</data>
  <data key="d6">This concept explains how knowledge transfer enhances the ability of code language models to generate code in low-resource programming languages by leveraging high-resource language knowledge.&lt;SEP&gt;This concept explains how knowledge transfer improves code generation across different programming languages within code LLMs.</data>
  <data key="d7">theory, transfer learning</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Objects of Study" target="AST">
  <data key="d5">8.0</data>
  <data key="d6">Abstract Syntax Trees (ASTs) are used as a core representation in program synthesis for structure and semantic analysis."|&lt;"program representation, syntax analysis</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Gulwani (2011); Gulwani et al. (2012)">
  <data key="d5">7.0</data>
  <data key="d6">Early datasets like FlashFill and Hearthstone used to benchmark neural program synthesis methods, focusing on specific tasks."|&lt;"benchmark datasets, domain-specific</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Husain et al. (2019)">
  <data key="d5">8.0</data>
  <data key="d6">CodeSearchNet dataset, a large corpus from GitHub for training and evaluating code understanding models."|&lt;"large code corpus, multi-language</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Lu et al. (2021)">
  <data key="d5">7.0</data>
  <data key="d6">CodeXGLUE benchmark suite, aggregating multiple programming benchmarks and metrics like CodeBLEU."|&lt;"benchmark suite, evaluation metrics</data>
  <data key="d7">7</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Hendrycks et al. (2021)">
  <data key="d5">8.0</data>
  <data key="d6">APPS benchmark for assessing functional correctness of code solutions from competitive programming problems."|&lt;"benchmark, competitive programming</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="positive integer n">
  <data key="d5">8.0</data>
  <data key="d6">n is the main object of study, defining the range for counting palindromic numbers.</data>
  <data key="d7">study scope, number range</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Taxonomies">
  <data key="d5">6.0</data>
  <data key="d6">Taxonomies classify objects of study into organized categories.</data>
  <data key="d7">classification, organization</data>
  <data key="d8">chunk-26540abbca06fd8f3d8be2237ce61efe</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Market">
  <data key="d5">8.0</data>
  <data key="d6">The market for code generation tools and machine learning packages is influenced by Codex's suggestions and user preferences.</data>
  <data key="d7">market influence, adoption patterns</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Open-source developers">
  <data key="d5">8.0</data>
  <data key="d6">Developers maintain open-source packages, and their practices are affected by Codex's suggestions, especially regarding deprecated methods and backward compatibility.</data>
  <data key="d7">developer practices, compatibility</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Code Contests Dataset">
  <data key="d5">9.0</data>
  <data key="d6">The code contests dataset provides the source data for training and evaluating models.</data>
  <data key="d7">dataset source, data collection</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Objects of Study" target="Knowledge Base">
  <data key="d5">8.0</data>
  <data key="d6">Structured repositories like Wikidata or Wikipedia used as external sources for retrieval."|</data>
  <data key="d7">knowledge repositories, external sources</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Objects of Study" target="Factual Knowledge">
  <data key="d5">16.0</data>
  <data key="d6">Factual knowledge encompasses verified information that models aim to learn and retrieve."|&lt;SEP&gt;Factual knowledge encompasses verified information that models aim to learn, store, and retrieve accurately."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Objects of Study" target="Bloomberggpt">
  <data key="d5">16.0</data>
  <data key="d6">Bloomberggpt is a finance-focused large language model for generating financial insights.</data>
  <data key="d7">financial application, specialized model</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Objects of Study" target="Parallel Programming Models">
  <data key="d5">14.0</data>
  <data key="d6">Different parallel programming frameworks (MPI, OpenMP, CUDA, etc.) are used as contexts within which generated code is evaluated for correctness and efficiency.&lt;SEP&gt;Different parallel programming models are used within tasks in ParEval to test the models' ability to generate code suited to various frameworks.</data>
  <data key="d7">framework diversity, problem context&lt;SEP&gt;model diversity, problem frameworks</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="COMPCODER">
  <data key="d5">8.0</data>
  <data key="d6">COMPCODER is a trained model on specific programming languages used to generate HPC code, serving as a research object for HPC code synthesis.</data>
  <data key="d7">model training, code synthesis</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="PartialMinimums">
  <data key="d5">16.0</data>
  <data key="d6">PartialMinimums exemplifies a parallel algorithm task used to evaluate model capabilities in code generation.&lt;SEP&gt;The PartialMinimums function is a computational problem used to evaluate model capabilities in parallel prefix sum algorithms.</data>
  <data key="d7">algorithm, evaluation</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Problem Variations">
  <data key="d5">8.0</data>
  <data key="d6">Variations of problems, such as reverse prefix sums, serve to evaluate model adaptability and generalization capabilities.</data>
  <data key="d7">algorithm testing, robustness</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="OpenMP, MPI, MPI+OpenMP, Kokkos, CUDA, HIP">
  <data key="d5">9.0</data>
  <data key="d6">These execution models are the focus of the code translation tasks, representing different parallel programming frameworks.</data>
  <data key="d7">parallel computing, software frameworks</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Parallel Code">
  <data key="d5">16.0</data>
  <data key="d6">Parallel code is the primary object of study, focusing on its performance, scalability, and efficiency in different models.&lt;SEP&gt;Parallel code is the subject of performance evaluation, focusing on how well it leverages hardware resources to achieve speedup and efficiency.</data>
  <data key="d7">code analysis, performance metrics&lt;SEP&gt;performance analysis, scalability assessment</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc&lt;SEP&gt;chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Resources">
  <data key="d5">8.0</data>
  <data key="d6">Resources such as processes or threads are used to execute parallel code, affecting the measured speedup and efficiency.</data>
  <data key="d7">hardware resources, performance evaluation</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Execution Models">
  <data key="d5">15.0</data>
  <data key="d6">Different parallel execution paradigms impact model performance, with serial and OpenMP generally yielding higher success rates."|"&lt;parallel paradigms, performance&lt;SEP&gt;Execution models like MPI, OpenMP, and Kokkos are studied for their compatibility with code translation and performance.</data>
  <data key="d7">8&lt;SEP&gt;programming paradigms, model comparison</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7&lt;SEP&gt;chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="Spatiotemporal Information">
  <data key="d5">16.0</data>
  <data key="d6">Spatiotemporal data pertains to the objects of study, capturing their location and temporal dynamics.</data>
  <data key="d7">spatial-temporal analysis, data modeling</data>
  <data key="d8">chunk-2c69fe176c580c07a9269d7a20e4e011</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="parallel code">
  <data key="d5">8.0</data>
  <data key="d6">Parallel code is the primary object of study, focusing on its generation, translation, performance, and scalability in high-performance computing contexts."|&gt;"performance analysis, scalability assessment</data>
  <data key="d7">8</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="execution models">
  <data key="d5">7.0</data>
  <data key="d6">Execution models like MPI, OpenMP, and Kokkos are studied for their compatibility with code translation, performance, and scalability."|&gt;"programming paradigms, model comparison</data>
  <data key="d7">7</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Objects of Study" target="High-level programming models">
  <data key="d5">8.0</data>
  <data key="d6">High-level programming models like Kokkos, Thrust, and SyCL are abstractions designed to improve portability and productivity in HPC but show varying performance across kernels."|&gt;"performance variation, community support</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Kernel">
  <data key="d5">8.0</data>
  <data key="d6">A kernel is a fundamental computational unit in parallel programming, often representing a function executed on a GPU or accelerator, such as in CUDA or OpenMP."|&gt;"core computational unit</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Legacy HPC applications">
  <data key="d5">8.0</data>
  <data key="d6">Legacy HPC applications are older, established scientific codes often written in Fortran, still relevant and used in current research."|&gt;"legacy codes, scientific relevance</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Benchmark repositories">
  <data key="d5">8.0</data>
  <data key="d6">Benchmark repositories like HecBench are collections of computational kernels and datasets used to evaluate performance and code generation quality."|&gt;"evaluation datasets, performance benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="GitHub Copilot">
  <data key="d5">24.0</data>
  <data key="d6">GitHub Copilot is studied for its code synthesis performance, usability, and security implications, compared to other program synthesis methods like genetic programming.&lt;SEP&gt;GitHub Copilot is the focus of security and performance evaluations, assessing its role as an AI-powered code synthesis tool.&lt;SEP&gt;GitHub Copilot is the subject of security and performance evaluations, focusing on its role as an AI-powered code generator.</data>
  <data key="d7">Code Generation Performance&lt;SEP&gt;Code Synthesis</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Preferred Networks and Inc. Preferred Infrastructure">
  <data key="d5">16.0</data>
  <data key="d6">These organizations develop CuPy, a GPU-accelerated library, impacting scientific computing workflows on Nvidia GPUs.&lt;SEP&gt;These organizations develop and support CuPy, a GPU-accelerated numerical library, impacting scientific computing workflows.</data>
  <data key="d7">Scientific Computing</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Github Copilot">
  <data key="d5">8.0</data>
  <data key="d6">GitHub Copilot is studied for its performance in code synthesis compared to genetic programming, assessing its practical utility.</data>
  <data key="d7">Code Generation Performance</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Code Generation Tools">
  <data key="d5">16.0</data>
  <data key="d6">Tools like Copilot are analyzed for usability, security, and performance in software development workflows.&lt;SEP&gt;Tools such as Copilot are analyzed for their performance, security, and usability in automated software development workflows.</data>
  <data key="d7">Software Engineering</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Objects of Study" target="Knowledge Source">
  <data key="d5">16.0</data>
  <data key="d6">Wikipedia serves as a primary external knowledge source for grounding factual responses in RAG models.</data>
  <data key="d7">source</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Objects of Study" target="Twilight (novel series)">
  <data key="d5">8.0</data>
  <data key="d6">The Twilight series is a primary object of study as a vampire-themed fantasy romance novel series, relevant for publication analysis.</data>
  <data key="d7">literature, thematic analysis</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objects of Study" target="Harper Connelly Mysteries">
  <data key="d5">8.0</data>
  <data key="d6">The Harper Connelly Mysteries is a literary series used as an example in context for publication and genre analysis.</data>
  <data key="d7">literature, genre study</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objects of Study" target="The Dark Heroine">
  <data key="d5">8.0</data>
  <data key="d6">The Dark Heroine series is included as part of the context for analyzing vampire-themed literature and publication history.</data>
  <data key="d7">literature, publication analysis</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objects of Study" target="The Victorians">
  <data key="d5">7.0</data>
  <data key="d6">The Victorian documentary series serves as a cultural and historical background object, relevant for contextual understanding.</data>
  <data key="d7">historical context, media</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objects of Study" target="Jeremy Paxman">
  <data key="d5">8.0</data>
  <data key="d6">Jeremy Paxman is relevant as a biographical figure connected to Victorian history and media.</data>
  <data key="d7">biographical, historical relevance</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objects of Study" target="Figure 11">
  <data key="d5">7.0</data>
  <data key="d6">Figure 11 illustrates the process of multi-hop question generation, serving as an example of methodology in automated reasoning systems.</data>
  <data key="d7">visual aid, methodology</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Disciplines" target="Hardware">
  <data key="d5">8.0</data>
  <data key="d6">The physical hardware infrastructure, including cores and processors, underpins the evaluation of parallel code performance metrics.</data>
  <data key="d7">hardware infrastructure, performance analysis</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Study Populations/Dataset" target="DRB-ML dataset">
  <data key="d5">8.0</data>
  <data key="d6">A dataset aimed at training and evaluating models for data race detection in HPC code.</data>
  <data key="d7">dataset, training, evaluation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Study Populations/Dataset" target="Number of Prompts | P">
  <data key="d5">8.0</data>
  <data key="d6">The set of prompts over which the overall average speedup is computed, representing the dataset for performance evaluation.</data>
  <data key="d7">dataset, performance evaluation</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Study Populations/Dataset" target="&lt;&gt;Various datasets and workloads">
  <data key="d5">7.0</data>
  <data key="d6">Used to evaluate performance, scalability, and portability of HPC techniques across different hardware and problem sizes.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Study Populations/Dataset" target="training set">
  <data key="d5">6.0</data>
  <data key="d6">The dataset used for training, sampling demonstrations, and evaluation of programs.</data>
  <data key="d7">training data, sample selection</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Applications/Implications" target="Labor market">
  <data key="d5">9.0</data>
  <data key="d6">AI code generation could substantially influence employment, wages, and skill requirements in tech fields.</data>
  <data key="d7">labor market impact, automation</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Applications/Implications" target="Impacts on worker productivity, wages, and quality of life">
  <data key="d5">8.0</data>
  <data key="d6">Understanding how AI tools like Codex affect employment conditions and economic well-being.</data>
  <data key="d7">worker impact, economic implications</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Applications/Implications" target="Code Development and Analysis Tools">
  <data key="d5">16.0</data>
  <data key="d6">Automated tools based on LLMs assist in developing, debugging, and optimizing parallel scientific codes.&lt;SEP&gt;Automated tools powered by LLMs can assist in developing, optimizing, and maintaining complex parallel codebases.</data>
  <data key="d7">software development, automation&lt;SEP&gt;software engineering, automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Applications/Implications" target="External Knowledge Integration">
  <data key="d5">16.0</data>
  <data key="d6">Integrating external knowledge enhances model performance, allowing for continuous learning and adaptation."|&lt;SEP&gt;Integrating external knowledge enhances model performance, supports lifelong learning, and allows models to adapt to new or changing information without extensive retraining."|</data>
  <data key="d7">performance improvement, lifelong learning</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications/Implications" target="Question-Answering with Human Feedback">
  <data key="d5">18.0</data>
  <data key="d6">WebGPT demonstrates how human feedback can be used to improve question-answering systems by providing verification and corrections."|&lt;SEP&gt;WebGPT demonstrates the use of human feedback to improve question-answering systems."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications/Implications" target="Browser-Assisted Question-Answering">
  <data key="d5">16.0</data>
  <data key="d6">WebGPT employs browser assistance and human feedback to enhance answer accuracy."|&lt;SEP&gt;WebGPT employs browser assistance combined with human feedback to improve answer correctness and user trust."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications/Implications" target="Interactive Planning">
  <data key="d5">16.0</data>
  <data key="d6">Interactive planning enables multi-task agents with large language models, supporting complex tasks.</data>
  <data key="d7">multi-tasking, agent development</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications/Implications" target="Wolfram Superpowers">
  <data key="d5">16.0</data>
  <data key="d6">Integrates Wolfram Alpha into ChatGPT, enhancing computational and knowledge capabilities.</data>
  <data key="d7">computational power, knowledge integration</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications/Implications" target="Implications for Software Development">
  <data key="d5">7.0</data>
  <data key="d6">The findings imply that while LLMs can assist in some aspects, they are not yet substitutes for expert human programmers in complex parallel computing tasks.</data>
  <data key="d7">practical impact, future development</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Applications/Implications" target="Societal Impact">
  <data key="d5">16.0</data>
  <data key="d6">The societal benefits include more factual and controllable NLP systems, but risks involve potential misuse for misleading content or automation of jobs.</data>
  <data key="d7">impact</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Applications/Implications" target="Trade-offs in model design">
  <data key="d5">18.0</data>
  <data key="d6">Smaller, fine-tuned models can be practical alternatives to larger models, offering a balance of speed, memory efficiency, and performance in HPC contexts.&lt;SEP&gt;Smaller, fine-tuned models can deliver high performance with less memory and faster inference, making them more practical for real-world HPC applications.</data>
  <data key="d7">practicality, efficiency</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Applications/Implications" target="HPC-I NSTRUCT">
  <data key="d5">8.0</data>
  <data key="d6">Findings inform best practices for data collection, synthetic data generation, and model fine-tuning in HPC contexts.</data>
  <data key="d7">application, impact</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Applications/Implications" target="Future HPC Developers">
  <data key="d5">16.0</data>
  <data key="d6">The models and insights developed will directly benefit HPC developers and inform future research into code LLMs for HPC.&lt;SEP&gt;The models and insights will benefit HPC developers and future research into code LLMs for HPC.</data>
  <data key="d7">application impact, research advancement</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="planet2" target="planet_names">
  <data key="d5">12.0</data>
  <data key="d6">planet2 is an element within the list of planet names, used in planetary calculations.</data>
  <data key="d7">reference, data structure</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="planet1_index" target="planet2_index">
  <data key="d5">10.0</data>
  <data key="d6">Indices determine the position of planets in a list, facilitating calculations involving neighboring planets.</data>
  <data key="d7">sequence, indexing</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="count_up_to" target="prime numbers">
  <data key="d5">16.0</data>
  <data key="d6">The count_up_to function generates a list of prime numbers less than a given number, used in mathematical computations.</data>
  <data key="d7">algorithm, prime detection</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="smallest_change" target="array">
  <data key="d5">18.0</data>
  <data key="d6">The smallest_change function computes the minimum edits required to make an array palindromic, relevant in array analysis and correction.</data>
  <data key="d7">algorithm, array manipulation</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in code generation" target="generative models">
  <data key="d5">20.0</data>
  <data key="d6">The analysis describes how models like Codex can encode societal biases, impacting generated code and content.</data>
  <data key="d7">concept, model behavior</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in code generation" target="Harms">
  <data key="d5">22.0</data>
  <data key="d6">Biases in generated code can lead to societal harms such as stereotypes and unfair resource allocation.</data>
  <data key="d7">result, societal impact</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in code generation" target="Review and verification">
  <data key="d5">12.0</data>
  <data key="d6">Bias detection tools are used to assess and mitigate biases in model outputs.</data>
  <data key="d7">tool, bias mitigation</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Review and verification" target="Bias probes">
  <data key="d5">12.0</data>
  <data key="d6">Bias detection tools are used to assess and mitigate biases in model outputs.</data>
  <data key="d7">tool, bias mitigation</data>
  <data key="d8">chunk-18c4f75a0246b1e0b98fe72403b2ee8e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Language Models" target="Prompts">
  <data key="d5">8.0</data>
  <data key="d6">Prompts for classifying protected classes can be leading and may reinforce harmful stereotypes.</data>
  <data key="d7">prompt bias, social bias</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Language Models" target="Bias in Code Generation">
  <data key="d5">9.0</data>
  <data key="d6">Models trained on biased datasets tend to produce code comments or outputs that reflect social prejudices.</data>
  <data key="d7">training bias, model output</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Language Models" target="Bias Reinforcement">
  <data key="d5">9.0</data>
  <data key="d6">Models may inadvertently reinforce societal biases present in their training data, leading to harmful outputs.</data>
  <data key="d7">bias reinforcement, societal impact</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias Analysis in Text" target="Bias in Code Generation">
  <data key="d5">7.0</data>
  <data key="d6">Co-occurrence tests reveal biases in model-generated comments, indicating reinforcement of stereotypes.</data>
  <data key="d7">bias evaluation, bias detection</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias Reinforcement" target="Bias in Model Responses">
  <data key="d5">9.0</data>
  <data key="d6">Models tend to reinforce societal biases present in training data, leading to prejudiced outputs.</data>
  <data key="d7">bias reinforcement, societal impact</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Prompts for Classification" target="Prompts">
  <data key="d5">8.0</data>
  <data key="d6">Prompts designed to classify social categories can be leading and reinforce biases.</data>
  <data key="d7">prompt bias, social bias</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Text Generated by Codex" target="Bias in Model Responses">
  <data key="d5">9.0</data>
  <data key="d6">Codex produces biased comments or code, reflecting societal biases learned during training.</data>
  <data key="d7">bias in output, societal bias</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Text Generated by Codex" target="Bias in Comment Generation">
  <data key="d5">8.0</data>
  <data key="d6">Codex's comments tend to reproduce biases similar to GPT-3 when explicitly prompted about sensitive groups.</data>
  <data key="d7">bias reproduction, prompt sensitivity</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias Detection Methods" target="Bias in Model Responses">
  <data key="d5">7.0</data>
  <data key="d6">Co-occurrence tests reveal biases in generated comments, indicating reinforcement of stereotypes.</data>
  <data key="d7">bias detection, bias measurement</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bias in Model Responses" target="Bias in Out-of-Distribution Usage">
  <data key="d5">6.0</data>
  <data key="d6">Models may produce more biased or harmful outputs when prompted with out-of-distribution inputs, indicating robustness issues.</data>
  <data key="d7">robustness, bias amplification</data>
  <data key="d8">chunk-10b08670a9cf75866c6b05fa5b5cfc12</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Model Limitations" target="Security Risks">
  <data key="d5">10.0</data>
  <data key="d6">Current limitations of Codex in generating effective malicious payloads or discovering complex vulnerabilities, which may improve with future research."|</data>
  <data key="d7">security vulnerabilities, model limitations</data>
  <data key="d8">chunk-bf1121c44634b616af61c49cd3adf29a</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Codex model" target="supply chain risk">
  <data key="d5">16.0</data>
  <data key="d6">Widespread use of Codex could introduce new supply chain vulnerabilities through insecure code generation.&lt;SEP&gt;Widespread use of Codex could introduce supply chain vulnerabilities due to insecure code generation.</data>
  <data key="d7">software security, infrastructure risk</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="cryptographic libraries" target="RSA keys">
  <data key="d5">16.0</data>
  <data key="d6">Insecure RSA keys can be generated by models if trained on or influenced by insecure configurations.&lt;SEP&gt;Insecure RSA keys can be generated if the model learns insecure configurations from training data.</data>
  <data key="d7">cryptography, security risk</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="cryptographic libraries" target="AES contexts">
  <data key="d5">16.0</data>
  <data key="d6">Models may produce insecure AES configurations, such as ECB mode, based on training data patterns.&lt;SEP&gt;Models may produce insecure AES configurations, such as ECB mode, if trained on insecure examples.</data>
  <data key="d7">cryptography, insecure configurations</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="RSA keys" target="security standards">
  <data key="d5">6.0</data>
  <data key="d6">RSA keys shorter than 2048 bits are considered insecure according to current security standards.</data>
  <data key="d7">security thresholds, cryptographic strength</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="AES contexts" target="security standards">
  <data key="d5">8.0</data>
  <data key="d6">Use of ECB mode in AES contexts is considered insecure by cryptography standards.</data>
  <data key="d7">cipher mode, security vulnerabilities</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="cryptographic vulnerabilities" target="cryptography experts">
  <data key="d5">36.0</data>
  <data key="d6">Cryptography experts analyze vulnerabilities in cryptographic code, including insecure configurations produced by models.&lt;SEP&gt;Cryptography experts assess and identify vulnerabilities in cryptographic code, which can be generated insecurely by models.&lt;SEP&gt;Experts assess the security of cryptographic implementations, including those generated by models.&lt;SEP&gt;Experts evaluate and identify cryptographic vulnerabilities in generated code, which models may inadvertently produce.</data>
  <data key="d7">security assessment, cryptography&lt;SEP&gt;security evaluation, cryptography</data>
  <data key="d8">chunk-aa0dc077f44be79e1c2829e5cb3b62c2</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="cryptography experts" target="conﬁgurations">
  <data key="d5">8.0</data>
  <data key="d6">Cryptography experts agree that certain configurations, like short RSA keys and ECB mode, are insecure and should be avoided.</data>
  <data key="d7">expert consensus, security guidelines</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="ECB" target="conﬁgurations">
  <data key="d5">7.0</data>
  <data key="d6">ECB is rarely desired because of its security vulnerabilities, influencing cryptographic configuration choices.</data>
  <data key="d7">security standards, cryptography</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="conﬁgurations" target="security standards">
  <data key="d5">7.0</data>
  <data key="d6">Security standards evolve over time, affecting what cryptographic configurations are considered secure or insecure.</data>
  <data key="d7">standards evolution, cryptographic best practices</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="code generation tools" target="Python">
  <data key="d5">7.0</data>
  <data key="d6">Python's high readability and educational prominence make it a key language for code generation and accessibility, influencing its growth and usage.</data>
  <data key="d7">language popularity, accessibility</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="code generation tools" target="effects on non-engineers">
  <data key="d5">10.0</data>
  <data key="d6">These tools can make programming more accessible to non-engineers and automate repetitive tasks, broadening skill access and changing skill requirements.</data>
  <data key="d7">accessibility, automation</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="programmers and engineers" target="software development tasks">
  <data key="d5">7.0</data>
  <data key="d6">AI tools like Codex can assist with code writing, documentation, testing, and reviews, affecting workflow and roles.</data>
  <data key="d7">task automation, workflow change</data>
  <data key="d8">chunk-d8479e6cf592a5f52ce4d6ea93fb04e9</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="economic implications" target="differential package import rates">
  <data key="d5">8.0</data>
  <data key="d6">Patterns in package import rates influence market dominance, economic rewards, and the robustness of the software ecosystem.</data>
  <data key="d7">market influence, economic effects</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Women" target="Data Science and Analysis Roles">
  <data key="d5">8.0</data>
  <data key="d6">Women are more represented in data science and analysis roles compared to other engineering roles, indicating gender disparities in technical employment.</data>
  <data key="d7">role disparity, gender representation</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Large Language Models Trained on Code" target="Code Generation Tools">
  <data key="d5">9.0</data>
  <data key="d6">Large language models like Codex are designed to generate code, which can automate tasks and influence programming workflows.</data>
  <data key="d7">automation, programming workflows</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Programming Languages" target="Community Trends">
  <data key="d5">7.0</data>
  <data key="d6">Lu, Wu, Sheng, Liu, and Yang analyze language popularity, informing trends in open source software."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Programming Languages" target="Lu, Wu, Sheng, Liu, Yang">
  <data key="d5">7.0</data>
  <data key="d6">They analyze the popularity and trends of programming languages in open source communities."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Python" target="Code Generation Tools">
  <data key="d5">7.0</data>
  <data key="d6">Python's high readability and educational prominence make it a key language for code generation and accessibility.</data>
  <data key="d7">language popularity, accessibility</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Python" target="CUDA">
  <data key="d5">18.0</data>
  <data key="d6">CUDA is supported in Python through libraries like cuPy and pyCUDA, enabling GPU acceleration of Python code for scientific computing.</data>
  <data key="d7">GPU acceleration, Python libraries</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Code Generation Tools" target="Effects on Non-Engineers">
  <data key="d5">10.0</data>
  <data key="d6">These tools can make programming more accessible to non-engineers and automate repetitive tasks, potentially broadening skill bases.</data>
  <data key="d7">accessibility, automation</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Differential Package Import Rates" target="Economic Implications">
  <data key="d5">8.0</data>
  <data key="d6">Variations in how Codex imports packages can influence the software ecosystem, affecting market dominance and economic rewards.</data>
  <data key="d7">market influence, economic effects</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Differential Package Import Rates" target="Safety and Security">
  <data key="d5">9.0</data>
  <data key="d6">Import patterns may lead to errors or vulnerabilities, impacting software safety and security.</data>
  <data key="d7">software safety, security risks</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="How do AI code generation models influence societal inequality and economic structures?">
  <data key="d5">6.0</data>
  <data key="d6">societal impact, economic influence</data>
  <data key="d7">6</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Future directions">
  <data key="d5">8.0</data>
  <data key="d6">Future research will explore Codex's economic impacts, biases, influence on documentation/testing, and societal implications.</data>
  <data key="d7">research scope, societal impact</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Research on code generation and automation">
  <data key="d5">8.0</data>
  <data key="d6">Research aims to quantify benefits, risks, and societal impacts of AI-driven code automation.</data>
  <data key="d7">research focus, societal implications</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="In-Context Learning">
  <data key="d5">14.0</data>
  <data key="d6">In-Context Learning examines how models learn from demonstrations within prompts."|&lt;SEP&gt;In-Context Learning explores how models learn from demonstrations within prompts to perform new tasks."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Applying LLMs to parallel and HPC code">
  <data key="d5">18.0</data>
  <data key="d6">Research explores the effectiveness and adaptation of LLMs in generating and analyzing high-performance computing code, including specific tasks like labeling pragmas and performance prediction.&lt;SEP&gt;Research investigates how well LLMs can generate, label, and optimize HPC code, including specific tasks like labeling OpenMP pragmas and predicting performance metrics.</data>
  <data key="d7">application, domain-specific adaptation&lt;SEP&gt;application, domain-specific tasks</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Munley et al.">
  <data key="d5">7.0</data>
  <data key="d6">Investigate whether LLMs can generate compiler verification tests for parallel HPC code, assessing their utility in compiler validation.</data>
  <data key="d7">verification, testing</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Chen et al.">
  <data key="d5">7.0</data>
  <data key="d6">Examine if LLMs can identify data races in parallel code and contribute to data race detection datasets.</data>
  <data key="d7">race detection, dataset creation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Godoy et al.">
  <data key="d5">6.0</data>
  <data key="d6">Evaluate the capability of LLMs to generate HPC kernels across limited problem sets and compare their performance.</data>
  <data key="d7">performance evaluation, kernel generation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Valero-Lara et al.">
  <data key="d5">6.0</data>
  <data key="d6">Assess whether LLMs can generate HPC kernels effectively using standard practices and limited problem sets.</data>
  <data key="d7">standard evaluation, kernel generation</data>
  <data key="d8">chunk-08927f4020e77ca8974c06ceacee30f7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="model comparison">
  <data key="d5">8.0</data>
  <data key="d6">The comparison aims to identify which LLMs perform best at parallel code generation based on pass@1 scores.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Scalability">
  <data key="d5">7.0</data>
  <data key="d6">The study hypothesizes that models can generate scalable parallel code across different resource counts.</data>
  <data key="d7">performance scalability, hypothesis testing</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="scalability">
  <data key="d5">7.0</data>
  <data key="d6">The research hypothesizes that models can produce scalable parallel code across varying resource counts and execution models."|&gt;"performance scalability, hypothesis testing</data>
  <data key="d7">7</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="Automated Generation of Programming Exercises">
  <data key="d5">16.0</data>
  <data key="d6">This research examines the feasibility and effectiveness of using LLMs to automatically create educational programming content.&lt;SEP&gt;This research explores the potential of LLMs to automatically generate programming exercises and explanations for educational purposes.</data>
  <data key="d7">Educational Technology</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Research Questions/Hypotheses" target="HPC-I NSTRUCT">
  <data key="d5">7.0</data>
  <data key="d6">The study hypothesizes that data quality and model configuration significantly influence the effectiveness of HPC code LLMs.</data>
  <data key="d7">hypothesis, impact</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="users (Stack Overflow, 2020)" target="data source">
  <data key="d5">8.0</data>
  <data key="d6">Provides data on user participation and role distribution, serving as a basis for analyzing representation and role differences.</data>
  <data key="d7">data source, user participation</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="women" target="data science and analysis roles">
  <data key="d5">8.0</data>
  <data key="d6">Women are more represented in data science and analysis roles compared to other engineering roles, indicating gender disparities in tech employment.</data>
  <data key="d7">role disparity, gender representation</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="differential package import rates" target="safety and security">
  <data key="d5">9.0</data>
  <data key="d6">Package import patterns can lead to errors or vulnerabilities, affecting software safety and security.</data>
  <data key="d7">software safety, security risks</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="research questions/hypotheses" target="societal impact">
  <data key="d5">6.0</data>
  <data key="d6">Investigates how AI code generation influences societal inequality, labor markets, and economic power structures.</data>
  <data key="d7">societal impact, economic influence</data>
  <data key="d8">chunk-706ca7a8d3fc8406a24b8328b849606b</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="research questions/hypotheses" target="future research">
  <data key="d5">9.0</data>
  <data key="d6">Future research aims to investigate the economic value, impacts on documentation, worker productivity, barriers to entry, and societal implications of Codex and similar systems.</data>
  <data key="d7">research focus, societal impact</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="long-run labor market" target="AI code generation technologies">
  <data key="d5">10.0</data>
  <data key="d6">Advancements in AI code generation could substantially affect the labor market, wages, and job quality for programmers and engineers.</data>
  <data key="d7">labor market impact, automation</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="deployment scenarios" target="research on code generation and automation">
  <data key="d5">8.0</data>
  <data key="d6">Studying different deployment scenarios helps understand the effects of Codex on productivity, code quality, and economic outcomes.</data>
  <data key="d7">study design, impact analysis</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Users" target="Prompt engineering">
  <data key="d5">7.0</data>
  <data key="d6">Users apply prompt engineering techniques to guide Codex's outputs, shaping the relevance and accuracy of suggestions.</data>
  <data key="d7">user interaction, output control</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Prompt engineering" target="Augmentation">
  <data key="d5">18.0</data>
  <data key="d6">Prompt engineering is a method of augmentation that involves crafting input prompts to guide the model's outputs toward domain relevance."|&lt;SEP&gt;Prompt engineering is a method of augmentation that involves designing input prompts to steer the model towards domain-specific responses."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training data" target="Biases in model outputs">
  <data key="d5">9.0</data>
  <data key="d6">Biases present in training data are reflected in Codex's suggestions, leading to biased recommendations that may entrench certain packages.</data>
  <data key="d7">data bias, model bias</data>
  <data key="d8">chunk-735818e62b066d03c2144cdb5db162c1</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Models" target="Training Problems">
  <data key="d5">16.0</data>
  <data key="d6">Training problems serve as datasets for fine-tuning language models like Codex or GPT-Neo.&lt;SEP&gt;Training problems serve as datasets to fine-tune models like Codex and GPT-Neo for better code generation.</data>
  <data key="d7">model training, dataset&lt;SEP&gt;training data, model improvement</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Models" target="Reference Solution">
  <data key="d5">18.0</data>
  <data key="d6">Reference solutions are used as target outputs during supervised fine-tuning of models.&lt;SEP&gt;Reference solutions are used as targets during model fine-tuning to guide correct output generation.</data>
  <data key="d7">training targets, supervised learning</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Models" target="OpenMP Prediction Tests">
  <data key="d5">9.0</data>
  <data key="d6">The models are evaluated through experiments that measure their ability to generate correct pragmas and reproduce them exactly, assessing their understanding and learning trends.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models" target="Functionally Correct Pragmas">
  <data key="d5">8.0</data>
  <data key="d6">High accuracy in generating functionally correct pragmas indicates strong comprehension of parallelization requirements.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models" target="Exact Reproduction">
  <data key="d5">7.0</data>
  <data key="d6">Models' ability to reproduce pragmas exactly reflects learned patterns in pragma construction and ordering.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models" target="Performance Prediction">
  <data key="d5">8.0</data>
  <data key="d6">Models can predict performance impacts of code changes, demonstrating their understanding of performance-related properties.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models" target="Evaluation">
  <data key="d5">18.0</data>
  <data key="d6">Evaluation compares different LLMs' ability to generate code based on prompts, using metrics to assess performance.&lt;SEP&gt;Evaluation involves comparing the performance of different LLMs in code generation tasks using metrics like pass@1 and success rates.</data>
  <data key="d7">performance assessment, benchmarking</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Models" target="Open-source models">
  <data key="d5">14.0</data>
  <data key="d6">Open-source models like Phind-V2 are compared against proprietary models to assess relative performance in code generation tasks.&lt;SEP&gt;Open-source models like Phind-V2 are compared with closed-source models to assess relative strengths and weaknesses in code generation."|"&lt;model comparison, open vs closed source</data>
  <data key="d7">7&lt;SEP&gt;model comparison, open-source vs closed-source</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Models" target="Kokkos">
  <data key="d5">12.0</data>
  <data key="d6">Kokkos influences how models generate code for high-level parallel abstractions, affecting their success rate."|"&lt;code generation, abstraction&lt;SEP&gt;Kokkos is a parallel programming model that influences how models generate code for high-level parallel constructs.</data>
  <data key="d7">6&lt;SEP&gt;code generation, abstraction</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Models" target="OpenMP">
  <data key="d5">14.0</data>
  <data key="d6">OpenMP's similarity to serial code facilitates easier code generation by models, resulting in higher accuracy."|"&lt;ease of code generation, similarity to serial&lt;SEP&gt;OpenMP's similarity to serial code makes it easier for models to generate correct code for OpenMP-based parallelization.</data>
  <data key="d7">7&lt;SEP&gt;ease of code generation, similarity to serial</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Models" target="CUDA/HIP">
  <data key="d5">12.0</data>
  <data key="d6">Models tend to perform less well on CUDA/HIP problems compared to serial or OpenMP, possibly due to complexity and training data limitations."|"&lt;performance variation, GPU frameworks&lt;SEP&gt;Models tend to perform less well on CUDA/HIP problems compared to serial or OpenMP, possibly due to complexity or training data limitations.</data>
  <data key="d7">6&lt;SEP&gt;performance variation, GPU frameworks</data>
  <data key="d8">chunk-0c18728de6eaa2dba619a8f9da01a39a</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Models" target="Label Accuracy">
  <data key="d5">7.0</data>
  <data key="d6">Label accuracy evaluates how well models classify claims correctly in FEVER.</data>
  <data key="d7">performance evaluation</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Generation and Classification Test Scores">
  <data key="d5">7.0</data>
  <data key="d6">Evaluation of models' performance using metrics like Bleu, Rouge-L, and accuracy scores."|</data>
  <data key="d7">performance metrics</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Jeopardy Question Generation">
  <data key="d5">7.0</data>
  <data key="d6">Models generate questions in the style of Jeopardy to evaluate generative capabilities."|</data>
  <data key="d7">generation, evaluation</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Generation Diversity">
  <data key="d5">7.0</data>
  <data key="d6">Generation diversity is assessed by comparing the ratios of distinct ngrams among models like RAG and BART.</data>
  <data key="d7">model comparison, diversity metrics</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Retrieval Ablations">
  <data key="d5">8.0</data>
  <data key="d6">Ablation studies assess the impact of freezing or replacing the retriever component in RAG models.</data>
  <data key="d7">model ablation, retrieval effectiveness</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Retrieval Ablation Studies">
  <data key="d5">8.0</data>
  <data key="d6">Ablation studies where the retriever component is frozen or replaced show the impact on retrieval and verification performance.</data>
  <data key="d7">experimental evaluation, retrieval impact</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Retrieval Index">
  <data key="d5">14.0</data>
  <data key="d6">The retrieval index stores evidence documents that can be hot-swapped to update the model without retraining.</data>
  <data key="d7">system-structure</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models" target="Code generation performance">
  <data key="d5">8.0</data>
  <data key="d6">The performance of HPC-Coder-V2 and other models is assessed based on their ability to generate correct, efficient code across different problem types and parallel models.</data>
  <data key="d7">performance assessment, model capability</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Models" target="Speed and memory efficiency">
  <data key="d5">8.0</data>
  <data key="d6">HPC-Coder-V2-1.3B is the fastest and most memory-efficient, yet still outperforms comparable models in code correctness, highlighting a balance between speed and accuracy.</data>
  <data key="d7">efficiency, performance</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Kulal et al. (2019)" target="Methods/Models">
  <data key="d5">8.0</data>
  <data key="d6">SPoC system produces functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics."|&lt;"program correctness, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evaluation" target="Common Framework">
  <data key="d5">14.0</data>
  <data key="d6">Evaluation assesses the performance of the adapted model and guides further refinements."|&lt;SEP&gt;Evaluation is the final stage, where the model's performance is tested and refined based on benchmarks and feedback."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Evaluation" target="Scientific Kernels for HPC">
  <data key="d5">20.0</data>
  <data key="d6">The evaluation assesses the correctness, performance, and suitability of generated kernels for HPC applications."|&gt;"Assessment&lt;SEP&gt;The evaluation assesses the generated kernels' correctness, performance, and suitability for HPC applications."|&gt;"Assessment</data>
  <data key="d7">10</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Evaluation" target="Prompt Engineering">
  <data key="d5">14.0</data>
  <data key="d6">Prompt engineering impacts the quality of generated code, influencing the success of HPC kernel development."|&gt;"Method-Impact&lt;SEP&gt;Prompt engineering impacts the quality of generated code, influencing the success of kernel generation and optimization."|&gt;"Method-Impact</data>
  <data key="d7">7</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Evaluation" target="Metrics for Quality">
  <data key="d5">16.0</data>
  <data key="d6">Metrics are used to quantify the quality, correctness, and trade-offs of generated code outputs."|&gt;"Assessment&lt;SEP&gt;Metrics are used to quantitatively score the output quality, guiding improvements in model prompts and configurations."|&gt;"Assessment</data>
  <data key="d7">8</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Evaluation" target="Code Generator">
  <data key="d5">16.0</data>
  <data key="d6">Evaluation assesses the effectiveness of the code generator in producing optimized code compared to benchmarks.</data>
  <data key="d7">performance benchmarking, validation</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Evaluation" target="Comparison of Hand-Written and Generated Code">
  <data key="d5">18.0</data>
  <data key="d6">This comparison evaluates the performance and correctness of automatically generated code against manually optimized code.</data>
  <data key="d7">performance comparison, validation</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Evaluation" target="System Environment">
  <data key="d5">14.0</data>
  <data key="d6">The hardware and system environment influence the measurement of code performance during evaluation.</data>
  <data key="d7">hardware dependency, performance measurement</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Evaluation" target="Self">
  <data key="d5">8.0</data>
  <data key="d6">Evaluation involves assessing the program's performance using the score metric.</data>
  <data key="d7">performance assessment, evaluation process</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Metrics" target="Taxonomy">
  <data key="d5">18.0</data>
  <data key="d6">The initial taxonomy aids in evaluating AI result accuracy and trustworthiness, but needs expansion for broader community adoption.&lt;SEP&gt;The proposed taxonomy aids in evaluating the accuracy and trustworthiness of AI-generated code, but needs expansion for wider adoption.</data>
  <data key="d7">evaluation framework, trust assessment&lt;SEP&gt;evaluation framework, trustworthiness</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Study Design" target="Note">
  <data key="d5">7.0</data>
  <data key="d6">Clarifies the constraints and expectations for the function, such as input limits and output format, guiding correct implementation.</data>
  <data key="d7">constraints, clarification</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Study Design" target="reproducibility">
  <data key="d5">8.0</data>
  <data key="d6">Ensuring that experiments can be reliably repeated across different conditions, supporting scientific rigor in modular pipeline evaluation."|</data>
  <data key="d7">experimental reliability</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Python Package Index" target="Source of Code">
  <data key="d5">10.0</data>
  <data key="d6">PyPI provides code modules and packages used as sources for problem objects and training data.&lt;SEP&gt;PyPI provides code objects that can be used for problem creation and model training.</data>
  <data key="d7">code sourcing, libraries</data>
  <data key="d8">chunk-18f91435491c87cd10d4cbb056061eda</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Manual Grading" target="evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Manual grading of generated docstrings assesses their correctness and completeness, compensating for the lack of automatic metrics.</data>
  <data key="d7">evaluation method, subjective assessment</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Manual Grading" target="Generated Docstrings">
  <data key="d5">8.0</data>
  <data key="d6">Manual grading assesses the correctness of generated docstrings, compensating for the lack of automatic evaluation.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="evaluation" target="programming frameworks">
  <data key="d5">16.0</data>
  <data key="d6">Frameworks are evaluated based on their efficiency, intuitiveness, and ability to facilitate modular program development for language models.</data>
  <data key="d7">software evaluation, performance metrics</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="evaluation" target="modules">
  <data key="d5">18.0</data>
  <data key="d6">Modules like Predict, ChainOfThought, and MultiChainComparison are evaluated for their effectiveness in reasoning tasks and their impact on accuracy.</data>
  <data key="d7">performance evaluation, reasoning</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="evaluation" target="GSM8K dataset">
  <data key="d5">16.0</data>
  <data key="d6">The dataset is used to assess the accuracy of reasoning modules and programs on math word problems.</data>
  <data key="d7">benchmark, task-specific evaluation</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Performance" target="Scheduling">
  <data key="d5">8.0</data>
  <data key="d6">Scheduling strategies impact overall performance by optimizing resource utilization and execution order."|&gt;"performance optimization</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Data Transfers">
  <data key="d5">7.0</data>
  <data key="d6">Data movement affects runtime and efficiency, influencing optimization decisions."|&gt;"performance factor</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Synchronization">
  <data key="d5">7.0</data>
  <data key="d6">Synchronization points can introduce overhead but are necessary for correctness, affecting performance."|&gt;"performance trade-off</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Dataset Size">
  <data key="d5">7.0</data>
  <data key="d6">Dataset size influences the effectiveness of optimization and hardware resource utilization."|&gt;"performance dependency</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Benchmark Results">
  <data key="d5">8.0</data>
  <data key="d6">Benchmark data shows the impact of optimization techniques on runtime and speedup."|&gt;"performance evaluation</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Hardware Resources">
  <data key="d5">8.0</data>
  <data key="d6">Utilization of CPU cores, GPUs, and nodes determines the potential speedup and scalability."|&gt;"resource impact</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance" target="Model Choice">
  <data key="d5">7.0</data>
  <data key="d6">The choice of base model affects the model's capacity to learn from synthetic data and generate parallel code effectively.</data>
  <data key="d7">model selection, learning capacity</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Performance" target="Comparison with other models">
  <data key="d5">8.0</data>
  <data key="d6">HPC-Coder-V2 models outperform some larger models in parallel code generation, demonstrating the effectiveness of their training strategies despite smaller size.</data>
  <data key="d7">performance superiority, training strategy</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Performance" target="7B Model">
  <data key="d5">14.0</data>
  <data key="d6">The 7B model achieves performance similar to larger models while generating tokens faster, highlighting efficiency benefits.&lt;SEP&gt;The 7B model achieves similar performance to larger models while generating tokens faster, indicating efficiency.</data>
  <data key="d7">model efficiency, performance comparison</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Performance" target="HPC-Coder Models">
  <data key="d5">18.0</data>
  <data key="d6">The HPC-Coder models (V2-1.3B, V2-6.7B, V2-16B) outperform other open-source models in generating parallel code, with faster speed and lower memory use.&lt;SEP&gt;The fine-tuned HPC-Coder models outperform other open-source models in generating parallel code and are faster and more memory-efficient.</data>
  <data key="d7">model evaluation, efficiency</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Generated Code" target="Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">The quality of generated code is measured by pass@k and manual evaluation, reflecting correctness and usefulness.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9ff3c43f969888578a1b963a36023834</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Generated Code" target="Generated">
  <data key="d5">16.0</data>
  <data key="d6">The code evaluated (generated by a toolchain) is compared against handwritten and baseline versions to assess performance and correctness."|&lt;SEP&gt;The code evaluated is generated by a toolchain, compared against handwritten and baseline versions."|</data>
  <data key="d7">Tools, Code Generation, Evaluation&lt;SEP&gt;Tools, Code Generation, Performance Evaluation</data>
  <data key="d8">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance Metrics" target="Equation (2)">
  <data key="d5">8.0</data>
  <data key="d6">Equation (2) mathematically models the expected maximum speedup of generated code based on sample runtimes and resources, providing insight into parallelization efficiency.</data>
  <data key="d7">modeling, performance estimation</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Metrics" target="Speedup max@k">
  <data key="d5">8.0</data>
  <data key="d6">Speedup max@k provides an upper bound on speedup across resource configurations, indicating peak potential of generated code.</data>
  <data key="d7">performance ceiling, peak performance</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Metrics" target="DSPy Program">
  <data key="d5">7.0</data>
  <data key="d6">The effectiveness of DSP Y pipelines is evaluated and improved based on specific metrics like accuracy gains and task success rates.</data>
  <data key="d7">evaluation, optimization</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Theories/Models" target="Domain Knowledge">
  <data key="d5">16.0</data>
  <data key="d6">Domain knowledge includes concepts, principles, facts, and patterns specific to a field, which can be explicit or implicit."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Explicit Knowledge">
  <data key="d5">16.0</data>
  <data key="d6">Explicit knowledge is structured, clearly defined information that can be directly retrieved and used by models."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Implicit Knowledge">
  <data key="d5">16.0</data>
  <data key="d6">Implicit knowledge is latent, embedded within data or systems, influencing model behavior without direct expression."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Pointer Sentinel Mixture Models">
  <data key="d5">14.0</data>
  <data key="d6">Pointer sentinel mixture models are a neural architecture designed to improve language modeling by combining pointer mechanisms with sentinel tokens."|&lt;SEP&gt;Pointer sentinel mixture models are a specific neural architecture designed to improve language modeling."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Augmented Language Models">
  <data key="d5">12.0</data>
  <data key="d6">Augmented language models are enhanced models integrating additional modules or data."|&lt;SEP&gt;Augmented language models enhance basic models with additional modules or data sources to improve performance and capabilities."|</data>
  <data key="d7">6</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Large Pre-trained Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Large pre-trained language models serve as foundational architectures in NLP."|&lt;SEP&gt;Large pre-trained models serve as foundational architectures trained on vast datasets for various NLP tasks."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Theories/Models" target="Execution Models">
  <data key="d5">16.0</data>
  <data key="d6">Different execution models represent paradigms for parallel and distributed programming, influencing code generation and performance.&lt;SEP&gt;Different execution paradigms like serial, OpenMP, MPI, CUDA, HIP, and Kokkos define how code is executed in parallel or distributed environments.</data>
  <data key="d7">parallel computing, programming paradigms</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Theories/Models" target="Llama 2">
  <data key="d5">9.0</data>
  <data key="d6">Llama 2 is the foundational architecture supporting models like CodeLlama, which are fine-tuned for code generation.</data>
  <data key="d7">model architecture, code fine-tuning</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Theories/Models" target="Gerry Tesauro">
  <data key="d5">8.0</data>
  <data key="d6">Gerry Tesauro developed or contributed to the theory of memory networks, which are used to improve reasoning and memory in NLP models.</data>
  <data key="d7">theoretical contribution, model</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Hindle et al. (2012)" target="Language Models">
  <data key="d5">6.0</data>
  <data key="d6">Investigated predictability of code using n-gram models, showing code is more predictable than natural language, aiding in code modeling.</data>
  <data key="d7">predictability, language modeling</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Hindle et al. (2012)" target="Research Studies">
  <data key="d5">6.0</data>
  <data key="d6">Investigated code predictability using n-gram language models, showing code is more predictable than natural language."|&lt;"predictability, language modeling</data>
  <data key="d7">6</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Language Models" target="Adapters">
  <data key="d5">16.0</data>
  <data key="d6">Adapters are inserted into language models to enable efficient domain adaptation without retraining the entire model.</data>
  <data key="d7">modular adaptation, efficiency</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="Parameter-Efficient Fine-Tuning">
  <data key="d5">18.0</data>
  <data key="d6">Parameter-efficient fine-tuning employs adapters to adapt large language models with fewer trainable parameters.</data>
  <data key="d7">training efficiency, domain adaptation</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="K-adapters">
  <data key="d5">16.0</data>
  <data key="d6">K-adapters inject knowledge learned from multiple domains into language models via concatenation, enhancing domain-specific performance.</data>
  <data key="d7">knowledge transfer, domain adaptation</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="Zero-Shot Reasoning">
  <data key="d5">14.0</data>
  <data key="d6">Research explores the ability of large language models to perform reasoning without explicit task-specific training, testing their generalization capacity.&lt;SEP&gt;Research investigates the reasoning capabilities of large language models without task-specific training.</data>
  <data key="d7">reasoning, generalization</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="arXiv">
  <data key="d5">18.0</data>
  <data key="d6">arXiv hosts preprints on language models, providing early research evidence and developments in the field.</data>
  <data key="d7">publication platform, research dissemination</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="Rationale-Augmented Ensembles">
  <data key="d5">16.0</data>
  <data key="d6">These ensembles incorporate rationales to improve reasoning and interpretability in language models.</data>
  <data key="d7">explainability, reasoning improvement</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="Self-Instruct">
  <data key="d5">16.0</data>
  <data key="d6">Self-Instruct aligns models with self-generated instructions to enhance instruction following.</data>
  <data key="d7">alignment, instruction tuning</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Language Models" target="Unsupervised Multitask Learners">
  <data key="d5">8.0</data>
  <data key="d6">Language models are described as unsupervised multitask learners capable of handling multiple NLP tasks without explicit supervision, demonstrating versatility in language understanding.</data>
  <data key="d7">model capability, multitask learning</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Language Models" target="Knowledge Packing in Language Models">
  <data key="d5">6.0</data>
  <data key="d6">Research investigates how much knowledge can be embedded within the parameters of language models, affecting their performance and capacity.</data>
  <data key="d7">model capacity, information embedding</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Language Models" target="Social Impacts of Language Models">
  <data key="d5">7.0</data>
  <data key="d6">The social impacts are analyzed in relation to how release strategies influence societal perceptions, ethical considerations, and potential risks.</data>
  <data key="d7">social implications, ethics</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Language Models" target="DSP Y">
  <data key="d5">9.0</data>
  <data key="d6">DSP Y is designed to build, optimize, and execute pipelines involving various language models for complex NLP tasks.</data>
  <data key="d7">tool integration, pipeline design</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Large Transformers" target="Models/Techniques">
  <data key="d5">9.0</data>
  <data key="d6">Large-scale transformer models are applied to program synthesis, leveraging extensive datasets for improved code generation.</data>
  <data key="d7">transformer models, large-scale training</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Models/Techniques" target="Devlin et al. (2018); Radford et al. (2019); Liu et al. (2019); Raffel et al. (2020); Brown et al. (2020)">
  <data key="d5">9.0</data>
  <data key="d6">Large-scale transformer models trained on vast datasets for code understanding and generation, leveraging natural language processing advances."|&lt;"transformer models, large-scale training</data>
  <data key="d7">9</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Models/Methods" target="SPoC">
  <data key="d5">8.0</data>
  <data key="d6">SPoC produces functionally correct code from pseudocode within a fixed compilation budget, related to pass@k metrics.</data>
  <data key="d7">code correctness, program synthesis</data>
  <data key="d8">chunk-ba9b42eee12401dc9fac455e631a8fd8</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Barone, A. V. M. and Sennrich, R" target="parallel corpus">
  <data key="d5">7.0</data>
  <data key="d6">A dataset of Python functions and documentation strings used for training and evaluating code generation models.</data>
  <data key="d7">objects of study, application</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Barrington, I. M. and Maciel, A" target="lecture on nondeterministic computation">
  <data key="d5">5.0</data>
  <data key="d6">Educational material explaining nondeterministic computation concepts.</data>
  <data key="d7">study design, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S" target="Dangers of stochastic parrots">
  <data key="d5">10.0</data>
  <data key="d6">The paper discusses risks associated with large language models, including biases and ethical concerns.</data>
  <data key="d7">content, discipline</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Bureau of Labor Statistics, U. D. o. L" target="Occupational data">
  <data key="d5">8.0</data>
  <data key="d6">Provides official statistics and occupational outlooks for roles like computer programmers and software developers.</data>
  <data key="d7">objects of study, application</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C" target="Training data extraction">
  <data key="d5">9.0</data>
  <data key="d6">Research on techniques for extracting training data from large language models, relevant to model interpretability and security.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I" target="Generative pretraining">
  <data key="d5">8.0</data>
  <data key="d6">Pretraining from pixels enables models to generate images, demonstrating advances in generative modeling.</data>
  <data key="d7">application, core concept</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Child, R., Gray, S., Radford, A., and Sutskever, I" target="Long sequence generation">
  <data key="d5">8.0</data>
  <data key="d6">Sparse transformers facilitate efficient long sequence generation, impacting NLP and sequence modeling.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="AI alignment" target="Christiano, P">
  <data key="d5">9.0</data>
  <data key="d6">Provides clarification on AI alignment issues, contributing to the discourse on safe and aligned AI development.</data>
  <data key="d7">content, discipline</data>
  <data key="d8">chunk-54d0e5495bc4b24b68d3cbc1c13d835e</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Maddison and Tarlow" target="Structured generative models">
  <data key="d5">14.0</data>
  <data key="d6">They discuss models that generate natural source code based on structured probabilistic frameworks.</data>
  <data key="d7">generative modeling, source code</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Data center energy-use estimates" target="Masanet et al.">
  <data key="d5">16.0</data>
  <data key="d6">Masanet et al. (2020) recalibrated estimates of energy consumption by data centers on a global scale.&lt;SEP&gt;Masanet et al. (2020) recalibrated estimates of global energy consumption by data centers, providing updated insights.</data>
  <data key="d7">energy consumption, data centers&lt;SEP&gt;energy, data centers</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="High-fidelity images" target="Menick and Kalchbrenner">
  <data key="d5">12.0</data>
  <data key="d6">They introduced models for generating high-quality images using subscale pixel networks and upscaling techniques.&lt;SEP&gt;They introduced techniques for generating high-quality images with subscale pixel networks.</data>
  <data key="d7">image generation, deep learning&lt;SEP&gt;image synthesis, deep learning</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Mikolov et al." target="Distributed representations of words">
  <data key="d5">7.0</data>
  <data key="d6">Mikolov et al. (2013) pioneered distributed word embeddings capturing semantic and syntactic properties.</data>
  <data key="d7">word embeddings, NLP</data>
  <data key="d8">chunk-c97846857e436f3dd2a3ffc454f7bc5d</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K" target="Research">
  <data key="d5">18.0</data>
  <data key="d6">Authors of foundational neural network models and learning techniques, contributing to AI advancements."|&lt;SEP&gt;Authors of foundational neural network models and representation learning techniques, contributing to AI research."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="O’Neill, M. and Spector, L">
  <data key="d5">15.0</data>
  <data key="d6">Authors discussing open issues in automatic programming and the challenges in evolving programs automatically."|&lt;SEP&gt;Authors discussing open issues in automatic programming, highlighting challenges and methodologies in evolving programs.</data>
  <data key="d7">8&lt;SEP&gt;automatic programming, research challenges</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J">
  <data key="d5">9.0</data>
  <data key="d6">Authors analyzing the environmental impact of large neural network training, emphasizing carbon footprint concerns.</data>
  <data key="d7">environmental impact, neural network training</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L">
  <data key="d5">18.0</data>
  <data key="d6">Authors developing deep contextualized word representations to enhance NLP models."|&lt;SEP&gt;Developers of deep contextualized word representations, advancing NLP understanding."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N">
  <data key="d5">18.0</data>
  <data key="d6">Authors exploring neural program learning via recursive tree search and planning, contributing to program synthesis methodologies."|&lt;SEP&gt;Authors exploring neural program learning, including recursive tree search and planning techniques."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I">
  <data key="d5">18.0</data>
  <data key="d6">Authors demonstrating that large language models can perform multiple NLP tasks without supervision, in an unsupervised, multitask manner."|&lt;SEP&gt;Authors showing that large language models can perform multiple NLP tasks in an unsupervised manner."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I">
  <data key="d5">18.0</data>
  <data key="d6">Developers of zero-shot text-to-image generation models, enabling image creation from textual prompts."|&lt;SEP&gt;Developers of zero-shot text-to-image models that generate images from textual prompts."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Research" target="Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I">
  <data key="d5">9.0</data>
  <data key="d6">Authors of large-scale language models trained via generative pre-training, greatly advancing NLP."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-b089b4639ea075693b83629858196628</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Transformers" target="Attention Mechanism">
  <data key="d5">18.0</data>
  <data key="d6">Attention is a core component of transformer models, enabling them to weigh input parts dynamically for better context understanding.&lt;SEP&gt;Transformers utilize attention mechanisms as their fundamental building block, allowing efficient processing of sequential data.</data>
  <data key="d7">model architecture, sequence processing&lt;SEP&gt;neural attention, context modeling</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Transformers" target="Thomas Wolf">
  <data key="d5">10.0</data>
  <data key="d6">Thomas Wolf and colleagues describe transformers as the state-of-the-art architecture for NLP tasks.</data>
  <data key="d7">architecture overview, NLP</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Learning Bug-Fixing Patches" target="Neural Machine Translation">
  <data key="d5">16.0</data>
  <data key="d6">Applying neural machine translation techniques to learn bug-fixing patches from real-world data connects neural models with practical software maintenance and automation."|&lt;SEP&gt;The study applies neural machine translation techniques to learn bug-fixing patches from real-world data, connecting neural models with practical software maintenance."|</data>
  <data key="d7">software engineering, neural translation</data>
  <data key="d8">chunk-861fc8218eac993fbe13b418ec4b837c</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="The function returns the computed result when the condition ifb &lt; 0 is false, providing the main output." target="return result">
  <data key="d5">6.0</data>
  <data key="d6">output, calculation</data>
  <data key="d7">6</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evidence" target="Example 1">
  <data key="d5">8.0</data>
  <data key="d6">Provides a specific case demonstrating the function's expected output for input 3, validating correctness.</data>
  <data key="d7">validation, example</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evidence" target="Example 2">
  <data key="d5">8.0</data>
  <data key="d6">Shows the function's output for input 12, further supporting the function's accuracy.</data>
  <data key="d7">validation, example</data>
  <data key="d8">chunk-758b1575f8afe059a0beaf2570df4bd6</data>
  <data key="d9">Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf</data>
</edge>
<edge source="Evidence" target="Claim">
  <data key="d5">8.0</data>
  <data key="d6">Evidence supports or refutes claims, forming the basis for classification.</data>
  <data key="d7">support, refutation</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Evidence" target="Retrieval">
  <data key="d5">8.0</data>
  <data key="d6">Retrieval fetches relevant evidence from Wikipedia to support claim classification.</data>
  <data key="d7">information retrieval</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Evidence" target="Entailment Reasoning">
  <data key="d5">8.0</data>
  <data key="d6">Entailment reasoning analyzes evidence to determine support or refutation of claims.</data>
  <data key="d7">reasoning, inference</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="HPC-Coder" target="Performance Modeling">
  <data key="d5">9.0</data>
  <data key="d6">HPC-Coder applies large language models to predict and analyze performance changes in scientific codes, enabling automated performance insights.</data>
  <data key="d7">performance prediction, automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="Code Completion">
  <data key="d5">8.0</data>
  <data key="d6">HPC-Coder can auto-complete functions in HPC codes, assisting developers in coding tasks.</data>
  <data key="d7">code automation, developer assistance</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="Decorate Loops with OpenMP Pragmas">
  <data key="d5">8.0</data>
  <data key="d6">HPC-Coder automatically decorates loops with OpenMP pragmas to facilitate parallel execution.</data>
  <data key="d7">parallelization, code automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="Fine-tuned on HPC and scientific code datasets">
  <data key="d5">8.0</data>
  <data key="d6">The model is trained on a curated dataset of HPC and scientific source codes to enhance its domain-specific capabilities.</data>
  <data key="d7">training data, domain adaptation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="HPC and scientific codes">
  <data key="d5">16.0</data>
  <data key="d6">HPC-Coder is trained on HPC and scientific codes to improve modeling and generation tasks.&lt;SEP&gt;HPC-Coder is trained on HPC and scientific codes to perform code generation, labeling, and performance prediction tasks effectively."|</data>
  <data key="d7">training data, domain specialization&lt;SEP&gt;training data, domain specificity</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="OpenMP pragmas">
  <data key="d5">18.0</data>
  <data key="d6">HPC-Coder can accurately label OpenMP pragmas, aiding in parallel code analysis."|&lt;SEP&gt;HPC-Coder can label OpenMP pragmas with 97% accuracy, aiding in parallel code analysis."|</data>
  <data key="d7">accuracy, parallel programming</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC-Coder" target="performance prediction">
  <data key="d5">20.0</data>
  <data key="d6">The model predicts relative performance of code changes with high accuracy, supporting performance optimization."|&lt;SEP&gt;The model predicts relative performance of source code changes with up to 92% accuracy, demonstrating its utility in performance modeling."|</data>
  <data key="d7">performance estimation, optimization&lt;SEP&gt;performance prediction, evaluation</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Modeling Parallel Programs" target="Daniel Nichols et al.">
  <data key="d5">14.0</data>
  <data key="d6">Daniel Nichols and colleagues used large language models to simulate and analyze parallel programming constructs.&lt;SEP&gt;Daniel Nichols and colleagues used large language models to simulate, analyze, and optimize parallel programming constructs, advancing understanding of parallel computing.</data>
  <data key="d7">modeling methodology, parallel programming&lt;SEP&gt;modeling methodology, parallel programming, simulation</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Modeling" target="Performance Changes in Scientific Applications">
  <data key="d5">9.0</data>
  <data key="d6">The models predict how modifications in code or execution environment impact performance, aiding optimization.</data>
  <data key="d7">performance prediction, optimization</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Performance Modeling" target="Transfer Learning">
  <data key="d5">16.0</data>
  <data key="d6">Transfer learning enables models to learn performance modeling with fewer data by leveraging knowledge from related tasks.&lt;SEP&gt;Transfer learning enables models to learn performance prediction with fewer data by leveraging knowledge from related tasks.</data>
  <data key="d7">data efficiency, learning transfer&lt;SEP&gt;data efficiency, model training</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Performance Modeling" target="PolyCoder+HPC">
  <data key="d5">16.0</data>
  <data key="d6">The model is used for analyzing and predicting performance properties of source code in HPC contexts.&lt;SEP&gt;The model is used for performance modeling of source code, showing its utility in analyzing and predicting code performance.</data>
  <data key="d7">application, performance analysis</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Performance Modeling" target="Equation (2)">
  <data key="d5">8.0</data>
  <data key="d6">Equation (2) models the expected maximum speedup of generated code by considering the runtimes of samples, the number of samples, and resources, providing insight into parallelization performance.</data>
  <data key="d7">modeling, performance estimation</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="OpenMP Pragmas" target="Code Completion">
  <data key="d5">7.0</data>
  <data key="d6">The models can automatically decorate loops with OpenMP pragmas to facilitate parallelization.</data>
  <data key="d7">parallelization, code automation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="OpenMP Pragmas" target="PolyCoder+HPC">
  <data key="d5">10.0</data>
  <data key="d6">PolyCoder+HPC is trained to generate and predict OpenMP pragmas with high accuracy, indicating its capability in understanding parallelization directives.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="OpenMP Pragmas" target="PolyCoder">
  <data key="d5">9.0</data>
  <data key="d6">PolyCoder also generates OpenMP pragmas with slightly lower accuracy, demonstrating its competence in modeling parallel code directives.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="OpenMP Pragmas" target="LLMs">
  <data key="d5">8.0</data>
  <data key="d6">Large Language Models understand the dependencies and syntax of OpenMP pragmas, enabling them to generate functionally correct directives.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Dataset of HPC and Scientific Codes" target="Fine-tuning of Pre-trained Models">
  <data key="d5">8.0</data>
  <data key="d6">The dataset is used to fine-tune large language models for improved performance on HPC-specific tasks.</data>
  <data key="d7">training data, model adaptation</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Fine-tuning of Pre-trained Models" target="HPC Codes">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning adjusts pre-trained language models on HPC-specific code datasets to improve task performance.</data>
  <data key="d7">model adaptation, domain-specific training</data>
  <data key="d8">chunk-c17dc636d560a988c23d0b7ee6ca279c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Transfer Learning" target="Text-to-Text Transformer">
  <data key="d5">7.0</data>
  <data key="d6">The text-to-text transformer architecture facilitates transfer learning by converting multiple NLP tasks into a unified text format, enabling models to leverage learned knowledge across tasks.</data>
  <data key="d7">methodology, model transfer</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Parallel Programming Models" target="LLMs">
  <data key="d5">16.0</data>
  <data key="d6">LLMs perform differently across various parallel models, with MPI and MPI+OpenMP being the most challenging for code generation."|&lt;SEP&gt;LLMs' ability to generate correct code varies across different parallel models, with MPI and MPI+OpenMP being the most challenging."|</data>
  <data key="d7">performance variation, complexity</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Parallel Programming Models" target="MPI">
  <data key="d5">9.0</data>
  <data key="d6">MPI's complexity makes it difficult for LLMs to generate correct code, especially when its code differs significantly from serial code."|</data>
  <data key="d7">difficulty, complexity</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Parallel Programming Models" target="Code Complexity">
  <data key="d5">9.0</data>
  <data key="d6">The greater the difference between serial and parallel code in a given model, the more difficult it is for LLMs to generate correct code."|</data>
  <data key="d7">code similarity, difficulty</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Data" target="Relative Performance Prediction">
  <data key="d5">9.0</data>
  <data key="d6">Performance datasets are used to train and evaluate the model's ability to predict code performance.</data>
  <data key="d7">performance modeling, evaluation</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Performance Prediction" target="Proxy Applications and Dataset">
  <data key="d5">9.0</data>
  <data key="d6">The larger programming dataset allows PolyCoder+HPC to better learn performance trends, resulting in higher prediction accuracy.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-c48289d50367366b4b4faa00d871ebb4</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC and scientific codes" target="large curated dataset">
  <data key="d5">8.0</data>
  <data key="d6">The dataset contains HPC and scientific codes used to train and evaluate the models.</data>
  <data key="d7">training data, dataset composition</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="OpenMP pragmas" target="HPC tasks">
  <data key="d5">6.0</data>
  <data key="d6">OpenMP pragmas are annotations in parallel code that LLMs can generate or label to facilitate parallel execution in HPC applications.</data>
  <data key="d7">parallel programming, code annotation</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="transformer-based language models" target="source code">
  <data key="d5">14.0</data>
  <data key="d6">Transformer models are applied to source code to enable tasks like code generation, labeling, and performance prediction."|&lt;SEP&gt;Transformer models underpin the activity of modeling, generating, and analyzing source code in this research."|</data>
  <data key="d7">model application, source code analysis&lt;SEP&gt;modeling, source code analysis</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Large Language Models (LLMs)" target="Pre-trained Language Models (PLMs)">
  <data key="d5">8.0</data>
  <data key="d6">LLMs are often scaled-up versions of PLMs, built upon the Transformer architecture, with enhanced capacity for downstream tasks.</data>
  <data key="d7">model development, architecture</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models (LLMs)" target="Domain Adaptation Techniques">
  <data key="d5">6.0</data>
  <data key="d6">Domain adaptation techniques aim to specialize LLMs for specific fields, though face challenges due to architecture inaccessibility.</data>
  <data key="d7">specialization, technical challenge</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Large Language Models (LLMs)" target="Knowledge Updating">
  <data key="d5">7.0</data>
  <data key="d6">Strategies for updating knowledge are crucial for maintaining accuracy and relevance, but are limited by model size and architecture.</data>
  <data key="d7">knowledge management, limitations</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="dataset of HPC and scientific codes" target="training and evaluation">
  <data key="d5">8.0</data>
  <data key="d6">The curated dataset is used to train, validate, and test the models' capabilities in code generation and performance prediction."|</data>
  <data key="d7">training data, evaluation</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="performance prediction" target="model's accuracy">
  <data key="d5">9.0</data>
  <data key="d6">The activity of predicting code performance achieves up to 92% accuracy, demonstrating effectiveness."|</data>
  <data key="d7">performance assessment</data>
  <data key="d8">chunk-c27a9a886dd19125bb1adb49ea7038f7</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Perplexity" target="Model Confidence">
  <data key="d5">14.0</data>
  <data key="d6">Perplexity measures how well the model predicts data, serving as an indicator of confidence and performance in language modeling.&lt;SEP&gt;Perplexity measures the model's confidence in its predictions, serving as an indicator of prediction quality and performance evaluation.</data>
  <data key="d7">model evaluation, confidence</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Perplexity" target="Accuracy">
  <data key="d5">6.0</data>
  <data key="d6">Lower perplexity generally correlates with better downstream task performance, although they measure different aspects of model quality.</data>
  <data key="d7">performance metrics, evaluation</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Accuracy" target="2016 Index">
  <data key="d5">16.0</data>
  <data key="d6">The 2016 index is used to assess the accuracy of leader identification, with mismatches indicating low accuracy.&lt;SEP&gt;The 2016 index is used to measure the accuracy of leader identification, with mismatches indicating low accuracy.</data>
  <data key="d7">benchmarking, measurement, evaluation&lt;SEP&gt;correlation, benchmarking</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Accuracy" target="2018 Index">
  <data key="d5">14.0</data>
  <data key="d6">The 2018 index is compared against actual leaders to evaluate accuracy, with low accuracy scores indicating mismatches.&lt;SEP&gt;The 2018 index is used to evaluate leader identification accuracy, with mismatches leading to low scores.</data>
  <data key="d7">benchmarking, evaluation&lt;SEP&gt;benchmarking, measurement, evaluation</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Text Generation" target="Sampling Methods">
  <data key="d5">28.0</data>
  <data key="d6">Sampling methods directly influence the diversity and quality of generated text by controlling randomness and token selection.&lt;SEP&gt;Sampling methods like temperature, top-k, and nucleus sampling are used to generate diverse and contextually appropriate text from models.&lt;SEP&gt;Sampling methods like temperature, top-k, and nucleus sampling influence the diversity, relevance, and coherence of generated text.</data>
  <data key="d7">generation quality, diversity control&lt;SEP&gt;text diversity, sampling techniques</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Text Generation" target="Sampling Techniques">
  <data key="d5">12.0</data>
  <data key="d6">Sampling techniques directly impact the diversity, relevance, and quality of generated text by controlling randomness during token selection.</data>
  <data key="d7">generation control, diversity</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Text Generation" target="Diverse Beam Search">
  <data key="d5">8.0</data>
  <data key="d6">Diverse beam search improves the diversity and quality of generated sequences in NLP tasks like text summarization and dialogue.</data>
  <data key="d7">generation strategies, diversity</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Nucleus Sampling" target="Token selection strategy">
  <data key="d5">20.0</data>
  <data key="d6">Nucleus sampling is a token selection method used during text generation to improve diversity and quality of output.&lt;SEP&gt;Nucleus sampling is used during text generation to select tokens based on cumulative probability, enhancing diversity and output quality.</data>
  <data key="d7">sampling method, generation quality&lt;SEP&gt;sampling method, output diversity</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Nucleus Sampling" target="Model Temperature">
  <data key="d5">15.0</data>
  <data key="d6">Model temperature influences the probability distribution from which tokens are sampled, affecting the diversity and creativity of generated code.&lt;SEP&gt;Model temperature influences the probability distribution from which tokens are sampled, affecting the diversity of generated code.</data>
  <data key="d7">sampling parameter, output diversity&lt;SEP&gt;sampling parameter, output variability</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Training on HPC Code" target="Large Dataset of HPC Code">
  <data key="d5">13.0</data>
  <data key="d6">A large dataset of HPC source code is used to train and fine-tune language models for HPC applications.</data>
  <data key="d7">domain-specific training, data collection</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Downstream Tasks" target="Model Selection">
  <data key="d5">14.0</data>
  <data key="d6">Selecting the best trained model involves evaluating models on validation metrics to optimize performance for specific HPC tasks.</data>
  <data key="d7">model evaluation, optimization</data>
  <data key="d8">chunk-80b64c5392960c5d4c391906f60b9a2c</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Downstream Tasks" target="Model Fine-Tuning">
  <data key="d5">9.0</data>
  <data key="d6">The fine-tuned models are evaluated on downstream tasks like code generation, pragma labeling, and performance prediction.</data>
  <data key="d7">evaluation, performance</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Model Selection" target="AutoML">
  <data key="d5">8.0</data>
  <data key="d6">AutoML automates the process of choosing the best models and hyperparameters, streamlining domain adaptation.</data>
  <data key="d7">automation, efficiency</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Setup" target="Model Fine-Tuning">
  <data key="d5">8.0</data>
  <data key="d6">The training setup involves fine-tuning pre-trained language models on HPC code datasets to adapt them for specific tasks.</data>
  <data key="d7">training, adaptation</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Training Setup" target="Parallel Deep Learning Framework">
  <data key="d5">8.0</data>
  <data key="d6">AxoNN enables the parallel training of large models across multiple GPUs, facilitating large-scale fine-tuning.</data>
  <data key="d7">training infrastructure, scalability</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model Fine-Tuning" target="HPC Source Code Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The dataset provides the source code required for fine-tuning the language models to understand HPC-specific syntax and semantics.</data>
  <data key="d7">training data, domain-specific</data>
  <data key="d8">chunk-e78a79fd58794ba9c2d3c38bb8b05368</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Programming Competition Solutions" target="Code Contests Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The solutions are collected from the code contests dataset, which aggregates data from multiple online programming competitions.</data>
  <data key="d7">dataset collection, data sources</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Run Time" target="Solution Pairs">
  <data key="d5">18.0</data>
  <data key="d6">Run times are recorded for each solution when executed on test cases, enabling the grouping into slower and faster pairs.&lt;SEP&gt;Run times are used to group solutions into pairs labeled as slower or faster, forming the basis for performance analysis.</data>
  <data key="d7">performance measurement, data grouping&lt;SEP&gt;performance measurement, solution comparison</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models Selected for Fine-tuning" target="Fine-tuning Setup">
  <data key="d5">8.0</data>
  <data key="d6">The selected models are fine-tuned using specific datasets, hyperparameters, and hardware configurations to optimize their performance for the task.</data>
  <data key="d7">model training, optimization</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models Selected for Fine-tuning" target="DeepSpeed">
  <data key="d5">7.0</data>
  <data key="d6">DeepSpeed is used as the backend framework to optimize the fine-tuning process of the models on GPU hardware.</data>
  <data key="d7">training efficiency, memory optimization</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models Selected for Fine-tuning" target="AdamW Optimizer">
  <data key="d5">8.0</data>
  <data key="d6">The AdamW optimizer is employed during fine-tuning to update model weights and minimize training loss.</data>
  <data key="d7">optimization algorithm, training process</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models Selected for Fine-tuning" target="Hyperparameters">
  <data key="d5">7.0</data>
  <data key="d6">Hyperparameters such as learning rate and Adam parameters are set to guide the training process and ensure convergence.</data>
  <data key="d7">training control, parameter tuning</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Models Selected for Fine-tuning" target="Training Process">
  <data key="d5">8.0</data>
  <data key="d6">Selected models are fine-tuned using the specified hardware, datasets, and hyperparameters to optimize their performance.</data>
  <data key="d7">model training, optimization</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="DeepSpeed" target="Training Hardware">
  <data key="d5">7.0</data>
  <data key="d6">DeepSpeed enables efficient distributed training on the specified GPU hardware setup.</data>
  <data key="d7">training efficiency, hardware utilization</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="AdamW Optimizer" target="Training Process">
  <data key="d5">8.0</data>
  <data key="d6">The AdamW optimizer updates model weights during fine-tuning, minimizing loss based on hyperparameters.</data>
  <data key="d7">optimization, training dynamics</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Hyperparameters" target="Training Process">
  <data key="d5">7.0</data>
  <data key="d6">Hyperparameters such as learning rate and beta values are set to guide training and ensure convergence.</data>
  <data key="d7">training control, parameter tuning</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Hyperparameters" target="Downstream Task Learning">
  <data key="d5">8.0</data>
  <data key="d6">Choosing appropriate hyperparameters is critical for effective downstream task learning.</data>
  <data key="d7">training optimization, model performance</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Hardware" target="Training Process">
  <data key="d5">7.0</data>
  <data key="d6">The hardware setup supports the computational requirements of the fine-tuning process.</data>
  <data key="d7">hardware support, computational resources</data>
  <data key="d8">chunk-93a5a4d02febb50f7431a4688bb7fe2f</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Training Process" target="RAG-Token Model">
  <data key="d5">7.0</data>
  <data key="d6">The joint training process optimizes both retriever and generator components to improve answer accuracy.</data>
  <data key="d7">model training, optimization</data>
  <data key="d8">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="MPI" target="Parallel Framework">
  <data key="d5">8.0</data>
  <data key="d6">MPI is a parallel framework that enables message passing between processes in distributed computing environments.</data>
  <data key="d7">framework-functionality</data>
  <data key="d8">chunk-47d2594b07c17f257a06cf89e43eef94</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="MPI" target="parallel programming model">
  <data key="d5">7.0</data>
  <data key="d6">MPI is used as a message-passing framework to evaluate the models' ability to generate MPI-based parallel code.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="MPI" target="Code Generation Difficulty">
  <data key="d5">9.0</data>
  <data key="d6">MPI's complexity and divergence from serial code make it the hardest for LLMs to generate correct code."|</data>
  <data key="d7">difficulty, complexity</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="MPI" target="Parallel Computing">
  <data key="d5">18.0</data>
  <data key="d6">MPI provides message-passing capabilities essential for parallel computing across distributed systems.</data>
  <data key="d7">tool, application</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Evaluation Metric" target="&lt;code generation model&gt;">
  <data key="d5">8.0</data>
  <data key="d6">The evaluation metric measures the correctness and proficiency of the generated code."|&gt;"assessment, performance</data>
  <data key="d7">8</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Evaluation Metric" target="Self">
  <data key="d5">7.0</data>
  <data key="d6">The evaluation metric (score) quantifies the model's performance during validation.</data>
  <data key="d7">performance measure, evaluation score</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="PolyCoder" target="HPC Dataset">
  <data key="d5">18.0</data>
  <data key="d6">PolyCoder is fine-tuned on the HPC dataset, which improves its ability to generate HPC-specific code.</data>
  <data key="d7">training data, domain adaptation</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="PolyCoder+HPC" target="HPC Code Generation">
  <data key="d5">18.0</data>
  <data key="d6">PolyCoder+HPC is trained to generate high-performance HPC code, achieving higher pass rates and accurate labeling of parallel constructs.&lt;SEP&gt;PolyCoder+HPC is trained to generate high-performance HPC code, demonstrating superior performance in generation and labeling tasks.</data>
  <data key="d7">application, performance improvement</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Evaluation Performance" target="Samples">
  <data key="d5">16.0</data>
  <data key="d6">Generated code samples are evaluated for correctness and compilation success, affecting overall performance metrics.</data>
  <data key="d7">code quality, evaluation</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Evaluation Performance" target="Sample Size">
  <data key="d5">6.0</data>
  <data key="d6">The number of samples generated influences the statistical reliability of performance metrics.</data>
  <data key="d7">sample quantity, statistical validity</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Catastrophic Forgetting" target="Downstream Task Learning">
  <data key="d5">8.0</data>
  <data key="d6">Fine-tuning for specific tasks can cause catastrophic forgetting of previously learned knowledge.</data>
  <data key="d7">knowledge retention, model stability</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Catastrophic Forgetting" target="Prompt Tuning">
  <data key="d5">6.0</data>
  <data key="d6">Lifelong learning prompts are designed to mitigate catastrophic forgetting by training prompts that retain knowledge of previous tasks while learning new ones.</data>
  <data key="d7">continual learning, memory retention</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Evaluation Figures" target="Evaluation Tasks">
  <data key="d5">7.0</data>
  <data key="d6">Evaluation figures visually compare model performance across specific tasks like code correctness and HPC features.</data>
  <data key="d7">performance visualization, task assessment</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Domain Adaptation" target="LLM">
  <data key="d5">16.0</data>
  <data key="d6">Domain adaptation involves using prompts or training methods to enable LLMs to perform well in new, unseen domains.</data>
  <data key="d7">application, generalization</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Figure 8" target="Resource Counts and Performance">
  <data key="d5">14.0</data>
  <data key="d6">Figure 8 depicts maximum speedup and efficiency across resource counts, analyzing how parallel code performance scales with resource availability and identifying plateauing behaviors."|&gt;"scalability, resource utilization&lt;SEP&gt;Figure 8 depicts maximum speedup and efficiency across resource counts, analyzing scalability and resource utilization.</data>
  <data key="d7">7&lt;SEP&gt;scalability, resource efficiency</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Figure 9" target="Model Translation">
  <data key="d5">16.0</data>
  <data key="d6">Figure 9 compares pass@1 scores for code translation between models and execution contexts, assessing translation accuracy.&lt;SEP&gt;Figure 9 compares pass@1 scores for translating code between models and execution contexts, assessing the models' ability to accurately convert code from serial to OpenMP, MPI, and CUDA to Kokkos, thereby evaluating translation effectiveness."|&gt;"translation accuracy, model capability</data>
  <data key="d7">8&lt;SEP&gt;translation performance, model capability</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Figure 10" target="search query">
  <data key="d5">14.0</data>
  <data key="d6">Figure 10 illustrates the process of deriving a search query from context and question, serving as an example of systematic methodology.&lt;SEP&gt;Figure 10 illustrates the process of generating a search query from context and question, serving as an example of methodology.</data>
  <data key="d7">visual aid, methodology illustration&lt;SEP&gt;visual illustration, methodology</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Speedup" target="Benchmarks">
  <data key="d5">16.0</data>
  <data key="d6">Performance improvements are quantified by speedup metrics achieved through PPLin optimization."|&gt;"performance measurement</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Speedup" target="Static Overhead">
  <data key="d5">14.0</data>
  <data key="d6">Reducing static overhead leads to higher speedups, especially in smaller applications or datasets."|&gt;"performance gain&lt;SEP&gt;Reducing static overhead leads to higher speedups, especially in smaller applications."|&gt;"performance gain</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Figure 11" target="search query">
  <data key="d5">14.0</data>
  <data key="d6">Figure 11 demonstrates the process of creating a second-hop search query in a multi-hop question-answering system.&lt;SEP&gt;Figure 11 demonstrates the process of generating a second-hop search query in a multi-hop question-answering system, illustrating methodology.</data>
  <data key="d7">methodology illustration, multi-hop process</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="code2vec" target="U. Alon et al. 2018">
  <data key="d5">16.0</data>
  <data key="d6">Creates distributed code representations for tasks like code similarity and classification.&lt;SEP&gt;Creates distributed code representations that can be used for code classification, similarity detection, and other code analysis tasks."|</data>
  <data key="d7">code embedding, machine learning</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="DeepDevPERF" target="Code Changes Dataset">
  <data key="d5">16.0</data>
  <data key="d6">DeepDevPERF uses code changes from Git commits with performance keywords to train and evaluate performance improvement suggestions.&lt;SEP&gt;DeepDevPERF utilizes code changes from Git commits with performance keywords to train and evaluate the model for performance improvement suggestions.</data>
  <data key="d7">data source, performance dataset&lt;SEP&gt;data source, training data</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="DeepDevPERF" target="Code Transformations">
  <data key="d5">14.0</data>
  <data key="d6">DeepDevPERF suggests code transformations aimed at performance enhancement, focusing on code modifications based on data.&lt;SEP&gt;DeepDevPERF suggests code transformations rather than relative performance learning, focusing on code modification for performance enhancement.</data>
  <data key="d7">method, performance optimization&lt;SEP&gt;methodology, performance optimization</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="M. Lewis et al.">
  <data key="d5">16.0</data>
  <data key="d6">Authors developed BART as a pre-training model to improve NLP tasks like generation, translation, and comprehension.&lt;SEP&gt;Authors developed BART, a pre-training NLP model designed to enhance language generation, translation, and comprehension.</data>
  <data key="d7">model development, NLP application</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="Natural Language Generation, Translation, and Comprehension">
  <data key="d5">10.0</data>
  <data key="d6">BART is designed to improve performance in these NLP tasks through denoising sequence-to-sequence pre-training.</data>
  <data key="d7">theoretical model, NLP tasks</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="Y. Liu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the development and evaluation of BART as a denoising sequence-to-sequence model.</data>
  <data key="d7">model development, NLP tasks</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="N. Goyal et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the development of BART, focusing on its capabilities in NLP tasks.</data>
  <data key="d7">model development, NLP application</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="M. Ghazvininejad et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the design and evaluation of BART for language tasks.</data>
  <data key="d7">model development, NLP tasks</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="A. Mohamed et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the research on BART's performance in NLP tasks.</data>
  <data key="d7">model development, NLP application</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="O. Levy et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the development and testing of BART for NLP.</data>
  <data key="d7">model development, NLP tasks</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="V. Stoyanov et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the development and evaluation of BART for language understanding.</data>
  <data key="d7">model development, NLP</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="L. Zettlemoyer et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors contributed to the theoretical foundation and application of BART in NLP.</data>
  <data key="d7">model development, NLP theory</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="BART" target="Factuality">
  <data key="d5">16.0</data>
  <data key="d6">BART is more prone to hallucination and less factual compared to RAG, as indicated by evaluation metrics and human judgments.</data>
  <data key="d7">model accuracy, hallucination</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Future Work" target="Analysis and Tool Development">
  <data key="d5">12.0</data>
  <data key="d6">Future plans include analyzing code performance further and creating practical tools for HPC developers.&lt;SEP&gt;Future research aims to develop tools and further analyze code performance using the specialized language models.</data>
  <data key="d7">research direction, application development&lt;SEP&gt;research direction, practical application</data>
  <data key="d8">chunk-cfcd5a8798f30da73fdaa3033d579b42</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Code Llama" target="Research Team or Authors">
  <data key="d5">16.0</data>
  <data key="d6">The listed researchers and authors contributed to the publication about Code Llama, indicating their role in developing or analyzing the foundation model for code."|&gt;"research contribution, authorship</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Android Mobile Malware Detection" target="Systematic Review">
  <data key="d5">7.0</data>
  <data key="d6">The systematic review compiles existing research and methodologies on Android malware detection using machine learning."|&gt;"study synthesis, methodology overview</data>
  <data key="d7">7</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Android Mobile Malware Detection" target="Machine Learning">
  <data key="d5">9.0</data>
  <data key="d6">Machine learning techniques are employed as the primary methodology for detecting malware in Android devices."|&gt;"method application, technical approach</data>
  <data key="d7">9</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="J. Gu, P. Salza, H. C. Gall" target="Assemble Foundation Models for Automatic Code Summarization">
  <data key="d5">16.0</data>
  <data key="d6">These authors developed models to automate code summarization, advancing AI capabilities in code understanding."|&gt;"research focus, model development&lt;SEP&gt;These authors developed models to automate code summarization, advancing the field of AI-driven code understanding."|&gt;"research focus, model development</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="T. Ahmed, P. Devanbu" target="Learning Code Summarization">
  <data key="d5">14.0</data>
  <data key="d6">Their research explores how to effectively learn code summaries from limited datasets, contributing to model training strategies."|&gt;"research question, data efficiency&lt;SEP&gt;Their work explores how to effectively learn code summaries from limited datasets, contributing to model training strategies."|&gt;"research question, data efficiency</data>
  <data key="d7">7</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="S. Haque, Z. Eberhart, A. Bansal, C. McMillan" target="Semantic Similarity Metrics">
  <data key="d5">16.0</data>
  <data key="d6">They created metrics to evaluate the quality of source code summaries, aiding in assessment of models."|&gt;"evaluation methodology, metrics&lt;SEP&gt;They created metrics to evaluate the quality of source code summaries, aiding in model assessment."|&gt;"evaluation methodology, metrics</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="W. U. Ahmad, S. Chakraborty, B. Ray, K.-W. Chang" target="Transformer-Based Approach">
  <data key="d5">18.0</data>
  <data key="d6">Developed a transformer-based method for source code summarization, leveraging advanced neural architectures."|&gt;"model architecture, technical innovation&lt;SEP&gt;Developed a transformer-based method for source code summarization, leveraging neural architectures."|&gt;"model architecture, technical innovation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="C. Richter, H. Wehrheim" target="Learning from Developer Mistakes">
  <data key="d5">16.0</data>
  <data key="d6">Their research involves localizing and repairing real bugs from actual developer fixes, contributing to debugging techniques."|&gt;"application, bug repair&lt;SEP&gt;Their research involves localizing and repairing real bugs from actual developer fixes, informing debugging techniques."|&gt;"application, bug repair</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="A. Kharkar, R. Z. Moghaddam, M. Jin, X. Liu, X. Shi, C. B. Clement, N. Sundaresan" target="Reducing False Positives in Bug Detectors">
  <data key="d5">16.0</data>
  <data key="d6">They aim to improve analytic bug detectors by reducing false positives, enhancing bug detection accuracy."|&gt;"application, bug detection&lt;SEP&gt;They aim to improve bug detection accuracy by reducing false positives in analytic bug detectors."|&gt;"application, bug detection</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="F. F. Xu, U. Alon, G. Neubig, V. J. Hellendoorn" target="Large Language Models of Code">
  <data key="d5">16.0</data>
  <data key="d6">They systematically evaluate how large language models perform on coding tasks, providing benchmarks and insights."|&gt;"evaluation, benchmarking&lt;SEP&gt;They systematically evaluate large language models' performance on coding tasks, providing benchmarks."|&gt;"evaluation, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="M. Allamanis" target="Adverse Effects of Code Duplication">
  <data key="d5">14.0</data>
  <data key="d6">He discussed how code duplication negatively impacts machine learning models trained on code, highlighting data quality issues."|&gt;"data quality, model performance&lt;SEP&gt;He discusses how code duplication negatively impacts machine learning models trained on code, affecting data quality and model performance."|&gt;"data quality, model performance</data>
  <data key="d7">7</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Radford, Wu, Child, Luan, Amodei, Sutskever" target="Language Models Are Unsupervised Multitask Learners">
  <data key="d5">18.0</data>
  <data key="d6">These authors introduced large-scale language models capable of multiple tasks without supervised training."|&gt;"model capability, multitask learning&lt;SEP&gt;They introduced large-scale language models capable of multiple tasks, foundational for AI language understanding."|&gt;"model capability, multitask learning</data>
  <data key="d7">9</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="V. A. Dobrev, T. V. Kolev, R. N. Rieben" target="High-Order Finite Element Methods">
  <data key="d5">16.0</data>
  <data key="d6">Their work advances numerical methods for Lagrangian hydrodynamics, impacting computational physics."|&gt;"methodology, computational modeling&lt;SEP&gt;Their work advances numerical techniques for Lagrangian hydrodynamics in scientific computing."|&gt;"methodology, computational physics</data>
  <data key="d7">8</data>
  <data key="d8">chunk-05dc85544943f63f4078b8c41f458d49</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="OpenAI" target="GPT-4">
  <data key="d5">23.0</data>
  <data key="d6">OpenAI developed GPT-4, a sophisticated language model detailed in its technical report.&lt;SEP&gt;OpenAI's GPT-4 model is described in a technical report detailing its architecture, capabilities, and potential applications in AI and software engineering.</data>
  <data key="d7">model architecture, AI capabilities&lt;SEP&gt;model development, technical documentation</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718&lt;SEP&gt;chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf&lt;SEP&gt;Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="OpenAI" target="GPT-4 Technical Report">
  <data key="d5">9.0</data>
  <data key="d6">OpenAI published the GPT-4 technical report detailing the architecture and capabilities of the GPT-4 model.</data>
  <data key="d7">model documentation, language model</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="OpenAI" target="OpenAI API and Python Library">
  <data key="d5">16.0</data>
  <data key="d6">OpenAI provides APIs and Python libraries that enable developers to incorporate GPT-4 and other models into their software tools for code generation, analysis, and automation.&lt;SEP&gt;OpenAI provides APIs and libraries for accessing and utilizing their language models in software applications.</data>
  <data key="d7">tool support, API integration</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Loshchilov and Hutter 2017" target="Fixing weight decay regularization in Adam">
  <data key="d5">16.0</data>
  <data key="d6">The study proposes a method to improve weight decay regularization in the Adam optimizer.&lt;SEP&gt;The study proposes a specific method to correct weight decay regularization in the Adam optimizer to enhance training stability and performance.</data>
  <data key="d7">optimization, regularization</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="M. Chen et al. 2021" target="Evaluating large language models trained on code">
  <data key="d5">18.0</data>
  <data key="d6">Evaluates the performance, capabilities, and limitations of large language models trained on source code across various tasks, including understanding, generation, and reasoning."|&lt;SEP&gt;Evaluates the performance, capabilities, and limitations of large language models trained on source code.</data>
  <data key="d7">language models, code understanding&lt;SEP&gt;language models, code understanding, evaluation</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="J. Devlin et al. 2019" target="BERT: Pre-training of deep bidirectional transformers">
  <data key="d5">16.0</data>
  <data key="d6">Introduces BERT, a pre-trained transformer model for natural language understanding.&lt;SEP&gt;Introduces BERT, a transformer-based pretraining model that significantly advances NLP tasks through deep bidirectional context learning."|</data>
  <data key="d7">transformer models, NLP</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Y. Liu et al. 2019" target="Roberta: A robustly optimized BERT">
  <data key="d5">14.0</data>
  <data key="d6">Presents RoBERTa, an improved pretraining approach that enhances BERT's performance by optimizing training strategies and data usage."|&lt;SEP&gt;Presents RoBERTa, an optimized version of BERT for better NLP task performance.</data>
  <data key="d7">model optimization, NLP</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Y. Wei et al. 2023" target="Magicoder: Source code is all you need">
  <data key="d5">12.0</data>
  <data key="d6">Introduces Magicoder, a large language model specialized for source code tasks.&lt;SEP&gt;Introduces Magicoder, a large language model tailored for source code understanding, generation, and programming assistance."|</data>
  <data key="d7">code models, programming&lt;SEP&gt;code models, programming, AI</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="J.-B. Doderlein et al. 2022" target="Piloting copilot and codex">
  <data key="d5">14.0</data>
  <data key="d6">Analyzes how prompt temperature and prompt strategies influence the behavior, quality, and reliability of AI code generation models like Copilot and Codex."|&lt;SEP&gt;Analyzes how prompt temperature and strategies affect AI code generation models.</data>
  <data key="d7">prompt engineering, AI code generation&lt;SEP&gt;prompt engineering, AI models</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="S. Barke et al. 2022" target="Grounded copilot">
  <data key="d5">16.0</data>
  <data key="d6">Studies how programmers interact with AI code-generation tools, focusing on usability, effectiveness, and user behavior."|&lt;SEP&gt;Studies programmer interactions with AI code-generation tools, focusing on usability and effectiveness.</data>
  <data key="d7">user interaction, AI tools</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="A. Sarkar et al. 2022" target="Programming with artificial intelligence">
  <data key="d5">14.0</data>
  <data key="d6">Explores programmers' experiences and perceptions when working with AI-assisted programming.&lt;SEP&gt;Explores programmers' subjective experiences, perceptions, and challenges when working with AI-assisted programming tools."|</data>
  <data key="d7">programmer experience, AI assistance&lt;SEP&gt;programming experience, AI assistance</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="L. Chen et al. 2023" target="Data race detection using large language models">
  <data key="d5">16.0</data>
  <data key="d6">Applies large language models to detect data races in software, improving concurrency analysis.&lt;SEP&gt;Uses large language models to detect data races in concurrent software, improving software reliability and correctness."|</data>
  <data key="d7">software analysis, concurrency&lt;SEP&gt;software analysis, concurrency, data race</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="C. Munley et al. 2023" target="Llm4vv">
  <data key="d5">14.0</data>
  <data key="d6">Develops an LLM-driven test suite for compiler validation to enhance compiler reliability.&lt;SEP&gt;Develops an LLM-driven test suite to automate and improve compiler validation, increasing confidence in compiler correctness."|</data>
  <data key="d7">compiler testing, validation&lt;SEP&gt;compiler validation, testing</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="S. VenkataKeerthy et al. 2020" target="IR2V">
  <data key="d5">16.0</data>
  <data key="d6">Provides scalable program embeddings based on LLVM IR for large-scale code analysis.&lt;SEP&gt;Provides scalable program embeddings based on LLVM IR, enabling large-scale code analysis across multiple programming languages."|</data>
  <data key="d7">program embeddings, scalability</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="S. Garg et al. 2022" target="Deepdev-perf">
  <data key="d5">14.0</data>
  <data key="d6">Applies deep learning techniques to analyze software performance, identify bottlenecks, and suggest optimizations."|&lt;SEP&gt;Uses deep learning to analyze and improve software performance.</data>
  <data key="d7">performance analysis, deep learning&lt;SEP&gt;performance optimization, deep learning</data>
  <data key="d8">chunk-fbc178ca51bae28816d191153e754f68</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Deepdev-perf" target="Software Performance">
  <data key="d5">9.0</data>
  <data key="d6">Deepdev-perf aims to improve software performance, directly impacting the efficiency and effectiveness of software systems.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Deepdev-perf" target="S. Garg et al.">
  <data key="d5">17.0</data>
  <data key="d6">Authors introduced Deepdev-perf as a methodology for enhancing software performance.&lt;SEP&gt;Authors introduced Deepdev-perf as a methodology for improving software performance, impacting software efficiency and effectiveness.</data>
  <data key="d7">methodology, application&lt;SEP&gt;research contribution, methodology</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="Deepdev-perf" target="R. Z. Moghaddam et al.">
  <data key="d5">8.0</data>
  <data key="d6">Authors developed Deepdev-perf, a deep learning approach aimed at optimizing software performance.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-25ebcde4744e2b19ba1d5d1fd25807b0</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="HPC Dataset" target="GPT2">
  <data key="d5">14.0</data>
  <data key="d6">GPT-2, lacking pre-training on code, performs poorly even after fine-tuning on HPC data.</data>
  <data key="d7">pre-training limitations, model capacity</data>
  <data key="d8">chunk-26223e383a1876273c0d63ed52102a6e</data>
  <data key="d9">Nichols et al. - 2024 - HPC-Coder Modeling Parallel Programs using Large Language Models.pdf</data>
</edge>
<edge source="LLMs" target="Genomic and proteomic data">
  <data key="d5">7.0</data>
  <data key="d6">Genomic and proteomic data serve as training data for LLMs to analyze biological functions and predict cellular interactions.</data>
  <data key="d7">data source, biological prediction</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Protein structures and interactions">
  <data key="d5">8.0</data>
  <data key="d6">LLMs predict protein structures and interactions, which are critical for understanding cellular processes and drug design.</data>
  <data key="d7">biological modeling, drug discovery</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Medical records">
  <data key="d5">9.0</data>
  <data key="d6">Medical records are processed by LLMs to identify patterns, aid in diagnosis, and recommend treatments.</data>
  <data key="d7">medical data analysis, clinical decision support</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Medical image analysis">
  <data key="d5">16.0</data>
  <data key="d6">LLMs are utilized to analyze medical images, identify features, and assist in diagnostics across modalities.&lt;SEP&gt;LLMs assist in analyzing medical images like X-rays and MRI scans to identify specific features using multi-modality learning.</data>
  <data key="d7">imaging analysis, healthcare diagnostics</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Climate scenario generation">
  <data key="d5">8.0</data>
  <data key="d6">LLMs are used to generate climate scenarios, aiding in environmental planning and policy-making.</data>
  <data key="d7">scenario modeling, climate research</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Few-shot learning">
  <data key="d5">7.0</data>
  <data key="d6">Few-shot learning enables LLMs to perform well with limited data, useful in specialized domains.</data>
  <data key="d7">learning techniques, domain adaptation</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="Zero-shot learning">
  <data key="d5">7.0</data>
  <data key="d6">Zero-shot learning allows LLMs to handle tasks without prior specific training, increasing flexibility.</data>
  <data key="d7">learning techniques, task versatility</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLMs" target="parallel code generation">
  <data key="d5">8.0</data>
  <data key="d6">LLMs are evaluated for their ability to generate parallel code, with performance varying across models.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="LLMs" target="pass@1 score">
  <data key="d5">9.0</data>
  <data key="d6">The pass@1 score measures the proportion of correct top outputs generated by LLMs, serving as a key performance indicator.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="LLMs" target="parEval">
  <data key="d5">10.0</data>
  <data key="d6">ParEval assesses the ability of LLMs to generate correct and efficient parallel code, thus evaluating their performance in this domain."|</data>
  <data key="d7">evaluation, performance assessment</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Domain Specification Techniques" target="Application Domains">
  <data key="d5">8.0</data>
  <data key="d6">These techniques are applied to various domains such as healthcare, finance, and legal sectors to tailor LLMs for specific needs and challenges.</data>
  <data key="d7">domain tailoring, application-specific adaptation</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Heterogeneity of Domain Data" target="Domain Knowledge">
  <data key="d5">7.0</data>
  <data key="d6">Understanding the heterogeneity of data is essential for effectively integrating domain knowledge into LLMs for better performance.</data>
  <data key="d7">data heterogeneity, knowledge integration</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain Knowledge" target="Domain Data Heterogeneity">
  <data key="d5">8.0</data>
  <data key="d6">Understanding and managing data heterogeneity is essential to effectively incorporate domain knowledge into LLMs for improved performance."|&gt;"data diversity, knowledge integration</data>
  <data key="d7">8</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain Knowledge" target="Model Specialization">
  <data key="d5">9.0</data>
  <data key="d6">Domain knowledge provides the foundational information necessary for tailoring models to specific fields, enhancing their accuracy and relevance.</data>
  <data key="d7">domain expertise, model tuning</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Open Challenges">
  <data key="d5">5.0</data>
  <data key="d6">Different application domains face unique challenges in applying domain specialization techniques, necessitating tailored solutions.</data>
  <data key="d7">domain-specific challenges</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Taxonomy of Domain Specialization Techniques">
  <data key="d5">5.0</data>
  <data key="d6">The taxonomy categorizes various techniques based on their applicability to different domains and their accessibility to LLMs."|&gt;"classification, domain applicability</data>
  <data key="d7">5</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Open Challenges in Domain Specialization">
  <data key="d5">4.0</data>
  <data key="d6">Different application domains face unique challenges that require tailored solutions for effective LLM adaptation."|&gt;"domain-specific challenges</data>
  <data key="d7">4</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Domain Specialization Techniques">
  <data key="d5">15.0</data>
  <data key="d6">Techniques are categorized based on their applicability to various application domains.&lt;SEP&gt;Techniques are tailored to specific application domains to address unique needs and enhance performance."|</data>
  <data key="d7">domain tailoring, application-specific optimization&lt;SEP&gt;method categorization, domain mapping</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Taxonomy of Techniques">
  <data key="d5">7.0</data>
  <data key="d6">Different application domains benefit from specific techniques classified within the taxonomy, aiding targeted adaptation.</data>
  <data key="d7">method application, domain specificity</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Taxonomy of Domain Specialization">
  <data key="d5">8.0</data>
  <data key="d6">The taxonomy categorizes various techniques to tailor LLMs for sectors like medical, legal, and financial fields.</data>
  <data key="d7">classification, sector-specific</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Challenges and Limitations">
  <data key="d5">6.0</data>
  <data key="d6">Different application fields face unique challenges, such as data scarcity or domain expertise gaps, affecting LLM effectiveness."|</data>
  <data key="d7">domain-specific obstacles, deployment issues</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Application Domains" target="Future Trends">
  <data key="d5">5.0</data>
  <data key="d6">Emerging trends aim to expand the capabilities and effectiveness of domain-specific LLMs across various fields."|</data>
  <data key="d7">research directions, innovations</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Open Challenges" target="Systematic Taxonomy">
  <data key="d5">6.0</data>
  <data key="d6">The taxonomy helps identify and categorize open challenges in domain specialization, guiding future research.</data>
  <data key="d7">classification, research guidance</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Open Challenges" target="Future Trends">
  <data key="d5">11.0</data>
  <data key="d6">Addressing open challenges is essential for advancing future research and application of LLM domain specialization.&lt;SEP&gt;Emerging trends aim to address current open challenges and expand the effectiveness of domain-specific LLMs.</data>
  <data key="d7">research direction, innovation&lt;SEP&gt;research directions, innovation</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906&lt;SEP&gt;chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain Specialization of LLMs" target="External Augmentation">
  <data key="d5">16.0</data>
  <data key="d6">External augmentation techniques incorporate domain knowledge into LLMs via external tools or resources without internal model changes."|&gt;"knowledge integration, external tools</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain Specialization of LLMs" target="Prompt Crafting">
  <data key="d5">18.0</data>
  <data key="d6">Prompt crafting involves designing prompts for LLMs to better extract domain knowledge, especially under limited model access."|&gt;"prompt design, knowledge elicitation&lt;SEP&gt;Prompt crafting involves designing prompts to better elicit domain knowledge from LLMs, especially under limited access."|&gt;"prompt design, knowledge elicitation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Pre-trained Language Models (PLMs)" target="Transformer Architecture">
  <data key="d5">9.0</data>
  <data key="d6">Transformer architecture is fundamental to the design of PLMs and LLMs, enabling effective processing of large-scale text data.</data>
  <data key="d7">model architecture, NLP</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Pre-trained Language Models (PLMs)" target="Model Scaling">
  <data key="d5">7.0</data>
  <data key="d6">Scaling techniques improve the capacity and performance of PLMs and LLMs by increasing size or data.</data>
  <data key="d7">model enhancement, performance</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain-Specific Knowledge" target="Knowledge Extraction">
  <data key="d5">8.0</data>
  <data key="d6">Knowledge extraction aims to gather domain-specific knowledge from data sources for model training and updating.</data>
  <data key="d7">data extraction, domain knowledge</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain-Specific Knowledge" target="Knowledge Distillation">
  <data key="d5">18.0</data>
  <data key="d6">Knowledge distillation transfers domain-specific knowledge from large models to smaller models, enhancing efficiency and relevance in targeted fields."|"&lt;"knowledge transfer, model efficiency&lt;SEP&gt;Knowledge distillation transfers domain-specific knowledge from large models to smaller, more efficient models, enabling domain adaptation and faster inference."|"&lt;"knowledge transfer, efficiency</data>
  <data key="d7">9</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Discoveries" target="Regulations">
  <data key="d5">7.0</data>
  <data key="d6">Discoveries influence regulations, which in turn can modify practices and standards in specialized domains.</data>
  <data key="d7">knowledge development, regulatory impact</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Discoveries" target="Best Practices">
  <data key="d5">6.0</data>
  <data key="d6">New discoveries lead to updated best practices that enhance domain-specific workflows and knowledge application.</data>
  <data key="d7">knowledge update, practice improvement</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="News Articles" target="Knowledge Extraction">
  <data key="d5">8.0</data>
  <data key="d6">News articles serve as primary sources for knowledge extraction that contribute to understanding current events and trends.</data>
  <data key="d7">information source, data collection</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Social Media Analysis" target="Fact-Checking">
  <data key="d5">8.0</data>
  <data key="d6">Social media analysis supports fact-checking efforts by analyzing content for accuracy and misinformation.</data>
  <data key="d7">verification, misinformation detection</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Re-Training" target="Model Re-Training">
  <data key="d5">9.0</data>
  <data key="d6">Re-training involves updating models with new data to maintain relevance and improve accuracy.</data>
  <data key="d7">model update, knowledge maintenance</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Re-Training" target="Knowledge Relevance">
  <data key="d5">8.0</data>
  <data key="d6">Re-training aims to enhance the relevance of models by incorporating current, domain-specific knowledge.</data>
  <data key="d7">model updating, relevance improvement</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Re-Training" target="Hyperparameter Tuning">
  <data key="d5">8.0</data>
  <data key="d6">Hyperparameter tuning optimizes training procedures, influencing model performance and knowledge integration.</data>
  <data key="d7">optimization, training efficiency</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Specialized Terminology" target="Complex Concepts">
  <data key="d5">7.0</data>
  <data key="d6">Understanding specialized terminology is essential for modeling complex domain concepts accurately.</data>
  <data key="d7">vocabulary, concept understanding</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hallucination" target="Vague User Instructions">
  <data key="d5">6.0</data>
  <data key="d6">Vague instructions can increase hallucination risks by providing ambiguous guidance to LLMs.</data>
  <data key="d7">instruction clarity, output accuracy</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Downstream Task Learning" target="High-Performance Hardware">
  <data key="d5">8.0</data>
  <data key="d6">Training large models requires high-performance hardware such as GPUs and TPUs.</data>
  <data key="d7">computational resources, hardware dependency</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Specialized Domains" target="Knowledge Bases">
  <data key="d5">8.0</data>
  <data key="d6">Knowledge bases are built from structured information extracted from domain-specific data sources, supporting specialized applications.</data>
  <data key="d7">knowledge management, data sources</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Bases" target="Knowledge Extraction Methods">
  <data key="d5">8.0</data>
  <data key="d6">Knowledge extraction methods provide the means to populate and update knowledge bases with relevant information.</data>
  <data key="d7">data processing, information updating</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Corpus" target="Knowledge Relevance">
  <data key="d5">7.0</data>
  <data key="d6">The training corpus determines the scope and quality of knowledge available to the model, affecting its relevance and performance.</data>
  <data key="d7">data quality, knowledge scope</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Continuous Learning" target="Knowledge Relevance">
  <data key="d5">8.0</data>
  <data key="d6">Continuous learning mechanisms help maintain the relevance of models by ongoing knowledge acquisition.</data>
  <data key="d7">adaptation, knowledge maintenance</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Resource-Intensive Processes" target="Model Complexity">
  <data key="d5">8.0</data>
  <data key="d6">Complex models with many parameters require resource-intensive training and updating processes.</data>
  <data key="d7">computational resources, scalability</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Complexity" target="Explainability">
  <data key="d5">8.0</data>
  <data key="d6">Increasing model complexity often decreases interpretability, creating a trade-off between performance and transparency.</data>
  <data key="d7">model interpretability, complexity</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Complexity" target="Challenges and Limitations">
  <data key="d5">7.0</data>
  <data key="d6">The complexity of models presents challenges in interpretability and resource requirements, impacting their practical deployment in domain-specific contexts."|</data>
  <data key="d7">model interpretability, resource constraints</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hyperparameter Tuning" target="Self">
  <data key="d5">8.0</data>
  <data key="d6">Hyperparameter tuning involves adjusting model parameters to improve evaluation scores, often via tools like Optuna.</data>
  <data key="d7">model tuning, parameter optimization</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Cross-Domain Adaptation" target="Evaluation Standards">
  <data key="d5">7.0</data>
  <data key="d6">Effective cross-domain adaptation requires standardized evaluation standards to measure success and guide improvements.</data>
  <data key="d7">benchmarking, assessment</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Gaps" target="Future Research Directions">
  <data key="d5">7.0</data>
  <data key="d6">Addressing knowledge gaps is essential for advancing research and developing better domain adaptation methods.</data>
  <data key="d7">research development, innovation</data>
  <data key="d8">chunk-51f3018c9cc02b8c33f697aa15c2c906</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Gaps" target="Knowledge Transfer">
  <data key="d5">7.0</data>
  <data key="d6">Addressing knowledge gaps through transfer techniques improves the relevance of LLMs for domain-specific tasks."|</data>
  <data key="d7">knowledge gap filling, domain adaptation</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Scaling" target="Data Scarcity">
  <data key="d5">7.0</data>
  <data key="d6">Limited data availability hampers the ability to effectively scale domain-specific models, requiring advanced techniques to mitigate this challenge.</data>
  <data key="d7">data limitations, scalability</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Scaling" target="Model Architecture">
  <data key="d5">14.0</data>
  <data key="d6">Architectural choices like MOE enable models to scale to larger sizes while maintaining efficiency and performance.&lt;SEP&gt;Architectural choices such as MOE enable models to scale while maintaining performance.</data>
  <data key="d7">model design, scalability</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Challenges in Domain Specialization" target="Future Trends in LLM Research">
  <data key="d5">8.0</data>
  <data key="d6">Future research aims to address open problems such as architecture inaccessibility and high computational costs in domain-specific LLMs.</data>
  <data key="d7">research directions, innovation</data>
  <data key="d8">chunk-bb9f9f9e90a0c162892181da55ffb7b4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ms" target="T5">
  <data key="d5">14.0</data>
  <data key="d6">T5 is an example of a sequence-to-sequence model used for tasks like translation and summarization, which Ms models utilize as a base."|&gt;"model application, sequence-to-sequence</data>
  <data key="d7">7</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="External Augmentation" target="Black Box">
  <data key="d5">12.0</data>
  <data key="d6">Black box models only expose output, making external augmentation the primary approach for domain adaptation."|&gt;"model access, API-based</data>
  <data key="d7">6</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Crafting" target="Grey Box">
  <data key="d5">14.0</data>
  <data key="d6">Grey box models allow limited internal access, enabling more refined prompt design for domain knowledge extraction."|&gt;"partial internal access, prompt design</data>
  <data key="d7">7</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Strategies" target="Pre-training Intervention">
  <data key="d5">14.0</data>
  <data key="d6">Pre-training involves modifying the initial training process to embed domain-specific knowledge."|&gt;"training process, domain knowledge</data>
  <data key="d7">7</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Strategies" target="Fine-tuning Intervention">
  <data key="d5">16.0</data>
  <data key="d6">Fine-tuning adapts the model by further training on domain-specific data."|&gt;"training adaptation, domain data</data>
  <data key="d7">8</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Training Strategies" target="Inference-time Intervention">
  <data key="d5">14.0</data>
  <data key="d6">Inference-time modifications alter model behavior during deployment to produce domain-specific outputs."|&gt;"deployment, behavior modification</data>
  <data key="d7">7</data>
  <data key="d8">chunk-3c24b46980eea930fd6590d61f0b0f25</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Approaches in different categories" target="Parameters to incorporate">
  <data key="d5">16.0</data>
  <data key="d6">Parameters to incorporate serve as the overarching concept linking various methods of domain-specific model customization, such as fine-tuning, prompt engineering, and external knowledge augmentation.&lt;SEP&gt;Parameters to incorporate underpin various domain adaptation methods, linking concepts like fine-tuning, prompt engineering, and external knowledge integration."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Approaches in different categories" target="Levels of specialization">
  <data key="d5">14.0</data>
  <data key="d6">Different levels of specialization (black box, grey box, white box) are associated with specific methods and degrees of domain knowledge integration."|&lt;SEP&gt;Different levels such as black box, grey box, and white box correspond to different degrees and methods of domain knowledge integration."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="External knowledge augmentation" target="Domain Knowledge Augmentation">
  <data key="d5">18.0</data>
  <data key="d6">External knowledge augmentation is a specific approach involving the use of external domain knowledge sources to improve model performance."|&lt;SEP&gt;External knowledge augmentation is a specific approach that involves incorporating external domain knowledge to improve model performance."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural adapters" target="Augmentation">
  <data key="d5">16.0</data>
  <data key="d6">Neural adapters are auxiliary modules added to models to incorporate domain knowledge during the augmentation stage."|&lt;SEP&gt;Neural adapters are modules added to models during augmentation to incorporate domain knowledge without retraining the entire model."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Common Framework" target="Definition">
  <data key="d5">14.0</data>
  <data key="d6">The definition stage involves clearly specifying the domain, objectives, and constraints to guide the entire process."|&lt;SEP&gt;The definition stage involves explicitly specifying the domain, objectives, and constraints to guide the adaptation process."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Common Framework" target="Augmentation">
  <data key="d5">14.0</data>
  <data key="d6">Augmentation involves incorporating domain knowledge into models or inputs after defining objectives."|&lt;SEP&gt;Augmentation is the second stage where domain-specific knowledge is incorporated into the model or inputs."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Common Framework" target="Optimization">
  <data key="d5">14.0</data>
  <data key="d6">Optimization follows augmentation, focusing on refining model performance."|&lt;SEP&gt;Optimization refines model performance through methods like gradient descent or prompt tuning."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Optimization" target="APT">
  <data key="d5">18.0</data>
  <data key="d6">The APT is subjected to global optimizations, such as reordering and explicit hardware mapping, to improve performance.</data>
  <data key="d7">optimization process, performance improvement</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="AMT Generation">
  <data key="d5">16.0</data>
  <data key="d6">Optimization results inform the generation of the AMT, which adds necessary synchronization and data transfer details for code generation.</data>
  <data key="d7">intermediate representation, synchronization</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Parallelism">
  <data key="d5">15.0</data>
  <data key="d6">Optimization efforts aim to enhance parallel execution in kernels and code structure, leading to reduced runtimes."|&gt;"performance enhancement&lt;SEP&gt;Optimization efforts aim to enhance parallelism in kernels and code structure to improve runtime."|&gt;"performance enhancement</data>
  <data key="d7">7&lt;SEP&gt;8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Kernel">
  <data key="d5">16.0</data>
  <data key="d6">Optimized kernels are central to improving overall benchmark performance."|&gt;"core activity</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Code Rewriting">
  <data key="d5">13.0</data>
  <data key="d6">Rewriting code facilitates better optimization and parallel execution."|&gt;"method of optimization&lt;SEP&gt;Transforming code into alternative forms facilitates better parallelism and reduces overhead."|&gt;"method of optimization</data>
  <data key="d7">6&lt;SEP&gt;7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Dependency Analysis">
  <data key="d5">8.0</data>
  <data key="d6">Dependency analysis determines what can be optimized by understanding data and control dependencies."|&gt;"method of optimization</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Jacobi Kernel">
  <data key="d5">12.0</data>
  <data key="d6">Optimization strategies such as kernel fusion are yet to be implemented for the Jacobi kernel."|</data>
  <data key="d7">Methodology,Kernel Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Monte Kernel">
  <data key="d5">12.0</data>
  <data key="d6">Optimization of the Monte kernel depends on improving reduction implementations and shared memory strategies."|</data>
  <data key="d7">Methodology,Kernel Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Scheduling Problem">
  <data key="d5">16.0</data>
  <data key="d6">The scheduling problem in complex benchmarks is hindered by exponential search spaces, requiring alternative strategies like beam search."|</data>
  <data key="d7">Study Design,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization" target="Self">
  <data key="d5">8.0</data>
  <data key="d6">Optimization refers to the process of tuning the program parameters to improve performance, guided by the objective function.</data>
  <data key="d7">hyperparameter tuning, model optimization</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Domain Knowledge Augmentation" target="Retrieval augmentation">
  <data key="d5">9.0</data>
  <data key="d6">Retrieval augmentation involves fetching external domain knowledge to supplement model responses, enhancing depth and accuracy."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-384c5ce5a8f4d5429d6ef56e99c277dd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Implicit Knowledge" target="Pretrained Language Models (PLMs)">
  <data key="d5">16.0</data>
  <data key="d6">PLMs store implicit knowledge as learned embeddings that can be accessed during inference."|</data>
  <data key="d7">latent knowledge, learned embeddings</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Implicit Knowledge" target="Attention Mechanisms">
  <data key="d5">18.0</data>
  <data key="d6">Attention mechanisms enable retrieval of task-relevant information from implicit knowledge via scoring and weighting."|</data>
  <data key="d7">information retrieval, relevance scoring</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Implicit Knowledge" target="Instruction Cycle">
  <data key="d5">14.0</data>
  <data key="d6">An instruction cycle involves retrieving, parsing, and storing information from implicit knowledge to solve complex problems."|</data>
  <data key="d7">knowledge processing, problem solving</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explicit Knowledge with LLM" target="Retrieval">
  <data key="d5">17.0</data>
  <data key="d6">Retrieval of external knowledge sources supplies domain-specific information to improve model accuracy and task relevance."|&lt;SEP&gt;Retrieving external knowledge sources supplies relevant domain-specific information to improve model predictions and task accuracy."|</data>
  <data key="d7">knowledge retrieval, external context&lt;SEP&gt;knowledge sourcing, external context</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Retrieval" target="Neural Retriever">
  <data key="d5">18.0</data>
  <data key="d6">Neural retrievers vectorize queries and knowledge to find relevant information based on similarity metrics."|&lt;SEP&gt;Neural retrievers vectorize queries and knowledge to identify relevant information based on similarity metrics."|</data>
  <data key="d7">search, similarity metrics</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Retrieval" target="Similarity Metrics">
  <data key="d5">8.0</data>
  <data key="d6">Similarity metrics such as cosine similarity are used to identify relevant information during retrieval."|</data>
  <data key="d7">search algorithms, relevance evaluation</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Retrieval" target="Semantic Parsing">
  <data key="d5">16.0</data>
  <data key="d6">Semantic parsing converts natural language questions into formal representations that facilitate effective retrieval of relevant evidence documents.&lt;SEP&gt;Semantic parsing is used to convert natural language into formal representations that facilitate effective retrieval of relevant evidence documents.</data>
  <data key="d7">concept-application</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Retrieval" target="RAG (Retrieval-Augmented Generation)">
  <data key="d5">18.0</data>
  <data key="d6">RAG combines retrieval mechanisms with generative models to produce factually grounded responses, improving accuracy and controllability.</data>
  <data key="d7">method-integration</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Retrieval" target="Factual Knowledge">
  <data key="d5">16.0</data>
  <data key="d6">Retrieval enhances the factual grounding of generated responses by fetching relevant evidence, thereby reducing hallucinations.</data>
  <data key="d7">information-boost</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Retrieval" target="Keshav Santhanam">
  <data key="d5">9.0</data>
  <data key="d6">Author of ColBERTv2, an efficient retrieval system via lightweight late interaction, 2021.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Retrieval" target="Omar Khattab">
  <data key="d5">9.0</data>
  <data key="d6">Co-author working on retrieval effectiveness, published in 2021.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Retrieval" target="Jon Saad-Falcon">
  <data key="d5">9.0</data>
  <data key="d6">Co-author contributing to retrieval system research, 2021.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Retrieval" target="Christopher Potts">
  <data key="d5">9.0</data>
  <data key="d6">Co-author involved in retrieval and language understanding research, 2021.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Retrieval" target="Matei Zaharia">
  <data key="d5">9.0</data>
  <data key="d6">Co-author working on retrieval systems, 2021.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Neural Retriever" target="Retrieval-augmented generation">
  <data key="d5">8.0</data>
  <data key="d6">The neural retriever is used within RAG to access relevant documents from Wikipedia, supporting factual accuracy and knowledge retrieval.</data>
  <data key="d7">retrieval mechanism, external knowledge</data>
  <data key="d8">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge Base" target="Explicit Knowledge Source">
  <data key="d5">14.0</data>
  <data key="d6">Knowledge bases serve as external repositories of information that can be retrieved to support language models."|</data>
  <data key="d7">data source, external repository</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explicit Knowledge Source" target="Model Predictions">
  <data key="d5">12.0</data>
  <data key="d6">Explicit sources provide context that helps refine or correct the model's outputs."|</data>
  <data key="d7">prediction refinement, context</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Latent Embeddings" target="Knowledge Retrieval">
  <data key="d5">16.0</data>
  <data key="d6">Latent embeddings are used to retrieve relevant information through attention-based weighted sums."|</data>
  <data key="d7">vector retrieval, attention</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="External Knowledge Integration" target="Knowledge Management">
  <data key="d5">8.0</data>
  <data key="d6">Effective management of external knowledge sources supports better retrieval, updating, and scaling of domain knowledge."|</data>
  <data key="d7">knowledge updating, data handling</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Source" target="External repositories">
  <data key="d5">8.0</data>
  <data key="d6">External repositories like Wikipedia or Wikidata serve as sources of domain-specific information for retrieval."|</data>
  <data key="d7">data sources, external repositories</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explainability" target="Transparency">
  <data key="d5">7.0</data>
  <data key="d6">High transparency and explainability involve making the retrieval and reasoning processes interpretable."|</data>
  <data key="d7">interpretability, model transparency</data>
  <data key="d8">chunk-8bf110218e64d416737ff57117ec74cc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explainability" target="Trust">
  <data key="d5">8.0</data>
  <data key="d6">Higher explainability fosters greater user trust by making model decisions transparent and understandable.</data>
  <data key="d7">trust, transparency</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explainability" target="Black-box Methods">
  <data key="d5">8.0</data>
  <data key="d6">Black-box approaches often lack interpretability, making it difficult to explain model decisions to users.</data>
  <data key="d7">opacity, interpretability</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explainability" target="Grey-box Methods">
  <data key="d5">7.0</data>
  <data key="d6">Grey-box methods balance interpretability and complexity, providing some insights into model processes.</data>
  <data key="d7">partial transparency</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Explainability" target="White-box Methods">
  <data key="d5">8.0</data>
  <data key="d6">White-box models are fully interpretable, facilitating transparency and detailed understanding of decision mechanisms.</data>
  <data key="d7">full interpretability</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Collaborative Integration Approach" target="Multi-Stage Pipeline">
  <data key="d5">16.0</data>
  <data key="d6">The approach employs a multi-stage pipeline where LLMs generate commands, execute tools, and process results to improve task performance.&lt;SEP&gt;The approach employs a multi-stage process where LLMs generate commands, call domain tools, and process outputs to improve task accuracy and efficiency.</data>
  <data key="d7">workflow, integration</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Python Script" target="Arithmetic Tasks">
  <data key="d5">14.0</data>
  <data key="d6">Python scripts are used by LLMs to perform calculations in tasks like solving the chicken and rabbit problem.&lt;SEP&gt;Python scripts are used by LLMs to perform calculations, such as solving the chicken and rabbit problem, demonstrating computational assistance.</data>
  <data key="d7">computational aid, problem-solving&lt;SEP&gt;computational assistance, task solving</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Task Planners" target="Multi-Tool Coordination">
  <data key="d5">16.0</data>
  <data key="d6">Models coordinate multiple domain tools and decompose complex tasks into subtasks, guiding the overall process.&lt;SEP&gt;Models coordinate multiple tools and decompose complex tasks into subtasks, guiding the overall process.</data>
  <data key="d7">task decomposition, process control</data>
  <data key="d8">chunk-d8772ec3475b4e5b1b113032265f9d74</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Engineering" target="LLM">
  <data key="d5">6.0</data>
  <data key="d6">Designing effective prompts is crucial for eliciting accurate and reliable responses from LLMs across various tasks.</data>
  <data key="d7">technique, optimization</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Engineering" target="LangChain">
  <data key="d5">18.0</data>
  <data key="d6">LangChain employs extensive prompt templates for various tasks, often exceeding thousands of characters, illustrating the challenges DSPy aims to address.</data>
  <data key="d7">prompt engineering challenge</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Complex Tasks" target="Decomposed Prompting">
  <data key="d5">16.0</data>
  <data key="d6">Decomposed prompting offers a modular approach to solve complex tasks by breaking them into simpler components.&lt;SEP&gt;Decomposed prompting provides a modular, step-by-step approach to solving complex problems by breaking them into simpler sub-tasks.</data>
  <data key="d7">modularity, problem-solving</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Zero-shot Discrete Prompts" target="LLM">
  <data key="d5">14.0</data>
  <data key="d6">Zero-shot prompts are used to test the LLM's ability to perform tasks without prior examples, relying solely on instructions.</data>
  <data key="d7">prompting, task performance</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Zero-shot Discrete Prompts" target="Unseen Domains">
  <data key="d5">7.0</data>
  <data key="d6">Zero-shot prompts aim to enable LLMs to operate effectively in domains they haven't been trained on, such as new areas of NLP or computer vision.</data>
  <data key="d7">domain generalization, transfer learning</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Few-shot Discrete Prompts" target="LLM">
  <data key="d5">12.0</data>
  <data key="d6">Few-shot prompts provide illustrative examples to guide the LLM in performing specific tasks with limited data.</data>
  <data key="d7">training data, task guidance</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Instruction Alignment Pre-training" target="LLM">
  <data key="d5">18.0</data>
  <data key="d6">This pre-training strategy improves LLMs' zero-shot performance by aligning their instructions with human expectations.</data>
  <data key="d7">training strategy, model performance</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Semantic Segmentation" target="Unseen Domains">
  <data key="d5">7.0</data>
  <data key="d6">Semantic segmentation is used as an example of a domain where zero-shot learning and adaptation techniques are applied to classify pixels in images from unseen domains.</data>
  <data key="d7">application, domain transfer</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Manuscript" target="GingGPT">
  <data key="d5">6.0</data>
  <data key="d6">The manuscript submitted to ACM details the research on GingGPT.</data>
  <data key="d7">publication process, research dissemination</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Performance Evaluation" target="LLM">
  <data key="d5">6.0</data>
  <data key="d6">Evaluation metrics are used to measure how effectively LLMs perform on tasks like reasoning, classification, and domain adaptation.</data>
  <data key="d7">assessment, benchmarking</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Performance Evaluation" target="Equation (3)">
  <data key="d5">16.0</data>
  <data key="d6">Equation (3) calculates the average speedup across prompts, assessing overall model performance in code generation relative to the baseline.&lt;SEP&gt;Equation (3) computes the average speedup across all prompts, assessing overall code performance relative to the sequential baseline.</data>
  <data key="d7">average performance, benchmarking&lt;SEP&gt;benchmarking, average performance</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Evaluation" target="Efficiency n@k">
  <data key="d5">8.0</data>
  <data key="d6">Efficiency n@k measures how effectively generated code utilizes hardware resources, normalized by the number of processes or threads.</data>
  <data key="d7">resource utilization, performance efficiency</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Evaluation" target="Julia, Python/Numba, Kokkos">
  <data key="d5">9.0</data>
  <data key="d6">Godoy et al. evaluate these programming models' performance and portability on exascale nodes, assessing efficiency and scalability."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Performance Evaluation" target="Godoy, Valero-Lara, Dettling, Trefftz, Jorquera, Sheehy, Miller, Gonzalez-Tallada, Vetter, Churavy">
  <data key="d5">9.0</data>
  <data key="d6">They evaluate the performance and portability of Julia, Python/Numba, and Kokkos on exascale nodes, assessing efficiency and scalability."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Model Parameters" target="LLM">
  <data key="d5">7.0</data>
  <data key="d6">Model parameters are fixed during inference, but their pre-training on large datasets underpins the model's ability to generalize through prompts.</data>
  <data key="d7">model architecture, inference</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="APGAR Scores" target="Hypothesis">
  <data key="d5">13.0</data>
  <data key="d6">The hypothesis directly links the statement about her having low APGAR scores to the clinical measurement being investigated.&lt;SEP&gt;The hypothesis posits that the subject had low APGAR scores, linking the research question to a specific clinical measurement.</data>
  <data key="d7">causal inference, clinical assessment&lt;SEP&gt;research question, clinical measurement</data>
  <data key="d8">chunk-03fe9b9ee91cb6d7d1e6d86d23f22ae9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="APGAR Scores" target="She">
  <data key="d5">8.0</data>
  <data key="d6">She is the subject whose APGAR scores are being assessed, making her directly related to the measurement results.</data>
  <data key="d7">subject, data source</data>
  <data key="d8">chunk-03fe9b9ee91cb6d7d1e6d86d23f22ae9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Tuning" target="Task-dependent Prompt Tuning">
  <data key="d5">8.0</data>
  <data key="d6">Task-dependent prompt tuning is a specific implementation of prompt tuning that optimizes prompts for individual tasks, enabling efficient adaptation of language models.</data>
  <data key="d7">model adaptation, task specificity</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Tuning" target="Transferability of Prompts">
  <data key="d5">8.0</data>
  <data key="d6">Transferability allows prompts trained in one setting to be effectively applied to different tasks and models, improving efficiency and performance.</data>
  <data key="d7">generalization, adaptation</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Tuning" target="Parameter Efficiency">
  <data key="d5">16.0</data>
  <data key="d6">Prompt tuning techniques enable efficient adaptation of large models with fewer parameters, improving scalability.&lt;SEP&gt;Prompt tuning techniques improve parameter efficiency, allowing large models to be adapted with fewer resources.</data>
  <data key="d7">efficiency, adaptation</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Content Enhancement" target="WARP">
  <data key="d5">7.0</data>
  <data key="d6">WARP initializes prompts using the embedding of the '[MASK]' token, facilitating better convergence in prompt tuning.</data>
  <data key="d7">initialization, convergence</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Content Enhancement" target="KnowPrompt">
  <data key="d5">8.0</data>
  <data key="d6">KnowPrompt designs learnable prompts based on aggregated label and frequent words representations, improving prompt initialization and effectiveness.</data>
  <data key="d7">prompt design, initialization</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Content Enhancement" target="Instance-dependent prompt tuning">
  <data key="d5">8.0</data>
  <data key="d6">Instance-dependent prompts are enhanced with external knowledge and adaptive positioning to better capture instance-specific context.</data>
  <data key="d7">context-aware prompts, knowledge integration</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="WARP" target="Prompt Construction">
  <data key="d5">7.0</data>
  <data key="d6">WARP adopts all three prompt intersections with '[MASK]' tokens for classification, demonstrating a specific prompt construction technique.</data>
  <data key="d7">classification, prompt design</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="KnowPrompt" target="Relation Extraction">
  <data key="d5">9.0</data>
  <data key="d6">KnowPrompt uses templates with '[MASK]' tokens and virtual type words to improve relation extraction by conditioning on entity types.</data>
  <data key="d7">relation prediction, entity typing</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt-tuning" target="Prompt-tuning">
  <data key="d5">6.0</data>
  <data key="d6">Prompt-tuning demonstrates robustness to initialization strategies in large models, indicating its effectiveness across different prompt configurations.</data>
  <data key="d7">model robustness, training efficiency</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt-tuning" target="Prompt Transfer">
  <data key="d5">9.0</data>
  <data key="d6">Prompt transfer techniques improve performance by transferring prompts across domains and tasks, facilitating domain adaptation and generalization.</data>
  <data key="d7">domain adaptation, transfer learning</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt-tuning" target="Self-supervised Prompt Pretraining">
  <data key="d5">7.0</data>
  <data key="d6">Pre-training prompts with self-supervised learning on unlabeled data creates effective starting points for downstream prompt tuning.</data>
  <data key="d7">unsupervised learning, initialization</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt-tuning" target="LFPT5">
  <data key="d5">8.0</data>
  <data key="d6">LFPT5 employs transfer learning to enhance prompt initialization and adaptation across models and tasks.</data>
  <data key="d7">transfer learning, model adaptation</data>
  <data key="d8">chunk-6f045498b0e63780c3aef4b9daf795b6</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Self-Supervised Learning" target="Transferability of Prompts">
  <data key="d5">7.0</data>
  <data key="d6">Pre-training with self-supervised learning enhances the transferability of prompts across tasks and models, enabling better adaptation and faster convergence.</data>
  <data key="d7">training paradigm, transfer learning</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prefix-tuning" target="Attention Blocks">
  <data key="d5">8.0</data>
  <data key="d6">Prefix-tuning prepends prompts to sentence embeddings and attention activations, leveraging autoregressive model properties to influence subsequent words.</data>
  <data key="d7">model influence, sequence modeling</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="KiPT" target="Event Detection">
  <data key="d5">8.0</data>
  <data key="d6">KiPT identifies trigger words based on semantic similarity and reformulates sequence tagging into generative event record outputs.</data>
  <data key="d7">trigger identification, event modeling</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Dynamic Prompting" target="Prompt Structure Optimization">
  <data key="d5">7.0</data>
  <data key="d6">Dynamic prompting learns to define prompt position, length, and content dynamically for each input, improving flexibility and effectiveness.</data>
  <data key="d7">adaptive prompts, real-time tuning</data>
  <data key="d8">chunk-d3bbf812d2ef1f847454ee16a672e8f9</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapters" target="Unsupervised Domain Adaptation">
  <data key="d5">14.0</data>
  <data key="d6">Unsupervised domain adaptation utilizes adapters to improve model performance across domains without labeled data.</data>
  <data key="d7">domain transfer, unsupervised learning</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapters" target="AdapterFusion">
  <data key="d5">16.0</data>
  <data key="d6">AdapterFusion combines multiple adapters to improve domain adaptation performance.</data>
  <data key="d7">ensemble, domain generalization</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapters" target="AdaMix">
  <data key="d5">16.0</data>
  <data key="d6">AdaMix stacks multiple adapters and uses stochastic routing to efficiently combine their outputs without significant additional computational cost.</data>
  <data key="d7">multi-adapter integration, efficiency</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapters" target="AdapterHub">
  <data key="d5">14.0</data>
  <data key="d6">AdapterHub provides a library of integrated adapters for easy deployment and experimentation across models.</data>
  <data key="d7">tool, usability</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapters" target="Adapterhub">
  <data key="d5">16.0</data>
  <data key="d6">Adapterhub provides tools for creating and applying adapters, facilitating transfer learning in transformer models.</data>
  <data key="d7">tool support, transfer learning</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural Adapters" target="Invertible Adapters">
  <data key="d5">14.0</data>
  <data key="d6">Invertible adapters are a type of neural adapter that can reverse their transformations, inspired by autoencoders.</data>
  <data key="d7">reversibility, autoencoder analogy</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural Adapters" target="Hypercomplex Multiplication Layers">
  <data key="d5">12.0</data>
  <data key="d6">Hypercomplex multiplication layers serve as parameter-efficient alternatives in adapter architectures, like Compacters.</data>
  <data key="d7">parameter efficiency, Kronecker products</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural Adapters" target="Pruning">
  <data key="d5">14.0</data>
  <data key="d6">Pruning techniques are applied to neural adapters to reduce parameters and improve efficiency.</data>
  <data key="d7">model compression, sparsity</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural Adapters" target="Residual Connections">
  <data key="d5">12.0</data>
  <data key="d6">Residual connections are used in neural adapters like MAD-X to stabilize training and facilitate learning.</data>
  <data key="d7">training stability, improved learning</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Self-Instruct Demonstrations" target="LLaMA-adapter">
  <data key="d5">12.0</data>
  <data key="d6">Self-instruct demonstrations are incorporated in LLaMA-adapter to enhance instruction-following and reasoning capabilities.</data>
  <data key="d7">instruction learning, multi-modal reasoning</data>
  <data key="d8">chunk-3346afc27fba0a6e366bb328d5882c1d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="AdapterFusion" target="Multiple Adapters">
  <data key="d5">14.0</data>
  <data key="d6">AdapterFusion combines embeddings from multiple adapters trained on different tasks to boost overall performance.</data>
  <data key="d7">multi-task learning, performance</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Network Pruning" target="SparseAdapter">
  <data key="d5">14.0</data>
  <data key="d6">SparseAdapter employs network pruning techniques to reduce training parameters, inspired by general pruning methods.</data>
  <data key="d7">technique, efficiency</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="DyLora" target="LoRA">
  <data key="d5">12.0</data>
  <data key="d6">DyLora improves LoRA by dynamically searching for optimal rank and fixed block size, addressing limitations in representation power.</data>
  <data key="d7">optimization, adaptation</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="UniPELT" target="Adapter Methods">
  <data key="d5">12.0</data>
  <data key="d6">UniPELT activates different adapter combinations via gating to adapt to specific data or task setups.</data>
  <data key="d7">flexibility, task adaptation</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Polytropon" target="Adapters and Routing">
  <data key="d5">14.0</data>
  <data key="d6">Polytropon jointly learns adapters and routing functions to improve multi-task learning across diverse tasks.</data>
  <data key="d7">multi-task, routing</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Gradient Masking" target="Parameter Relevance">
  <data key="d5">16.0</data>
  <data key="d6">Gradient masking is used to focus training updates on relevant parameters based on their importance, improving fine-tuning efficiency."|"&lt;"technique, parameter relevance&lt;SEP&gt;Gradient masking is used to focus updates on relevant parameters based on their importance for a specific task, improving fine-tuning efficiency."|"&lt;"technique, parameter selection</data>
  <data key="d7">8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Child-Tuning" target="Dynamic Parameter Selection">
  <data key="d5">15.0</data>
  <data key="d6">Both approaches aim to improve fine-tuning by selecting optimal sub-networks or parameters based on task relevance and gradients."|"&lt;"methodology, optimization&lt;SEP&gt;Both methods aim to improve fine-tuning by selecting the most relevant parameters or sub-networks based on gradients and task relevance."|"&lt;"methodology, optimization</data>
  <data key="d7">7&lt;SEP&gt;8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-Tuning Challenges" target="Applications of Domain-Specific LLMs">
  <data key="d5">6.0</data>
  <data key="d6">Addressing challenges such as compliance and resource demands is essential for effective application of domain-specific LLMs."|"&lt;"problem, application</data>
  <data key="d7">6</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-Tuning Challenges" target="Regulations and Compliance">
  <data key="d5">6.0</data>
  <data key="d6">Ensuring models comply with legal and ethical standards during fine-tuning is a key challenge."|"&lt;"challenge, regulation</data>
  <data key="d7">6</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Fine-Tuning Challenges" target="High-Performance Hardware">
  <data key="d5">7.0</data>
  <data key="d6">Effective fine-tuning of large models requires access to expensive, specialized hardware."|"&lt;"challenge, hardware</data>
  <data key="d7">7</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Biomedical Applications">
  <data key="d5">17.0</data>
  <data key="d6">Domain-specific LLMs are employed in biomedical fields for tasks like genomics analysis and drug discovery, impacting healthcare."|"&lt;"application, domain impact&lt;SEP&gt;Domain-specific LLMs support biomedical research, drug discovery, and clinical decision-making."|"&lt;"application, biomedical</data>
  <data key="d7">8&lt;SEP&gt;9</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Natural Science Applications">
  <data key="d5">16.0</data>
  <data key="d6">LLMs are employed in earth science and biomedicine for data analysis, hypothesis generation, and scientific discovery."|"&lt;"application, science&lt;SEP&gt;LLMs support earth science and biomedicine by enabling advanced data analysis and hypothesis generation."|"&lt;"application, scientific research</data>
  <data key="d7">8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Social Science Applications">
  <data key="d5">14.0</data>
  <data key="d6">In social sciences, LLMs facilitate information extraction, summarization, and decision-making support."|"&lt;"application, societal impact&lt;SEP&gt;LLMs facilitate information extraction, summarization, and decision support in social sciences."|"&lt;"application, social sciences</data>
  <data key="d7">7</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Entity Recognition">
  <data key="d5">9.0</data>
  <data key="d6">LLMs recognize entities like genes, legal clauses, or biological terms within domain texts."|"&lt;"application, entity recognition</data>
  <data key="d7">9</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Text Summarization">
  <data key="d5">8.0</data>
  <data key="d6">Generating summaries of complex texts to aid understanding in specialized fields."|"&lt;"application, summarization</data>
  <data key="d7">8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Prediction and Recommendation">
  <data key="d5">8.0</data>
  <data key="d6">Forecasting trends and offering recommendations based on domain data."|"&lt;"application, prediction</data>
  <data key="d7">8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Conversational Agents">
  <data key="d5">7.0</data>
  <data key="d6">Implementing domain-specific chatbots or virtual assistants for expert guidance."|"&lt;"application, conversational agents</data>
  <data key="d7">7</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Applications of Domain-Specific LLMs" target="Code Generation and Analysis">
  <data key="d5">8.0</data>
  <data key="d6">Using LLMs to generate, analyze, or improve code in software engineering tasks."|"&lt;"application, code analysis</data>
  <data key="d7">8</data>
  <data key="d8">chunk-bf6a553300b045fc9b2a5d7c34c274bc</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Update" target="Index Hot-Swapping">
  <data key="d5">16.0</data>
  <data key="d6">Index hot-swapping demonstrates how RAG's knowledge base can be updated with newer data like the Dec 2018 Wikipedia dump.&lt;SEP&gt;Swapping indices from Dec 2016 to Dec 2018 demonstrates updating RAG's knowledge base with more recent information.</data>
  <data key="d7">knowledge updating, index management</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge Update" target="World Leaders">
  <data key="d5">7.0</data>
  <data key="d6">The list of world leaders is used to test the effectiveness of index updates in reflecting current knowledge.</data>
  <data key="d7">knowledge accuracy, index relevance</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge Update" target="Wikipedia Dumps">
  <data key="d5">8.0</data>
  <data key="d6">Different Wikipedia snapshots (2016 and 2018) are used to evaluate how well RAG updates and maintains current knowledge.</data>
  <data key="d7">knowledge freshness, index comparison</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Useful in the field of biology" target="Fundamental biomedical research">
  <data key="d5">8.0</data>
  <data key="d6">The application of LLMs supports basic biological research through data analysis and prediction of biological functions and mechanisms.</data>
  <data key="d7">domain application, biomedical research</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Useful in the field of biology" target="Clinical healthcare support">
  <data key="d5">9.0</data>
  <data key="d6">LLMs are used to analyze medical records, assist in diagnosis, and support personalized treatment, improving healthcare outcomes.</data>
  <data key="d7">healthcare application, medical support</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Clinical healthcare support" target="Medical records">
  <data key="d5">9.0</data>
  <data key="d6">Medical records are processed by LLMs to identify patterns, assist diagnoses, and support personalized treatments.</data>
  <data key="d7">clinical data analysis, decision support</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Earth Science" target="Methods and tools">
  <data key="d5">7.0</data>
  <data key="d6">Earth science employs methods like Earth observation, spatial analysis, and simulation modeling, with LLMs aiding in knowledge dissemination and data processing.</data>
  <data key="d7">interdisciplinary methods, data analysis</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Earth Science" target="Large language models like ChatGPT">
  <data key="d5">8.0</data>
  <data key="d6">ChatGPT and similar LLMs act as question-answer systems, assisting researchers to gain knowledge, generate ideas, and process satellite data.</data>
  <data key="d7">knowledge support, AI assistance</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Earth Science" target="Satellite and aerial data">
  <data key="d5">8.0</data>
  <data key="d6">Satellite and aerial data are used in Earth science to monitor environmental changes, land-use, and climate phenomena.</data>
  <data key="d7">data collection, environmental monitoring</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Earth Science" target="Spatial analysis">
  <data key="d5">8.0</data>
  <data key="d6">Spatial analysis techniques are employed to interpret geographic and environmental data, understanding interactions and changes.</data>
  <data key="d7">analytical techniques, interdisciplinary research</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Earth Science" target="Climate change, land-use change, natural disasters, environmental development, urbanization">
  <data key="d5">9.0</data>
  <data key="d6">These core concepts are studied through data, modeling, and analysis to understand and address environmental challenges.</data>
  <data key="d7">core concepts, environmental studies</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Methods" target="Fine-tuning, few-shot, zero-shot learning">
  <data key="d5">7.0</data>
  <data key="d6">LLMs</data>
  <data key="d7">These methods adapt LLMs for specific Earth science tasks and downstream applications.</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Finance and Law" target="Model specialization">
  <data key="d5">16.0</data>
  <data key="d6">Domain-specific fine-tuning allows LLMs to understand complex terminologies, regulations, and legal language.&lt;SEP&gt;LLMs require domain-specific fine-tuning to understand complex terminologies, regulations, and legal language for accurate content generation.</data>
  <data key="d7">domain adaptation, specialized training</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Finance and Law" target="Ethical guardrails">
  <data key="d5">14.0</data>
  <data key="d6">Implementing safeguards ensures ethical operation of LLMs in sensitive financial and legal contexts.&lt;SEP&gt;Implementing safeguards to ensure ethical operation of LLMs in high-stakes financial and legal contexts.</data>
  <data key="d7">ethics, safety</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model specialization" target="Human Computer Interaction and Software Engineering">
  <data key="d5">8.0</data>
  <data key="d6">LLMs are tailored to understand user inputs and assist in coding, bug detection, and interface design.</data>
  <data key="d7">domain-specific training, developer support</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model specialization" target="HCI and Software Engineering">
  <data key="d5">8.0</data>
  <data key="d6">LLMs are tailored to understand user inputs, improve interface design, and assist coding tasks.</data>
  <data key="d7">domain-specific training, user support</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Biomedical research" target="Genomic data">
  <data key="d5">8.0</data>
  <data key="d6">Genomic data provides foundational information for biomedical research, enabling analysis of genetic contributions to health and disease.</data>
  <data key="d7">data source, biological analysis</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Biomedical research" target="Proteomic data">
  <data key="d5">7.0</data>
  <data key="d6">Proteomic data complements genomic data, helping to understand protein functions and interactions in biological systems.</data>
  <data key="d7">data source, biological analysis</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Biomedical research" target="Protein structures">
  <data key="d5">8.0</data>
  <data key="d6">LLMs predict protein structures to understand cellular mechanisms and aid drug development.</data>
  <data key="d7">biological modeling, drug discovery</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Google Earth Engine" target="Satellite and aerial data">
  <data key="d5">8.0</data>
  <data key="d6">Google Earth Engine is a platform used to process satellite data for Earth science research.</data>
  <data key="d7">data processing, environmental analysis</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Google Earth Engine" target="Question-answer systems">
  <data key="d5">7.0</data>
  <data key="d6">Google Earth Engine supports question-answer systems by providing processed satellite data and analysis tools.</data>
  <data key="d7">knowledge support, environmental data</data>
  <data key="d8">chunk-f3c94db4d83b03209953445c36e8cb0d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Balancing General and Domain Knowledge" target="Explainability and Trust">
  <data key="d5">9.0</data>
  <data key="d6">Maintaining a balance between specific and general knowledge enhances the model's ability to provide accurate, trustworthy responses across diverse contexts.</data>
  <data key="d7">conceptual balance, model performance</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Adapting to Domain Evolution" target="Future Techniques in Domain Specialization">
  <data key="d5">8.0</data>
  <data key="d6">Adapting to evolving domain knowledge is essential for the continued relevance of specialized LLMs, which can be supported by emerging techniques.</data>
  <data key="d7">domain adaptability, continuous learning</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Scalability" target="Future Techniques in Domain Specialization">
  <data key="d5">7.0</data>
  <data key="d6">Scaling domain-specific training involves overcoming computational and data challenges, which future methods aim to address.</data>
  <data key="d7">resource efficiency, large-scale training</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hybrid Approaches" target="Future Techniques in Domain Specialization">
  <data key="d5">8.0</data>
  <data key="d6">Hybrid methods combine different strategies to optimize resource use and model performance in domain adaptation.</data>
  <data key="d7">method integration, resource optimization</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Meta-Learning or AutoML Techniques" target="Future Techniques in Domain Specialization">
  <data key="d5">7.0</data>
  <data key="d6">Meta-learning and AutoML automate the selection of best strategies for domain adaptation, reducing the need for extensive expertise.</data>
  <data key="d7">automation, strategy optimization</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Incorporating More Explicit World Knowledge" target="Future Techniques in Domain Specialization">
  <data key="d5">9.0</data>
  <data key="d6">Integrating structured knowledge sources like knowledge graphs enhances model understanding of domain relationships and improves accuracy.</data>
  <data key="d7">structured knowledge, domain relationships</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Human-in-the-loop Learning" target="Future Techniques in Domain Specialization">
  <data key="d5">8.0</data>
  <data key="d6">Involving human feedback allows continuous refinement and relevance of domain-specific LLMs.</data>
  <data key="d7">user interaction, continuous learning</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Human-in-the-loop Learning" target="Model Refinement">
  <data key="d5">8.0</data>
  <data key="d6">Human feedback directly influences model updates, ensuring the model remains aligned with domain expertise.</data>
  <data key="d7">feedback, continuous improvement</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Active Learning" target="Future Techniques in Domain Specialization">
  <data key="d5">8.0</data>
  <data key="d6">Active querying for unfamiliar concepts improves model handling of specialized topics and user engagement.</data>
  <data key="d7">interactive learning, domain unfamiliarity</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Active Learning" target="Knowledge Acquisition">
  <data key="d5">8.0</data>
  <data key="d6">Models actively seek out information on uncertain concepts, improving their understanding of specialized topics.</data>
  <data key="d7">querying, knowledge gain</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain Evolution" target="Model Adaptation">
  <data key="d5">7.0</data>
  <data key="d6">Evolving domain knowledge necessitates continuous model updates to maintain relevance and accuracy.</data>
  <data key="d7">model updating, relevance</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Graphs" target="Model Understanding">
  <data key="d5">8.0</data>
  <data key="d6">Incorporating knowledge graphs into models enhances their ability to understand complex relationships within domain data.</data>
  <data key="d7">structured knowledge, domain comprehension</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge Graphs" target="Graph Neural Networks">
  <data key="d5">8.0</data>
  <data key="d6">Graph neural networks process structured knowledge from knowledge graphs to improve model reasoning in a domain-specific context.</data>
  <data key="d7">structured data processing</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Meta-Learning" target="Model Optimization">
  <data key="d5">8.0</data>
  <data key="d6">Meta-learning strategies optimize the process of selecting training data, hyperparameters, and adaptation techniques for domain-specific models.</data>
  <data key="d7">learning to learn, optimization</data>
  <data key="d8">chunk-8405c501f648b7138a3c272cd94a6fd5</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Medical Ontology" target="Domain-Specific Techniques">
  <data key="d5">8.0</data>
  <data key="d6">Medical ontology provides structured knowledge that can be incorporated into domain-specific techniques to improve LLM understanding and reasoning in healthcare."|</data>
  <data key="d7">knowledge integration, domain modeling</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Medical Ontology" target="Knowledge Elicitation Techniques">
  <data key="d5">8.0</data>
  <data key="d6">Medical ontology serves as a structured knowledge source that can be utilized through elicitation techniques to enhance domain-specific LLMs in healthcare."|</data>
  <data key="d7">knowledge integration, structured knowledge</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Domain-Specific Techniques" target="Knowledge Elicitation">
  <data key="d5">9.0</data>
  <data key="d6">Effective techniques facilitate the extraction and incorporation of domain knowledge into LLMs, enhancing their accuracy in specialized fields."|</data>
  <data key="d7">knowledge transfer, model training</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Model Interpretability" target="Challenges in Domain Adaptation">
  <data key="d5">6.0</data>
  <data key="d6">Higher interpretability facilitates better understanding and trust, helping overcome some challenges in domain adaptation."|</data>
  <data key="d7">model transparency, trust</data>
  <data key="d8">chunk-4068ec6ee794d8ae5ed33ac825ff37d4</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Generative AI" target="HPC Numerical Kernels">
  <data key="d5">16.0</data>
  <data key="d6">AI tools like Codex and Copilot are evaluated for their capacity to generate HPC numerical kernels targeting parallel programming models.&lt;SEP&gt;Generative AI tools like Codex and Copilot are evaluated for their ability to generate HPC numerical kernels targeting parallel models.</data>
  <data key="d7">AI application, code generation</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Legal NLP" target="Josef Valvoda">
  <data key="d5">7.0</data>
  <data key="d6">Their 2023 publication examines the role of negative precedent in predicting legal outcomes.</data>
  <data key="d7">legal NLP, precedent analysis</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Ori Ram et al.">
  <data key="d5">14.0</data>
  <data key="d6">Preprint discussing in-context retrieval-augmented language models, contributing to advancements in model retrieval capabilities.&lt;SEP&gt;Preprint discussing retrieval-augmented language models, advancing retrieval techniques in NLP.</data>
  <data key="d7">retrieval, augmentation&lt;SEP&gt;retrieval-augmented models, in-context learning</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Adam Roberts et al.">
  <data key="d5">16.0</data>
  <data key="d6">Research on knowledge capacity of language models, focusing on parameter packing and information density.&lt;SEP&gt;Research on the capacity of language models to store knowledge, focusing on parameter utilization.</data>
  <data key="d7">knowledge capacity, parameters</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Joshua Robinson et al.">
  <data key="d5">14.0</data>
  <data key="d6">Research demonstrating the use of large language models for multiple choice question answering.&lt;SEP&gt;Study leveraging large language models for multiple choice question answering, demonstrating model capabilities.</data>
  <data key="d7">question answering, NLP tasks</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Timo Schick et al.">
  <data key="d5">14.0</data>
  <data key="d6">Research on Toolformer, a model that can teach itself to use external tools for improved performance.&lt;SEP&gt;Research on Toolformer, a model that can teach itself to use external tools, enhancing self-sufficiency and task performance.</data>
  <data key="d7">self-teaching, tool use</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Timo Schick and Hinrich Schütze">
  <data key="d5">12.0</data>
  <data key="d6">Research exploiting cloze questions for few-shot classification and inference, improving few-shot learning techniques.&lt;SEP&gt;Research exploiting cloze questions for few-shot classification and inference.</data>
  <data key="d7">cloze questions, few-shot learning</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Dale Schuurmans">
  <data key="d5">16.0</data>
  <data key="d6">Research on memory-augmented large language models, demonstrating their computational universality.&lt;SEP&gt;Research on memory-augmented large language models, establishing their computational universality.</data>
  <data key="d7">memory augmentation, universality</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Yongliang Shen et al.">
  <data key="d5">14.0</data>
  <data key="d6">Introduction of HuggingGPT, combining ChatGPT with HuggingFace tools for multi-task AI solutions.&lt;SEP&gt;Introduction of HuggingGPT, integrating ChatGPT with HuggingFace tools for multi-task AI solutions.</data>
  <data key="d7">multi-task AI, tool integration</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="arXiv" target="Preprint">
  <data key="d5">16.0</data>
  <data key="d6">Preprints are published on arXiv to share preliminary research findings in language modeling.</data>
  <data key="d7">research dissemination, early evidence</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Anastasia Razdaibiedina et al.">
  <data key="d5">14.0</data>
  <data key="d6">Research on progressive prompts for continual learning in language models.</data>
  <data key="d7">continual learning, prompts</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ma" target="Taylor Berg-Kirkpatrick">
  <data key="d5">12.0</data>
  <data key="d6">Ma is an author collaborating with Taylor Berg-Kirkpatrick on transfer learning research.&lt;SEP&gt;Ma is an author collaborating with Taylor Berg-Kirkpatrick on transfer learning studies."|</data>
  <data key="d7">author collaboration, transfer learning</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ma" target="Graham Neubig">
  <data key="d5">10.0</data>
  <data key="d6">Ma co-authored with Graham Neubig on transfer learning models in NLP."|&lt;SEP&gt;Ma co-authored with Graham Neubig on transfer learning models."|</data>
  <data key="d7">author collaboration, model development</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Shwai He" target="Liang Ding">
  <data key="d5">14.0</data>
  <data key="d6">Shwai He and Liang Ding co-authored work on improving parameter efficiency of adapters in NLP."|&lt;SEP&gt;Shwai He and Liang Ding co-authored work on improving parameter-efficiency of adapters in NLP."|</data>
  <data key="d7">collaborative research, model efficiency</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Daize Dong" target="Miao Zhang">
  <data key="d5">12.0</data>
  <data key="d6">Daize Dong and Miao Zhang co-authored research on transfer learning techniques."|&lt;SEP&gt;Daize Dong and Miao Zhang collaborated on research related to transfer learning techniques."|</data>
  <data key="d7">collaborative research, NLP methods&lt;SEP&gt;collaborative research, transfer learning methods</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Dacheng Tao" target="Dan Hendrycks">
  <data key="d5">10.0</data>
  <data key="d6">Dacheng Tao and Dan Hendrycks contributed to studies on neural activation functions and transfer learning."|&lt;SEP&gt;Dacheng Tao and Dan Hendrycks contributed to studies on neural activation functions for neural networks."|</data>
  <data key="d7">research contribution, neural activations</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Evan Hernandez" target="Belinda Z Li">
  <data key="d5">16.0</data>
  <data key="d6">Evan Hernandez and Belinda Z Li both research knowledge representation and manipulation in language models."|&lt;SEP&gt;Evan Hernandez and Belinda Z Li both research knowledge representations and their manipulation in language models."|</data>
  <data key="d7">research focus, knowledge manipulation</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neil Houlsby" target="Andrei Giurgiu">
  <data key="d5">14.0</data>
  <data key="d6">Neil Houlsby and Andrei Giurgiu co-authored work on parameter-efficient transfer learning for NLP."|</data>
  <data key="d7">author collaboration, transfer learning</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Stanislaw Jastrzebski" target="Bruna Morrone">
  <data key="d5">12.0</data>
  <data key="d6">Stanislaw Jastrzebski and Bruna Morrone collaborated on NLP transfer learning research."|&lt;SEP&gt;Stanislaw Jastrzebski and Bruna Morrone collaborated on transfer learning research in NLP."|</data>
  <data key="d7">collaborative research, NLP models</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Quentin De Laroussilhe" target="Andrea Gesmundo">
  <data key="d5">12.0</data>
  <data key="d6">Quentin De Laroussilhe and Andrea Gesmundo worked together on transfer learning approaches."|</data>
  <data key="d7">collaborative research, NLP techniques</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Mona Attariyan" target="Sylvain Gelly">
  <data key="d5">12.0</data>
  <data key="d6">Mona Attariyan and Sylvain Gelly collaborated on NLP transfer learning and model efficiency research."|&lt;SEP&gt;Mona Attariyan and Sylvain Gelly collaborated on NLP transfer learning studies."|</data>
  <data key="d7">collaborative research, model efficiency&lt;SEP&gt;collaborative research, transfer learning</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Jeremy Howard" target="Sebastian Ruder">
  <data key="d5">14.0</data>
  <data key="d6">Jeremy Howard and Sebastian Ruder co-authored work on language model fine-tuning and text classification."|&lt;SEP&gt;Jeremy Howard and Sebastian Ruder co-authored work on language model fine-tuning."|</data>
  <data key="d7">author collaboration, NLP fine-tuning</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Cheng-Yu Hsieh" target="Yasuhisa Fujii">
  <data key="d5">12.0</data>
  <data key="d6">Cheng-Yu Hsieh and Yasuhisa Fujii contributed to data-efficient NLP training."|&lt;SEP&gt;Cheng-Yu Hsieh and Yasuhisa Fujii contributed to data-efficient training and prompt-tuning in NLP."|</data>
  <data key="d7">collaborative research, training efficiency</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Edward J Hu" target="Yelong Shen">
  <data key="d5">16.0</data>
  <data key="d6">Edward J Hu and Yelong Shen both research large language models and their adaptation techniques."|&lt;SEP&gt;Edward J Hu and Yelong Shen both research large language models and their adaptations."|</data>
  <data key="d7">research focus, model adaptation</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yelong Shen" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Co-author working on chain-of-thought techniques, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Phillip Wallis" target="Zeyuan Allen-Zhu">
  <data key="d5">12.0</data>
  <data key="d6">Phillip Wallis and Zeyuan Allen-Zhu work on NLP model optimization and knowledge integration."|&lt;SEP&gt;Phillip Wallis and Zeyuan Allen-Zhu work on NLP model optimization and knowledge."|</data>
  <data key="d7">collaborative research, optimization</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yuanzhi Li" target="Shean Wang">
  <data key="d5">14.0</data>
  <data key="d6">Yuanzhi Li and Shean Wang collaborate on research related to large language models and knowledge representation."|</data>
  <data key="d7">research collaboration, knowledge representation&lt;SEP&gt;research collaboration, model knowledge</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Lu Wang" target="Weizhu Chen">
  <data key="d5">12.0</data>
  <data key="d6">Lu Wang and Weizhu Chen co-authored on large language model research and optimization."|</data>
  <data key="d7">collaborative research, model efficiency</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Weizhu Chen" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Co-author working on chain-of-thought demonstration techniques, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Shengding Hu" target="Ning Ding">
  <data key="d5">12.0</data>
  <data key="d6">Shengding Hu and Ning Ding work on knowledge-enhanced NLP methodologies."|</data>
  <data key="d7">collaborative research, knowledge incorporation&lt;SEP&gt;collaborative research, knowledge integration</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Huadong Wang" target="Zhiyuan Liu">
  <data key="d5">12.0</data>
  <data key="d6">Huadong Wang and Zhiyuan Liu research knowledge incorporation and prompt-tuning in language models."|&lt;SEP&gt;Huadong Wang and Zhiyuan Liu research knowledge incorporation in language models."|</data>
  <data key="d7">collaborative research, knowledge integration</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Jingang Wang" target="Juanzi Li">
  <data key="d5">12.0</data>
  <data key="d6">Jingang Wang and Juanzi Li collaborate on knowledge-aware NLP models."|</data>
  <data key="d7">collaborative research, knowledge integration</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Wei Wu" target="Maosong Sun">
  <data key="d5">12.0</data>
  <data key="d6">Wei Wu and Maosong Sun contribute to NLP transfer learning and knowledge incorporation."|</data>
  <data key="d7">collaborative research, transfer learning</data>
  <data key="d8">chunk-3f464ce07046c0201c72becc8d51da3e</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Manuscript submitted to ACM" target="GingGPT">
  <data key="d5">6.0</data>
  <data key="d6">The manuscript regarding GingGPT has been submitted to ACM, indicating peer review and dissemination.</data>
  <data key="d7">publication process, peer review</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Informal Proofs" target="Guiding Formal Theorem Provers">
  <data key="d5">12.0</data>
  <data key="d6">Informal proofs are used to guide the operation of formal theorem provers, facilitating proof generation and validation.&lt;SEP&gt;Informal proofs serve as guidance to formal theorem provers, helping automate the proof process and improve their effectiveness.</data>
  <data key="d7">proof guidance, automation&lt;SEP&gt;proof guidance, formal verification</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Learning" target="Language Understanding and Generation">
  <data key="d5">16.0</data>
  <data key="d6">Prompt learning techniques are employed to improve language understanding and generation tasks, including instance-aware prompt methods.&lt;SEP&gt;Prompt learning techniques are used to enhance models' capabilities in understanding and generating language, including approaches like instance-aware prompting.</data>
  <data key="d7">task performance, methodology</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hypercomplex Adapter Layers" target="Neural Network Efficiency">
  <data key="d5">14.0</data>
  <data key="d6">These adapter layers enable efficient low-rank adaptation, reducing parameters while maintaining or improving model performance.&lt;SEP&gt;These adapter layers enable efficient low-rank adaptations, reducing the parameter footprint while maintaining performance.</data>
  <data key="d7">efficiency, adaptation&lt;SEP&gt;efficiency, model adaptation</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Decomposed Prompting" target="Large Language Models as Zero-Shot Reasoners">
  <data key="d5">16.0</data>
  <data key="d6">Decomposed prompting facilitates zero-shot reasoning by breaking down complex tasks into manageable components for large language models.</data>
  <data key="d7">prompting, reasoning</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Internet-Augmented Dialogue Generation" target="Dialogue Systems">
  <data key="d5">16.0</data>
  <data key="d6">Integrating internet access into dialogue systems enhances response quality and knowledge retrieval.&lt;SEP&gt;Integrating internet access into dialogue systems enhances their ability to provide accurate, up-to-date responses and access external knowledge.</data>
  <data key="d7">knowledge access, system enhancement</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Clinical Language Models" target="Utility">
  <data key="d5">12.0</data>
  <data key="d6">Debates concern whether specialized clinical language models are still necessary given the capabilities of general models, impacting healthcare NLP applications.&lt;SEP&gt;Debates focus on whether specialized clinical language models are still necessary given advances in general models.</data>
  <data key="d7">specialization, utility</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Retrieval-Augmented Generation" target="Knowledge-Intensive NLP Tasks">
  <data key="d5">8.0</data>
  <data key="d6">RAG enhances the performance of models on knowledge-intensive NLP tasks by integrating retrieval mechanisms with generative models.</data>
  <data key="d7">application, model enhancement</data>
  <data key="d8">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Retrieval-augmented Generation" target="Knowledge-Intensive Tasks">
  <data key="d5">18.0</data>
  <data key="d6">This approach combines retrieval with generation to improve performance on knowledge-intensive NLP tasks.&lt;SEP&gt;This approach enhances performance on knowledge-intensive NLP tasks by combining retrieval mechanisms with generative models.</data>
  <data key="d7">knowledge access, task performance</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Chemical Reaction Arrays" target="Designing">
  <data key="d5">16.0</data>
  <data key="d6">Designing chemical reaction arrays involves using tools like phactor and ChatGPT to systematically plan and execute experiments.&lt;SEP&gt;Designing chemical reaction arrays involves using tools like phactor and ChatGPT to systematically plan experiments.</data>
  <data key="d7">methodology, experimental design</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="ChatGPT" target="GingGPT">
  <data key="d5">16.0</data>
  <data key="d6">GingGPT is based on or utilizes ChatGPT for solving AI tasks as part of its framework.&lt;SEP&gt;GingGPT utilizes or is based on ChatGPT for solving AI tasks.</data>
  <data key="d7">model foundation, AI architecture&lt;SEP&gt;model utilization, AI architecture</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Amjad Almahairi" target="Llama 2">
  <data key="d5">16.0</data>
  <data key="d6">Researcher contributing to Llama 2, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Long Ouyang" target="Training language models to follow instructions with human feedback">
  <data key="d5">18.0</data>
  <data key="d6">Long Ouyang's work supports the hypothesis that human feedback improves language model instruction-following abilities.</data>
  <data key="d7">research focus, model training</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="WebGPT" target="ChatGPT plugins">
  <data key="d5">16.0</data>
  <data key="d6">WebGPT utilizes ChatGPT plugins to extend its question-answering capabilities, integrating human feedback mechanisms.</data>
  <data key="d7">integration, enhancement</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="GPT-4" target="ParEval">
  <data key="d5">8.0</data>
  <data key="d6">GPT-4 is used within the ParEval methodology to generate code completions and evaluate performance.</data>
  <data key="d7">evaluation, natural language processing</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="GPT-4" target="Kokkos Search Problems">
  <data key="d5">16.0</data>
  <data key="d6">GPT-4 does not perform well on Kokkos search problems despite excelling elsewhere, indicating model limitations."|&lt;SEP&gt;GPT-4 does not perform well on Kokkos search problems despite excelling in other areas, indicating specific model limitations."|</data>
  <data key="d7">model performance, problem type</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="GPT-4" target="OpenAI (2023)">
  <data key="d5">18.0</data>
  <data key="d6">OpenAI reports that GPT-4 achieves 92% performance, pre-trained on GSM8K subset, indicating an advanced capability in reasoning tasks."|&gt;"model comparison, pre-training data</data>
  <data key="d7">9</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GPT-4" target="Problem Types">
  <data key="d5">16.0</data>
  <data key="d6">GPT-4 is evaluated on various problem types to measure its performance in different computational contexts.&lt;SEP&gt;GPT-4's evaluation across problem types demonstrates its capacity to perform well on various computational challenges.</data>
  <data key="d7">model assessment, task performance&lt;SEP&gt;model performance, task versatility</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="GPT-4" target="Execution Models">
  <data key="d5">16.0</data>
  <data key="d6">GPT-4's evaluation across execution models demonstrates its ability to operate effectively in diverse parallel and serial environments.&lt;SEP&gt;GPT-4's evaluation across execution models demonstrates its adaptability to different computational paradigms.</data>
  <data key="d7">model adaptability, computational paradigms&lt;SEP&gt;model versatility, computational paradigms</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Mad-x" target="Multi-task transfer">
  <data key="d5">16.0</data>
  <data key="d6">Mad-x enables multi-task cross-lingual transfer learning using adapters, enhancing multilingual NLP capabilities.</data>
  <data key="d7">multilingual transfer, framework</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hierarchical Domain-specific Language Models" target="Legal Decision Classification">
  <data key="d5">18.0</data>
  <data key="d6">Hierarchical models with attention improve the classification of legal decisions, demonstrating domain-specific effectiveness.</data>
  <data key="d7">model effectiveness, domain adaptation</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt Tuning of T5" target="Lifelong Few-Shot Learning">
  <data key="d5">16.0</data>
  <data key="d6">Prompt tuning of T5 facilitates lifelong few-shot learning by optimizing prompts for specific tasks.</data>
  <data key="d7">method, learning efficiency</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="The Journal of Machine Learning Research" target="Colin Raffel et al.">
  <data key="d5">16.0</data>
  <data key="d6">Published research exploring transfer learning limits with a unified text-to-text transformer.&lt;SEP&gt;Published research on transfer learning limits with transformer models, contributing to foundational knowledge in NLP.</data>
  <data key="d7">research publication, transfer learning</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="medRxiv" target="Arya Rao et al.">
  <data key="d5">12.0</data>
  <data key="d6">Study evaluating ChatGPT as an adjunct in radiologic decision-making, assessing practical healthcare applications.&lt;SEP&gt;Study evaluating ChatGPT as an adjunct in radiology, exploring practical healthcare applications.</data>
  <data key="d7">healthcare, decision support</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="medRxiv" target="Malik Sallam">
  <data key="d5">18.0</data>
  <data key="d6">Systematic review on ChatGPT's utility in healthcare education, research, and practice, discussing future perspectives and limitations.&lt;SEP&gt;Systematic review on ChatGPT's utility in healthcare education, research, and practice, highlighting future perspectives and limitations.</data>
  <data key="d7">healthcare, systematic review</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Advances in neural information processing systems" target="Sylvestre-Alvise Rebuffi et al.">
  <data key="d5">12.0</data>
  <data key="d6">Research on learning multiple visual domains with residual adapters, relevant for multi-domain models.&lt;SEP&gt;Research on learning multiple visual domains with residual adapters, relevant to multi-domain learning models.</data>
  <data key="d7">multi-domain learning, residual adapters</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Advances in neural information processing systems" target="Hanul Shin et al.">
  <data key="d5">16.0</data>
  <data key="d6">Research on continual learning with deep generative replay to address catastrophic forgetting.&lt;SEP&gt;Research on continual learning with deep generative replay, addressing catastrophic forgetting.</data>
  <data key="d7">continual learning, generative replay</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Nature Machine Intelligence" target="Jerret Ross et al.">
  <data key="d5">16.0</data>
  <data key="d6">Research on chemical language representations capturing molecular structures and properties for ML applications in chemistry.&lt;SEP&gt;Research on chemical language representations capturing molecular structures and properties, enabling ML in chemistry.</data>
  <data key="d7">molecular structures, chemical properties</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="HuggingFace" target="GingGPT">
  <data key="d5">14.0</data>
  <data key="d6">GingGPT is implemented or supported within the HuggingFace ecosystem, leveraging its tools and libraries.&lt;SEP&gt;GingGPT is implemented or supported within the HuggingFace platform, leveraging its tools and libraries.</data>
  <data key="d7">platform integration, tool support&lt;SEP&gt;platform support, tool integration</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Toolformer" target="Timo Schick">
  <data key="d5">10.0</data>
  <data key="d6">Author of Toolformer, a model enabling language models to teach themselves to use tools, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Jane Dwivedi-Yu">
  <data key="d5">10.0</data>
  <data key="d6">Co-author working on Toolformer project, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Roberto Dess ı">
  <data key="d5">10.0</data>
  <data key="d6">Co-author involved in tool-use in language models, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Roberta Raileanu">
  <data key="d5">10.0</data>
  <data key="d6">Co-author contributing to tool-learning in language models, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Maria Lomeli">
  <data key="d5">10.0</data>
  <data key="d6">Co-author involved in tool-use research, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Luke Zettlemoyer">
  <data key="d5">10.0</data>
  <data key="d6">Co-author working on language models and tools, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Nicola Cancedda">
  <data key="d5">10.0</data>
  <data key="d6">Co-author contributing to tool-use in language models, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Toolformer" target="Thomas Scialom">
  <data key="d5">10.0</data>
  <data key="d6">Co-author involved in tool-learning for language models, 2023.</data>
  <data key="d7">10</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GingGPT" target="arXiv:2303.17580">
  <data key="d5">18.0</data>
  <data key="d6">The manuscript arXiv:2303.17580 discusses the development and application of GingGPT.&lt;SEP&gt;The research manuscript arXiv:2303.17580 discusses GingGPT and its approach to solving AI tasks.</data>
  <data key="d7">research documentation, publication&lt;SEP&gt;research publication, methodology</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim" target="Deep generative replay">
  <data key="d5">5.0</data>
  <data key="d6">Their 2017 paper contributes to continual learning methods in neural information processing.</data>
  <data key="d7">methodology, research contribution</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan" target="Model distillation">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 preprint focuses on transferring reasoning capabilities into smaller models via semantic decompositions.</data>
  <data key="d7">model compression, reasoning transfer</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, Dani Yogatama" target="End-to-end training">
  <data key="d5">3.0</data>
  <data key="d6">Their 2021 work involves training multi-document readers and retrievers for open-domain QA.</data>
  <data key="d7">methodology, QA systems</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg" target="Task planning using LLMs">
  <data key="d5">2.0</data>
  <data key="d6">Their workshop paper discusses generating robot task plans with language models.</data>
  <data key="d7">application, robotics</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu" target="Black-box tuning">
  <data key="d5">2.0</data>
  <data key="d6">Their 2022 work addresses tuning language models as a service without internal access.</data>
  <data key="d7">model tuning, API services</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yi-Lin Sung, Jaemin Cho, Mohit Bansal" target="Parameter-efficient transfer">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 preprint proposes ladder side-tuning for efficient transfer learning.</data>
  <data key="d7">transfer learning, efficiency</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Dídac Surís, Sachit Menon, Carl Vondrick" target="Visual inference">
  <data key="d5">1.0</data>
  <data key="d6">Their 2023 preprint introduces reasoning via Python execution for visual inference tasks.</data>
  <data key="d7">multimodal reasoning, visual AI</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al." target="Foundation models">
  <data key="d5">9.0</data>
  <data key="d6">Their 2023 preprint presents Llama, an open and efficient foundation language model.</data>
  <data key="d7">model development, foundation models</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi" target="Parameter search-free adaptation">
  <data key="d5">8.0</data>
  <data key="d6">Their 2022 preprint discusses DyLoRA, a method for parameter-efficient tuning using dynamic low-rank adaptation.</data>
  <data key="d7">model tuning, efficiency</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Josef Valvoda, Ryan Cotterell, Simone Teufel" target="Legal outcome prediction">
  <data key="d5">7.0</data>
  <data key="d6">Their 2023 publication examines the role of negative precedent in legal outcome predictions.</data>
  <data key="d7">legal NLP, precedent analysis</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin" target="Transformer architecture">
  <data key="d5">10.0</data>
  <data key="d6">Their 2017 paper 'Attention is all you need' introduces the transformer model, foundational to modern NLP.</data>
  <data key="d7">model architecture, attention mechanism</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, Daniel Matthew Cer" target="Model adaptation">
  <data key="d5">6.0</data>
  <data key="d6">Their 2021 paper on SPoT discusses better frozen model adaptation through soft prompt transfer.</data>
  <data key="d7">model adaptation, prompt transfer</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu" target="Memory-augmented models">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 paper introduces G-MAP, a memory-augmented pre-trained language model for domain-specific tasks.</data>
  <data key="d7">model architecture, domain adaptation</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu" target="Document-level translation">
  <data key="d5">3.0</data>
  <data key="d6">Their 2023 preprint discusses translation at the document level using large language models.</data>
  <data key="d7">application, machine translation</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, Tarek Abdelzaher" target="Knowledge graph reasoning">
  <data key="d5">2.0</data>
  <data key="d6">Their study addresses few-shot reasoning over temporal knowledge graphs.</data>
  <data key="d7">knowledge graphs, reasoning</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hanul Shin" target="Deep generative replay">
  <data key="d5">5.0</data>
  <data key="d6">Their 2017 paper discusses continual learning techniques involving deep generative replay.</data>
  <data key="d7">learning methodology, continual learning</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Kumar Shridhar" target="Model distillation">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 preprint focuses on distilling reasoning capabilities into smaller models via semantic decompositions.</data>
  <data key="d7">model compression, reasoning transfer</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Devendra Singh" target="End-to-end training">
  <data key="d5">3.0</data>
  <data key="d6">Their 2021 paper describes training multi-document readers and retrievers for open-domain question answering.</data>
  <data key="d7">QA methodology, training approach</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ishika Singh" target="Task planning using LLMs">
  <data key="d5">2.0</data>
  <data key="d6">Their workshop paper discusses generating robot task plans using large language models.</data>
  <data key="d7">application, robotics, planning</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yusheng Su" target="Prompt transferability">
  <data key="d5">3.0</data>
  <data key="d6">Their 2022 work investigates how prompt tuning techniques transfer across NLP tasks.</data>
  <data key="d7">prompt tuning, transferability</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Tianxiang Sun" target="Black-box tuning">
  <data key="d5">2.0</data>
  <data key="d6">Their 2022 research explores tuning language models as a service without internal access.</data>
  <data key="d7">model tuning, API</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yi-Lin Sung" target="Parameter-efficient transfer">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 preprint proposes ladder side-tuning for efficient transfer learning.</data>
  <data key="d7">transfer learning, efficiency</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Dídac Surís" target="Visual inference">
  <data key="d5">1.0</data>
  <data key="d6">Their 2023 preprint introduces reasoning via Python execution for visual tasks.</data>
  <data key="d7">multimodal reasoning, visual AI</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hugo Touvron" target="Foundation models">
  <data key="d5">9.0</data>
  <data key="d6">Their 2023 preprint presents Llama, an open and efficient foundation language model.</data>
  <data key="d7">model development, foundation models</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Hugo Touvron" target="Llama 2">
  <data key="d5">16.0</data>
  <data key="d6">Author of Llama 2, open foundation and fine-tuned chat models, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Ashish Vaswani" target="Transformer architecture">
  <data key="d5">10.0</data>
  <data key="d6">Their 2017 paper 'Attention is all you need' introduces the transformer, foundational for modern NLP models.</data>
  <data key="d7">model architecture, attention mechanism</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Jakob Uszkoreit" target="Eunsol Choi">
  <data key="d5">4.0</data>
  <data key="d6">Both are contributing to advancements in question answering for long documents."|&lt;SEP&gt;Both work on models and techniques for question answering and language modeling."|</data>
  <data key="d7">2</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Illia Polosukhin" target="Eunsol Choi">
  <data key="d5">4.0</data>
  <data key="d6">Research collaboration in neural question answering systems."|&lt;SEP&gt;Research collaboration in question answering methodologies."|</data>
  <data key="d7">2</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Tu Vu" target="Model adaptation">
  <data key="d5">6.0</data>
  <data key="d6">Their 2021 paper discusses improved frozen model adaptation through soft prompt transfer.</data>
  <data key="d7">model adaptation, prompt transfer</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Zhongwei Wan" target="Memory-augmented models">
  <data key="d5">4.0</data>
  <data key="d6">Their 2022 paper introduces G-MAP, a memory-augmented pre-trained language model for domain-specific tasks.</data>
  <data key="d7">model architecture, domain adaptation</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Wei Zhang" target="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d5">14.0</data>
  <data key="d6">Wei Zhang contributed to evidence aggregation approaches for answer reranking.&lt;SEP&gt;Wei Zhang contributed to evidence aggregation techniques in NLP.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Longyue Wang" target="Document-level translation">
  <data key="d5">3.0</data>
  <data key="d6">Their 2023 preprint discusses large language models applied to document-level machine translation.</data>
  <data key="d7">application, translation</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Ruijie Wang" target="Knowledge graph reasoning">
  <data key="d5">2.0</data>
  <data key="d6">Their study addresses few-shot reasoning over temporal knowledge graphs.</data>
  <data key="d7">knowledge graphs, reasoning</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Preprint" target="Gao et al. (2023a)">
  <data key="d5">18.0</data>
  <data key="d6">Gao et al. (2023a) being a preprint indicates that the findings or methods are shared early, prior to peer review, to facilitate feedback and rapid dissemination.&lt;SEP&gt;Preprint indicates that Gao et al. (2023a) is a preliminary research output shared for early dissemination.&lt;SEP&gt;Preprint status indicates early dissemination of research findings, allowing community feedback and rapid sharing.</data>
  <data key="d7">Evidence Types&lt;SEP&gt;Study Designs, Evidence Types</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Preprint" target="context">
  <data key="d5">12.0</data>
  <data key="d6">The preprint provides background and preliminary findings relevant to the research process described in the context.&lt;SEP&gt;The preprint provides background information, preliminary findings, or methodological context relevant to the overall research process.</data>
  <data key="d7">research background, preliminary data</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Learning to Sample and Aggregate" target="Few-Shot Reasoning">
  <data key="d5">14.0</data>
  <data key="d6">The framework aims to improve few-shot reasoning capabilities over temporal knowledge graphs.</data>
  <data key="d7">reasoning enhancement, knowledge graphs</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="K-Adapter" target="Knowledge Infusion">
  <data key="d5">16.0</data>
  <data key="d6">K-Adapter is a method for infusing external knowledge into pre-trained models.</data>
  <data key="d7">knowledge integration, model enhancement</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Self-Consistency" target="Chain of Thought Reasoning">
  <data key="d5">18.0</data>
  <data key="d6">Self-Consistency improves chain-of-thought reasoning by encouraging consistent intermediate outputs.</data>
  <data key="d7">reasoning accuracy, consistency</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Interactive Planning" target="Describe, Explain, Plan and Select">
  <data key="d5">14.0</data>
  <data key="d6">This framework facilitates open-world multi-tasking through structured planning in language models.</data>
  <data key="d7">multi-task, planning</data>
  <data key="d8">chunk-130f4b29fc5e5322f823d6edfc5ef3c3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Yi Tay" target="Recitation-Augmented Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Co-author contributing to recitation-augmented models, 2022.</data>
  <data key="d7">14</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Minlie Huang" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Co-author involved in generating chain-of-thought demonstrations, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhuosheng Zhang" target="Automatic chain of thought prompting">
  <data key="d5">9.0</data>
  <data key="d6">Zhuosheng Zhang's work focuses on automating chain of thought prompting in large language models.</data>
  <data key="d7">research contribution, prompting automation</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Denny Zhou" target="Recitation-Augmented Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Co-author involved in recitation-augmented models, 2022.</data>
  <data key="d7">14</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Denny Zhou" target="Rationale-augmented ensembles">
  <data key="d5">8.0</data>
  <data key="d6">Denny Zhou's work involves ensemble methods that incorporate rationale explanations to enhance reasoning in language models.</data>
  <data key="d7">research contribution, model improvement</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Denny Zhou" target="Self-consistency">
  <data key="d5">7.0</data>
  <data key="d6">Denny Zhou contributed to the development of self-consistency techniques to improve chain of thought reasoning.</data>
  <data key="d7">technique development, reasoning enhancement</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Jason Wei" target="Chain of thought prompting">
  <data key="d5">9.0</data>
  <data key="d6">Jason Wei's research demonstrates that chain of thought prompting elicits reasoning in large language models.</data>
  <data key="d7">research findings, prompting technique</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Xuezhi Wang" target="Recitation-Augmented Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Co-author involved in recitation-augmented language models, 2022.</data>
  <data key="d7">14</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Alec Radford" target="Improving language understanding">
  <data key="d5">7.0</data>
  <data key="d6">Author of work on generative pre-training to enhance language comprehension, 2018.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Open Challenges in Domain Specialization" target="Future Research Trends">
  <data key="d5">3.0</data>
  <data key="d6">Emerging research directions aim to address current limitations and expand the capabilities of domain-specific LLMs."|&gt;"research development, innovation</data>
  <data key="d7">3</data>
  <data key="d8">chunk-1c77a184d805ecadcb8dd5b0d1057e7c</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LLM" target="Zero-shot-CoT">
  <data key="d5">16.0</data>
  <data key="d6">Zero-shot-CoT prompts enhance the reasoning ability of LLMs by prompting them to think step-by-step, leading to improved performance on reasoning tasks.</data>
  <data key="d7">reasoning, prompting techniques</data>
  <data key="d8">chunk-daac5179ef8fa77fe775da37eb1276f3</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LoRA" target="Pre-trained Weights">
  <data key="d5">18.0</data>
  <data key="d6">LoRA implants low-rank modules in parallel to frozen pre-trained weights, enabling efficient fine-tuning without latency.</data>
  <data key="d7">parameter reduction, inference efficiency</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="LoRA" target="KronA">
  <data key="d5">16.0</data>
  <data key="d6">Kronecker adapter replaces SVD modules in LoRA with Kronecker product modules to enhance expressivity while maintaining parameter efficiency.</data>
  <data key="d7">representation power, efficiency</data>
  <data key="d8">chunk-bc782afdd6c18f4e206e464038047556</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Neural Language Models" target="Scaling Laws">
  <data key="d5">16.0</data>
  <data key="d6">Scaling laws describe how increasing model size, data, and compute impacts the performance of neural language models.&lt;SEP&gt;Scaling laws describe how the performance of neural language models improves with increased size and data.</data>
  <data key="d7">model scaling, performance&lt;SEP&gt;performance scaling, model size</data>
  <data key="d8">chunk-50aebbb39645b3b6929fbd72650593cd</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Knowledge-Intensive Tasks" target="RAG">
  <data key="d5">9.0</data>
  <data key="d6">RAG models are designed to improve performance on tasks requiring access to external knowledge, such as question answering and fact verification.</data>
  <data key="d7">task performance, knowledge access</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen Kan, Soujanya Poria" target="Research on Domain Adaptation">
  <data key="d5">18.0</data>
  <data key="d6">These researchers contribute to methodologies for efficient domain adaptation in machine learning, especially using adapters."|&lt;SEP&gt;These researchers contribute to methodologies for efficient domain adaptation using adapters in machine learning."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Research on Language Model Tuning" target="Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa">
  <data key="d5">16.0</data>
  <data key="d6">These authors explore frameworks for parameter-efficient language model tuning and unified approaches."|&lt;SEP&gt;These authors explore frameworks for parameter-efficient language model tuning."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov" target="Research on Model Editing">
  <data key="d5">10.0</data>
  <data key="d6">These researchers investigate locating, editing, and supporting factual knowledge in language models."|</data>
  <data key="d7">10</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov" target="Research on Locating and Editing Factual Associations">
  <data key="d5">10.0</data>
  <data key="d6">These researchers focus on methods to locate, edit, and support factual associations and memory in language models."|</data>
  <data key="d7">10</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving" target="Research on Verified Quotes in Language Models">
  <data key="d5">9.0</data>
  <data key="d6">These authors focus on teaching models to support answers with verified quotes."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving" target="Research on Supporting Answers with Verified Quotes">
  <data key="d5">9.0</data>
  <data key="d6">These authors develop techniques for teaching language models to provide answers supported by verified quotes."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-09444836044cdc7760bd666a5b6e6c6d</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Decomposed In-Context Learning" target="Self-Correction">
  <data key="d5">18.0</data>
  <data key="d6">This methodology improves Text-to-SQL accuracy by decomposing the learning process and applying self-correction techniques.</data>
  <data key="d7">technique, performance enhancement</data>
  <data key="d8">chunk-f7b1fd0aca9367181c74acfbf0c25a9b</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="ICLR" target="Victor Sanh et al.">
  <data key="d5">16.0</data>
  <data key="d6">Research showing multitask prompted training enables zero-shot task generalization in language models.&lt;SEP&gt;Research showing that multitask prompted training enables zero-shot task generalization in language models.</data>
  <data key="d7">zero-shot learning, multitask training&lt;SEP&gt;zero-shot, multitask training</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Proceedings of EMNLP" target="Thomas Scialom et al.">
  <data key="d5">14.0</data>
  <data key="d6">Research on fine-tuned language models as continual learners, emphasizing adaptability across tasks.&lt;SEP&gt;Research on fine-tuned language models as continual learners, emphasizing adaptability.</data>
  <data key="d7">fine-tuning, continual learning</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="IEEE Big Data Conference" target="Shohreh Shaghaghian et al.">
  <data key="d5">12.0</data>
  <data key="d6">Research on customizing contextualized language models for legal document review, enhancing legal NLP applications.&lt;SEP&gt;Research on customizing contextualized language models for legal document review, enhancing legal tech applications.</data>
  <data key="d7">legal NLP, customization</data>
  <data key="d8">chunk-8945fa45930b3a8b5decf88507c783df</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Prompt transferability" target="Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al">
  <data key="d5">3.0</data>
  <data key="d6">Their 2022 paper explores the transferability of prompt tuning techniques in NLP.</data>
  <data key="d7">methodology, NLP adaptation</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="Parameter search-free adaptation" target="Mojtaba Valipour">
  <data key="d5">8.0</data>
  <data key="d6">Their 2022 preprint discusses DyLoRA, a method for efficient parameter tuning of pre-trained models.</data>
  <data key="d7">model tuning, efficiency</data>
  <data key="d8">chunk-0fedeaf17a30ea46b3aacf96752c0053</data>
  <data key="d9">Domain Specialization as the Key to Make Large Language Models Disruptive - 2305.18703v7.pdf</data>
</edge>
<edge source="ParEval" target="Evaluation of Language Models">
  <data key="d5">18.0</data>
  <data key="d6">ParEval functions as the benchmark framework to systematically evaluate and compare the performance of different language models on parallel coding tasks.&lt;SEP&gt;ParEval serves as the benchmark framework to evaluate the effectiveness of language models in generating parallel code across multiple tasks.</data>
  <data key="d7">benchmarking, performance assessment</data>
  <data key="d8">chunk-4b0500ce797f3002cd2bbbd0d398db00</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval" target="Section 7">
  <data key="d5">8.0</data>
  <data key="d6">Section 7 details the evaluation metrics and procedures used in the ParEval methodology.</data>
  <data key="d7">evaluation metrics, methodology description</data>
  <data key="d8">chunk-8044b7aee8628a78f321eae0291f7292</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval" target="pass@1 score">
  <data key="d5">9.0</data>
  <data key="d6">ParEval uses pass@1 as a key metric to evaluate correctness of generated code.</data>
  <data key="d7">evaluation accuracy, correctness</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval" target="Efficiency@1">
  <data key="d5">8.0</data>
  <data key="d6">ParEval assesses efficiency of generated code using the Efficiency@1 metric.</data>
  <data key="d7">performance measurement</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval" target="Speedup@1">
  <data key="d5">8.0</data>
  <data key="d6">ParEval evaluates the scaling behavior of generated code with the Speedup@1 metric.</data>
  <data key="d7">scalability assessment</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval" target="HPC-Coder-V2">
  <data key="d5">20.0</data>
  <data key="d6">HPC-Coder-V2's performance is evaluated against ParEval, demonstrating superior parallel code generation capabilities.&lt;SEP&gt;HPC-Coder-V2's performance is evaluated on ParEval, demonstrating its superior ability to generate parallel code compared to other models.</data>
  <data key="d7">evaluation&lt;SEP&gt;evaluation, benchmarking</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="ParEval" target="HPC-Coder-V2 models">
  <data key="d5">18.0</data>
  <data key="d6">HPC-Coder-V2 models are evaluated using ParEval to determine their correctness and effectiveness in code generation tasks.&lt;SEP&gt;HPC-Coder-V2 models are evaluated using ParEval to measure their correctness and performance in code generation tasks.</data>
  <data key="d7">evaluation, benchmarking</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="correctness" target="generated code">
  <data key="d5">16.0</data>
  <data key="d6">The correctness of generated code determines whether it produces the correct outputs and adheres to the task requirements.&lt;SEP&gt;The correctness of generated code determines whether it produces the expected outputs and passes validation.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="correctness" target="Providing correct implementations">
  <data key="d5">7.0</data>
  <data key="d6">Supplying correct code examples enhances LLMs' correctness in code generation.</data>
  <data key="d7">training, accuracy</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="correctness" target="parallel code">
  <data key="d5">9.0</data>
  <data key="d6">Correctness of generated code is measured by pass@1, indicating how accurately the code implements the intended algorithms."|</data>
  <data key="d7">accuracy, evaluation</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="scalability" target="parallel code">
  <data key="d5">8.0</data>
  <data key="d6">Scalability assesses how well the generated code maintains performance as problem size or resources increase."|</data>
  <data key="d7">performance, evaluation</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Transformer-based models" target="Large Language Models for Code">
  <data key="d5">19.0</data>
  <data key="d6">Transformer models underpin the architecture of large language models used for code understanding and generation.&lt;SEP&gt;Transformers serve as the core architecture behind large language models used for code understanding and generation.</data>
  <data key="d7">architecture, NLP, code modeling&lt;SEP&gt;model architecture, NLP, code generation</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Large Language Models for Code" target="Pre-training corpus">
  <data key="d5">17.0</data>
  <data key="d6">Large corpora like The Stack and The Pile serve as the foundational data sources for training large language models in code-related tasks.&lt;SEP&gt;The large datasets like The Stack and The Pile are used to pre-train models such as CodeLlama, providing diverse data for effective learning.</data>
  <data key="d7">training data, dataset&lt;SEP&gt;training data, foundational resources</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Large Language Models for Code" target="CodeLlama">
  <data key="d5">15.0</data>
  <data key="d6">CodeLlama is a fine-tuned natural language model specialized on code data to improve code generation capabilities.&lt;SEP&gt;CodeLlama is a specific example of a large language model fine-tuned on code data, demonstrating the application of such models.</data>
  <data key="d7">model specialization, fine-tuning</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Applying LLMs to parallel and HPC code" target="HPC-specific models">
  <data key="d5">8.0</data>
  <data key="d6">Models like HPCCoder are trained or fine-tuned specifically for HPC code to improve tasks like code generation and performance prediction.</data>
  <data key="d7">specialized models, domain adaptation</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Applying LLMs to parallel and HPC code" target="HPC tasks">
  <data key="d5">7.0</data>
  <data key="d6">HPC tasks such as code generation, pragma labeling, and performance prediction are used as benchmarks to evaluate LLM effectiveness in HPC environments.</data>
  <data key="d7">task evaluation, domain-specific testing</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Benchmarking LLMs for code-related tasks" target="Evaluation frameworks">
  <data key="d5">16.0</data>
  <data key="d6">Benchmarks like HumanEval and MBPP are used to evaluate the performance and capabilities of code-generating language models.&lt;SEP&gt;Benchmarks such as HumanEval, MBPP, DS-1000, GSM8K, and CoderEval are used to evaluate the performance of models on code generation tasks.</data>
  <data key="d7">evaluation, performance assessment&lt;SEP&gt;performance assessment, evaluation</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="HPC tasks" target="Performance prediction">
  <data key="d5">5.0</data>
  <data key="d6">Performance prediction involves estimating runtime or efficiency metrics of HPC code, serving as a key evaluation task for LLM applications in HPC.</data>
  <data key="d7">performance modeling, evaluation</data>
  <data key="d8">chunk-107fb981331e6facca90c9849f179ee2</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Problem Types" target="HPC-Coder-V2-6.7B">
  <data key="d5">16.0</data>
  <data key="d6">HPC-Coder-V2-6.7B is evaluated on various problem types, demonstrating its ability to handle different computational challenges.&lt;SEP&gt;HPC-Coder-V2-6.7B is evaluated on various problem types, indicating its versatility in different computational scenarios.</data>
  <data key="d7">model evaluation, problem versatility&lt;SEP&gt;model versatility, problem applicability</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Problem Types" target="HPC-Coder-V2-16B">
  <data key="d5">16.0</data>
  <data key="d6">HPC-Coder-V2-16B's assessment across problem types demonstrates its broad applicability in high-performance computing tasks.&lt;SEP&gt;HPC-Coder-V2-16B's assessment across problem types shows its broad applicability in high-performance computing tasks.</data>
  <data key="d7">model capability, problem diversity</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Problem Types" target="Phind-V2-34B">
  <data key="d5">16.0</data>
  <data key="d6">Phind-V2-34B is tested on multiple problem types, indicating its versatility in different computational scenarios.&lt;SEP&gt;Phind-V2-34B is tested on multiple problem types, reflecting its performance across different computational challenges.</data>
  <data key="d7">model evaluation, problem diversity&lt;SEP&gt;model evaluation, task diversity</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Execution Models" target="HPC-Coder-V2-6.7B">
  <data key="d5">16.0</data>
  <data key="d6">HPC-Coder-V2-6.7B is assessed across different execution models, indicating its adaptability to various computational frameworks.&lt;SEP&gt;HPC-Coder-V2-6.7B is evaluated across different execution models, reflecting its adaptability to various computational frameworks.</data>
  <data key="d7">model flexibility, framework compatibility</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Execution Models" target="HPC-Coder-V2-16B">
  <data key="d5">16.0</data>
  <data key="d6">HPC-Coder-V2-16B's evaluation across execution models highlights its capacity to operate in diverse high-performance environments.&lt;SEP&gt;HPC-Coder-V2-16B's performance across execution models highlights its versatility in high-performance environments.</data>
  <data key="d7">model adaptability, parallel frameworks&lt;SEP&gt;model versatility, computational frameworks</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Execution Models" target="Phind-V2-34B">
  <data key="d5">16.0</data>
  <data key="d6">Phind-V2-34B is tested on multiple execution models, showcasing its performance in various parallel and serial processing scenarios.</data>
  <data key="d7">model robustness, parallel processing</data>
  <data key="d8">chunk-727ea750122a5bed30ad781918446611</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Geometry" target="Component counting">
  <data key="d5">6.0</data>
  <data key="d6">Component counting is a fundamental concept in geometric analysis, involving quantifying parts of a geometric structure.</data>
  <data key="d7">concept, analysis</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Geometry" target="Convex Hull">
  <data key="d5">14.0</data>
  <data key="d6">The convex hull is a geometric construct used to analyze the shape and spatial extent of a set of points in computational geometry.&lt;SEP&gt;The convex hull is a geometric object computed to analyze the shape and spatial properties of a set of points.</data>
  <data key="d7">geometric analysis, spatial data</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Geometry" target="Component Counting">
  <data key="d5">6.0</data>
  <data key="d6">Component counting is a fundamental concept in geometric analysis, involving quantifying parts of a geometric object to understand its structure.</data>
  <data key="d7">concept, analysis</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Fourier Transform" target="Transform">
  <data key="d5">16.0</data>
  <data key="d6">Fourier Transform techniques are used to decompose signals into frequency components, aiding in signal analysis and processing.&lt;SEP&gt;Fourier Transforms decompose signals into frequency components, essential in signal analysis and processing.</data>
  <data key="d7">signal processing, mathematical technique</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Transform" target="Array Mapping">
  <data key="d5">14.0</data>
  <data key="d6">Mapping a constant function to array elements is a data transformation technique used in computational algorithms.&lt;SEP&gt;Mapping a constant function to each element of an array is a data transformation process used in computational tasks.</data>
  <data key="d7">data manipulation, array processing</data>
  <data key="d8">chunk-dfe74b691919432b3bb0f4652ee2468c</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="CUDA" target="Kokkos">
  <data key="d5">14.0</data>
  <data key="d6">Kokkos and CUDA are both parallel programming frameworks, with Kokkos being less efficient but similar in their approach, and both are used to evaluate LLM code generation."|&lt;SEP&gt;Kokkos is observed to be less efficient than CUDA but similar in performance, due to their similar parallelism structures.</data>
  <data key="d7">similarity, efficiency</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="CUDA" target="C++">
  <data key="d5">18.0</data>
  <data key="d6">CUDA is a parallel computing platform in C++ for GPU acceleration, especially NVIDIA GPUs.&lt;SEP&gt;CUDA is a parallel computing platform in C++ for GPU acceleration, especially NVIDIA GPUs."|&gt;"GPU computing, acceleration</data>
  <data key="d7">9&lt;SEP&gt;GPU computing, acceleration</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="CUDA" target="Memory Locality">
  <data key="d5">16.0</data>
  <data key="d6">CUDA enables explicit control over memory management to exploit data locality, improving performance.&lt;SEP&gt;CUDA enables explicit control over memory management to exploit data locality, reducing latency and improving overall performance.</data>
  <data key="d7">performance optimization</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="CUDA" target="Thread Pool">
  <data key="d5">14.0</data>
  <data key="d6">A thread pool in CUDA manages GPU threads to handle multiple tasks concurrently, simplifying parallel execution.&lt;SEP&gt;A thread pool in CUDA manages worker threads to execute tasks concurrently, simplifying parallel execution and resource management.</data>
  <data key="d7">resource management</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="HIP" target="C++">
  <data key="d5">16.0</data>
  <data key="d6">HIP provides portable GPU programming in C++ for AMD and NVIDIA hardware.&lt;SEP&gt;HIP provides portable GPU programming in C++ for AMD and NVIDIA hardware."|&gt;"hardware portability, GPU programming</data>
  <data key="d7">8&lt;SEP&gt;hardware portability, GPU programming</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Kokkos" target="parallel programming model">
  <data key="d5">7.0</data>
  <data key="d6">Kokkos is used as a tool for parallel code execution and performance testing.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Kokkos" target="C++">
  <data key="d5">16.0</data>
  <data key="d6">Kokkos is a library for performance portability in C++ across different hardware architectures.&lt;SEP&gt;Kokkos is a library for performance portability in C++ across different hardware architectures."|&gt;"portability, performance</data>
  <data key="d7">8&lt;SEP&gt;portability, performance</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Kokkos" target="Libraries">
  <data key="d5">16.0</data>
  <data key="d6">Kokkos provides performance portability for HPC applications across different hardware architectures, supporting node-level parallelism."|&gt;"performance portability, HPC</data>
  <data key="d7">8</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="C++" target="OpenMP">
  <data key="d5">16.0</data>
  <data key="d6">OpenMP is a methodology used within C++ for parallel programming to leverage multi-core processors.&lt;SEP&gt;OpenMP is a methodology used within C++ for parallel programming to leverage multi-core processors."|&gt;"usage, parallelism</data>
  <data key="d7">8&lt;SEP&gt;usage, parallelism</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="C++" target="OpenACC">
  <data key="d5">14.0</data>
  <data key="d6">OpenACC is a methodology used within C++ to simplify programming for heterogeneous systems, especially GPUs.&lt;SEP&gt;OpenACC is a methodology used within C++ to simplify programming for heterogeneous systems, especially GPUs."|&gt;"usage, acceleration</data>
  <data key="d7">7&lt;SEP&gt;usage, acceleration</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="C++" target="Thrust">
  <data key="d5">14.0</data>
  <data key="d6">Thrust is a C++ library for parallel algorithms targeting GPU acceleration via CUDA.&lt;SEP&gt;Thrust is a C++ library for parallel algorithms targeting GPU acceleration via CUDA."|&gt;"parallel algorithms, GPU</data>
  <data key="d7">7&lt;SEP&gt;parallel algorithms, GPU</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="C++" target="SyCL">
  <data key="d5">16.0</data>
  <data key="d6">SyCL enables portable heterogeneous computing in C++, supporting multiple hardware backends.&lt;SEP&gt;SyCL enables portable heterogeneous computing in C++, supporting multiple hardware backends."|&gt;"heterogeneous computing, portability</data>
  <data key="d7">8&lt;SEP&gt;heterogeneous computing, portability</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Llama 2" target="Louis Martin">
  <data key="d5">16.0</data>
  <data key="d6">Researcher involved in Llama 2 development, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Kevin Stone">
  <data key="d5">16.0</data>
  <data key="d6">Contributor to Llama 2 models, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Peter Albert">
  <data key="d5">16.0</data>
  <data key="d6">Involved in Llama 2 development, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Yasmine Babaei">
  <data key="d5">16.0</data>
  <data key="d6">Contributor to Llama 2 models, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Nikolay Bashlykov">
  <data key="d5">16.0</data>
  <data key="d6">Researcher involved in Llama 2 development, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Soumya Batra">
  <data key="d5">16.0</data>
  <data key="d6">Contributor to Llama 2, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Prrajjwal Bhargava">
  <data key="d5">16.0</data>
  <data key="d6">Researcher involved in Llama 2, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="Shruti Bhosale">
  <data key="d5">16.0</data>
  <data key="d6">Contributor to Llama 2 models, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="et al.">
  <data key="d5">16.0</data>
  <data key="d6">Indicates multiple authors involved in Llama 2 publication, 2023.</data>
  <data key="d7">16</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama 2" target="arXiv:2307.09288">
  <data key="d5">10.0</data>
  <data key="d6">The preprint describes the development and features of Llama 2, an open foundation and fine-tuned chat model.</data>
  <data key="d7">model description, research publication</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="StarCoderBase" target="The Stack">
  <data key="d5">8.0</data>
  <data key="d6">The Stack dataset is used to train and evaluate StarCoderBase, providing extensive code and natural language data for model development."|</data>
  <data key="d7">supporting training and evaluation of StarCoderBase</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="StarCoderBase" target="model architecture">
  <data key="d5">7.0</data>
  <data key="d6">StarCoderBase is based on the SantaCoder model architecture, influencing its code infilling capabilities."|</data>
  <data key="d7">model design</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="The Stack" target="training datasets">
  <data key="d5">8.0</data>
  <data key="d6">The Stack provides extensive code and natural language data used to train language models like StarCoderBase."|</data>
  <data key="d7">training data source</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="The Stack" target="Denis Kocetkov">
  <data key="d5">7.0</data>
  <data key="d6">Denis Kocetkov is associated with or contributed to 'The Stack', a large open-source code dataset used for training and evaluating software analysis models.</data>
  <data key="d7">dataset development, source code analysis</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Phind-CodeLlama-V2" target="training datasets">
  <data key="d5">6.0</data>
  <data key="d6">Phind-CodeLlama-V2 was fine-tuned on over 1.5 billion tokens of code data, affecting its performance."|</data>
  <data key="d7">training process</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="GPT-3.5 and GPT-4" target="training datasets">
  <data key="d5">6.0</data>
  <data key="d6">GPT-3.5 and GPT-4 were trained on proprietary data, with their training datasets not publicly disclosed."|</data>
  <data key="d7">training data</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="model comparison" target="Model architecture">
  <data key="d5">7.0</data>
  <data key="d6">Different architectures like SantaCoder and CodeLlama influence model capabilities and performance."|</data>
  <data key="d7">design influence</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="benchmark datasets" target="Model comparison">
  <data key="d5">8.0</data>
  <data key="d6">Models are compared based on their performance on datasets like HumanEval and BigCode, using defined metrics."|</data>
  <data key="d7">evaluation</data>
  <data key="d8">chunk-aba26da203f4fd14ca664e347342978e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Equation (3)" target="efficiency𝑛@𝑘">
  <data key="d5">5.0</data>
  <data key="d6">Equation (3) defines the calculation of efficiency𝑛@𝑘, establishing the mathematical foundation for performance evaluation.</data>
  <data key="d7">mathematical modeling, performance metrics</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Equation (4)" target="Performance Optimization">
  <data key="d5">8.0</data>
  <data key="d6">Equation (4) estimates the maximum achievable speedup over varying resource counts, guiding optimization of parallel code performance.</data>
  <data key="d7">max speedup, resource scaling</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Equation (4)" target="Performance Bound">
  <data key="d5">8.0</data>
  <data key="d6">Equation (4) estimates the maximum possible speedup over different resource counts, guiding understanding of the upper performance limit for generated code.</data>
  <data key="d7">performance ceiling, scalability</data>
  <data key="d8">chunk-aebb652b359c14534612e14559754bbc</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Parallel Code" target="HPC-specific Models">
  <data key="d5">8.0</data>
  <data key="d6">HPC-specific models are designed to generate and optimize parallel code for high-performance computing applications.</data>
  <data key="d7">design</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="performance of the generated code" target="efficiency𝑛@𝑘">
  <data key="d5">8.0</data>
  <data key="d6">The performance of generated code is evaluated using efficiency𝑛@𝑘, which measures how effectively the code utilizes parallel resources and scales.</data>
  <data key="d7">performance measurement, parallel efficiency</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="performance of the generated code" target="efficiencymax@𝑘">
  <data key="d5">7.0</data>
  <data key="d6">efficiencymax@𝑘 provides the maximum potential efficiency at attempt 𝑘, helping to understand the best possible scaling of code.</data>
  <data key="d7">performance optimization, maximum efficiency</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="efficiency𝑛@𝑘" target="parallel resources">
  <data key="d5">6.0</data>
  <data key="d6">Parallel resources are the basis for measuring efficiency𝑛@𝑘, as the metric evaluates how well the generated code makes use of these resources.</data>
  <data key="d7">resource utilization, parallel computing</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="efficiency𝑛@𝑘" target="Equation (5)">
  <data key="d5">4.0</data>
  <data key="d6">Equation (5) provides the formula to compute efficiency𝑛@𝑘, normalizing speedup by attempts, which is central to performance analysis.</data>
  <data key="d7">performance calculation, normalization</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Benchmarks (HumanEval, MBPP, DS-1000)" target="Speedup1@𝑘">
  <data key="d5">3.0</data>
  <data key="d6">Benchmarks like HumanEval, MBPP, and DS-1000 are used to compare generated code's speedup against human baseline, assessing efficiency in sequential scenarios.</data>
  <data key="d7">benchmark evaluation, code performance</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="ParEval test harness" target="Code evaluation">
  <data key="d5">1.0</data>
  <data key="d6">The ParEval test harness compiles and runs generated code, recording correctness and execution time to evaluate code quality.</data>
  <data key="d7">testing framework, correctness assessment</data>
  <data key="d8">chunk-d1ed35f0489d99675c43fbbee04d61fd</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="test harness" target="generated code">
  <data key="d5">8.0</data>
  <data key="d6">The test harness runs the generated code and compares outputs to assess correctness and measure performance.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="generated code" target="correctness check">
  <data key="d5">7.0</data>
  <data key="d6">The correctness check verifies if outputs match the expected results based on the sequential baseline.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="generated code" target="sequential baseline">
  <data key="d5">8.0</data>
  <data key="d6">The sequential baseline serves as a standard for correctness and performance comparison.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="generated code" target="parallel programming model">
  <data key="d5">7.0</data>
  <data key="d6">The code is expected to utilize specific parallel programming models like OpenMP or MPI for execution.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel execution models" target="parallel code">
  <data key="d5">6.0</data>
  <data key="d6">Parallel execution models are the target frameworks or paradigms that the generated code aims to utilize effectively.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="problem sizes" target="code execution">
  <data key="d5">7.0</data>
  <data key="d6">Problem sizes are chosen to ensure code runs within a reasonable time limit, typically less than three minutes.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="execution time limit" target="code">
  <data key="d5">6.0</data>
  <data key="d6">Codes exceeding the three-minute threshold are labeled as incorrect, establishing a performance constraint.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel programming model" target="OpenMP">
  <data key="d5">7.0</data>
  <data key="d6">OpenMP is used as a framework for parallel code generation and evaluation.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-0263fe0a192189f09d40327b5d776e30</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="OpenMP" target="Fortran">
  <data key="d5">16.0</data>
  <data key="d6">OpenMP is used with Fortran to enable parallelization of legacy HPC applications, leveraging its mature support for Fortran codes.</data>
  <data key="d7">parallel computing, legacy code&lt;SEP&gt;parallelization, legacy code support</data>
  <data key="d8">chunk-49dc2215d7813b7181583fe38186e482</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenMP" target="Discipline">
  <data key="d5">6.0</data>
  <data key="d6">OpenMP provides directives-based parallel programming support for shared-memory systems, often used together with GPU computing frameworks.</data>
  <data key="d7">Shared-Memory Parallelism</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenMP" target="NVIDIA">
  <data key="d5">6.0</data>
  <data key="d6">OpenMP is a shared-memory parallel programming API supported alongside NVIDIA GPU programming frameworks, used for multi-core CPU parallelism.</data>
  <data key="d7">Shared-Memory Parallelism</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenMP" target="Parallelization Patterns">
  <data key="d5">16.0</data>
  <data key="d6">OpenMP provides a model for parallelizing hotspots in code, forming recurring structures in parallel algorithms, such as work sharing and synchronization."|&gt;"parallel programming, code structure&lt;SEP&gt;OpenMP provides a model for parallelizing hotspots in code, forming recurring structures in parallel algorithms.</data>
  <data key="d7">8&lt;SEP&gt;parallel programming, code structure</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="OpenMP" target="aggregate_metrics">
  <data key="d5">19.0</data>
  <data key="d6">OpenMP directives are used within `aggregate_metrics` to parallelize the outer loop, distributing workload across multiple CPU cores."|&gt;"parallel processing, performance optimization&lt;SEP&gt;OpenMP's `parallel for` directive with `reduction(+:sum)` ensures thread-safe accumulation of the total sum across multiple threads."|&gt;"parallel computing, thread safety</data>
  <data key="d7">10&lt;SEP&gt;9</data>
  <data key="d8">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Structured, Dense Problems" target="Transform Problems">
  <data key="d5">10.0</data>
  <data key="d6">Transform problems are easier for LLMs to solve due to their data-parallel nature."|</data>
  <data key="d7">problem type, ease of parallelization</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Transform Problems" target="pass@1 Score">
  <data key="d5">10.0</data>
  <data key="d6">Transform problems have the highest pass@1 scores across LLMs, indicating they are the easiest to parallelize."|</data>
  <data key="d7">performance, problem type</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Sparse Linear Algebra" target="Difficulty">
  <data key="d5">9.0</data>
  <data key="d6">Sparse linear algebra problems are the most difficult for LLMs to parallelize, likely due to their inherent complexity."|</data>
  <data key="d7">difficulty, problem complexity</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Sparse Linear Algebra" target="pass@1 Score">
  <data key="d5">9.0</data>
  <data key="d6">Sparse linear algebra problems have the lowest pass@1 scores, reflecting their difficulty."|</data>
  <data key="d7">difficulty, performance</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="GPT-3.5" target="DSP Y Program">
  <data key="d5">8.0</data>
  <data key="d6">DSP Y-optimized pipelines significantly outperform standard few-shot prompting on GPT-3.5 models in accuracy and efficiency.</data>
  <data key="d7">performance enhancement, prompting</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GPT-3.5" target="DSPy">
  <data key="d5">7.0</data>
  <data key="d6">DSPy</data>
  <data key="d7">DSPy improves the performance of GPT-3.5 in NLP tasks without hand-crafted prompts.</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Figure 6" target="Models and Performance">
  <data key="d5">18.0</data>
  <data key="d6">Figure 6 demonstrates pass@1 scores and speedup for models, emphasizing GPT-4's superior performance and the trend of efficiency and speedup across models and parallel prompts."|&gt;"model performance, accuracy&lt;SEP&gt;Figure 6 demonstrates pass@1 scores and speedup for models, emphasizing GPT-4's superior performance.</data>
  <data key="d7">9&lt;SEP&gt;model performance, accuracy</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="speedup 𝑛@1" target="Parallel Code Performance">
  <data key="d5">18.0</data>
  <data key="d6">speedup 𝑛@1 measures how much faster parallel code runs compared to sequential, indicating performance gains.&lt;SEP&gt;speedup 𝑛@1 measures how much faster parallel code runs compared to the sequential baseline, indicating the effectiveness of parallelization and the performance gains achieved."|&gt;"performance metrics, effectiveness</data>
  <data key="d7">9&lt;SEP&gt;performance metrics, efficiency</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="efficiency 𝑛@1" target="Parallel Code Performance">
  <data key="d5">18.0</data>
  <data key="d6">efficiency 𝑛@1 assesses resource utilization efficiency in parallel code.&lt;SEP&gt;efficiency 𝑛@1 assesses the utilization of parallel resources relative to maximum possible speedup, indicating resource efficiency and scalability."|&gt;"resource utilization, scalability</data>
  <data key="d7">9&lt;SEP&gt;resource utilization, scalability</data>
  <data key="d8">chunk-a1b2a0e4b767a0658ae2b70b61e208a7</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="Correct implementations">
  <data key="d5">7.0</data>
  <data key="d6">Providing correct serial code implementations improves LLMs' ability to generate correct parallel code.</data>
  <data key="d7">training data, correctness improvement</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="Open- and closed-source models">
  <data key="d5">6.0</data>
  <data key="d6">Closed-source models outperform open-source models in parallel code generation.</data>
  <data key="d7">model performance, comparison</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="parEval">
  <data key="d5">9.0</data>
  <data key="d6">ParEval benchmarks LLMs' ability to generate parallel code.</data>
  <data key="d7">evaluation scope, application</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="correct implementation">
  <data key="d5">8.0</data>
  <data key="d6">Providing correct serial or other model code improves LLMs' accuracy in generating correct parallel code."|</data>
  <data key="d7">training data, correctness</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="open-source models">
  <data key="d5">7.0</data>
  <data key="d6">Closed-source models outperform open-source models in generating correct and scalable parallel code."|</data>
  <data key="d7">model performance, comparison</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="parallel code" target="providing correct implementations">
  <data key="d5">7.0</data>
  <data key="d6">Supplying correct code examples in serial or other models helps LLMs produce correct parallel code."|</data>
  <data key="d7">training, correctness</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Code translation" target="C2Rust">
  <data key="d5">12.0</data>
  <data key="d6">C2Rust facilitates translating C code to Rust, improving safety and performance in systems programming.</data>
  <data key="d7">tool, purpose</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Model performance" target="Fine-tuning strategies">
  <data key="d5">20.0</data>
  <data key="d6">Effective fine-tuning improves code quality, especially for parallel code, without significantly increasing resource demands.&lt;SEP&gt;Effective fine-tuning improves the models' ability to generate high-quality parallel code without significant resource sacrifice.</data>
  <data key="d7">training strategies, optimization&lt;SEP&gt;training, optimization</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="benchmark" target="parEval">
  <data key="d5">8.0</data>
  <data key="d6">ParEval functions as a benchmark to evaluate LLMs' parallel code generation capabilities."|</data>
  <data key="d7">evaluation tool, benchmarking</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="correctness and performance" target="parEval">
  <data key="d5">8.0</data>
  <data key="d6">ParEval aims to improve both the correctness and runtime performance of generated parallel code.</data>
  <data key="d7">benchmark goals, improvement</data>
  <data key="d8">chunk-873e4d15525881588c22e6d0a81e435e</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Jan Leike" target="arXiv:arXiv:2107.03374">
  <data key="d5">16.0</data>
  <data key="d6">The study by Jan Leike et al. is hosted on arXiv as a preprint evaluating large language models trained on code, providing datasets and results.</data>
  <data key="d7">research publication, dataset hosting</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Ilya Sutskever" target="Improving language understanding">
  <data key="d5">7.0</data>
  <data key="d6">A researcher central to language model development, 2018.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Karl Cobbe" target="arXiv:2110.14168">
  <data key="d5">16.0</data>
  <data key="d6">The study by Karl Cobbe et al. is hosted on arXiv as a preprint focusing on training verifiers for math word problems, providing datasets and methodology.</data>
  <data key="d7">research publication, problem-solving verification</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Xueying Du" target="arXiv:2308.01861">
  <data key="d5">16.0</data>
  <data key="d6">The ClassEval benchmark for evaluating large language models on class-level code generation is hosted on arXiv as a preprint, providing datasets and evaluation methodology.</data>
  <data key="d7">benchmark, code evaluation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Leo Gao" target="arXiv:2101.00027">
  <data key="d5">16.0</data>
  <data key="d6">The Pile dataset is hosted on arXiv as a large diverse text dataset for language modeling, supporting various research activities.</data>
  <data key="d7">dataset, language modeling</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="William Godoy" target="ICPP-W 2023">
  <data key="d5">16.0</data>
  <data key="d6">William Godoy's evaluation of OpenAI Codex for HPC is presented at the ICPP-W 2023 conference, sharing results on model performance in HPC contexts.</data>
  <data key="d7">conference presentation, HPC model evaluation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Jian Gu" target="IEEE SANER 2022">
  <data key="d5">16.0</data>
  <data key="d6">Jian Gu's work on assembling foundation models for code summarization was presented at SANER 2022, providing methodology and results.</data>
  <data key="d7">conference presentation, code summarization</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Sakib Haque" target="IEEE ICPC 2022">
  <data key="d5">16.0</data>
  <data key="d6">Sakib Haque's research on semantic similarity metrics for code summaries was presented at ICPC 2022, contributing to evaluation techniques.</data>
  <data key="d7">conference presentation, code evaluation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Edward J. Hu" target="arXiv:2106.09685">
  <data key="d5">8.0</data>
  <data key="d6">The LoRA adaptation technique is detailed in an arXiv preprint, explaining low-rank adaptation methods for large models.</data>
  <data key="d7">methodology, model adaptation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="arXiv:2106.09685" target="Edward Hu">
  <data key="d5">8.0</data>
  <data key="d6">The LoRA adaptation technique is detailed in an arXiv preprint, explaining low-rank adaptation methods for large models.</data>
  <data key="d7">methodology, model adaptation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Tal Kadosh" target="arXiv:2308.09440">
  <data key="d5">16.0</data>
  <data key="d6">This preprint discusses transforming LLMs for HPC code, including datasets and adaptation techniques.</data>
  <data key="d7">methodology, HPC adaptation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Md Abul Kalam Azad" target="MSR 2023">
  <data key="d5">16.0</data>
  <data key="d6">Empirical study on HPC performance bugs was presented at MSR 2023, providing datasets and analysis on software bugs in HPC.</data>
  <data key="d7">conference presentation, software bugs</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Learning to Reduce False Positives in Analytic Bug Detectors" target="Anant Kharkar">
  <data key="d5">16.0</data>
  <data key="d6">Anant Kharkar authored the research focused on improving bug detector accuracy by reducing false positives.&lt;SEP&gt;Anant Kharkar is an author contributing to research on improving bug detection accuracy in software analysis.</data>
  <data key="d7">research contribution, bug detection improvement</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Anant Kharkar" target="IEEE/ACM 44th Software Engineering">
  <data key="d5">16.0</data>
  <data key="d6">Research on reducing false positives in bug detectors was presented at ICSE 2022, sharing datasets and techniques.</data>
  <data key="d7">conference presentation, bug detection</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Denis Kocetkov" target="The Stack Dataset">
  <data key="d5">7.0</data>
  <data key="d6">Denis Kocetkov is associated with the creation or utilization of the 'The Stack' large source code dataset, which provides extensive code for analysis and training models.</data>
  <data key="d7">dataset development, source code analysis</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Yacine Jernite" target="Transformers with KNN-based Memory">
  <data key="d5">8.0</data>
  <data key="d6">Jernite supports transformer augmentation with memory and retrieval techniques."|&lt;SEP&gt;Jernite's research supports transformer augmentation with memory mechanisms."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="VerilogEval" target="Mingjie Liu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Mingjie Liu and colleagues created VerilogEval, a benchmark for evaluating language models' performance in Verilog hardware description language code generation.</data>
  <data key="d7">benchmark creation, hardware description language, evaluation</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="LLM4VV" target="Christian Munley et al.">
  <data key="d5">8.0</data>
  <data key="d6">Christian Munley and team developed LLM4VV, a large language model-based test suite for compiler validation, aiming to improve compiler correctness testing methods.</data>
  <data key="d7">validation, compiler testing, AI-driven testing</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="PyTorch" target="Din-sql: Decomposed In-Context Learning of Text-to-SQL with Self-Correction">
  <data key="d5">14.0</data>
  <data key="d6">PyTorch provides the deep learning infrastructure used to implement the text-to-SQL model with self-correction features."|</data>
  <data key="d7">implementation, deep learning</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Performance Deep Learning" target="arXiv:1912.01703 [cs.LG]">
  <data key="d5">8.0</data>
  <data key="d6">The arXiv paper documents research related to performance deep learning, providing foundational insights and methodologies.</data>
  <data key="d7">research documentation, foundational knowledge</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Deep Learning" target="Phind">
  <data key="d5">12.0</data>
  <data key="d6">Phind-CodeLlama-34B-v2 is a tool related to code generation, which can be used to implement or test performance deep learning models.&lt;SEP&gt;Phind-CodeLlama-34B-v2 is a tool that can be used to implement or experiment with performance deep learning models, especially in code understanding and generation tasks."|&lt;"tool application, research support</data>
  <data key="d7">6&lt;SEP&gt;tool application, implementation</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Deep Learning" target="Library. arXiv:1912.01703 [cs.LG]">
  <data key="d5">8.0</data>
  <data key="d6">The arXiv preprint provides foundational research and methodologies related to performance deep learning, contributing to the theoretical and practical understanding of the field."|&lt;"research foundation, methodologies</data>
  <data key="d7">8</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Cedric Richter and Heike Wehrheim" target="Learning to localize and repair real bugs from real bug fixes">
  <data key="d5">7.0</data>
  <data key="d6">The study explores whether models can learn bug localization and repair, contributing to improving software reliability.</data>
  <data key="d7">software engineering, bug fixing</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Cedric Richter and Heike Wehrheim" target="Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes">
  <data key="d5">7.0</data>
  <data key="d6">The study explores whether models can learn to identify, localize, and repair bugs based on real bug fix data, aiming to improve automated debugging."|&lt;"software engineering, bug repair</data>
  <data key="d7">7</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Baptiste Rozière, Jonas Gehring, et al." target="Code Llama: Open Foundation Models for Code">
  <data key="d5">18.0</data>
  <data key="d6">The paper introduces a foundation model framework for code understanding and generation, aiming to advance AI capabilities in programming.&lt;SEP&gt;The paper introduces the development of large foundation models tailored for code understanding and generation, advancing AI in programming."|&lt;"model development, AI for code</data>
  <data key="d7">9&lt;SEP&gt;model development, AI for code</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Code Llama: Open Foundation Models for Code" target="arXiv:2308.12950 [cs.CL]">
  <data key="d5">16.0</data>
  <data key="d6">The arXiv publication details the architecture, capabilities, and applications of Code Llama models, contributing to open-source AI for code."|&lt;"research dissemination, model description&lt;SEP&gt;The arXiv publication details the development and capabilities of the Code Llama models, contributing to the field of large language models for code.</data>
  <data key="d7">8&lt;SEP&gt;research dissemination, model description</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="M. Snir" target="MPI–the Complete Reference: The MPI core">
  <data key="d5">18.0</data>
  <data key="d6">The book provides comprehensive knowledge on MPI, a core methodology for parallel programming in high-performance computing environments."|&lt;"methodology, parallel computing&lt;SEP&gt;The book provides comprehensive knowledge on MPI, a core methodology for parallel programming in high-performance computing.</data>
  <data key="d7">9&lt;SEP&gt;methodology, parallel computing</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Xiangru Tang, Bill Qian, Rick Gao, et al." target="BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge">
  <data key="d5">8.0</data>
  <data key="d6">The benchmark assesses models' ability to generate bioinformatics code with contextual understanding, aiding research in computational biology.</data>
  <data key="d7">bioinformatics, benchmarking</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge" target="Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein">
  <data key="d5">8.0</data>
  <data key="d6">The benchmark evaluates models' ability to generate bioinformatics code with pragmatic understanding, supporting research in computational biology."|&lt;"bioinformatics, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Hugo Touvron et al." target="Llama 2: Open Foundation and Fine-Tuned Chat Models">
  <data key="d5">20.0</data>
  <data key="d6">The paper presents Llama 2, a large language model framework that supports open access and fine-tuning for diverse NLP and code tasks.&lt;SEP&gt;The paper presents Llama 2, an open and fine-tuned large language model supporting NLP and code understanding, pushing forward AI research."|&lt;"model development, NLP, code understanding</data>
  <data key="d7">10&lt;SEP&gt;model development, NLP, code understanding</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Christian R. Trott, Damien Lebrun-Grandié, et al." target="Kokkos 3: Programming Model Extensions for the Exascale Era">
  <data key="d5">18.0</data>
  <data key="d6">The research extends Kokkos to support exascale computing, emphasizing scalable parallel programming techniques.&lt;SEP&gt;The research extends Kokkos to support exascale computing, focusing on scalable, portable parallel programming models."|&lt;"parallel programming, high-performance computing</data>
  <data key="d7">9&lt;SEP&gt;parallel programming, high-performance computing</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, et al." target="Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation">
  <data key="d5">16.0</data>
  <data key="d6">The study compares the effectiveness of Llama-2 and GPT-3 in generating HPC kernels, informing model selection for scientific computing."|&lt;"model comparison, HPC, code generation&lt;SEP&gt;The study compares two LLMs to evaluate their effectiveness in generating HPC kernels, informing model selection for scientific computing.</data>
  <data key="d7">8&lt;SEP&gt;model comparison, HPC, code generation</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Hao Yu, Bo Shen, Dezhi Ran, et al." target="CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models">
  <data key="d5">18.0</data>
  <data key="d6">The benchmark assesses the pragmatic and contextual accuracy of code generated by large language models, advancing evaluation methods in AI code generation."|&lt;"benchmarking, pragmatics, code generation&lt;SEP&gt;The benchmark evaluates the pragmatic and contextual accuracy of code generated by large language models, advancing understanding of model capabilities in real-world coding tasks.</data>
  <data key="d7">9&lt;SEP&gt;benchmarking, code generation, pragmatics</data>
  <data key="d8">chunk-764fdbb8cfee52f0f6d62be5589676f9</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Performance Optimization" target="Numba">
  <data key="d5">8.0</data>
  <data key="d6">Seibert, Pitrou, and colleagues describe Numba as a JIT compiler that accelerates Python numerical code."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Code Complexity" target="Parallel Models">
  <data key="d5">9.0</data>
  <data key="d6">The greater the difference between serial and parallel code in a model, the harder it is for LLMs to generate correct code."|</data>
  <data key="d7">difficulty, code similarity</data>
  <data key="d8">chunk-71ff485ed5de2f1a53b77145285b1a49</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Code Complexity" target="Keywords">
  <data key="d5">12.0</data>
  <data key="d6"> Selecting specific keywords enhances AI answer proficiency, but requires careful, context-sensitive choices.&lt;SEP&gt;Choosing correct, specific keywords improves AI answer proficiency.</data>
  <data key="d7">keyword optimization, language sensitivity&lt;SEP&gt;keyword selection, language sensitivity</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Luyu Gao" target="arXiv:2211.10435">
  <data key="d5">16.0</data>
  <data key="d6">PAL (Program-aided Language Models) is described in an arXiv preprint as a methodology for enhancing language models with program assistance.</data>
  <data key="d7">methodology, code generation</data>
  <data key="d8">chunk-71987847f5d7f5bd36428e254673c5cb</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="DS-1000 Benchmark" target="Yuhang Lai et al.">
  <data key="d5">16.0</data>
  <data key="d6">Yuhang Lai and colleagues developed DS-1000, a benchmark dataset for data science code generation tasks.&lt;SEP&gt;Yuhang Lai and colleagues developed the DS-1000 benchmark dataset for data science code generation tasks, providing a reliable evaluation resource.</data>
  <data key="d7">benchmark creation, data science code generation&lt;SEP&gt;benchmark creation, data science, code generation</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Large Source Code Dataset" target="Raymond Li et al.">
  <data key="d5">14.0</data>
  <data key="d6">Raymond Li and team contributed to or analyzed a large-scale source code dataset for machine learning and software engineering research.&lt;SEP&gt;Raymond Li and team contributed to or analyzed large-scale source code datasets for machine learning applications.</data>
  <data key="d7">dataset analysis, software engineering&lt;SEP&gt;dataset analysis, source code modeling</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Mingjie Liu et al." target="VerilogEval Benchmark">
  <data key="d5">8.0</data>
  <data key="d6">Mingjie Liu and colleagues developed VerilogEval, a benchmark for evaluating language models in Verilog hardware description code generation.</data>
  <data key="d7">benchmark creation, hardware description language</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="Christian Munley et al." target="LLM for Compiler Validation">
  <data key="d5">8.0</data>
  <data key="d6">Christian Munley and team developed LLM4VV, a large language model-driven test suite for compiler validation.</data>
  <data key="d7">validation methodology, compiler testing</data>
  <data key="d8">chunk-c0b8d49beacd06e5cfcb389ea3120718</data>
  <data key="d9">Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf</data>
</edge>
<edge source="GPT-4 Technical Report" target="Training Language Models to Follow Instructions with Human Feedback">
  <data key="d5">18.0</data>
  <data key="d6">The GPT-4 technical report details how human feedback was used to train and align GPT-4 for better instruction following."|</data>
  <data key="d7">training, alignment</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kernel Generation" target="OpenAI Codex">
  <data key="d5">18.0</data>
  <data key="d6">OpenAI Codex acts as the core technology enabling the automatic generation of numerical HPC kernels across multiple programming models and languages.&lt;SEP&gt;OpenAI Codex facilitates the generation of numerical kernels for HPC, acting as a core technology enabling AI-assisted code creation.</data>
  <data key="d7">technology, code generation</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Kernel Generation" target="Programming Models">
  <data key="d5">14.0</data>
  <data key="d6">Different programming models influence the effectiveness and quality of AI-generated kernels, with more mature models yielding better results."|&lt;SEP&gt;Different programming models influence the quality and performance of AI-generated kernels, with maturity levels affecting output quality.</data>
  <data key="d7">model influence, code quality&lt;SEP&gt;model influence, output quality</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Kernel Generation" target="GitHub Copilot">
  <data key="d5">8.0</data>
  <data key="d6">GitHub Copilot, powered by OpenAI Codex, is used to generate and suggest code implementations for HPC kernels.</data>
  <data key="d7">tool, code assistance</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="William F. Godoy" target="Evaluation of OpenAI Codex">
  <data key="d5">16.0</data>
  <data key="d6">William F. Godoy evaluates the effectiveness and limitations of OpenAI Codex in generating HPC kernels across multiple programming models.&lt;SEP&gt;William F. Godoy leads the evaluation, analyzing the quality, limitations, and applicability of AI-generated HPC kernels across different programming environments."|</data>
  <data key="d7">research, assessment&lt;SEP&gt;research, evaluation</data>
  <data key="d8">chunk-8184ca90cae86b58652f2117d440bd17</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GitHub Copilot" target="OpenAI Codex">
  <data key="d5">18.0</data>
  <data key="d6">GitHub Copilot is based on OpenAI Codex, leveraging its source code understanding for AI-assisted programming."|&gt;"Tool-Foundation&lt;SEP&gt;GitHub Copilot is based on OpenAI Codex, utilizing its source code understanding and generation capabilities for AI-assisted programming."|&gt;"Tool-Foundation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GitHub Copilot" target="Substitute for Human Programming">
  <data key="d5">8.0</data>
  <data key="d6">Imai's empirical study assesses whether Copilot can replace or assist human pair programming effectively."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GitHub Copilot" target="Code Suggestions">
  <data key="d5">8.0</data>
  <data key="d6">Nguyen and Nadi empirically evaluate Copilot's code suggestions for effectiveness and developer impact."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GitHub Copilot" target="Imai">
  <data key="d5">8.0</data>
  <data key="d6">Imai's empirical study investigates whether Copilot can substitute for human pair programming, evaluating its effectiveness and limitations."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GitHub Copilot" target="Nguyen, Nadi">
  <data key="d5">8.0</data>
  <data key="d6">Nguyen and Nadi empirically evaluate Copilot's code suggestions, assessing effectiveness and developer impact."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenAI Codex" target="&lt;Results&gt;">
  <data key="d5">10.0</data>
  <data key="d6">OpenAI Codex's code generation quality decreases as kernel complexity increases, with better results on simpler kernels and languages with more public code.</data>
  <data key="d7">performance, code quality</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Broader Computer Science Research" target="Related efforts">
  <data key="d5">12.0</data>
  <data key="d6">Related research highlights the growing focus on LLMs' impacts across NLP, code synthesis, and automation."|&gt;"Research Context&lt;SEP&gt;Related research highlights the increasing focus on LLMs' impact on NLP, code synthesis, and automation in computer science."|&gt;"Research Context</data>
  <data key="d7">6</data>
  <data key="d8">chunk-38bf816094395541632173a71659ec3b</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenACC" target="Discipline">
  <data key="d5">6.0</data>
  <data key="d6">OpenACC offers a standardized API for GPU acceleration, complementing CUDA-based tools in high-performance computing environments.</data>
  <data key="d7">Parallel Programming</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="OpenACC" target="NVIDIA">
  <data key="d5">6.0</data>
  <data key="d6">OpenACC is an API standard for GPU acceleration that complements NVIDIA's CUDA ecosystem, enabling directives-based parallel programming.</data>
  <data key="d7">Parallel Programming</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Thrust" target="NVIDIA">
  <data key="d5">14.0</data>
  <data key="d6">Thrust library is a CUDA-based parallel algorithms library provided by NVIDIA to facilitate GPU programming.&lt;SEP&gt;Thrust library is used alongside CUDA to facilitate parallel algorithm implementation for GPU computations.</data>
  <data key="d7">Parallel Algorithms</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Post fix function" target="&lt;kernel&gt;">
  <data key="d5">12.0</data>
  <data key="d6">Using a postfix function prompt guides code generation towards specific function styles in experiments.&lt;SEP&gt;Using a postfix function prompt guides code generation towards specific function styles in experiments."|&gt;"prompt pattern, code generation</data>
  <data key="d7">6&lt;SEP&gt;prompt pattern, code generation</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GEMV" target="&lt;kernel&gt;">
  <data key="d5">16.0</data>
  <data key="d6">GEMV is a matrix-vector multiplication kernel used as a benchmark for evaluating code correctness and proficiency.&lt;SEP&gt;GEMV is a matrix-vector multiplication kernel used as a benchmark for evaluating code correctness and proficiency."|&gt;"benchmark, kernel</data>
  <data key="d7">8&lt;SEP&gt;benchmark, kernel</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GEMV" target="&lt;experiment&gt;">
  <data key="d5">8.0</data>
  <data key="d6">The GEMV kernel is used as a benchmark in the experiments to evaluate code correctness and proficiency levels."|&gt;"evaluation, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GEMM" target="&lt;kernel&gt;">
  <data key="d5">16.0</data>
  <data key="d6">GEMM is a matrix-matrix multiplication kernel analyzed for code correctness and model proficiency.&lt;SEP&gt;GEMM is a matrix-matrix multiplication kernel analyzed for code correctness and model proficiency."|&gt;"benchmark, kernel</data>
  <data key="d7">8&lt;SEP&gt;benchmark, kernel</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GEMM" target="&lt;experiment&gt;">
  <data key="d5">8.0</data>
  <data key="d6">The GEMM kernel is analyzed in the experiments for code correctness and model proficiency."|&gt;"evaluation, benchmarking</data>
  <data key="d7">8</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="SpMV" target="&lt;kernel&gt;">
  <data key="d5">14.0</data>
  <data key="d6">SpMV is a sparse matrix-vector multiplication kernel used in performance evaluation.&lt;SEP&gt;SpMV is a sparse matrix-vector multiplication kernel used in performance evaluation."|&gt;"benchmark, sparse computation</data>
  <data key="d7">7&lt;SEP&gt;benchmark, sparse computation</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="SpMV" target="&lt;experiment&gt;">
  <data key="d5">7.0</data>
  <data key="d6">The SpMV kernel is used to assess code correctness and proficiency in sparse matrix operations."|&gt;"evaluation, benchmarking</data>
  <data key="d7">7</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Jacobi" target="&lt;kernel&gt;">
  <data key="d5">14.0</data>
  <data key="d6">Jacobi iteration is used as a linear solver kernel in the study.&lt;SEP&gt;Jacobi iteration is used as a linear solver kernel in the study."|&gt;"algorithm, kernel</data>
  <data key="d7">7&lt;SEP&gt;algorithm, kernel</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Jacobi" target="&lt;experiment&gt;">
  <data key="d5">7.0</data>
  <data key="d6">The Jacobi kernel is evaluated for code correctness and proficiency in the experiments."|&gt;"evaluation, benchmarking</data>
  <data key="d7">7</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="CG" target="&lt;kernel&gt;">
  <data key="d5">14.0</data>
  <data key="d6">Conjugate Gradient is an iterative solver kernel analyzed for code correctness.&lt;SEP&gt;Conjugate Gradient is an iterative solver kernel analyzed for code correctness."|&gt;"algorithm, kernel</data>
  <data key="d7">7&lt;SEP&gt;algorithm, kernel</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="CG" target="&lt;experiment&gt;">
  <data key="d5">7.0</data>
  <data key="d6">The CG kernel is assessed for correctness and proficiency in the experimental setup."|&gt;"evaluation, benchmarking</data>
  <data key="d7">7</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Kernel" target="Code Rewriting">
  <data key="d5">7.0</data>
  <data key="d6">Rewritten kernels, like in hotspot, enable better optimization and parallel execution."|&gt;"core activity</data>
  <data key="d7">7</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Prompt Pattern" target="&lt;code generation model&gt;">
  <data key="d5">6.0</data>
  <data key="d6">The prompt pattern specifies how input instructions are formatted to guide code generation."|&gt;"prompt design, input format</data>
  <data key="d7">6</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Correctness Level" target="&lt;code generation model&gt;">
  <data key="d5">7.0</data>
  <data key="d6">The correctness level indicates the accuracy of code output, ranging from non-knowledge to expert."|&gt;"quality, accuracy</data>
  <data key="d7">7</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Analysis" target="&lt;study&gt;">
  <data key="d5">7.0</data>
  <data key="d6">The analysis interprets the results to understand the effectiveness of code generation approaches."|&gt;"interpretation, insights</data>
  <data key="d7">7</data>
  <data key="d8">chunk-64e7fd510c0d613c9975fe3385d6f3a0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="numpy" target="&lt;kernel&gt;">
  <data key="d5">8.0</data>
  <data key="d6">NumPy provides foundational numerical operations that are widely used in Python implementations of kernels and algorithms.</data>
  <data key="d7">software dependency, computational foundation</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="pyCUDA" target="&lt;kernel&gt;">
  <data key="d5">10.0</data>
  <data key="d6">pyCUDA allows direct GPU programming from Python, facilitating custom kernel development and execution.</data>
  <data key="d7">GPU programming, custom kernels</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Numba" target="&lt;kernel&gt;">
  <data key="d5">7.0</data>
  <data key="d6">Numba compiles Python functions to machine code, enabling faster execution of numerical kernels.</data>
  <data key="d7">performance optimization, just-in-time compilation</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="CuPy" target="&lt;kernel&gt;">
  <data key="d5">9.0</data>
  <data key="d6">CuPy accelerates NumPy-like operations on GPUs, enabling high-performance kernel execution.</data>
  <data key="d7">hardware acceleration, performance enhancement</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Julia" target="&lt;Tools&gt;">
  <data key="d5">24.0</data>
  <data key="d6">Julia is used for scientific computing and offers features like LLVM-based compilation, supporting high-performance kernel development.&lt;SEP&gt;Julia supports GPU programming through packages like CUDA.jl and AMDGPU.jl, enabling cross-vendor GPU kernels.&lt;SEP&gt;Julia's syntax and features are influenced by Fortran and designed for mathematical and performance-focused applications.</data>
  <data key="d7">GPU support, cross-platform&lt;SEP&gt;language design, scientific computing&lt;SEP&gt;performance, scientific computing</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Julia" target="Knopp">
  <data key="d5">8.0</data>
  <data key="d6">Knopp contributed to multi-threading support in Julia, enhancing its capabilities for high-performance parallel computing."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="LLVM" target="&lt;&gt;Chris Lattner and Vikram Adve">
  <data key="d5">16.0</data>
  <data key="d6">A compiler infrastructure enabling program analysis and transformation, essential for optimizing performance across architectures.&lt;SEP&gt;A compiler infrastructure framework that enables program analysis, transformation, and optimization to improve performance across architectures.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="public repositories" target="&lt;Objects of Study&gt;">
  <data key="d5">8.0</data>
  <data key="d6">Public repositories provide the source code that influences the AI's ability to generate accurate kernels, especially in popular languages.</data>
  <data key="d7">data source, influence</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="complexity" target="&lt;Variables&gt;">
  <data key="d5">7.0</data>
  <data key="d6">Kernel complexity directly impacts the difficulty of generating correct code with AI models.</data>
  <data key="d7">difficulty, AI performance</data>
  <data key="d8">chunk-07477bd37f9598e434f96bc52e2aea53</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="LLM Technologies" target="HPC Ecosystem">
  <data key="d5">16.0</data>
  <data key="d6">Emerging LLMs such as GPT-3 influence the development and integration of AI tools within HPC ecosystems.&lt;SEP&gt;Emerging LLMs such as GPT-3 influence the integration and development of HPC software ecosystems.</data>
  <data key="d7">technology impact, ecosystem integration</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Human-in-the-loop" target="Metadata-rich Suggestions">
  <data key="d5">16.0</data>
  <data key="d6">Incorporating human feedback and detailed suggestions can refine AI outputs and facilitate decision-making.&lt;SEP&gt;Incorporating human feedback and detailed suggestions can refine AI outputs and support decision-making processes.</data>
  <data key="d7">refinement process, decision support&lt;SEP&gt;refinement, decision support</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="HPC Modernization Initiatives" target="AI Tools">
  <data key="d5">14.0</data>
  <data key="d6">Modernization projects are exploring the incorporation of AI tools to enhance HPC infrastructure and workflows.&lt;SEP&gt;Modernization projects are exploring the integration of AI tools to enhance workflows and infrastructure in HPC.</data>
  <data key="d7">technology adoption, project scope</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Ecosystem Features" target="Revolutionary Capabilities">
  <data key="d5">16.0</data>
  <data key="d6">Automation of build systems, packaging, validation, and pipelines could be revolutionized by AI, impacting HPC workflows.&lt;SEP&gt;Automation of features like build systems and pipelines could be revolutionized by AI, impacting HPC domains.</data>
  <data key="d7">automation, technological advancement</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Educational Aspects" target="AI Technologies">
  <data key="d5">14.0</data>
  <data key="d6">AI-driven tools could redefine HPC education by providing advanced training and learning methodologies.&lt;SEP&gt;AI-driven tools could transform HPC education by providing innovative training and learning solutions.</data>
  <data key="d7">education, technological impact</data>
  <data key="d8">chunk-04aecdc2586d2c7966e70e02cac846b0</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Christian R. Trott" target="Kokkos 3">
  <data key="d5">16.0</data>
  <data key="d6">Christian R. Trott's work on Kokkos 3 extends programming models for exascale computing, enabling scalable parallel programming."|&lt;SEP&gt;Christian R. Trott's work on Kokkos 3 extends programming models for exascale systems, enabling scalable, portable parallel programming."|</data>
  <data key="d7">programming models, exascale</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Julia, Python/Numba, Kokkos" target="Performance and Portability">
  <data key="d5">9.0</data>
  <data key="d6">These tools are evaluated for their scalability, efficiency, and portability in high-performance computing environments."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Christopher D. Manning" target="Answering complex open-domain questions">
  <data key="d5">5.0</data>
  <data key="d6">Key author involved in question answering research, 2019.</data>
  <data key="d7">5</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zheming Jin" target="SYCL Benchmarks">
  <data key="d5">7.0</data>
  <data key="d6">Jin's technical report discusses the implementation of Rodinia benchmarks in SYCL for heterogeneous computing performance evaluation."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Hecbench" target="Jin">
  <data key="d5">8.0</data>
  <data key="d6">Jin's Hecbench suite is used to benchmark HPC hardware and software performance."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="PyExaFMM" target="Scientific Computing">
  <data key="d5">8.0</data>
  <data key="d6">Kailasa et al. developed PyExaFMM for high-performance scientific simulations using Python and Numba."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="pycuda" target="GPU Programming">
  <data key="d5">8.0</data>
  <data key="d6">Klöckner's pycuda library enables scripting and runtime code generation for NVIDIA GPUs."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="pyopencl" target="GPU Programming">
  <data key="d5">8.0</data>
  <data key="d6">Klöckner's pyopencl library provides GPU programming capabilities across platforms using OpenCL."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Exascale Computing" target="Large-Scale Computing">
  <data key="d5">8.0</data>
  <data key="d6">Kothe, Lee, and Qualters discuss the challenges and strategies for achieving exascale performance in US scientific computing."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Exascale Computing" target="Kothe, Lee, Qualters">
  <data key="d5">8.0</data>
  <data key="d6">They discuss challenges, strategies, and future directions for exascale supercomputing in the United States."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Cupy" target="NVIDIA">
  <data key="d5">8.0</data>
  <data key="d6">Cupy leverages NVIDIA GPU hardware and software tools such as CUDA and Thrust to enable high-performance GPU computations.</data>
  <data key="d7">Tools, Hardware Acceleration</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="NVIDIA" target="CUDA Toolkit">
  <data key="d5">15.0</data>
  <data key="d6">NVIDIA provides the CUDA Toolkit for GPU programming, which supports libraries like CuPy and Thrust for high-performance computations.&lt;SEP&gt;The CUDA Toolkit provides essential development resources for GPU programming, supporting CuPy and other GPU-accelerated libraries.</data>
  <data key="d7">Software Development&lt;SEP&gt;Tools, Hardware Acceleration</data>
  <data key="d8">chunk-7d291b4c5bc433808e3d40f3b84dae35</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Extreme Heterogeneity 2018" target="DOE ASCR Workshop on Extreme Heterogeneity">
  <data key="d5">16.0</data>
  <data key="d6">The report was produced as a result of discussions and findings from the workshop.</data>
  <data key="d7">research collaboration, workshop output</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Generalizing from a Few Examples" target="Yaqing Wang et al.">
  <data key="d5">13.0</data>
  <data key="d6">The survey discusses techniques for enabling models to learn effectively from few examples, advancing the field of machine learning.</data>
  <data key="d7">research contribution, model generalization</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Few-Shot Learning" target="Yaqing Wang et al.">
  <data key="d5">18.0</data>
  <data key="d6">The survey provides an overview of methods enabling models to generalize from limited data, contributing to the broader understanding of machine learning generalization.</data>
  <data key="d7">literature review, model generalization</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Michel Wermelinger" target="Using GitHub Copilot to Solve Simple Programming Problems">
  <data key="d5">34.0</data>
  <data key="d6">The study demonstrates practical application of GitHub Copilot in programming education and problem solving.&lt;SEP&gt;The study illustrates how AI tools like Copilot can be integrated into educational practices for programming.</data>
  <data key="d7">application, programming assistance&lt;SEP&gt;educational technology, AI application</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Assessing the Quality of GitHub Copilot’s Code Generation" target="Burak Yetistiren et al.">
  <data key="d5">37.0</data>
  <data key="d6">The evaluation assesses how well Copilot's generated code meets quality standards in software engineering.&lt;SEP&gt;The research investigates the effectiveness and limitations of AI-generated code in real-world scenarios.</data>
  <data key="d7">performance evaluation, AI in software engineering&lt;SEP&gt;quality assessment, AI code generation</data>
  <data key="d8">chunk-26eb2867fee2b7e0f5aa834b5b5d9096</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="SYCL Benchmarks" target="Jin">
  <data key="d5">7.0</data>
  <data key="d6">Jin's technical report discusses implementing Rodinia benchmarks in SYCL to evaluate heterogeneous computing performance."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Scientific Computing" target="Kailasa, Wang, Barba, Betcke">
  <data key="d5">8.0</data>
  <data key="d6">They developed PyExaFMM, a high-performance Python and Numba-based software for scientific multipole computations."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="GPU Programming" target="Klöckner">
  <data key="d5">8.0</data>
  <data key="d6">Klöckner's pycuda and pyopencl libraries enable scripting, runtime code generation, and GPU acceleration in Python."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Helmuth, Kelly" target="Program Synthesis Benchmark">
  <data key="d5">8.0</data>
  <data key="d6">They developed PSB2 to benchmark program synthesis algorithms, facilitating standardized evaluation."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5da9b3b8fd36c0504bd8f232a5c91252</data>
  <data key="d9">Godoy et al. - 2023 - Evaluation of OpenAI Codex for HPC Parallel Progra.pdf</data>
</edge>
<edge source="Discipline" target="Springer International Publishing">
  <data key="d5">10.0</data>
  <data key="d6">Springer International Publishing is a major publisher disseminating scientific research in computer science, HPC, and related fields."|&lt;SEP&gt;Springer publishes scientific research, including computer science and HPC topics."|</data>
  <data key="d7">publishing, scientific research</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Pre-trained Language Models" target="Retrieval-augmented generation">
  <data key="d5">7.0</data>
  <data key="d6">Pre-trained language models serve as the parametric component in RAG, providing a foundation for knowledge storage and language generation.</data>
  <data key="d7">model architecture, foundation</data>
  <data key="d8">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Non-parametric Memory" target="Retrieval-augmented generation">
  <data key="d5">9.0</data>
  <data key="d6">Non-parametric memory provides external, editable knowledge that complements the parametric model in RAG.</data>
  <data key="d7">knowledge access, external memory</data>
  <data key="d8">chunk-4bb8bb2fc231e52a13ed0e91512e55eb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Wikipedia" target="FEVER">
  <data key="d5">18.0</data>
  <data key="d6">Wikipedia provides evidence for classifying claims, linking the core concept of FEVER to its evidence source.&lt;SEP&gt;Wikipedia provides the evidence source used for classifying claims, linking core concepts to evidence objects of study with retrieval processes.</data>
  <data key="d7">evidence retrieval, fact verification</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="State-of-the-Art Results" target="RAG">
  <data key="d5">8.0</data>
  <data key="d6">The combination of retrieval and generation in RAG leads to state-of-the-art results on multiple datasets, outperforming previous approaches.</data>
  <data key="d7">performance improvement, data benchmarks</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Models' Knowledge Updating" target="RAG">
  <data key="d5">7.0</data>
  <data key="d6">The non-parametric memory component allows for knowledge updates, reflecting changes in the external world.</data>
  <data key="d7">model adaptability, external knowledge</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG-Token Model" target="Decoding Methods">
  <data key="d5">8.0</data>
  <data key="d6">The RAG-Token model employs specific decoding strategies like beam search to generate outputs based on retrieved documents.</data>
  <data key="d7">decoding strategies, sequence generation</data>
  <data key="d8">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG-Token Model" target="Retriever (DPR)">
  <data key="d5">9.0</data>
  <data key="d6">The DPR retriever provides relevant document representations that are used by the RAG-Token model to generate responses.</data>
  <data key="d7">retrieval, content generation</data>
  <data key="d8">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG-Token Model" target="Generator (BART)">
  <data key="d5">8.0</data>
  <data key="d6">The BART generator uses concatenated input and retrieved documents to produce the final output.</data>
  <data key="d7">content synthesis, sequence modeling</data>
  <data key="d8">chunk-20aafa58f3d7697ab6ba5015b31cda9b</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Model Architecture" target="DeepSeek-Coder">
  <data key="d5">9.0</data>
  <data key="d6">DeepSeek-Coder is a specific family of models with architectures based on llama and MOE, designed for code modeling.</data>
  <data key="d7">model design, architecture</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Fast Decoding" target="Experiments">
  <data key="d5">7.0</data>
  <data key="d6">Fast Decoding is applied during the experiments to improve inference efficiency in RAG models.</data>
  <data key="d7">decoding efficiency</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Fast Decoding" target="decoding procedure">
  <data key="d5">7.0</data>
  <data key="d6">The Fast Decoding procedure is designed to reduce the need for additional forward passes during inference, improving efficiency."|</data>
  <data key="d7">methodology</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Wikipedia dump (December 2018)" target="Document encoder">
  <data key="d5">8.0</data>
  <data key="d6">The Wikipedia dump provides the corpus from which documents are split and embeddings are generated for retrieval."|</data>
  <data key="d7">knowledge source</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Wikipedia dump (December 2018)" target="document encoder">
  <data key="d5">8.0</data>
  <data key="d6">The Wikipedia dump provides the corpus from which documents are encoded into embeddings for retrieval."|</data>
  <data key="d7">knowledge source</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Document encoder" target="">
  <data key="d5">8.0</data>
  <data key="d6">The document encoder processes Wikipedia chunks to produce embeddings for similarity search."|</data>
  <data key="d7">retrieval process</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Document encoder" target="MIPS index with FAISS">
  <data key="d5">8.0</data>
  <data key="d6">The MIPS index stores document embeddings for fast retrieval during training and testing."|</data>
  <data key="d7">retrieval infrastructure</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge-intensive tasks" target="Open-domain Question Answering">
  <data key="d5">16.0</data>
  <data key="d6">Open-domain QA exemplifies the use of retrieval and generation to answer questions with external knowledge."|&lt;SEP&gt;Open-domain QA is a primary example of a knowledge-intensive task utilizing Wikipedia retrieval to answer questions."|</data>
  <data key="d7">application</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge-intensive tasks" target="Abstractive Question Answering">
  <data key="d5">16.0</data>
  <data key="d6">Abstractive QA involves generating answers beyond span extraction, leveraging retrieved knowledge for natural language responses."|&lt;SEP&gt;Abstractive QA leverages retrieved knowledge to generate free-form answers that are not limited to span extraction."|</data>
  <data key="d7">generation&lt;SEP&gt;generation capability</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge-intensive tasks" target="Jeopardy Question Generation">
  <data key="d5">16.0</data>
  <data key="d6">Generating Jeopardy questions involves factual knowledge about entities, testing the model's ability to generate precise, knowledge-based questions."|&lt;SEP&gt;Generating Jeopardy questions requires factual knowledge about entities, representing a complex knowledge-intensive generation challenge."|</data>
  <data key="d7">generation&lt;SEP&gt;generation challenge</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Knowledge-intensive tasks" target="Fact Veriﬁcation (FEVER)">
  <data key="d5">16.0</data>
  <data key="d6">FEVER involves retrieving evidence from Wikipedia to support or refute claims, exemplifying a retrieval and reasoning process."|&lt;SEP&gt;FEVER involves retrieving evidence from Wikipedia to support or refute claims, requiring reasoning over external knowledge."|</data>
  <data key="d7">evidence retrieval&lt;SEP&gt;retrieval and reasoning</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Jeopardy Question Generation" target="RAG-Token">
  <data key="d5">16.0</data>
  <data key="d6">RAG-Token performs better than RAG-Sequence on Jeopardy question generation, indicating its superior effectiveness in this task.</data>
  <data key="d7">model comparison, performance</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Y" target="candidate set Y">
  <data key="d5">8.0</data>
  <data key="d6">Y is the set of candidate documents or options generated during decoding, central to the process of knowledge retrieval and answer generation."|</data>
  <data key="d7">retrieval process</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FAISS" target="MIPS index">
  <data key="d5">8.0</data>
  <data key="d6">FAISS is used to build the MIPS index that enables fast approximate nearest neighbor search of document embeddings."|</data>
  <data key="d7">retrieval infrastructure</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FAISS" target="Hierarchical Navigable Small World (HNSW)">
  <data key="d5">8.0</data>
  <data key="d6">HNSW algorithm within FAISS facilitates efficient retrieval of relevant documents."|</data>
  <data key="d7">retrieval algorithm</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FEVER" target="Retrieval Problem">
  <data key="d5">8.0</data>
  <data key="d6">FEVER models address the retrieval problem by retrieving relevant evidence from Wikipedia to support claim classification.</data>
  <data key="d7">retrieval, evidence gathering</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FEVER" target="Entailment Reasoning">
  <data key="d5">8.0</data>
  <data key="d6">FEVER involves reasoning over retrieved evidence to determine the support or refutation of claims.</data>
  <data key="d7">reasoning, inference</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FEVER" target="Claim-Class Pairs">
  <data key="d5">7.0</data>
  <data key="d6">Claim-class pairs are used as training data to teach models how to classify claims based on evidence.</data>
  <data key="d7">training data, classification</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FEVER" target="Class Labels">
  <data key="d5">7.0</data>
  <data key="d6">The class labels support, refutes, and not enough info categorize claims in the FEVER framework.</data>
  <data key="d7">classification labels, categorization</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="FEVER" target="RAG">
  <data key="d5">8.0</data>
  <data key="d6">RAG is used to analyze document retrieval overlap with gold evidence in the FEVER dataset.</data>
  <data key="d7">retrieval evaluation, evidence correspondence</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Claim-Class Pairs" target="Claim">
  <data key="d5">8.0</data>
  <data key="d6">Claim-class pairs are used to train models to classify claims based on evidence.</data>
  <data key="d7">training, classification</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Class Labels" target="Claim">
  <data key="d5">7.0</data>
  <data key="d6">Class labels categorize the relationship between claims and evidence into supports, refutes, or not enough info.</data>
  <data key="d7">classification labels</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG Models" target="Open-domain QA Tasks">
  <data key="d5">16.0</data>
  <data key="d6">RAG models are applied to open-domain question answering, demonstrating superior performance over previous models."|&lt;SEP&gt;RAG models are applied to open-domain question answering, demonstrating superior performance.</data>
  <data key="d7">model application, performance&lt;SEP&gt;performance, model application</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG Models" target="Factuality">
  <data key="d5">18.0</data>
  <data key="d6">RAG models hallucinate less and generate more factually correct text than BART, demonstrating higher factuality.</data>
  <data key="d7">accuracy, factual correctness</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG Models" target="Generation Diversity">
  <data key="d5">7.0</data>
  <data key="d6">RAG models demonstrate higher diversity in generated outputs compared to BART, measured by n-gram ratios.</data>
  <data key="d7">model comparison, diversity</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Fact Verification" target="FEVER Dataset">
  <data key="d5">14.0</data>
  <data key="d6">The FEVER dataset is used to evaluate the models' ability to verify factual correctness of generated responses, with RAG achieving competitive results.</data>
  <data key="d7">factual accuracy, evaluation</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Purgatorio" target="Paradiso">
  <data key="d5">8.0</data>
  <data key="d6">Both are parts of Dante's Divine Comedy, representing different stages of the afterlife journey.</data>
  <data key="d7">literary structure, thematic progression</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Thorne" target="Vlachos">
  <data key="d5">7.0</data>
  <data key="d6">Thorne and Vlachos collaborate on claim classification models, such as RoBERTa, for fact verification.</data>
  <data key="d7">collaboration, research partnership</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RoBERTa" target="claim classification">
  <data key="d5">9.0</data>
  <data key="d6">RoBERTa is used to classify claims as true or false based on evidence sentences.</data>
  <data key="d7">model application, text classification</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RoBERTa" target="Claim Verification">
  <data key="d5">9.0</data>
  <data key="d6">RoBERTa is employed to classify claims as true or false based on evidence sentences.</data>
  <data key="d7">model application, classification</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG" target="Seq2seq Models">
  <data key="d5">8.0</data>
  <data key="d6">RAG builds upon seq2seq models by integrating retrieval mechanisms to enhance knowledge-based generation.</data>
  <data key="d7">model architecture, knowledge integration</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG" target="DPR">
  <data key="d5">9.0</data>
  <data key="d6">DPR provides relevant documents to RAG, acting as the non-parametric memory source for retrieval.</data>
  <data key="d7">retrieval component, external knowledge source</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG" target="BART/T5">
  <data key="d5">8.0</data>
  <data key="d6">BART and T5 serve as the generative backbone in RAG, conditioned on retrieved documents to produce responses.</data>
  <data key="d7">generation backbone, pre-trained models</data>
  <data key="d8">chunk-e185b4ac016fe0595175efaf985eeacb</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG" target="Claim Verification">
  <data key="d5">9.0</data>
  <data key="d6">RAG is used to retrieve relevant evidence and generate responses for claim verification tasks.</data>
  <data key="d7">application, evidence retrieval</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Top k Documents" target="Gold Evidence">
  <data key="d5">8.0</data>
  <data key="d6">The top retrieved documents are compared to gold evidence annotations to measure retrieval accuracy.</data>
  <data key="d7">retrieval accuracy, evidence matching</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Top k Documents" target="Claim Verification">
  <data key="d5">8.0</data>
  <data key="d6">The top retrieved documents are analyzed to see if they contain the gold evidence supporting or refuting claims.</data>
  <data key="d7">retrieval accuracy, evidence support</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BM25" target="Retrieval Mechanism">
  <data key="d5">14.0</data>
  <data key="d6">BM25 is used as a baseline retrieval method to compare with learned retrievers in RAG.&lt;SEP&gt;BM25 is used as a baseline retrieval method to compare with learned retrievers like dense retrieval in RAG.</data>
  <data key="d7">baseline comparison, retrieval method&lt;SEP&gt;baseline, retrieval method</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BM25" target="Probabilistic Relevance Framework">
  <data key="d5">8.0</data>
  <data key="d6">BM25 is an implementation based on the probabilistic relevance framework, used for ranking documents in information retrieval tasks.</data>
  <data key="d7">retrieval algorithms, relevance ranking</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="World Leaders" target="Index Updates">
  <data key="d5">7.0</data>
  <data key="d6">The list of world leaders is used to evaluate the accuracy of RAG's knowledge when using different Wikipedia indices.</data>
  <data key="d7">knowledge accuracy, index relevance</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Claim Verification" target="FEVER Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The FEVER dataset provides annotated evidence to evaluate the performance of claim verification models.</data>
  <data key="d7">dataset, evaluation</data>
  <data key="d8">chunk-02a741669060d492fb68529287d315dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Indices" target="Mismatched Indices">
  <data key="d5">12.0</data>
  <data key="d6">Errors in index matching cause low accuracy, highlighting limitations of index-based evaluation.&lt;SEP&gt;Low accuracy results are caused by mismatched indices, which fail to correctly identify leaders.</data>
  <data key="d7">error, limitation, measurement&lt;SEP&gt;error, measurement</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG (Retrieval-Augmented Generation)" target="Effect of Retrieving More Documents">
  <data key="d5">18.0</data>
  <data key="d6">Retrieving more documents at test time can improve model performance metrics but may also affect runtime.&lt;SEP&gt;Retrieving more documents at test time influences model performance and runtime, with performance improvements up to a peak.</data>
  <data key="d7">performance optimization, retrieval quantity&lt;SEP&gt;performance, retrieval, efficiency</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Figure 3" target="Retrieval Quantity and Performance">
  <data key="d5">14.0</data>
  <data key="d6">Figure 3 depicts how increasing retrieved documents influences model performance across different metrics and models.&lt;SEP&gt;Figure 3 illustrates the relationship between number of retrieved documents and performance metrics for different models.</data>
  <data key="d7">visualization, performance analysis</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Related Work" target="Single-Task Retrieval">
  <data key="d5">16.0</data>
  <data key="d6">Prior research demonstrates that retrieval enhances various NLP tasks, supporting the effectiveness of retrieval-augmented models.&lt;SEP&gt;Prior research shows retrieval improves various NLP tasks, demonstrating the benefit of retrieval-augmented models.</data>
  <data key="d7">literature, task improvement&lt;SEP&gt;literature, task improvement, validation</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Related Work" target="General-Purpose Architectures">
  <data key="d5">12.0</data>
  <data key="d6">Pre-trained models like GPT-2, BART, T5 achieve strong performance without retrieval but can be extended with retrieval modules for further gains.&lt;SEP&gt;Pre-trained models like GPT-2, BART, T5 achieve strong performance, with potential for retrieval augmentation.</data>
  <data key="d7">model architecture, performance&lt;SEP&gt;model architecture, performance, extension</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Related Work" target="Learned Retrieval">
  <data key="d5">14.0</data>
  <data key="d6">Methods that optimize document retrieval through neural models, reinforcement learning, or latent variables to improve task-specific results.&lt;SEP&gt;Optimizing document retrieval with neural models and reinforcement learning enhances downstream task results.</data>
  <data key="d7">retrieval optimization, neural methods, reinforcement learning&lt;SEP&gt;retrieval optimization, neural models</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Related Work" target="Memory-based Architectures">
  <data key="d5">14.0</data>
  <data key="d6">External memory modules that allow models to attend over raw text or embeddings, improving factual accuracy and interpretability.&lt;SEP&gt;Using external memory and attending over raw text or embeddings improves factual accuracy and interpretability.</data>
  <data key="d7">memory networks, factual accuracy&lt;SEP&gt;memory networks, factual correctness, interpretability</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Related Work" target="Retrieve-and-Edit">
  <data key="d5">12.0</data>
  <data key="d6">Retrieval and editing methods that generate outputs by retrieving similar content and making edits, successful in translation and semantic parsing.&lt;SEP&gt;Retrieving similar content and editing it to generate outputs has been successful in translation and semantic parsing.</data>
  <data key="d7">retrieval, editing, domain applications</data>
  <data key="d8">chunk-7d9c04128411fd2962a424d26641c6ad</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Bias" target="Societal Impact">
  <data key="d5">14.0</data>
  <data key="d6">Bias in external sources like Wikipedia can influence model outputs, necessitating bias mitigation strategies.</data>
  <data key="d7">risk</data>
  <data key="d8">chunk-6c82d084cc6bf7773cde2e068c3bcedc</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Association for Computational Linguistics" target="Eunsol Choi">
  <data key="d5">2.0</data>
  <data key="d6">Eunsol Choi is affiliated with the Association for Computational Linguistics, contributing to research in question answering and language understanding."|&lt;SEP&gt;Eunsol Choi is affiliated with the Association for Computational Linguistics, contributing to research in question answering, language understanding, and NLP methodologies."|</data>
  <data key="d7">1</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Eunsol Choi" target="Daniel Hewlett">
  <data key="d5">4.0</data>
  <data key="d6">Both are researchers contributing to question answering and long document comprehension."|&lt;SEP&gt;Both researchers are involved in question answering research, indicating collaboration or related work."|</data>
  <data key="d7">2</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Eunsol Choi" target="Alexandre Lacoste">
  <data key="d5">4.0</data>
  <data key="d6">Related research on question answering in long documents."|&lt;SEP&gt;Research related to long document question answering."|</data>
  <data key="d7">2</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Eunsol Choi" target="Jonathan Berant">
  <data key="d5">4.0</data>
  <data key="d6">Shared focus on question answering and semantic parsing."|&lt;SEP&gt;Shared research focus on question answering systems."|</data>
  <data key="d7">2</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Eunsol Choi" target="Entities as Experts">
  <data key="d5">4.0</data>
  <data key="d6">Choi's research on entities as experts ties into sparse memory models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Christopher Clark" target="Simple and Effective Multi-Paragraph Reading Comprehension">
  <data key="d5">8.0</data>
  <data key="d6">Clark's work on comprehension models relates to the broader field of question answering and reading comprehension."|&lt;SEP&gt;Clark's work relates to question answering and reading comprehension models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Matt Gardner" target="Simple and Effective Multi-Paragraph Reading Comprehension">
  <data key="d5">8.0</data>
  <data key="d6">Gardner's research supports effective multi-paragraph comprehension, relevant to question answering."|&lt;SEP&gt;Gardner's research supports effective reading comprehension techniques applicable to question answering."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Jacob Devlin" target="BERT: Pre-training of Deep Bidirectional Transformers">
  <data key="d5">4.0</data>
  <data key="d6">Devlin's BERT model is a foundational tool for language understanding, relevant to question answering."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Ming-Wei Chang" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Chang's research contributes to knowledge grounding in dialogue systems."|&lt;SEP&gt;Chang's research on knowledge grounding enhances neural QA systems."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Emily Dinan" target="Wizard of Wikipedia">
  <data key="d5">8.0</data>
  <data key="d6">Dinan's work on knowledge-powered conversational agents relates to question answering and dialogue systems."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Stephen Roller" target="Wizard of Wikipedia">
  <data key="d5">8.0</data>
  <data key="d6">Roller's research is connected to knowledge-based conversational AI."|&lt;SEP&gt;Roller's research supports knowledge-grounded dialogue, relevant to question answering."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Kurt Shuster" target="Wizard of Wikipedia">
  <data key="d5">8.0</data>
  <data key="d6">Shuster's work on neural conversation models is related to knowledge-based QA systems."|&lt;SEP&gt;Shuster's work supports knowledge-powered dialogue systems."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Angela Fan" target="Neural Story Generation">
  <data key="d5">8.0</data>
  <data key="d6">Fan's research on hierarchical neural models supports language understanding and generation tasks including QA."|&lt;SEP&gt;Fan's research on hierarchical neural story generation relates to narrative creation and language modeling."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Angela Fan" target="Transformers with KNN-based Memory">
  <data key="d5">8.0</data>
  <data key="d6">Fan's work on augmenting transformers with memory enhances language understanding and generation."|&lt;SEP&gt;Fan's work on augmenting transformers with memory mechanisms enhances question answering and language understanding."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Michael Auli" target="Neural Story Generation">
  <data key="d5">8.0</data>
  <data key="d6">Auli's work on language models supports neural understanding relevant to QA."|&lt;SEP&gt;Auli's work on language models supports story generation tasks."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Jason Weston" target="Neural Story Generation">
  <data key="d5">8.0</data>
  <data key="d6">Weston contributes to neural models for storytelling and language understanding."|&lt;SEP&gt;Weston contributes to neural models that can be applied in story and language understanding, related to QA."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Ethan Perez" target="Transformers with KNN-based Memory">
  <data key="d5">8.0</data>
  <data key="d6">Perez's research on neural memory modules supports question answering models."|&lt;SEP&gt;Perez's work advances neural models with memory components."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="David Grangier" target="Transformers with KNN-based Memory">
  <data key="d5">8.0</data>
  <data key="d6">Grangier's research enhances transformer-based language models."|&lt;SEP&gt;Grangier's work on transformer enhancements supports language understanding."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="http://arxiv.org/abs/1705.08807" target="Tool">
  <data key="d5">8.0</data>
  <data key="d6">URL to the paper discussing AI surpassing human performance."|&lt;SEP&gt;URL to the paper on AI surpassing human performance."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Jiatao Gu" target="Search engine guided neural machine translation">
  <data key="d5">26.0</data>
  <data key="d6">Gu's research integrates search engine techniques into neural machine translation models."|&lt;SEP&gt;Gu's research on search-guided neural translation integrates search engines into translation models."|&lt;SEP&gt;contributes to</data>
  <data key="d7">4&lt;SEP&gt;Core Concepts</data>
  <data key="d8">chunk-b5a1ae2fdbff2671d10e6572fa6fc442&lt;SEP&gt;chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Yong Wang" target="Search engine guided neural machine translation">
  <data key="d5">8.0</data>
  <data key="d6">Wang's work supports search-guided translation methods."|&lt;SEP&gt;Wang's work supports search-guided translation techniques."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Kyunghyun Cho" target="Search engine guided neural machine translation">
  <data key="d5">8.0</data>
  <data key="d6">Cho's research involves neural translation models enhanced with search guidance."|&lt;SEP&gt;Cho's research on neural translation models includes search guidance mechanisms."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Victor O.K. Li" target="Search engine guided neural machine translation">
  <data key="d5">4.0</data>
  <data key="d6">Li's work on integrating search engines into translation models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Thibault Févry" target="Entities as Experts">
  <data key="d5">8.0</data>
  <data key="d6">Févry's research on entity-based sparse memory access relates to knowledge representation in QA."|&lt;SEP&gt;Févry's research on sparse memory access with entity supervision relates to entity-based knowledge representation."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Livio Baldini Soares" target="Entities as Experts">
  <data key="d5">8.0</data>
  <data key="d6">Soares' work on entity supervision supports knowledge-grounded models."|&lt;SEP&gt;Soares' work supports entity supervision in neural models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Nicholas FitzGerald" target="Entities as Experts">
  <data key="d5">8.0</data>
  <data key="d6">FitzGerald contributes to entity supervision research."|&lt;SEP&gt;FitzGerald's research on sparse memory access with entities enhances QA models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Marjan Ghazvininejad" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Ghazvininejad's research on knowledge-grounded models relates to conversational AI."|&lt;SEP&gt;Ghazvininejad's research supports knowledge grounding in neural dialogue and QA systems."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Chris Brockett" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Brockett's work supports integrating external knowledge into neural dialogue models."|&lt;SEP&gt;Brockett's work supports knowledge integration in neural conversation models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Bill Dolan" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Dolan's work supports integrating knowledge into conversational agents."|&lt;SEP&gt;Dolan's work supports knowledge-based neural dialogue and QA."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Jianfeng Gao" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Gao's research advances knowledge-grounded neural dialogue models."|&lt;SEP&gt;Gao's research enhances knowledge-grounded dialogue models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Wentu Yih" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Yih's work supports external knowledge integration in neural dialogue systems."|&lt;SEP&gt;Yih's work supports knowledge integration in neural conversation models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Michel Galley" target="Knowledge-Grounded Neural Conversation">
  <data key="d5">8.0</data>
  <data key="d6">Galley's research advances neural models grounded in external knowledge."|&lt;SEP&gt;Galley's research supports knowledge grounding in neural conversational AI."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Katja Grace" target="AI exceeding human performance">
  <data key="d5">8.0</data>
  <data key="d6">Grace's research involves expert surveys estimating when AI will surpass human intelligence."|&lt;SEP&gt;Grace's research involves expert surveys estimating when AI will surpass human performance."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="John Salvatier" target="AI exceeding human performance">
  <data key="d5">8.0</data>
  <data key="d6">Salvatier contributes to assessments on AI progress and performance."|&lt;SEP&gt;Salvatier's work involves expert opinions on AI progress timelines."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Owain Evans" target="AI exceeding human performance">
  <data key="d5">8.0</data>
  <data key="d6">Evans' research involves assessing when AI might outperform humans."|&lt;SEP&gt;Evans' work involves evaluating expert opinions on AI capabilities."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Kelvin Guu" target="REALM: Retrieval-augmented language model pre-training">
  <data key="d5">16.0</data>
  <data key="d6">contributes to</data>
  <data key="d7">Theories/Models</data>
  <data key="d8">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Tatsunori B. Hashimoto" target="retrieve-and-edit framework">
  <data key="d5">16.0</data>
  <data key="d6">contributes to</data>
  <data key="d7">Theories/Models</data>
  <data key="d8">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Urvashi Khandelwal" target="generalization through memorization">
  <data key="d5">14.0</data>
  <data key="d6">contributes to</data>
  <data key="d7">Theories/Models</data>
  <data key="d8">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Mike Lewis" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Co-author in language model research, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Tom Kwiatkowski" target="Entities as Experts">
  <data key="d5">4.0</data>
  <data key="d6">Kwiatkowski's work supports entity supervision in neural models."|</data>
  <data key="d7">4</data>
  <data key="d8">chunk-f649fd9f0abb33568aa0dd91ee5a605f</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Tom Kwiatkowski" target="Natural Questions dataset">
  <data key="d5">16.0</data>
  <data key="d6">contributes to</data>
  <data key="d7">Theories/Models</data>
  <data key="d8">chunk-b5a1ae2fdbff2671d10e6572fa6fc442</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Natural Questions: a Benchmark for Question Answering Research" target="Guillaume Lample et al.">
  <data key="d5">16.0</data>
  <data key="d6">The benchmark dataset and research framework provided by this study serve as a standard for evaluating question answering systems.</data>
  <data key="d7">benchmark, evaluation</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Large memory layers with product keys" target="Guillaume Lample et al.&quot;|&lt;&quot;The architecture proposes large memory layers to enhance neural network capacity, which can be applied in question answering models.">
  <data key="d5">7.0</data>
  <data key="d6">model architecture, memory</data>
  <data key="d7">7</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Large memory layers with product keys" target="Guillaume Lample et al.">
  <data key="d5">7.0</data>
  <data key="d6">The architecture proposes large memory layers to enhance neural network capacity, which can be applied in question answering models.</data>
  <data key="d7">model architecture, memory</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Latent retrieval for weakly supervised open domain question answering" target="Kenton Lee, Ming-Wei Chang, and Kristina Toutanova">
  <data key="d5">18.0</data>
  <data key="d6">This methodology improves open-domain QA performance by utilizing latent retrieval techniques in weakly supervised settings.</data>
  <data key="d7">retrieval, weak supervision</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="A diversity-promoting objective function for neural conversation models" target="Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan">
  <data key="d5">16.0</data>
  <data key="d6">This objective function aims to increase diversity in generated conversations, improving system engagement and relevance.</data>
  <data key="d7">diversity, dialogue models</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Robust neural machine translation with joint textual and phonetic embedding" target="Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He">
  <data key="d5">14.0</data>
  <data key="d6">This approach enhances translation robustness by combining textual and phonetic features.</data>
  <data key="d7">translation, embeddings</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Generating Wikipedia by summarizing long sequences" target="Peter J. Liu et al.">
  <data key="d5">14.0</data>
  <data key="d6">This methodology advances automatic content generation by effectively summarizing lengthy sequences into encyclopedic articles.</data>
  <data key="d7">sequence summarization, content generation</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs" target="Yury A. Malkov and D. A. Yashunin">
  <data key="d5">14.0</data>
  <data key="d6">This algorithm improves the speed and robustness of nearest neighbor searches in high-dimensional data.</data>
  <data key="d7">search algorithms, graphs</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="The next decade in AI: four steps towards robust artificial intelligence" target="Gary Marcus">
  <data key="d5">16.0</data>
  <data key="d6">This research proposes strategic steps to develop more reliable and robust AI systems in the future.</data>
  <data key="d7">AI development, robustness</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="How decoding strategies affect the verifiability of generated text" target="Luca Massarelli, Fabio Petroni, et al.">
  <data key="d5">16.0</data>
  <data key="d6">This empirical study examines how different decoding methods influence the trustworthiness of generated language.</data>
  <data key="d7">decoding strategies, verifiability</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Mixed precision training" target="Paulius Micikevicius et al.">
  <data key="d5">14.0</data>
  <data key="d6">This technique accelerates neural network training while preserving accuracy, applicable across various NLP models.</data>
  <data key="d7">training, efficiency</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Towards exploiting background knowledge for building conversation systems" target="Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra">
  <data key="d5">16.0</data>
  <data key="d6">This study investigates how background knowledge can be integrated to improve conversational AI systems.</data>
  <data key="d7">background knowledge, dialogue systems</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Fact Extraction and Verification Dataset (FEVER)" target="NLP Research">
  <data key="d5">8.0</data>
  <data key="d6">FEVER provides a benchmark for evaluating models on their ability to extract and verify facts, advancing research in factual accuracy.</data>
  <data key="d7">evaluation, dataset</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Mitigating Model Biases" target="Model Biases">
  <data key="d5">7.0</data>
  <data key="d6">Techniques like elastic weight consolidation aim to reduce biases and prevent catastrophic forgetting in models, enhancing fairness and stability.</data>
  <data key="d7">bias reduction, continual learning</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="GLUE Benchmark" target="NLP Models">
  <data key="d5">8.0</data>
  <data key="d6">GLUE provides a standardized platform to evaluate and compare the performance of NLP models across multiple tasks.</data>
  <data key="d7">benchmarking, model evaluation</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="SuperGLUE Benchmark" target="NLP Models">
  <data key="d5">7.0</data>
  <data key="d6">SuperGLUE offers a more challenging evaluation environment for assessing the robustness and generalization of language understanding systems.</data>
  <data key="d7">robustness, evaluation</data>
  <data key="d8">chunk-19e79205b492f438828e7baefec8588e</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Alché-Buc, E. Fox, and R. Garnett" target="Advances in Neural Information Processing Systems 32">
  <data key="d5">16.0</data>
  <data key="d6">The publication is authored by these editors and presents research in neural information processing.&lt;SEP&gt;This publication is edited by these editors and presents recent advances in neural information processing.</data>
  <data key="d7">publication, authorship&lt;SEP&gt;publication, editors</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Shuohang Wang" target="R3: Reinforced ranker-reader for open-domain question answering">
  <data key="d5">18.0</data>
  <data key="d6">Shuohang Wang authored the paper presenting the R3 model, which enhances answer retrieval in open-domain QA.&lt;SEP&gt;Shuohang Wang is the lead author of this paper, which introduces a reinforcement learning model for question answering.</data>
  <data key="d7">author, research topic</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Shuohang Wang" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author contributing to prompt reliability research, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Mo Yu" target="R3: Reinforced ranker-reader for open-domain question answering">
  <data key="d5">16.0</data>
  <data key="d6">Mo Yu co-authored the paper, contributing to the development of reinforcement learning techniques for answer ranking.&lt;SEP&gt;Mo Yu co-authored the paper, contributing to the development of the model and methodology.</data>
  <data key="d7">co-author, research contribution</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Xiaoxiao Guo" target="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d5">14.0</data>
  <data key="d6">Xiaoxiao Guo contributed to research on evidence aggregation strategies to improve answer ranking accuracy.&lt;SEP&gt;Xiaoxiao Guo contributed to research on evidence aggregation techniques that improve answer ranking.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Zhiguo Wang" target="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d5">14.0</data>
  <data key="d6">Zhiguo Wang contributed to evidence aggregation research in NLP.&lt;SEP&gt;Zhiguo Wang contributed to evidence aggregation techniques aimed at answer reranking.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Tim Klinger" target="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d5">14.0</data>
  <data key="d6">Tim Klinger contributed to the development of evidence aggregation methods for question answering.&lt;SEP&gt;Tim Klinger worked on evidence aggregation methods to enhance answer ranking performance.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Shiyu Chang" target="Evidence aggregation for answer re-ranking in open-domain question answering">
  <data key="d5">14.0</data>
  <data key="d6">Shiyu Chang contributed to evidence aggregation methods for improving answer ranking.&lt;SEP&gt;Shiyu Chang contributed to evidence aggregation research for answer reranking.</data>
  <data key="d7">research contribution, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Gerry Tesauro" target="Memory networks">
  <data key="d5">8.0</data>
  <data key="d6">Gerry Tesauro developed or contributed to the theory of memory networks, enhancing reasoning in NLP models.</data>
  <data key="d7">theoretical contribution, model</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Retrieve and refine models for dialogue" target="Jason Weston, Emily Dinan, and Alexander Miller">
  <data key="d5">16.0</data>
  <data key="d6">These researchers collaborated on models that retrieve relevant information and refine responses for dialogue systems.&lt;SEP&gt;These researchers worked on improving sequence generation models for dialogue systems through retrieval and refinement techniques.</data>
  <data key="d7">research collaboration, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Huggingface’s transformers" target="Thomas Wolf et al.">
  <data key="d5">16.0</data>
  <data key="d6">Thomas Wolf and colleagues developed the transformers library, a tool for NLP model implementation.&lt;SEP&gt;Thomas Wolf and colleagues developed the transformers library, providing tools for NLP research.</data>
  <data key="d7">tool development, NLP</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Addressing semantic drift in question generation" target="Shiyue Zhang and Mohit Bansal">
  <data key="d5">16.0</data>
  <data key="d6">They conducted research to address semantic drift issues in semi-supervised question generation for improved QA.&lt;SEP&gt;They conducted research to address semantic drift issues in semi-supervised question generation.</data>
  <data key="d7">research focus, NLP challenge</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Reasoning over semantic-level graph for fact checking" target="Wanjun Zhong, Jingjing Xu, Jiahai Wang, and Jian Yin">
  <data key="d5">16.0</data>
  <data key="d6">These researchers collaborated on methods that perform reasoning over semantic graphs to verify facts.&lt;SEP&gt;These researchers collaborated on methods to perform reasoning over semantic graphs for fact verification.</data>
  <data key="d7">collaboration, methodology</data>
  <data key="d8">chunk-b576e2170fa3612deda92305d2850446</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="" target="document encoder">
  <data key="d5">8.0</data>
  <data key="d6">The document encoder processes Wikipedia articles into embeddings for similarity search."|</data>
  <data key="d7">retrieval</data>
  <data key="d8">chunk-99d4d675d4565ed5a3ce6fb12d24e2ee</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Open-domain QA Models" target="RAG-Token and RAG-Sequence">
  <data key="d5">7.0</data>
  <data key="d6">Different variants of RAG models, such as RAG-Token and RAG-Sequence, are evaluated for performance on QA tasks.</data>
  <data key="d7">model variants, evaluation</data>
  <data key="d8">chunk-3a383a22868857a4e9fe55978176d008</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="RAG-Token" target="Document Posterior">
  <data key="d5">14.0</data>
  <data key="d6">The posterior distribution over documents helps the model determine which source is most relevant during response generation, especially when combining content from multiple documents.</data>
  <data key="d7">document relevance, content integration</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Document 1" target="Factual Context">
  <data key="d5">12.0</data>
  <data key="d6">Provides historical context on American literature, relevant for verifying facts about authors and works.</data>
  <data key="d7">knowledge base, fact checking</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Factual Context" target="Document 2">
  <data key="d5">12.0</data>
  <data key="d6">Provides historical context on 1920s expatriate artists, relevant for verifying facts about literary movements.</data>
  <data key="d7">knowledge base, fact checking</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Factual Context" target="Document 3">
  <data key="d5">6.0</data>
  <data key="d6">Provides biographical details about Hemingway's wartime experiences and novel 'A Farewell to Arms'.</data>
  <data key="d7">biographical context, fact verification</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Factual Context" target="Document 4">
  <data key="d5">6.0</data>
  <data key="d6">Provides context on 1920s expatriate artists, supporting verification of related facts.</data>
  <data key="d7">historical context, verification</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Factual Context" target="Document 5">
  <data key="d5">6.0</data>
  <data key="d6">Provides information about the author of 'The Sun Also Rises' and related works.</data>
  <data key="d7">literary attribution, fact checking</data>
  <data key="d8">chunk-16f8966714d5949d2ce008e4018d6933</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="BART: Denoising sequence-to-sequence pre-training" target="Mike Lewis et al.">
  <data key="d5">18.0</data>
  <data key="d6">BART's pre-training approach enhances language understanding and generation, impacting various NLP tasks including question answering.</data>
  <data key="d7">pre-training, language models</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Acute-eval: Improved dialogue evaluation" target="Margaret Li, Jason Weston, and Stephen Roller">
  <data key="d5">16.0</data>
  <data key="d6">This evaluation methodology improves assessment accuracy of dialogue systems through optimized questions and multi-turn comparisons.</data>
  <data key="d7">evaluation, dialogue systems</data>
  <data key="d8">chunk-33d74fdf1c7d92bf60e54b62b86377dd</data>
  <data key="d9">NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf</data>
</edge>
<edge source="Memory and Power Walls" target="Heterogeneous Hardware">
  <data key="d5">14.0</data>
  <data key="d6">The limitations imposed by memory and power constraints drive the adoption of heterogeneous hardware architectures in HPC.&lt;SEP&gt;The limitations imposed by memory bandwidth and power consumption drive the adoption of heterogeneous hardware architectures in HPC."|</data>
  <data key="d7">7&lt;SEP&gt;system design challenge, hardware heterogeneity</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Parallel Pattern Language">
  <data key="d5">16.0</data>
  <data key="d6">The code generator is an implementation of the Parallel Pattern Language, producing optimized source code for heterogeneous HPC systems.&lt;SEP&gt;The code generator is an implementation of the Parallel Pattern Language, producing optimized source code for shared memory, distributed memory, and GPU offloading."|</data>
  <data key="d7">8&lt;SEP&gt;tool implementation, code automation</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Rodinia Benchmark Suite">
  <data key="d5">18.0</data>
  <data key="d6">The code generator is evaluated by compiling and optimizing applications from the Rodinia benchmark suite to demonstrate performance gains."|&lt;SEP&gt;The code generator is evaluated by compiling and optimizing applications from the Rodinia benchmark suite.</data>
  <data key="d7">9&lt;SEP&gt;benchmark testing, performance evaluation</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Overheads during Compile Time Optimization">
  <data key="d5">12.0</data>
  <data key="d6">Overheads are a current limitation, impacting the efficiency of static code analysis and transformations."|&lt;SEP&gt;Overheads are a limitation of the current prototype, affecting the efficiency of compile-time global optimizations.</data>
  <data key="d7">6&lt;SEP&gt;performance overhead, analysis complexity</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Evaluation against Rodinia Benchmarks">
  <data key="d5">9.0</data>
  <data key="d6">The evaluation demonstrates the code generator's ability to produce optimized code that achieves speedups on benchmark applications."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="PPLprototype">
  <data key="d5">18.0</data>
  <data key="d6">The code generator is a central component of the PPLprototype, enabling automated source code creation for parallel applications based on high-level specifications."|&gt;"software tool, code automation&lt;SEP&gt;The code generator is a core component of the PPLprototype, enabling automatic source code creation for parallel applications.</data>
  <data key="d7">9&lt;SEP&gt;software tool, code automation</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="parallel patterns">
  <data key="d5">20.0</data>
  <data key="d6">The code generator employs CUDA, MPI, and pthreads to implement parallel patterns supporting synchronization and data movement across hardware units.&lt;SEP&gt;The code generator employs CUDA, MPI, and pthreads to implement parallel patterns supporting synchronization and data movement across hardware units."|</data>
  <data key="d7">10&lt;SEP&gt;performance optimization, implementation</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Inlining">
  <data key="d5">16.0</data>
  <data key="d6">The code generator performs inlining to flatten code hierarchies, facilitating further optimization.&lt;SEP&gt;The code generator performs inlining to simplify code structure, enabling further optimizations like loop unrolling and flattening.</data>
  <data key="d7">code transformation</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="PPL">
  <data key="d5">16.0</data>
  <data key="d6">The PPL approach includes a code generator that produces optimized code for heterogeneous architectures."|</data>
  <data key="d7">Tools,Performance Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Performance Slowdown (17%)">
  <data key="d5">14.0</data>
  <data key="d6">The code generator exhibits a 17% slowdown compared to hand-optimized code, indicating optimization opportunities."|</data>
  <data key="d7">Performance,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Speedup (14%)">
  <data key="d5">16.0</data>
  <data key="d6">Achieving a 14% speedup over the baseline demonstrates the effectiveness of the current approach."|</data>
  <data key="d7">Performance Improvement</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code Generator" target="Memory Model">
  <data key="d5">14.0</data>
  <data key="d6">Improving the memory model can reduce local copies, enhancing performance and efficiency."|</data>
  <data key="d7">Objects of Study,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Rodinia Benchmark Suite" target="PPL">
  <data key="d5">16.0</data>
  <data key="d6">The PPL toolchain is evaluated against kernels from the Rodinia suite to assess performance and applicability.&lt;SEP&gt;The PPL toolchain is evaluated against kernels from the Rodinia suite to validate its performance and applicability across diverse workloads."|&gt;"benchmarking, evaluation</data>
  <data key="d7">8&lt;SEP&gt;benchmarking, evaluation</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Rodinia Benchmark Suite" target="&lt;&gt;Shuai Che et al.">
  <data key="d5">16.0</data>
  <data key="d6">A benchmarking suite used to evaluate the performance and scalability of heterogeneous computing systems across various workloads.&lt;SEP&gt;A benchmarking suite used to evaluate the performance of heterogeneous computing systems.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimizations" target="Assignment between Tasklets and Architecture">
  <data key="d5">20.0</data>
  <data key="d6">Global optimizations include calculating an assignment of tasklets to hardware during compile time to improve performance.&lt;SEP&gt;Global optimizations include calculating an assignment of tasklets to specific hardware resources during compile time to improve efficiency."|</data>
  <data key="d7">10&lt;SEP&gt;optimization process, workload distribution</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimizations" target="Parallel Pattern Language">
  <data key="d5">8.0</data>
  <data key="d6">The language's semantic constructs enable application-wide optimizations during code generation."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="Static Global Optimizations">
  <data key="d5">16.0</data>
  <data key="d6">Parallel patterns facilitate static global optimizations by providing semantic information about parallelism and data flow.&lt;SEP&gt;Parallel patterns provide semantic information that enables static global optimizations across the entire application."|</data>
  <data key="d7">8&lt;SEP&gt;semantic analysis, optimization enablement</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="PPL">
  <data key="d5">18.0</data>
  <data key="d6">PPL utilizes parallel patterns to represent algorithms, facilitating verification and optimization of real-world applications.</data>
  <data key="d7">algorithm modeling, pattern representation</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="DSL">
  <data key="d5">16.0</data>
  <data key="d6">Parallel patterns are defined within the DSL, enabling element-wise operations and structured parallelism in application descriptions.</data>
  <data key="d7">application modeling, element-wise computation</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="Execution Units">
  <data key="d5">16.0</data>
  <data key="d6">Execution units are defined by the program to divide device cores for parallel computation, enabling flexible resource partitioning and workload distribution.&lt;SEP&gt;Execution units are defined by the program to divide device cores for parallel computation, enabling flexible resource partitioning.</data>
  <data key="d7">resource allocation, parallelism</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="APT">
  <data key="d5">18.0</data>
  <data key="d6">The APT includes nodes that define control-flow and computations, categorized into expressions and statements, forming the basis for dependency analysis and optimization.&lt;SEP&gt;The APT includes nodes that define control-flow and computations, which are categorized into expressions and statements, forming the basis for dependency analysis.</data>
  <data key="d7">program analysis, hierarchical modeling&lt;SEP&gt;program analysis, hierarchical representation</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="Data Dependencies">
  <data key="d5">18.0</data>
  <data key="d6">Data dependencies are derived from expressions within parallel patterns, enabling static analysis of data flow and correct execution ordering.&lt;SEP&gt;Data dependencies are derived from expressions within parallel patterns, enabling static analysis of data flow and execution order.</data>
  <data key="d7">dependency analysis, parallel execution</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallel Patterns" target="Annotations">
  <data key="d5">16.0</data>
  <data key="d6">Annotations similar to OpenMP can be used to identify parallel patterns in source code, aiding parallelization."|</data>
  <data key="d7">Methodology,Code Annotation</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="PPLprototype" target="Abstract Pattern Tree (APT)">
  <data key="d5">14.0</data>
  <data key="d6">The APT is utilized within the PPLprototype to facilitate inlining and loop unrolling, enhancing optimization.&lt;SEP&gt;The APT supports the PPL in performing inlining and loop unrolling, which are key optimization techniques for performance enhancement."|&gt;"data structure, optimization techniques</data>
  <data key="d7">7&lt;SEP&gt;data structure, optimization techniques</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="LULESH proxy-app" target="PPL">
  <data key="d5">16.0</data>
  <data key="d6">The port of LULESH to the PPL demonstrates the practical use and scalability of the toolchain in real-world applications."|&gt;"application port, scalability&lt;SEP&gt;The port of LULESH to the PPL demonstrates the toolchain's capacity for real-world application and scalability testing.</data>
  <data key="d7">8&lt;SEP&gt;application port, scalability</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parallelization Patterns" target="Optimization of Parallel Programs">
  <data key="d5">18.0</data>
  <data key="d6">Recurring patterns like map, reduction, and stencil are exploited within the PPL to optimize parallel code performance."|&gt;"performance optimization, algorithm design&lt;SEP&gt;Recurring structures like map, reduction, and stencil are central to optimizing parallel code performance.</data>
  <data key="d7">9&lt;SEP&gt;performance optimization, algorithm design</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization of Parallel Programs" target="Polyhedral Compilers">
  <data key="d5">14.0</data>
  <data key="d6">Polyhedral models are used to optimize loop nests in parallel programs, especially for complex nested loops, often integrated into tools like the PPL."|&gt;"loop optimization, polyhedral models&lt;SEP&gt;Polyhedral models are used to optimize loop nests in parallel programs, often in conjunction with tools like the PPL."|&gt;"loop optimization, polyhedral models</data>
  <data key="d7">7</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimization of Parallel Programs" target="GPU Code Optimization">
  <data key="d5">12.0</data>
  <data key="d6">GPU-specific optimization techniques improve kernel performance, often involving specialized compilation strategies and transformations."|&gt;"GPU performance, kernel optimization&lt;SEP&gt;GPU-specific optimization techniques improve performance for GPU kernels, relevant to the broader context of parallel program optimization."|&gt;"GPU performance, kernel optimization</data>
  <data key="d7">6</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="OpenSBLI" target="Heterogeneous computing architectures">
  <data key="d5">14.0</data>
  <data key="d6">OpenSBLI is applied to heterogeneous computing architectures for fluid dynamics simulations, illustrating its use in diverse hardware environments.</data>
  <data key="d7">application, hardware</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Lift" target="Michel Steuwer">
  <data key="d5">16.0</data>
  <data key="d6">Michel Steuwer's research on Lift contributes to high-performance GPU code generation by providing a functional IR for parallelization."|&lt;SEP&gt;Michel Steuwer's research on Lift provides a functional IR that enables high-performance GPU code generation for data-parallel applications."|</data>
  <data key="d7">research, GPU programming</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="YASK" target="Charles Yount">
  <data key="d5">16.0</data>
  <data key="d6">Charles Yount developed or contributed to YASK, a framework for high-performance stencil code generation."|</data>
  <data key="d7">framework, stencil code-generation</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="YASK" target="Josh Tobin">
  <data key="d5">16.0</data>
  <data key="d6">Josh Tobin works on the development and tuning of YASK for scientific HPC applications."|&lt;SEP&gt;Josh Tobin works on the development, tuning, and optimization of YASK for scientific HPC applications."|</data>
  <data key="d7">code-generation, HPC&lt;SEP&gt;code-generation, HPC, optimization</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="YASK" target="Alexander Breuer">
  <data key="d5">16.0</data>
  <data key="d6">Alexander Breuer contributes to the development and optimization of YASK for high-performance stencil computations."|&lt;SEP&gt;Alexander Breuer contributes to the optimization and implementation of YASK for high-performance stencil computations."|</data>
  <data key="d7">optimization, stencil computations</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Raja" target="Libraries">
  <data key="d5">16.0</data>
  <data key="d6">Raja offers performance portability for HPC applications, enabling efficient execution on diverse hardware."|&gt;"performance portability, HPC</data>
  <data key="d7">8</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="SDFGs" target="DaCe">
  <data key="d5">16.0</data>
  <data key="d6">DaCe uses SDFGs as a core representation to implement rule-based optimizations for hotspots in high-performance computations.</data>
  <data key="d7">optimization framework, data flow</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="SDFGs" target="Data Dependency Analysis">
  <data key="d5">18.0</data>
  <data key="d6">SDFGs enable detailed dependency analysis and aliasing elimination during compile time, facilitating optimization of dynamic applications."|&lt;SEP&gt;SDFGs enable detailed dependency analysis and aliasing elimination during compile time."|</data>
  <data key="d7">Theories/Models,Analysis Techniques</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="HPC Expert" target="Optimization Strategy">
  <data key="d5">14.0</data>
  <data key="d6">HPC experts define the rules for optimization strategies applied to computational hotspots, enabling targeted performance improvements.</data>
  <data key="d7">expertise, rule definition</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Parsing" target="APT Generation">
  <data key="d5">16.0</data>
  <data key="d6">Parsing evaluates user input in DSL and hardware description, enabling subsequent generation of the hierarchical APT representation.</data>
  <data key="d7">input evaluation, compiler pipeline</data>
  <data key="d8">chunk-3487322ee335c5f4b3b40717a7de46d1</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Data Dependencies" target="Global Optimization">
  <data key="d5">16.0</data>
  <data key="d6">Global optimization techniques utilize data dependencies to reorder and fuse computations, improving efficiency and reducing synchronization.&lt;SEP&gt;Global optimization techniques utilize data dependencies to reorder, fuse, and optimize computations, reducing synchronization and improving performance.</data>
  <data key="d7">optimization, performance enhancement</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization" target="LP">
  <data key="d5">14.0</data>
  <data key="d6">LP models are used to perform global optimization in the current prototype, addressing issues in the code generator and optimization processes.&lt;SEP&gt;LP models are used within the current prototype to address issues in the code generator and to guide optimization strategies for global problem solving."|&gt;"optimization, modeling</data>
  <data key="d7">7&lt;SEP&gt;optimization, modeling</data>
  <data key="d8">chunk-2379c714e7223550ff7a193355127f04</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization" target="Synchronization Efficiency">
  <data key="d5">16.0</data>
  <data key="d6">Synchronization efficiency measures how well synchronization overhead is minimized during global optimization, enhancing parallelism.&lt;SEP&gt;Synchronization efficiency measures how well synchronization overhead is minimized during global optimization, maximizing parallel execution.</data>
  <data key="d7">performance metric, optimization</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization" target="Inter-Processor Dataflow Efficiency">
  <data key="d5">16.0</data>
  <data key="d6">This efficiency evaluates the minimization of data transfer and execution costs across multiple processors, informing the optimization process.&lt;SEP&gt;This efficiency evaluates the minimization of data transfer and execution time across multiple processing units, guiding resource mapping and optimization.</data>
  <data key="d7">cost minimization, resource utilization&lt;SEP&gt;cost reduction, resource utilization</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization" target="Intra-Processor Dataflow Efficiency">
  <data key="d5">16.0</data>
  <data key="d6">Intra-processor dataflow efficiency aims to maximize data reuse within the same execution unit, reducing cache misses and increasing performance.&lt;SEP&gt;Intra-processor dataflow efficiency focuses on maximizing data reuse within a single processing unit, improving cache performance.</data>
  <data key="d7">data reuse, cache efficiency&lt;SEP&gt;data reuse, performance</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization" target="Memory Locality">
  <data key="d5">18.0</data>
  <data key="d6">Memory locality is a key aspect targeted by global optimization techniques to enhance GPU memory access efficiency.&lt;SEP&gt;Memory locality is a key focus of global optimization to enhance data transfer efficiency and reduce bottlenecks in GPU memory access.</data>
  <data key="d7">optimization</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tasklets" target="AMT">
  <data key="d5">16.0</data>
  <data key="d6">The AMT incorporates tasklet representations, which are mapped onto hardware resources, enabling distributed and concurrent execution.&lt;SEP&gt;The AMT incorporates tasklet representations, which are mapped onto hardware resources, enabling distributed, concurrent execution and resource optimization.</data>
  <data key="d7">resource mapping, parallel execution</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tasklets" target="Mapping and Assignments">
  <data key="d5">16.0</data>
  <data key="d6">Tasklets are assigned to specific hardware units via mapping data, enabling efficient workload distribution and resource utilization.&lt;SEP&gt;Tasklets are assigned to specific hardware units via mapping data, optimizing execution and resource utilization.</data>
  <data key="d7">resource allocation, optimization</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Data Transfer and Synchronization" target="AMT">
  <data key="d5">16.0</data>
  <data key="d6">The AMT includes nodes for data transfer and synchronization, derived from data flow and mapping, facilitating communication in heterogeneous systems.</data>
  <data key="d7">communication, data management</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="GPU Memory Management" target="AMT">
  <data key="d5">16.0</data>
  <data key="d6">Special nodes in the AMT model offloading computations to GPUs and managing GPU memory, supporting heterogeneous execution.&lt;SEP&gt;Special nodes in the AMT represent GPU memory management, enabling offloading and efficient heterogeneous execution.</data>
  <data key="d7">hardware resource, offloading</data>
  <data key="d8">chunk-21fd6197daae9574ed63e4ade119d5f8</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="synchronization" target="data flow">
  <data key="d5">18.0</data>
  <data key="d6">Synchronization ensures proper sequencing and data coherence across different hardware units, facilitating correct parallel execution."|&lt;SEP&gt;Synchronization ensures proper sequencing and data consistency within the data flow across different execution units in heterogeneous systems.</data>
  <data key="d7">9&lt;SEP&gt;system coordination, data consistency</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="data flow" target="Remote write accesses">
  <data key="d5">16.0</data>
  <data key="d6">Remote write accesses do not require synchronization but involve invalidating copies to maintain data coherence across devices.&lt;SEP&gt;Remote write accesses do not require synchronization but involve invalidating copies to maintain data coherence across devices."|</data>
  <data key="d7">8&lt;SEP&gt;data coherence, memory management</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="data flow" target="GPU memory">
  <data key="d5">14.0</data>
  <data key="d6">GPUs hold only the data necessary for subsequent computations due to limited memory capacity, influencing data transfer and storage strategies.&lt;SEP&gt;GPUs hold only the data necessary for subsequent computations due to limited memory capacity, influencing data transfer and storage strategies."|</data>
  <data key="d7">7&lt;SEP&gt;memory optimization, data management</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="data flow" target="Hierarchical synchronization">
  <data key="d5">18.0</data>
  <data key="d6">Synchronization occurs at multiple levels to coordinate data transfers and computations within and across devices in the system.&lt;SEP&gt;Synchronization occurs at multiple levels: intra-device, intra-node, and inter-device, coordinating data and tasks across the system."|</data>
  <data key="d7">9&lt;SEP&gt;system coordination, data integrity</data>
  <data key="d8">chunk-b943bff32a36cd0ce9ab43893da92b5b</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Synchronization" target="CUDA Calls">
  <data key="d5">16.0</data>
  <data key="d6">CUDA calls are wrapped and used to synchronize data movement and kernel execution, ensuring correct operation order.&lt;SEP&gt;CUDA calls like cudaMemcpy and stream synchronization primitives are used to coordinate data transfer and kernel execution order, ensuring correct program behavior.</data>
  <data key="d7">execution control</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Synchronization" target="CUDA Streams">
  <data key="d5">16.0</data>
  <data key="d6">CUDA streams enable asynchronous execution of kernels and data transfers, allowing overlapping operations and synchronization for performance gains.&lt;SEP&gt;CUDA streams facilitate asynchronous execution and synchronization of GPU tasks, enabling overlapping data transfers and kernel runs.</data>
  <data key="d7">performance</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Pattern Implementation" target="Execution Range">
  <data key="d5">14.0</data>
  <data key="d6">Pattern implementation involves defining execution ranges to allocate work among threads efficiently.&lt;SEP&gt;Pattern implementation involves defining execution ranges to allocate work evenly among threads, balancing load and maximizing parallel efficiency.</data>
  <data key="d7">work distribution</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Pattern Implementation" target="Loop Unrolling">
  <data key="d5">14.0</data>
  <data key="d6">Loop unrolling enhances performance by reducing loop control overhead and increasing instruction-level parallelism within GPU kernels.&lt;SEP&gt;Loop unrolling is used within pattern implementation to reduce loop overhead and improve parallel execution efficiency.</data>
  <data key="d7">performance enhancement</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Inlining" target="Hierarchical Structure">
  <data key="d5">18.0</data>
  <data key="d6">Inlining flattens hierarchical code structures like nested functions or loops, enabling better optimization and parallelization.&lt;SEP&gt;Inlining flattens nested functions and loops, transforming hierarchical code into a form suitable for parallel optimization and code generation.</data>
  <data key="d7">code optimization</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Inlining" target="Scope Overlaps">
  <data key="d5">12.0</data>
  <data key="d6">Inlining must address scope overlaps by renaming variables to prevent conflicts and maintain correctness during code flattening.&lt;SEP&gt;Inlining must address scope overlaps by renaming variables to prevent conflicts during code transformation.</data>
  <data key="d7">code correctness</data>
  <data key="d8">chunk-e376cabd5c2bdbbf8ecdd095bfa831b5</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Function Arguments" target="Deep Copy">
  <data key="d5">14.0</data>
  <data key="d6">Function arguments are deep copied to prevent side effects and data races during parallel execution.</data>
  <data key="d7">data isolation, parallel safety</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Nested Parallel Patterns" target="Write-After-Write Dependencies">
  <data key="d5">16.0</data>
  <data key="d6">Nested parallel patterns can introduce write-after-write dependencies that must be managed to ensure correct parallel execution.</data>
  <data key="d7">dependency management, parallel correctness</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Jump Label" target="Function Call">
  <data key="d5">16.0</data>
  <data key="d6">Jump labels are associated with function calls to manage control flow, especially when inlining functions or handling returns.</data>
  <data key="d7">control flow management, inlining</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Hashing" target="Variable Replacement">
  <data key="d5">14.0</data>
  <data key="d6">Hashing is used to generate unique identifiers for variables, facilitating their replacement during code transformation processes.</data>
  <data key="d7">variable management, code transformation</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Deep Copy" target="Variable Replacement">
  <data key="d5">12.0</data>
  <data key="d6">Deep copying supports variable replacement by creating independent copies of data structures to avoid conflicts in parallel code.</data>
  <data key="d7">data integrity, parallel safety</data>
  <data key="d8">chunk-5f6ee49c82aaaf22c167cac791d43a0f</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="5.1 Environment" target="RWTH Aachen University">
  <data key="d5">14.0</data>
  <data key="d6">The environment is set up and maintained at RWTH Aachen University, where the systems are located."|&lt;SEP&gt;The experimental environment is located at RWTH Aachen University, where the hardware and software are configured for testing."|</data>
  <data key="d7">Location, Infrastructure</data>
  <data key="d8">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Gurobi" target="Random seed">
  <data key="d5">14.0</data>
  <data key="d6">Multiple seeds are used to mitigate the instability caused by Gurobi's stochastic initial solution process."|&lt;SEP&gt;Multiple seeds are used within Gurobi to mitigate solution instability caused by its stochastic initial solution process."|</data>
  <data key="d7">Tools, Variables</data>
  <data key="d8">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code sizes/configurations" target="Benchmarks">
  <data key="d5">12.0</data>
  <data key="d6">Different benchmark configurations and dataset sizes are used to evaluate the performance and scalability of generated code."|&lt;SEP&gt;Different benchmark configurations are used to evaluate the performance of the generated code."|</data>
  <data key="d7">Study Design, Variables</data>
  <data key="d8">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="PPLin" target="Benchmarks">
  <data key="d5">18.0</data>
  <data key="d6">PPLin is applied to port, adapt, and optimize the benchmarks within the Rodinia suite."|&gt;"application of methodology, benchmark adaptation</data>
  <data key="d7">9</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Benchmarks" target="Multi-node Parallelism">
  <data key="d5">16.0</data>
  <data key="d6">Some benchmarks utilize multiple nodes to distribute workloads, increasing performance and scalability."|&gt;"application of parallel computing&lt;SEP&gt;Some benchmarks utilize multiple nodes to distribute workloads, increasing performance."|&gt;"application of parallel computing</data>
  <data key="d7">8</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance Analysis" target="Speedup Visualization">
  <data key="d5">6.0</data>
  <data key="d6">Graphical representations help illustrate performance improvements across benchmarks."|&gt;"visualization of results</data>
  <data key="d7">6</data>
  <data key="d8">chunk-71ae4149c46ebd3eaae68a58bc6a57e2</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Scheduling" target="Filtering beam search">
  <data key="d5">16.0</data>
  <data key="d6">Filtered beam search is used to improve scheduling efficiency by exploring promising options while pruning less optimal ones.</data>
  <data key="d7">methodology, application</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="C/C++ Support" target="PPL">
  <data key="d5">12.0</data>
  <data key="d6">Introducing C/C++ support in PPL would facilitate porting existing applications and improve reusability."|</data>
  <data key="d7">Implementation,Ease of Porting</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Data Dependency Analysis" target="Dynamic Workloads">
  <data key="d5">16.0</data>
  <data key="d6">Analyzing data dependencies can help identify dynamic workloads at compile time for better optimization."|</data>
  <data key="d7">Methodology,Analysis Techniques</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Dependency Chain" target="Runtime Arguments">
  <data key="d5">14.0</data>
  <data key="d6">Extracted dependency chains can identify variables that influence control flow and parallel patterns at runtime."|</data>
  <data key="d7">Variables,Analysis Techniques</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Legion" target="Dynamic Applications">
  <data key="d5">16.0</data>
  <data key="d6">Legion supports executing patterns of unknown size, aiding dynamic load balancing."|</data>
  <data key="d7">Tools,Application Support</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="StarPU" target="Dynamic Applications">
  <data key="d5">16.0</data>
  <data key="d6">StarPU enables migration of tasklets during execution, supporting dynamic load balancing in distributed memory systems."|</data>
  <data key="d7">Tools,Application Support</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Random Seed" target="Scheduling Problem">
  <data key="d5">12.0</data>
  <data key="d6">Variations in random seed influence variable naming and problem duration, affecting reproducibility."|</data>
  <data key="d7">Variables,Impact</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Beam Search" target="Scheduling Problem">
  <data key="d5">18.0</data>
  <data key="d6">Beam search provides pruning and stability for scheduling problems, improving predictability."|</data>
  <data key="d7">Methodology,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Pruning Strategies" target="Scheduling Problem">
  <data key="d5">16.0</data>
  <data key="d6">Pruning strategies like n-best limit search space, aiding in scalable scheduling."|</data>
  <data key="d7">Methodology,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Scheduling Problem" target="LP-Solvers">
  <data key="d5">14.0</data>
  <data key="d6">Replacing LP-solvers with beam search can lead to more stable and predictable scheduling outcomes."|</data>
  <data key="d7">Tools,Optimization</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance Portability on Heterogeneous Architectures" target="&lt;&gt;Proceedings of the ACM Digital Library">
  <data key="d5">16.0</data>
  <data key="d6">The document reports on research related to performance portability across various architectures, including methodologies and tools for achieving this goal.&lt;SEP&gt;The document reports research efforts focused on achieving and evaluating performance portability across diverse hardware architectures using various methodologies and tools.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="DaCe-Data Centric Parallel Programming" target="&lt;&gt;Tal Ben-Nun et al.">
  <data key="d5">18.0</data>
  <data key="d6">Provides a data-centric model for parallel programming that aims to improve performance portability and efficiency.&lt;SEP&gt;Provides a data-centric programming model designed to improve performance portability and efficiency in heterogeneous systems.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Filtered Beam Search Method" target="&lt;&gt;Ernesto G Birgin et al.">
  <data key="d5">14.0</data>
  <data key="d6">A scheduling method designed to optimize permutation flowshop problems, relevant for scheduling tasks in high-performance computing.&lt;SEP&gt;A scheduling methodology aimed at optimizing permutation flowshop problems, relevant for task scheduling in high-performance computing environments.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Power Wall" target="&lt;&gt;Pradip Bose">
  <data key="d5">12.0</data>
  <data key="d6">Describes a fundamental hardware limitation that influences the design and performance potential of computing architectures.&lt;SEP&gt;Describes the power consumption limitations that influence hardware performance and architectural design.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Münster Skeleton Library Muesli" target="&lt;&gt;Philipp Ciechanowicz et al.">
  <data key="d5">14.0</data>
  <data key="d6">Provides tools for skeletal programming, aiding in structured parallel computation management.&lt;SEP&gt;Provides tools to facilitate structured parallel programming via skeletal abstractions, aiding in performance optimization.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Algorithmic Skeletons" target="&lt;&gt;Murray Cole">
  <data key="d5">16.0</data>
  <data key="d6">A conceptual framework for managing parallel algorithms through structured abstractions, facilitating performance optimization.&lt;SEP&gt;A high-level conceptual framework for managing parallel algorithms, promoting structured and efficient parallel programming.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="CUDA Toolkit and Libraries" target="&lt;&gt;Massimiliano Fatica">
  <data key="d5">18.0</data>
  <data key="d6">Tools essential for developing, optimizing, and deploying GPU-accelerated high-performance applications.&lt;SEP&gt;Tools for developing and optimizing GPU-accelerated applications, integral for performance portability.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="GCC 13.2 Manual" target="&lt;&gt;Free Software Foundation">
  <data key="d5">14.0</data>
  <data key="d6">Documentation guiding the compilation and optimization process for performance-critical code.&lt;SEP&gt;Documentation providing guidance on compiling and optimizing code for performance and portability.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Top500 List" target="&lt;&gt;Prometeus GmbH">
  <data key="d5">16.0</data>
  <data key="d6">Provides rankings of supercomputers, useful for benchmarking and evaluating high-performance systems.&lt;SEP&gt;Provides rankings of the most powerful supercomputers, serving as benchmarks for evaluating HPC performance.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Locality-Aware Scheduling" target="&lt;&gt;Maxime Gonthier et al.">
  <data key="d5">18.0</data>
  <data key="d6">A scheduling methodology that improves task placement efficiency by considering data locality, boosting performance on modern architectures.&lt;SEP&gt;An approach to improve task scheduling efficiency by considering data locality, enhancing performance on modern architectures.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Polly" target="&lt;&gt;Tobias Grosser et al.">
  <data key="d5">16.0</data>
  <data key="d6">Performs polyhedral optimizations to enhance loop execution efficiency, contributing to performance improvements in parallel code.&lt;SEP&gt;Performs polyhedral optimizations to improve loop execution efficiency in parallel programs.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Gurobi Optimizer" target="&lt;&gt;Gurobi Optimization, LLC">
  <data key="d5">16.0</data>
  <data key="d6">A solver used for large-scale optimization problems, supporting high-performance decision-making tasks.&lt;SEP&gt;An optimization solver used to solve large-scale linear, quadratic, and integer programming problems efficiently, supporting performance tuning.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="OpenSHMEM Using MPI-3" target="&lt;&gt;Jeff R. Hammond et al.">
  <data key="d5">14.0</data>
  <data key="d6">A methodology for implementing shared memory programming models using MPI-3 features, facilitating scalable parallelism.&lt;SEP&gt;A methodology that leverages MPI-3's one-sided communication to implement scalable shared memory programming models.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Optimizing Parallel Reduction in CUDA" target="&lt;&gt;Mark Harris et al.">
  <data key="d5">16.0</data>
  <data key="d6">Techniques for enhancing reduction operations in CUDA, improving parallel computation efficiency.&lt;SEP&gt;Techniques to improve the efficiency of reduction operations in CUDA, which are critical for many parallel algorithms.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Scalable Communication Protocols" target="&lt;&gt;Torsten Hoefler et al.">
  <data key="d5">16.0</data>
  <data key="d6">Protocols designed for efficient data exchange in distributed systems with dynamic sparse data, enabling scalable high-performance computing.&lt;SEP&gt;Protocols designed to facilitate efficient data exchange in distributed systems with dynamic sparse data, critical for scalable HPC.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="oneAPI Threading Building Blocks" target="&lt;&gt;Intel Corporation">
  <data key="d5">14.0</data>
  <data key="d6">A library for portable multithreading and task parallelism, supporting performance portability.&lt;SEP&gt;A portable C++ library for multithreading and task parallelism, supporting performance portability across architectures.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="LULESH 2.0" target="&lt;&gt;Karlin et al.">
  <data key="d5">16.0</data>
  <data key="d6">A hydrodynamics simulation code used as a benchmark to evaluate system performance, scalability, and portability in HPC systems.&lt;SEP&gt;A hydrodynamics simulation code used to evaluate system performance and scalability in high-performance computing.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Automatic Code Generation for Discontinuous Galerkin Methods" target="&lt;&gt;Dominic Kempf et al.">
  <data key="d5">18.0</data>
  <data key="d6">Techniques for automating code creation to optimize numerical methods on modern architectures.&lt;SEP&gt;Techniques for automating high-performance code generation for numerical methods, facilitating portability and efficiency.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="RAJA Performance Portability Layer" target="&lt;&gt;Lawrence Livermore National Laboratory">
  <data key="d5">18.0</data>
  <data key="d6">A software layer facilitating performance-portable programming across diverse hardware architectures.&lt;SEP&gt;A software layer that abstracts hardware details to support performance-portable code development across diverse architectures.</data>
  <data key="d7">9</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Large-Scale Stencil Computation" target="&lt;&gt;Various datasets">
  <data key="d5">14.0</data>
  <data key="d6">A common computational pattern in scientific simulations, used to evaluate performance and optimization strategies on many-core processors.&lt;SEP&gt;A computational pattern used to evaluate performance of numerical algorithms on many-core processors, impacting scientific simulations.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-5cd0b27c19a2314eb0b2f819a6edfa2e</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Mingzhen Li, Yi Liu, Hailong Yang, Yongmin Hu, Qingxiao Sun, Bangduo Chen, Xin You, Xiaoyan Liu, Zhongzhi Luan, Depei Qian" target="Proceedings of the 50th International Conference on Parallel Processing">
  <data key="d5">12.0</data>
  <data key="d6">The authors published their research in this conference proceedings, indicating peer-reviewed dissemination of their work.</data>
  <data key="d7">publication, dissemination</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Large-scale stencil computation" target="Many-core processors">
  <data key="d5">16.0</data>
  <data key="d6">Large-scale stencil computations are optimized for execution on many-core processors to leverage parallelism.</data>
  <data key="d7">application, hardware</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Code optimization" target="Parallel Processing">
  <data key="d5">14.0</data>
  <data key="d6">Code optimization techniques are applied within the context of parallel processing to improve computational performance.</data>
  <data key="d7">methodology, context</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Performance Variability in Mixed-Integer Programming" target="Memory Wall">
  <data key="d5">18.0</data>
  <data key="d6">Memory wall is a key factor influencing performance variability in computational optimization problems.</data>
  <data key="d7">core concept, impact</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Korali" target="Bayesian Uncertainty Quantification">
  <data key="d5">16.0</data>
  <data key="d6">Korali provides a framework for Bayesian uncertainty quantification, enabling probabilistic analysis in high-performance computing tasks.</data>
  <data key="d7">application, methodology</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Efficiency of Algorithmic Structures" target="Parallel Algorithms">
  <data key="d5">14.0</data>
  <data key="d6">The study evaluates the efficiency of certain algorithmic structures in optimizing parallel algorithms.</data>
  <data key="d7">methodology, focus</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="C2Rust Manual" target="C2Rust Development Team">
  <data key="d5">12.0</data>
  <data key="d6">The C2Rust Manual offers guidance on translating C code into Rust, supporting software safety and modernization efforts."|&lt;SEP&gt;The C2Rust Manual provides guidance on translating C code to Rust, aiding in software safety and modernization."|</data>
  <data key="d7">software tools, code translation</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Damien Lebrun-Grandié" target="Kokkos 3">
  <data key="d5">14.0</data>
  <data key="d6">Damien Lebrun-Grandié contributes to the development of Kokkos 3 as a programming model extension for exascale high-performance computing."|&lt;SEP&gt;Damien Lebrun-Grandié contributes to the development of Kokkos 3 for high-performance, scalable computing."|</data>
  <data key="d7">programming model extensions, exascale</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Daniel Arndt" target="Kokkos 3">
  <data key="d5">14.0</data>
  <data key="d6">Daniel Arndt is involved in extending programming models like Kokkos for exascale architectures."|&lt;SEP&gt;Daniel Arndt works on extending programming models like Kokkos for exascale architectures."|</data>
  <data key="d7">programming models, exascale</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Lukas Trümper" target="Automatic Mapping of Parallel Pattern-Based Algorithms">
  <data key="d5">18.0</data>
  <data key="d6">Lukas Trümper's research focuses on automating the mapping of parallel algorithms onto heterogeneous architectures to improve efficiency."|&lt;SEP&gt;Lukas Trümper's research focuses on automating the mapping of parallel pattern-based algorithms onto heterogeneous architectures to improve efficiency and flexibility."|</data>
  <data key="d7">algorithm mapping, heterogenous architectures</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Julian Miller" target="Automatic Mapping of Parallel Pattern-Based Algorithms">
  <data key="d5">16.0</data>
  <data key="d6">Julian Miller collaborates on automating parallel algorithm deployment across diverse hardware architectures."|&lt;SEP&gt;Julian Miller collaborates on automating parallel algorithm deployment on diverse hardware."|</data>
  <data key="d7">automation, parallel algorithms</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Christian Terboven" target="Automatic Mapping of Parallel Pattern-Based Algorithms">
  <data key="d5">14.0</data>
  <data key="d6">Christian Terboven contributes to frameworks that facilitate mapping parallel patterns onto various architectures."|&lt;SEP&gt;Christian Terboven works on frameworks that facilitate the automatic mapping of parallel patterns onto various hardware architectures."|</data>
  <data key="d7">frameworks, parallel algorithms</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Matthias S. Müller" target="Automatic Mapping of Parallel Pattern-Based Algorithms">
  <data key="d5">14.0</data>
  <data key="d6">Matthias S. Müller focuses on optimizing parallel algorithms for high-performance and heterogeneous computing environments."|&lt;SEP&gt;Matthias S. Müller works on optimizing parallel algorithms for high-performance computing environments."|</data>
  <data key="d7">optimization, parallel algorithms</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Tyler Whitney" target="Parallel Patterns Library (PPL)">
  <data key="d5">16.0</data>
  <data key="d6">Tyler Whitney developed or contributed to the Parallel Patterns Library (PPL) to support parallel programming in C++."|&lt;SEP&gt;Tyler Whitney developed or contributed to the Parallel Patterns Library (PPL), which provides abstractions and tools for parallel programming in C++."|</data>
  <data key="d7">library, parallel programming</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Kent Sharkey" target="Parallel Patterns Library (PPL)">
  <data key="d5">14.0</data>
  <data key="d6">Kent Sharkey is an author involved in the development or research of PPL for parallel algorithm implementation."|&lt;SEP&gt;Kent Sharkey is associated with the development or research of PPL for parallel algorithm implementation."|</data>
  <data key="d7">library, parallel algorithms</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Next Turn" target="Colin Robertson">
  <data key="d5">12.0</data>
  <data key="d6">Colin Robertson is involved with Next Turn, contributing to its development or application in parallel computing workflows."|&lt;SEP&gt;Colin Robertson is involved with Next Turn, contributing to parallel computing solutions."|</data>
  <data key="d7">platform, parallel computing&lt;SEP&gt;platforms, parallel computing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Next Turn" target="Mike Jones">
  <data key="d5">12.0</data>
  <data key="d6">Mike Jones works on tools or frameworks related to Next Turn for parallel processing."|&lt;SEP&gt;Mike Jones works on tools or frameworks within Next Turn for parallel algorithm support."|</data>
  <data key="d7">tools, parallel algorithms&lt;SEP&gt;tools, parallel processing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Next Turn" target="Mike Blome">
  <data key="d5">12.0</data>
  <data key="d6">Mike Blome contributes to the development of Next Turn's parallel computing tools."|&lt;SEP&gt;Mike Blome contributes to the development of Next Turn's tools or frameworks for parallel computing."|</data>
  <data key="d7">tools, parallel computing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Next Turn" target="Gordon Hogenson">
  <data key="d5">12.0</data>
  <data key="d6">Gordon Hogenson is involved in research or development activities related to Next Turn's platform or tools."|&lt;SEP&gt;Gordon Hogenson is involved in research or development related to Next Turn's platform."|</data>
  <data key="d7">platform, tools&lt;SEP&gt;platforms, parallel computing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Next Turn" target="Saisang Cai">
  <data key="d5">12.0</data>
  <data key="d6">Saisang Cai contributes to parallel computing solutions within Next Turn's ecosystem."|&lt;SEP&gt;Saisang Cai contributes to parallel computing solutions within Next Turn."|</data>
  <data key="d7">tools, parallel processing</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Samuel Williams" target="Roofline">
  <data key="d5">18.0</data>
  <data key="d6">Samuel Williams contributed to developing the Roofline performance model for analyzing multicore architectures."|&lt;SEP&gt;Samuel Williams contributed to the development of the Roofline performance model, which is used for visual performance analysis of multicore architectures."|</data>
  <data key="d7">performance modeling, multicore architectures</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Andrew Waterman" target="Roofline">
  <data key="d5">16.0</data>
  <data key="d6">Andrew Waterman applies the Roofline model to analyze and optimize multicore system performance."|&lt;SEP&gt;Andrew Waterman utilized the Roofline model to analyze and optimize multicore system performance."|</data>
  <data key="d7">performance analysis, multicore systems</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="David Patterson" target="Roofline">
  <data key="d5">16.0</data>
  <data key="d6">David Patterson is associated with the Roofline model's development or application in performance optimization."|&lt;SEP&gt;David Patterson is associated with the development and application of the Roofline performance model for performance optimization."|</data>
  <data key="d7">performance modeling, multicore architectures</data>
  <data key="d8">chunk-28e67ca955e33292ea0d23b1e10bb450</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Global Optimization via Linear Programming" target="High-Level IR">
  <data key="d5">8.0</data>
  <data key="d6">The high-level IR facilitates global optimization processes, including LP-based workload assignment."|</data>
  <data key="d7">8</data>
  <data key="d8">chunk-e6eb755f9f73ae3f7c76aff0920fdfae</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Measurements" target="Systems">
  <data key="d5">16.0</data>
  <data key="d6">All measurements are performed on the CLAIX18 systems, which are equipped with specified hardware components."|</data>
  <data key="d7">Objects of Study, Experimental Setup</data>
  <data key="d8">chunk-f51d6561a383e764adb74f234469e637</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Applications" target="Dynamic Workloads">
  <data key="d5">18.0</data>
  <data key="d6">Applications with dynamic workloads like b+tree and BFS require runtime adaptation beyond static approaches."|</data>
  <data key="d7">Study Design,Application Behavior</data>
  <data key="d8">chunk-0223d4c2805a1c339a5639baaa802cfc</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="C to Polyhedral MLIR" target="Polygeist">
  <data key="d5">16.0</data>
  <data key="d6">Polygeist transforms C code into polyhedral MLIR for optimization, bridging programming languages and compiler frameworks.</data>
  <data key="d7">tool, purpose</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="GPU graph algorithms compiler" target="Throughput optimization">
  <data key="d5">14.0</data>
  <data key="d6">The compiler optimizes throughput of graph algorithms on GPUs, enhancing performance in data-parallel tasks.</data>
  <data key="d7">tool, application</data>
  <data key="d8">chunk-2823358e9d763974915b4e0c1e7dc3ca</data>
  <data key="d9">Schmitz et al. - 2024 - Parallel Pattern Language Code Generation.pdf</data>
</edge>
<edge source="Language Models (LMs)" target="Weak Supervision">
  <data key="d5">6.0</data>
  <data key="d6">Weak supervision techniques are now implemented directly within language models, reducing reliance on task-specific heuristics and hand-built rules.</data>
  <data key="d7">supervision, model training</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Prompt Templates" target="DSPy Modules">
  <data key="d5">8.0</data>
  <data key="d6">DSP Y replaces static prompt templates with modular, parameterized components that can learn, adapt, and optimize prompts.</data>
  <data key="d7">prompt automation, modularity</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Prompt Templates" target="Prompt Length">
  <data key="d5">18.0</data>
  <data key="d6">Large prompt templates in existing libraries like LangChain contain thousands of characters, complicating manual prompt engineering and motivating automated approaches.</data>
  <data key="d7">prompt size challenge</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Mathematical Word Problems" target="DSPy Program">
  <data key="d5">10.0</data>
  <data key="d6">DSP Y pipelines are capable of reasoning through and solving math word problems, demonstrating complex reasoning abilities.</data>
  <data key="d7">task application, reasoning</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Multi-hop Retrieval" target="DSPy Program">
  <data key="d5">9.0</data>
  <data key="d6">DSP Y pipelines perform multi-step retrieval tasks requiring complex reasoning and information integration.</data>
  <data key="d7">information retrieval, reasoning</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Complex Question Answering" target="DSPy Program">
  <data key="d5">9.0</data>
  <data key="d6">DSP Y enables models to answer complex questions through multi-stage reasoning pipelines.</data>
  <data key="d7">question answering, reasoning</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Llama2-13b-Chat" target="DSP Y Program">
  <data key="d5">8.0</data>
  <data key="d6">DSP Y pipelines tailored for Llama2-13b-chat outperform baseline prompt chains, demonstrating competitive performance.</data>
  <data key="d7">performance, resource efficiency</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy Program" target="Expert-Created Demonstrations">
  <data key="d5">7.0</data>
  <data key="d6">DSP Y can achieve performance comparable to manually crafted prompt chains, providing an automated alternative.</data>
  <data key="d7">benchmark comparison, prompt design</data>
  <data key="d8">chunk-5ff9371dcd9f0dcbeb25583bbba6123c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Chain of Thought">
  <data key="d5">8.0</data>
  <data key="d6">DSPy incorporates Chain of Thought prompting techniques as modules to facilitate reasoning in NLP pipelines.</data>
  <data key="d7">prompting techniques, modular design</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Teleprompters">
  <data key="d5">17.0</data>
  <data key="d6">DSPy uses teleprompters as optimization strategies to improve pipeline performance through automated learning.&lt;SEP&gt;Teleprompters in DSPy optimize pipeline modules through techniques like model selection and reinforcement learning to maximize performance.</data>
  <data key="d7">optimization strategies, system improvement&lt;SEP&gt;optimization, performance</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c&lt;SEP&gt;chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Math word problems">
  <data key="d5">7.0</data>
  <data key="d6">DSPy is applied to solve math word problems, demonstrating its effectiveness in structured NLP tasks.</data>
  <data key="d7">application, NLP tasks</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Multi-hop question answering">
  <data key="d5">6.0</data>
  <data key="d6">DSPy is evaluated on multi-hop QA tasks to assess its capability in complex reasoning.</data>
  <data key="d7">application, reasoning</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="LLaMA2-13b-chat">
  <data key="d5">7.0</data>
  <data key="d6">DSPy</data>
  <data key="d7">DSPy modules are designed to improve performance of large language models like LLaMA2-13b-chat.</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="T5-Large">
  <data key="d5">6.0</data>
  <data key="d6">DSPy</data>
  <data key="d7">DSPy modules are used with T5-Large to demonstrate system efficiency and effectiveness.</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Self-improving NLP systems">
  <data key="d5">8.0</data>
  <data key="d6">DSPy</data>
  <data key="d7">The framework enables the development of NLP systems that bootstrap self-improvement through modular prompts and optimization.</data>
  <data key="d8">chunk-061790de1df7d16a1cc9252b2f75290c</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Signatures">
  <data key="d5">9.0</data>
  <data key="d6">DSPy uses natural language signatures to abstract prompting and finetuning, enabling modular and automated pipeline construction.</data>
  <data key="d7">abstraction, automation</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Modules">
  <data key="d5">9.0</data>
  <data key="d6">Modules in DSPy represent stages in a pipeline that are optimized collectively, replacing hand-crafted prompts with high-level specifications.</data>
  <data key="d7">modular design, optimization</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Model Selection Techniques">
  <data key="d5">7.0</data>
  <data key="d6">DSPy employs model selection methods such as cross-validation, RL, and Bayesian optimization to improve pipeline components.</data>
  <data key="d7">optimization methods, model tuning</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="Empirical Findings">
  <data key="d5">8.0</data>
  <data key="d6">The paper reports empirical results demonstrating DSPy's ability to build high-quality LM systems without manual prompt engineering.</data>
  <data key="d7">evaluation, performance</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="modules">
  <data key="d5">20.0</data>
  <data key="d6">DSPy provides a framework to define, compile, and optimize modules for language models, replacing traditional prompts with reusable components.</data>
  <data key="d7">tool for modular programming, optimization</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="hand-written prompts">
  <data key="d5">18.0</data>
  <data key="d6">DSPy aims to replace hand-crafted prompts with modular components, reducing reliance on artful prompt construction.</data>
  <data key="d7">prompt engineering, modularization</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="program compilation">
  <data key="d5">16.0</data>
  <data key="d6">Compilation transforms DSPy programs into optimized forms, enhancing model performance and accuracy.</data>
  <data key="d7">program optimization, performance improvement</data>
  <data key="d8">chunk-0696a875ae026175ba36c9e8529f7efd</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy" target="ReAct">
  <data key="d5">16.0</data>
  <data key="d6">ReAct is implemented within DSPy as a multi-step tool for question answering, involving iterative reasoning and tool use."|&gt;"tool use, multi-step reasoning</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="ReAct" target="Yao et al. (2022)">
  <data key="d5">27.0</data>
  <data key="d6">Yao et al. (2022) establishes or evaluates the ReAct framework as a model for reasoning and acting in AI systems.&lt;SEP&gt;Yao et al. (2022) introduces or evaluates the ReAct framework as a theoretical model for reasoning and acting in AI systems.&lt;SEP&gt;Yao et al. (2022) introduces or evaluates the ReAct framework, establishing a theoretical model for reasoning and acting in AI.</data>
  <data key="d7">Theories/Models&lt;SEP&gt;Theories/Models, Study Designs</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="T5-Large" target="multihop program">
  <data key="d5">16.0</data>
  <data key="d6">T5-Large is used for finetuning the multihop program, aiming to improve answer accuracy through transfer learning."|&gt;"finetuning, model performance</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Radford et al. 2018; Brown et al. 2020" target="Instruction Tuning">
  <data key="d5">9.0</data>
  <data key="d6">Radford et al. 2018; Brown et al. 2020</data>
  <data key="d7">Instruction tuning is a central mechanism for training foundation models to follow instructions and perform tasks based on prompts.&lt;SEP&gt;Instruction tuning is a key mechanism for training foundation models, enabling them to perform complex tasks via instruction-based prompts.</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Instruction Tuning" target="Prompting">
  <data key="d5">7.0</data>
  <data key="d6">Instruction tuning enhances prompting techniques by providing models with structured instructions, improving their ability to generate desired outputs.</data>
  <data key="d7">training, prompting</data>
  <data key="d8">chunk-26c6a2c314e1450a2c8d666abac03b85</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy Signatures" target="Predict Module">
  <data key="d5">16.0</data>
  <data key="d6">DSPy Signatures serve as the blueprint for the Predict module, guiding prompt formatting and output parsing.</data>
  <data key="d7">design, modularity</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predict Module" target="ChainOfThought">
  <data key="d5">14.0</data>
  <data key="d6">ChainOfThought enhances the Predict module by adding reasoning steps before final outputs, improving model reasoning.</data>
  <data key="d7">reasoning, step-by-step thinking</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predict Module" target="Structured Prompting">
  <data key="d5">12.0</data>
  <data key="d6">Structured prompting frameworks like DSPy improve the predict module's ability to generate consistent outputs across tasks.</data>
  <data key="d7">prompt design, consistency</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predict Module" target="Parameterization">
  <data key="d5">10.0</data>
  <data key="d6">Parameterization allows customization of the Predict module's prompts for specific tasks and models.</data>
  <data key="d7">customization, flexibility</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predict Module" target="Structured Formatting and Parsing">
  <data key="d5">6.0</data>
  <data key="d6">DSPy manages structured output formatting and parsing within the Predict module to ensure reliable data extraction.</data>
  <data key="d7">output management, reliability</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Self-Improving Prompts" target="Bootstrapping">
  <data key="d5">8.0</data>
  <data key="d6">Self-improving prompts are developed through bootstrapping techniques that refine prompts iteratively.</data>
  <data key="d7">learning, adaptation</data>
  <data key="d8">chunk-484caa2ba6025add81052bdcfed482a4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="ensemble" target="DSPy programs">
  <data key="d5">20.0</data>
  <data key="d6">Ensembling combines multiple candidate programs to enhance accuracy and robustness through majority voting.</data>
  <data key="d7">robustness, voting mechanism</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy programs" target="compile">
  <data key="d5">18.0</data>
  <data key="d6">DSPy programs are compiled into optimized forms using various strategies, including bootstrap and ensembling.</data>
  <data key="d7">program optimization, compilation process</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy programs" target="zero-shot">
  <data key="d5">16.0</data>
  <data key="d6">Zero-shot evaluation assesses program performance without prior demonstrations, highlighting the importance of compilation strategies.</data>
  <data key="d7">evaluation methodology, program performance</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="DSPy programs" target="bootstrap">
  <data key="d5">20.0</data>
  <data key="d6">Bootstrap techniques generate and refine demonstration chains to self-improve program accuracy.</data>
  <data key="d7">self-improvement, demonstration generation</data>
  <data key="d8">chunk-8148b67fb57a98685f653eb2824cfd6f</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="llama2-34b" target="program with the 13b variant">
  <data key="d5">14.0</data>
  <data key="d6">The 13b program performs competitively with larger models despite not employing human reasoning chains, indicating efficiency and effectiveness.&lt;SEP&gt;The 13b program performs competitively with larger models despite not using human reasoning chains, indicating efficiency and effectiveness.</data>
  <data key="d7">model performance, reasoning techniques</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhao et al. (2023b)" target="GPT-3.5-turbo">
  <data key="d5">16.0</data>
  <data key="d6">Zhao et al. (2023b) reports high accuracy (80.8%) for GPT-3.5-turbo using Chain-of-Thought prompting, establishing a benchmark for reasoning performance.</data>
  <data key="d7">benchmark, reasoning accuracy</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="HotPotQA" target="search index of Wikipedia 2017 abstracts">
  <data key="d5">16.0</data>
  <data key="d6">The search index is used to retrieve relevant passages for multi-hop reasoning in HotPotQA, facilitating information flow."|&gt;"retrieval, reasoning</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="BasicMultiHop" target="question answering">
  <data key="d5">16.0</data>
  <data key="d6">The BasicMultiHop program simulates multi-hop reasoning by retrieving passages iteratively and generating answers, aiming to improve performance on complex questions."|&gt;"multi-hop reasoning, information flow</data>
  <data key="d7">8</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="BootstrapFewShotWithRandomSearch" target="multihop program">
  <data key="d5">14.0</data>
  <data key="d6">This bootstrapping technique enhances the multihop program's performance by combining few-shot prompting with random search strategies."|&gt;"training, performance enhancement</data>
  <data key="d7">7</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="answer EM" target="multihop question answering">
  <data key="d5">18.0</data>
  <data key="d6">Answer EM scores are used to evaluate the effectiveness of the multihop programs, with higher scores indicating better accuracy."|&gt;"evaluation, accuracy</data>
  <data key="d7">9</data>
  <data key="d8">chunk-9596cd861216e68aa0fdb35edbc9a132</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Optuna" target="Best Program">
  <data key="d5">16.0</data>
  <data key="d6">Optuna searches and identifies the best program configuration based on the evaluation scores.&lt;SEP&gt;Optuna searches for and identifies the best program configuration based on the objective function.</data>
  <data key="d7">hyperparameter optimization, model selection&lt;SEP&gt;hyperparameter optimization, model tuning</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Robust Multi-Hop Reasoning at Scale" target="Condensed Retrieval">
  <data key="d5">18.0</data>
  <data key="d6">The concept of scalable multi-hop reasoning is supported by the use of condensed retrieval techniques to handle large-scale data efficiently.</data>
  <data key="d7">technique, scalability</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Robust Multi-Hop Reasoning at Scale" target="Relevance-guided Supervision for OpenQA with ColBERT">
  <data key="d5">14.0</data>
  <data key="d6">Supervision techniques like relevance-guided supervision enhance the ability of models to perform multi-hop reasoning in open question answering.</data>
  <data key="d7">supervision, reasoning</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Demonstrate-search-predict" target="Retrieval-augmented Generation for Knowledge-Intensive NLP Tasks">
  <data key="d5">16.0</data>
  <data key="d6">The demonstrate-search-predict approach is an example of retrieval-augmented generation methods designed to improve knowledge-intensive NLP tasks.</data>
  <data key="d7">methodology, NLP</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Internet-augmented Language Models through Few-Shot Prompting" target="Training Language Models to Follow Instructions with Human Feedback">
  <data key="d5">16.0</data>
  <data key="d6">Incorporating internet data via few-shot prompting enhances models' ability to follow instructions and answer questions effectively."|</data>
  <data key="d7">augmentation, instruction-following</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Retrieval-augmented Generation for Knowledge-Intensive NLP Tasks" target="WebGPT: Browser-assisted Question-Answering with Human Feedback">
  <data key="d5">16.0</data>
  <data key="d6">WebGPT employs browser assistance and human feedback to improve retrieval and answer quality in open-domain QA systems."|</data>
  <data key="d7">browser, human feedback</data>
  <data key="d8">chunk-ebb9fff9b6ca895234d2fcb26b588fe4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="LlamaIndex" target="IRS chatbot">
  <data key="d5">24.0</data>
  <data key="d6">LlamaIndex is used within the IRS chatbot to retrieve relevant documents, facilitating accurate and source-supported responses.&lt;SEP&gt;LlamaIndex supports the IRS chatbot by enabling document retrieval, which helps the chatbot answer user queries based on relevant information.&lt;SEP&gt;LlamaIndex supports the IRS chatbot by providing document retrieval capabilities essential for answering user queries.</data>
  <data key="d7">Tools&lt;SEP&gt;Tools, Applications/Implications</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Associates, Inc." target="research source">
  <data key="d5">1.0</data>
  <data key="d6">Provides proceedings and papers related to neural information processing and machine learning research.</data>
  <data key="d7">1</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Mohammadreza Pourreza" target="Din-sql">
  <data key="d5">2.0</data>
  <data key="d6">Author of the paper on decomposed in-context learning of text-to-SQL with self-correction, 2023.</data>
  <data key="d7">2</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Davood Rafiei" target="Din-sql">
  <data key="d5">2.0</data>
  <data key="d6">Co-author of the paper on text-to-SQL with self-correction, 2023.</data>
  <data key="d7">2</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Ofir Press" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Author involved in research on language model compositionality, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Muru Zhang" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Co-author contributing to the same research on language models, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Sewon Min" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Co-author contributing to language model compositionality research, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Ludwig Schmidt" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Co-author in language model compositionality study, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Noah A Smith" target="Measuring and narrowing the compositionality gap">
  <data key="d5">3.0</data>
  <data key="d6">Co-author in language model research, 2022.</data>
  <data key="d7">3</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Reid Pryzant" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Author working on prompt optimization with gradient descent and beam search, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Dan Iter" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Co-author on prompt optimization research, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Jerry Li" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Co-author on prompt optimization methods, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Yin Tat Lee" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Co-author contributing to prompt optimization research, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Chenguang Zhu" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Co-author involved in prompt optimization research, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Michael Zeng" target="Automatic prompt optimization">
  <data key="d5">4.0</data>
  <data key="d6">Co-author in prompt optimization research, 2023.</data>
  <data key="d7">4</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Peng Qi" target="Answering complex open-domain questions">
  <data key="d5">5.0</data>
  <data key="d6">Author of research on iterative query generation for answering complex questions, 2019.</data>
  <data key="d7">5</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Peng Qi" target="Retrieve, rerank, read, then iterate">
  <data key="d5">6.0</data>
  <data key="d6">Author of a methodology for answering questions through iterative retrieval and reading, 2020.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Xiaowen Lin" target="Answering complex open-domain questions">
  <data key="d5">5.0</data>
  <data key="d6">Co-author in research on iterative question answering, 2019.</data>
  <data key="d7">5</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Leo Mehr" target="Answering complex open-domain questions">
  <data key="d5">5.0</data>
  <data key="d6">Co-author contributing to open-domain question answering research, 2019.</data>
  <data key="d7">5</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zijian Wang" target="Answering complex open-domain questions">
  <data key="d5">5.0</data>
  <data key="d6">Co-author working on question answering methods, 2019.</data>
  <data key="d7">5</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Haejun Lee" target="Retrieve, rerank, read, then iterate">
  <data key="d5">6.0</data>
  <data key="d6">Co-author in question answering methodology, 2020.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Oghenetegiri Sido" target="Retrieve, rerank, read, then iterate">
  <data key="d5">6.0</data>
  <data key="d6">Co-author working on question answering, 2020.</data>
  <data key="d7">6</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Karthik Narasimhan" target="Improving language understanding">
  <data key="d5">7.0</data>
  <data key="d6">Co-author on language pre-training research, 2018.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Tim Salimans" target="Improving language understanding">
  <data key="d5">7.0</data>
  <data key="d6">Co-author contributing to language model pre-training, 2018.</data>
  <data key="d7">7</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Alexander J Ratner" target="Data programming">
  <data key="d5">8.0</data>
  <data key="d6">Author of work on creating large training sets quickly via data programming, 2016.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Christopher M De Sa" target="Data programming">
  <data key="d5">8.0</data>
  <data key="d6">Co-author in research on data programming for training data generation, 2016.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Sen Wu" target="Data programming">
  <data key="d5">8.0</data>
  <data key="d6">Co-author contributing to data programming methodology, 2016.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Daniel Selsam" target="Data programming">
  <data key="d5">8.0</data>
  <data key="d6">Co-author on large training set creation through data programming, 2016.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Christopher R ´e" target="Data programming">
  <data key="d5">8.0</data>
  <data key="d6">Co-author involved in data programming and neural information processing, 2016.</data>
  <data key="d7">8</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhihong Shao" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Author working on generating chain-of-thought demonstrations for large language models, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Yeyun Gong" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Co-author contributing to chain-of-thought demonstration research, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Nan Duan" target="Synthetic Prompting">
  <data key="d5">11.0</data>
  <data key="d6">Co-author contributing to chain-of-thought demonstrations, 2023.</data>
  <data key="d7">11</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Noah Shinn" target="Reflexion">
  <data key="d5">12.0</data>
  <data key="d6">Author working on autonomous agent with self-reflection and dynamic memory, 2023.</data>
  <data key="d7">12</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Beck Labash" target="Reflexion">
  <data key="d5">12.0</data>
  <data key="d6">Co-author involved in autonomous agent development, 2023.</data>
  <data key="d7">12</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Ashwin Gopinath" target="Reflexion">
  <data key="d5">12.0</data>
  <data key="d6">Co-author working on autonomous agents with self-reflection, 2023.</data>
  <data key="d7">12</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Chenglei Si" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Author working on making GPT-3 reliable through prompting techniques, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhe Gan" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author involved in prompt engineering for GPT-3, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhengyuan Yang" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author working on prompt reliability, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Jianfeng Wang" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author working on prompting techniques for GPT-3, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Jordan Boyd-Graber" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author involved in prompt reliability research, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Lijuan Wang" target="Prompting GPT-3">
  <data key="d5">13.0</data>
  <data key="d6">Co-author contributing to prompt reliability, 2022.</data>
  <data key="d7">13</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Zhiqing Sun" target="Recitation-Augmented Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Author working on recitation-augmented models, 2022.</data>
  <data key="d7">14</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Yiming Yang" target="Recitation-Augmented Language Models">
  <data key="d5">14.0</data>
  <data key="d6">Co-author working on recitation-augmented language models, 2022.</data>
  <data key="d7">14</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Seiya Tokui" target="Chainer">
  <data key="d5">15.0</data>
  <data key="d6">Author of the deep learning framework Chainer, 2015.</data>
  <data key="d7">15</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kenta Oono" target="Chainer">
  <data key="d5">15.0</data>
  <data key="d6">Co-developer of the Chainer framework, 2015.</data>
  <data key="d7">15</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Shohei Hido" target="Chainer">
  <data key="d5">15.0</data>
  <data key="d6">Contributor to the Chainer deep learning framework, 2015.</data>
  <data key="d7">15</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Justin Clayton" target="Chainer">
  <data key="d5">15.0</data>
  <data key="d6">Contributor to the Chainer framework, 2015.</data>
  <data key="d7">15</data>
  <data key="d8">chunk-d7fb767860f2acfdf6c421fa6f949b0b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Harsh Trivedi" target="Interleaving retrieval with chain-of-thought reasoning">
  <data key="d5">8.0</data>
  <data key="d6">Harsh Trivedi's work focuses on integrating retrieval techniques with chain-of-thought reasoning for knowledge-intensive questions.</data>
  <data key="d7">research focus, methodology</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Fei Wang" target="Backpropagation with callbacks">
  <data key="d5">9.0</data>
  <data key="d6">Fei Wang's research provides foundational methods for efficient differentiable programming in neural networks.</data>
  <data key="d7">methodology, neural networks</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Large language models as optimizers" target="Chengrun Yang">
  <data key="d5">8.0</data>
  <data key="d6">Chengrun Yang's recent work explores using large language models as optimization tools.</data>
  <data key="d7">application, research hypothesis</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="HotpotQA" target="Zhilin Yang">
  <data key="d5">9.0</data>
  <data key="d6">Zhilin Yang and colleagues present HotpotQA, a dataset for multi-hop question answering and explainability.</data>
  <data key="d7">dataset, research tool</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="React" target="Shunyu Yao">
  <data key="d5">8.0</data>
  <data key="d6">Shunyu Yao and team developed React, a framework for reasoning and acting synergistically in language models.</data>
  <data key="d7">framework, reasoning and acting</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Star" target="Eric Zelikman">
  <data key="d5">8.0</data>
  <data key="d6">Eric Zelikman and colleagues developed Star, a system for bootstrapping reasoning with iterative processes.</data>
  <data key="d7">system development, reasoning</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Answering questions by meta-reasoning" target="Ori Yoran">
  <data key="d5">8.0</data>
  <data key="d6">Ori Yoran's research involves meta-reasoning over multiple chains of thought to improve question answering.</data>
  <data key="d7">research focus, methodology</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Andrew Zhao" target="ExpeL">
  <data key="d5">8.0</data>
  <data key="d6">Andrew Zhao's research introduces ExpeL, an approach where LLM agents learn through experience.</data>
  <data key="d7">learning methodology, agent development</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Xu Zhao" target="Automatic model selection">
  <data key="d5">8.0</data>
  <data key="d6">Xu Zhao's study investigates automatic model selection techniques for reasoning tasks with large language models.</data>
  <data key="d7">research question, methodology</data>
  <data key="d8">chunk-2a8c8185b8c21a596df9bcd098f56ca4</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GenerateSearchQuery" target="dspy.Signature">
  <data key="d5">16.0</data>
  <data key="d6">The GenerateSearchQuery class extends dspy.Signature, defining a specific signature for generating search queries based on context and questions.</data>
  <data key="d7">class extension, signature definition</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GenerateSearchQuery" target="dspy.InputField">
  <data key="d5">14.0</data>
  <data key="d6">The context and question inputs are defined as dspy.InputFields within the GenerateSearchQuery signature, specifying their descriptions and roles.</data>
  <data key="d7">input field specification</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="GenerateSearchQuery" target="dspy.OutputField">
  <data key="d5">16.0</data>
  <data key="d6">The query output is defined as a dspy.OutputField with dtype=dspy.SearchQuery, representing the generated search query.</data>
  <data key="d7">output field specification</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="LangChain" target="Zero-shot ReAct">
  <data key="d5">24.0</data>
  <data key="d6">Zero-shot ReAct is enabled by LangChain, which provides the necessary tools for reasoning, chaining, and tool use without task-specific training.&lt;SEP&gt;Zero-shot ReAct is implemented using LangChain, which provides the infrastructure for reasoning, tool invocation, and chain execution in AI models.&lt;SEP&gt;Zero-shot ReAct utilizes LangChain as a tool to implement the reasoning framework without task-specific training data.</data>
  <data key="d7">Tools&lt;SEP&gt;Tools, Methodologies</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="LangChain" target="QA with sources">
  <data key="d5">24.0</data>
  <data key="d6">QA with sources is a methodology built upon LangChain's capabilities, enabling question-answering systems to retrieve and cite relevant external documents.&lt;SEP&gt;QA with sources is implemented using LangChain infrastructure, enabling retrieval-based question answering.&lt;SEP&gt;QA with sources utilizes LangChain's infrastructure for document retrieval and source citation, supporting explainability and accuracy.</data>
  <data key="d7">Tools&lt;SEP&gt;Tools, Methodologies</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Prompt Size" target="Prompt Engineering Challenges">
  <data key="d5">9.0</data>
  <data key="d6">Large prompt templates (in words and characters) exemplify the complexity and challenge of manual prompt engineering in current systems.</data>
  <data key="d7">prompt engineering challenge</data>
  <data key="d8">chunk-05507b9dde03ab6b84f0583f82e0877a</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Gao et al. (2023a)" target="Math word problems (PAL)">
  <data key="d5">16.0</data>
  <data key="d6">Gao et al. (2023a) investigates the application and performance of models on math word problems, linking the study to the objects of study.&lt;SEP&gt;Gao et al. (2023a) likely investigates or evaluates the performance of models on math word problems, linking the study to the task of mathematical reasoning.</data>
  <data key="d7">Core Concepts, Study Designs&lt;SEP&gt;Theories/Models, Study Designs</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Math word problems (PAL)" target="Gao et al. (2023b)">
  <data key="d5">21.0</data>
  <data key="d6">Gao et al. (2023b) builds upon or extends Gao et al. (2023a), focusing on further evaluation or methodological improvements.&lt;SEP&gt;Gao et al. (2023b) builds upon or relates to Gao et al. (2023a), focusing on further aspects of math problem solving or model evaluation.&lt;SEP&gt;Gao et al. (2023b) relates to the same objects of study, focusing on further evaluation or different aspects of mathematical problem solving.</data>
  <data key="d7">Core Concepts&lt;SEP&gt;Study Designs&lt;SEP&gt;Study Designs, Evidence Types</data>
  <data key="d8">chunk-d47a0e3d5d118be72a9f7b527b440956</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Inappropriate Physical Contact">
  <data key="d5">16.0</data>
  <data key="d6">Kelvin Hopkins is accused of engaging in inappropriate physical contact.&lt;SEP&gt;Kelvin Hopkins was accused of engaging in inappropriate physical contact, which is part of the misconduct allegations.</data>
  <data key="d7">accusation, misconduct</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Ava Etemadzadeh">
  <data key="d5">20.0</data>
  <data key="d6">Ava Etemadzadeh was allegedly harassed by Kelvin Hopkins, leading to the allegations.&lt;SEP&gt;Kelvin Hopkins allegedly harassed Ava Etemadzadeh, leading to suspension and investigation.</data>
  <data key="d7">harassment, victim&lt;SEP&gt;victim, harassment</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Labour Party">
  <data key="d5">14.0</data>
  <data key="d6">Kelvin Hopkins was suspended by the Labour Party following the allegations.</data>
  <data key="d7">institutional response, disciplinary action</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Sexual Harassment">
  <data key="d5">9.0</data>
  <data key="d6">Kelvin Hopkins is accused of sexual harassment towards Ava Etemadzadeh.</data>
  <data key="d7">accusation, harassment</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Allegations">
  <data key="d5">10.0</data>
  <data key="d6">The allegations against Kelvin Hopkins involve inappropriate physical contact and sexual harassment.</data>
  <data key="d7">accusation, misconduct</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Investigation">
  <data key="d5">12.0</data>
  <data key="d6">The Labour Party conducted an investigation into Kelvin Hopkins' conduct.</data>
  <data key="d7">investigation, misconduct</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Kelvin Hopkins" target="Disciplinary Action">
  <data key="d5">13.0</data>
  <data key="d6">The suspension by the Labour Party is a disciplinary action following the investigation.</data>
  <data key="d7">disciplinary process, consequences</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Inappropriate Physical Contact" target="Sexual Harassment">
  <data key="d5">6.0</data>
  <data key="d6">Inappropriate physical contact is a form of sexual harassment, which Kelvin Hopkins was accused of.</data>
  <data key="d7">misconduct, harassment</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Sexual Harassment" target="Ava Etemadzadeh">
  <data key="d5">10.0</data>
  <data key="d6">Ava Etemadzadeh was allegedly subjected to sexual harassment by Kelvin Hopkins.</data>
  <data key="d7">victim, allegation</data>
  <data key="d8">chunk-e7179702fce1ff6441001accbe9a590e</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Leah" target="Chocolates">
  <data key="d5">14.0</data>
  <data key="d6">Leah possesses chocolates, which are used as objects of study in the problem.&lt;SEP&gt;Leah's chocolates contribute to the total chocolates calculation, as part of the initial sum.</data>
  <data key="d7">component of total&lt;SEP&gt;possession</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Sister" target="Chocolates">
  <data key="d5">14.0</data>
  <data key="d6">Sister's chocolates contribute to the total chocolates calculation, as part of the initial sum.&lt;SEP&gt;The sister also possesses chocolates, contributing to the total initial quantity.</data>
  <data key="d7">component of total&lt;SEP&gt;possession</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Chocolates" target="Total Chocolates">
  <data key="d5">8.0</data>
  <data key="d6">The total chocolates are the sum of Leah's and her sister's chocolates, forming the initial total.</data>
  <data key="d7">composition</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Total Chocolates" target="Chocolates Eaten">
  <data key="d5">8.0</data>
  <data key="d6">Total chocolates minus chocolates eaten yields chocolates left, linking initial quantity to remaining quantity.</data>
  <data key="d7">mathematical relationship</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Chocolates Eaten" target="Chocolates Left">
  <data key="d5">16.0</data>
  <data key="d6">Chocolates eaten are subtracted from total chocolates to determine how many are left.&lt;SEP&gt;Chocolates eaten are subtracted from total chocolates to find chocolates left.</data>
  <data key="d7">causal&lt;SEP&gt;causal relationship</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Parking Lot" target="Total Cars">
  <data key="d5">8.0</data>
  <data key="d6">Initial cars plus arriving cars determine the total number of cars in the parking lot.</data>
  <data key="d7">arithmetic</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Cars Initial" target="Cars Arrived">
  <data key="d5">8.0</data>
  <data key="d6">The total number of cars in the parking lot is the sum of initial cars and cars that arrive.</data>
  <data key="d7">addition</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Cars Initial" target="Total Cars">
  <data key="d5">16.0</data>
  <data key="d6">Initial cars plus arriving cars determine the total in the parking lot.&lt;SEP&gt;Initial cars plus cars that arrive give the total cars in the parking lot.</data>
  <data key="d7">arithmetic&lt;SEP&gt;arithmetic sum</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Cars Arrived" target="Total Cars">
  <data key="d5">8.0</data>
  <data key="d6">The number of arriving cars adds to the initial count to give the total.</data>
  <data key="d7">addition</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Trees in Grove" target="Trees Added">
  <data key="d5">8.0</data>
  <data key="d6">The difference between Trees After and Initial Trees indicates how many trees were planted.</data>
  <data key="d7">subtraction</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Initial Trees" target="Trees After">
  <data key="d5">8.0</data>
  <data key="d6">The difference between trees after planting and initial trees indicates how many trees were added.</data>
  <data key="d7">subtraction</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Trees After" target="Trees Added">
  <data key="d5">8.0</data>
  <data key="d6">Trees added are calculated as the difference between the final and initial number of trees.</data>
  <data key="d7">arithmetic difference</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Solution Function" target="Mathematical Operations">
  <data key="d5">8.0</data>
  <data key="d6">The functions execute addition or subtraction to solve the posed problems.</data>
  <data key="d7">methodological</data>
  <data key="d8">chunk-d85df6d86061474b90ef1ebb5195d860</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Rebel Without a Cause" target="Elia Kazan">
  <data key="d5">16.0</data>
  <data key="d6">Elia Kazan directed the film Rebel Without a Cause, which is a notable work in American cinema.&lt;SEP&gt;Elia Kazan directed the film Rebel Without a Cause, which is a significant work in American cinema.</data>
  <data key="d7">film direction, American cinema</data>
  <data key="d8">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Elia Kazan" target="Pavel Urysohn">
  <data key="d5">5.0</data>
  <data key="d6">Both are influential figures in their respective fields—Kazan in film and theatre, Urysohn in mathematics—highlighting contributions to arts and sciences.</data>
  <data key="d7">interdisciplinary influence, contributions</data>
  <data key="d8">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Arthur’s Magazine" target="First for Women">
  <data key="d5">12.0</data>
  <data key="d6">Arthur’s Magazine predates First for Women, indicating the chronological order of these publications.&lt;SEP&gt;Arthur’s Magazine was published earlier (1844-1846) than First for Women (started in 1989), showing chronological publishing order.</data>
  <data key="d7">publication history, chronological order&lt;SEP&gt;publication history, chronology</data>
  <data key="d8">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Pavel Urysohn" target="Leonid Levin">
  <data key="d5">8.0</data>
  <data key="d6">Both Urysohn and Levin are prominent mathematicians, but Urysohn is known for dimension theory while Levin is known for computational work, indicating different specializations within mathematics.&lt;SEP&gt;Both Urysohn and Levin are prominent mathematicians, but Urysohn is known for topology and dimension theory, while Levin is known for computational complexity, indicating different specialization areas within mathematics.</data>
  <data key="d7">mathematical disciplines, specialization&lt;SEP&gt;mathematical specialization, discipline</data>
  <data key="d8">chunk-e384ac8e647dff2556f72fe5244aa59d</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Ukrainian People" target="Groups of Citizens">
  <data key="d5">7.0</data>
  <data key="d6">The Ukrainian People's fearlessness and determination inspire global admiration and support.</data>
  <data key="d7">inspiration, national resilience</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="COVID-19" target="Societal Reset">
  <data key="d5">12.0</data>
  <data key="d6">The pandemic has prompted a societal reset, encouraging unity and reframing societal divisions.</data>
  <data key="d7">societal change, unity</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Trust and Safety" target="Police Officers Mora and Rivera">
  <data key="d5">16.0</data>
  <data key="d6">Their sacrifice and the subsequent funerals symbolize community commitment to trust and safety.</data>
  <data key="d7">community sacrifice, trust</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="American Democracy" target="Nation’s Character and Future">
  <data key="d5">16.0</data>
  <data key="d6">The history of fighting for liberty and resilience continues to shape the nation's future and character.</data>
  <data key="d7">national identity, historical resilience</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="evaluate_program" target="Program">
  <data key="d5">16.0</data>
  <data key="d6">The evaluate_program function assesses the performance of the program on a given dataset, producing a score.&lt;SEP&gt;The function evaluates the program on a dataset, producing a performance score.</data>
  <data key="d7">performance assessment, model evaluation&lt;SEP&gt;performance evaluation, model assessment</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="evaluate_program" target="Score">
  <data key="d5">18.0</data>
  <data key="d6">The score is the output of the evaluation function, indicating how well the program performs.&lt;SEP&gt;The score is the output of the evaluation, indicating how well the program performs on the dataset.</data>
  <data key="d7">performance metric, evaluation result</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predictor1" target="Demos">
  <data key="d5">14.0</data>
  <data key="d6">Predictor1's demos are used as examples or data points within the demonstrations set for evaluation or training.&lt;SEP&gt;Predictor1's demos are used as training or evaluation examples within the demonstrations set.</data>
  <data key="d7">training data, evaluation set&lt;SEP&gt;training data, example set</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Predictor2" target="Demos">
  <data key="d5">12.0</data>
  <data key="d6">Predictor2's demos are similarly used for evaluation, complementing or contrasting with Predictor1.&lt;SEP&gt;Predictor2's demos are used similarly for evaluation or comparison with Predictor1.</data>
  <data key="d7">training data, evaluation set&lt;SEP&gt;training data, example set</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Demos" target="BootstrapFewshot">
  <data key="d5">18.0</data>
  <data key="d6">BootstrapFewshot compiles a large set of demonstrations to facilitate few-shot learning or model training.&lt;SEP&gt;BootstrapFewshot generates a large set of demonstrations to facilitate few-shot learning.</data>
  <data key="d7">data augmentation, sample generation&lt;SEP&gt;sample generation, data augmentation</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Program" target="Trial">
  <data key="d5">16.0</data>
  <data key="d6">The trial stores the current program configuration as a user attribute for comparison during optimization.&lt;SEP&gt;The trial stores the program as a user attribute for comparison and selection during optimization.</data>
  <data key="d7">model selection, hyperparameter tuning&lt;SEP&gt;optimization process, model selection</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Objective Function" target="Self">
  <data key="d5">7.0</data>
  <data key="d6">The objective function defines the goal of optimization, such as maximizing score.</data>
  <data key="d7">optimization goal, performance metric</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Training Set" target="Self">
  <data key="d5">16.0</data>
  <data key="d6">The training set is assigned to the object’s attribute for use in model training.</data>
  <data key="d7">dataset assignment, training data</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Validation Set" target="Self">
  <data key="d5">14.0</data>
  <data key="d6">The validation set is set to evaluate the model's performance during optimization, defaulting to the training set if not provided.&lt;SEP&gt;The validation set is used to evaluate the model during tuning, defaulting to training data if not specified.</data>
  <data key="d7">dataset assignment, validation</data>
  <data key="d8">chunk-f43aff49a19ae4052ebd930786a46d8b</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="question" target="Twilight (novel series)">
  <data key="d5">16.0</data>
  <data key="d6">The question references the publication year of the first vampire-themed fantasy romance novel in the Twilight series.&lt;SEP&gt;The question references the publication year of the first vampire-themed novel in the Twilight series.</data>
  <data key="d7">literature reference, publication date&lt;SEP&gt;literature, publication date</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="question" target="Harper Connelly Mysteries">
  <data key="d5">14.0</data>
  <data key="d6">The question involves details about the publication of the Harper Connelly Mysteries series.&lt;SEP&gt;The question involves the publication details of the Harper Connelly Mysteries series.</data>
  <data key="d7">literary series, publication info&lt;SEP&gt;literature series, publication info</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="question" target="The Dark Heroine">
  <data key="d5">16.0</data>
  <data key="d6">The question asks about the first publication year of The Dark Heroine series.</data>
  <data key="d7">literature, publication date</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="question" target="The Victorians">
  <data key="d5">16.0</data>
  <data key="d6">The question concerns the birth year of Jeremy Paxman, related to the Victorian documentary series context.&lt;SEP&gt;The question concerns the birth year of Jeremy Paxman, relevant to the Victorian cultural context discussed in the series.</data>
  <data key="d7">biographical detail, historical context</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="question" target="Jeremy Paxman">
  <data key="d5">16.0</data>
  <data key="d6">The question seeks Jeremy Paxman's birth year, which is relevant to the Victorian era context and historical background.&lt;SEP&gt;The question seeks Jeremy Paxman's birth year, which is relevant to the Victorian era context.</data>
  <data key="d7">biographical info, historical relevance</data>
  <data key="d8">chunk-66e5a3f496b5e1530da70355696b5224</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="Russian Invasion" target="Sanctions and Oil Reserves">
  <data key="d5">18.0</data>
  <data key="d6">Sanctions and strategic reserve releases are tools used by the U.S. and allies to respond to the invasion and influence Russia's economy and energy markets.</data>
  <data key="d7">geopolitical response, economic tools</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="ARPA-H" target="Health Breakthroughs">
  <data key="d5">16.0</data>
  <data key="d6">ARPA-H aims to drive technological innovations in health, inspired by DARPA's success in technological breakthroughs.</data>
  <data key="d7">health innovation, technological research</data>
  <data key="d8">chunk-5aca7edc6ce1296a9dd4c03894ff63de</data>
  <data key="d9">DSPY - COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES - 2310.03714v1.pdf</data>
</edge>
<edge source="HPC-INSTRUCT Dataset" target="HPC-Coder-V2">
  <data key="d5">9.0</data>
  <data key="d6">HPC-Coder-V2 is trained on the HPC-INSTRUCT dataset, which improves its ability to generate parallel code.</data>
  <data key="d7">training data, model performance</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-INSTRUCT Dataset" target="Synthetic Data Generation Methodology">
  <data key="d5">8.0</data>
  <data key="d6">HPC-INSTRUCT was created using a methodology that maps existing parallel code to instruct-answer pairs for training.</data>
  <data key="d7">data creation, methodology</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-Coder-V2" target="HPC-I NSTRUCT">
  <data key="d5">9.0</data>
  <data key="d6">HPC-I NSTRUCT provides the synthetic data used to fine-tune HPC-Coder-V2, enabling it to better generate parallel code.</data>
  <data key="d7">training data, model fine-tuning</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-Coder-V2" target="Synthetic Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The synthetic dataset is used to train and fine-tune HPC-Coder-V2 to improve its parallel code generation performance.</data>
  <data key="d7">training, data utilization</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-Coder-V2" target="ParEval benchmark">
  <data key="d5">18.0</data>
  <data key="d6">HPC-Coder-V2 is evaluated using the ParEval benchmark suite, which tests its performance across multiple problem types and execution models.&lt;SEP&gt;HPC-Coder-V2's performance is evaluated using the ParEval benchmark suite, which tests across various problem types and execution models.</data>
  <data key="d7">evaluation, benchmarking</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="StarCoder2" target="The Stack v2 Dataset">
  <data key="d5">14.0</data>
  <data key="d6">Models trained on The Stack v2 dataset perform comparably or worse, highlighting the importance of data quality over quantity.&lt;SEP&gt;Models trained on The Stack v2 dataset perform similarly or worse than models trained on less data, highlighting the importance of data quality.</data>
  <data key="d7">data impact&lt;SEP&gt;data quality, training effectiveness</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="StarCoder2" target="Comparison with Other Models">
  <data key="d5">8.0</data>
  <data key="d6">StarCoder2 is compared with Magicoder, Phind-V2, Gemini-1.5-flash, GPT-3.5, and GPT-4 to evaluate relative performance in code generation tasks.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Challenges in HPC Code Generation" target="Hurdles in Generating Parallel Code">
  <data key="d5">8.0</data>
  <data key="d6">The lack of high-quality parallel code data and the intrinsic difficulty of parallel code generation are key hurdles.</data>
  <data key="d7">challenges, data scarcity</data>
  <data key="d8">chunk-99b6ab3dd30e07713288e985bc1e0726</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-I NSTRUCT" target="Model Fine-tuning Impact">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuning on the HPC instruction dataset enhances model performance in generating parallel HPC code."|</data>
  <data key="d7">9</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-I NSTRUCT" target="Synthetic Data">
  <data key="d5">8.0</data>
  <data key="d6">Synthetic data is used to train and evaluate HPC LLMs, addressing data scarcity and quality issues.</data>
  <data key="d7">data augmentation, training</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-I NSTRUCT" target="Data Quality">
  <data key="d5">8.0</data>
  <data key="d6">High data quality in the synthetic dataset improves model training effectiveness compared to just increasing data quantity.</data>
  <data key="d7">data quality, training effectiveness</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Data Representation" target="Impact on Models">
  <data key="d5">8.0</data>
  <data key="d6">The way data is structured influences how effectively models can learn patterns for parallel code generation.</data>
  <data key="d7">data influence, learning effectiveness</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Synthetic Dataset HPC-I NSTRUCT" target="Fine-tuning HPC-Coder-V2">
  <data key="d5">8.0</data>
  <data key="d6">The synthetic dataset is used to fine-tune the HPC-Coder-V2 model to improve its parallel code generation capabilities.</data>
  <data key="d7">training data, model enhancement</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Research Questions" target="Study Impact">
  <data key="d5">6.0</data>
  <data key="d6">The research questions guide the investigation into how different factors influence model performance in parallel code generation.</data>
  <data key="d7">study focus, influence</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Instruction Masking" target="Performance Impact">
  <data key="d5">10.0</data>
  <data key="d6">Instruction masking during fine-tuning affects the quality and relevance of the model's responses in parallel code generation tasks.</data>
  <data key="d7">fine-tuning technique, response quality</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Synthetic Data" target="HPC-INSTRUCT">
  <data key="d5">18.0</data>
  <data key="d6">HPC-INSTRUCT is a synthetic dataset created from open-source snippets and multiple LLMs to improve code LLM fine-tuning for HPC applications.&lt;SEP&gt;HPC-INSTRUCT is a synthetic dataset generated from open-source code snippets and multiple LLMs to improve the training of HPC-specific code LLMs.</data>
  <data key="d7">dataset creation, synthetic data generation</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Synthetic Data" target="Seed Code Snippets">
  <data key="d5">16.0</data>
  <data key="d6">Seed code snippets are used as prompts inspired by open-source HPC code to generate diverse synthetic data samples in various programming languages.&lt;SEP&gt;Seed code snippets serve as prompts inspired by open-source HPC code to generate diverse synthetic code samples across multiple languages and paradigms.</data>
  <data key="d7">prompt engineering, data diversity</data>
  <data key="d8">chunk-e33bd393ed70103fd66f1645d1888b19</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="HPC-INSTRUCT" target="Fine-Tuning Dataset">
  <data key="d5">16.0</data>
  <data key="d6">HPC-INSTRUCT is used as the primary dataset for fine-tuning the HPC models, impacting their ability to generate parallel code.&lt;SEP&gt;HPC-INSTRUCT is used to fine-tune the HPC code models, impacting their ability to generate parallel code.</data>
  <data key="d7">dataset influence, model training</data>
  <data key="d8">chunk-cdd832b6a605bd8b46aec4e0615ea5df</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Ablation Studies" target="Choice of Base Model">
  <data key="d5">8.0</data>
  <data key="d6">Ablation studies compare different base models to assess how their intrinsic properties influence the outcome of fine-tuning.</data>
  <data key="d7">study comparison, model influence</data>
  <data key="d8">chunk-8ea08005bc62e40cce7faf8139912fa4</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Ablation Studies" target="Impact Analysis">
  <data key="d5">9.0</data>
  <data key="d6">Ablation studies analyze how different training configurations and data amounts impact model performance in code generation.</data>
  <data key="d7">study design, impact analysis</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="DeepSeek-Coder" target="Architecture">
  <data key="d5">9.0</data>
  <data key="d6">DeepSeek-Coder models are based on llama and mixture-of-experts architectures, which influence their scalability and performance.</data>
  <data key="d7">model design, architecture</data>
  <data key="d8">chunk-f3db4d8ebe5bafa5d9d7ae540d506e1f</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Data Quality" target="Synthetic Data Generation">
  <data key="d5">7.0</data>
  <data key="d6">Synthetic data aims to enhance data quality for training HPC code models, addressing limitations in real data availability."|</data>
  <data key="d7">7</data>
  <data key="d8">chunk-a491810c484f5945ed0c9ade326607dd</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Magicoder" target="Training Data and Fine-Tuning">
  <data key="d5">7.0</data>
  <data key="d6">Magicoder is a fine-tuned model based on DeepseekCoder-6.7B, trained on synthetic data generated from open-source code, aimed at enhancing code generation capabilities.</data>
  <data key="d7">training data, fine-tuning</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Gemini-1.5-flash" target="Commercial Model">
  <data key="d5">5.0</data>
  <data key="d6">Gemini-1.5-flash is an API-accessible commercial model from Google, used in practical applications of code generation.</data>
  <data key="d7">commercial application, API</data>
  <data key="d8">chunk-87968a35d8273787998be04f169d03c8</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Code generation performance" target="HPC-Coder-V2 and other models">
  <data key="d5">8.0</data>
  <data key="d6">The performance metrics compare how well different models generate correct, efficient parallel code across problem types and execution models.</data>
  <data key="d7">performance comparison, model effectiveness</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Model sizes" target="Performance and resource requirements">
  <data key="d5">14.0</data>
  <data key="d6">Different sizes of HPC-Coder-V2 models show trade-offs, with larger models generally performing better but requiring more memory and time.&lt;SEP&gt;Larger models tend to perform better but require more memory and computational resources; smaller models like 1.3B can outperform larger ones in speed and efficiency.</data>
  <data key="d7">scalability, resource trade-offs</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Parallel execution models" target="Model evaluation">
  <data key="d5">16.0</data>
  <data key="d6">Models perform differently depending on the execution model, with serial and OpenMP codes being generated more accurately than MPI codes, indicating varying effectiveness across parallel paradigms.&lt;SEP&gt;Models perform differently depending on the execution model, with serial and OpenMP codes being generated more accurately than MPI codes.</data>
  <data key="d7">model robustness, execution model impact&lt;SEP&gt;performance variation, execution model impact</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="W. Liang" target="Luo, Y">
  <data key="d5">6.0</data>
  <data key="d6">Both authors collaborated on research related to large language models and code intelligence, contributing to the cited publications.</data>
  <data key="d7">author collaboration, research context</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Deepseek-coder" target="Reproducibility">
  <data key="d5">15.0</data>
  <data key="d6">Deepseek-coder scripts and models are made available online to ensure reproducibility of research results.&lt;SEP&gt;Scripts and models related to Deepseek-coder are made publicly available to ensure reproducibility of research results.</data>
  <data key="d7">reproducibility, open access</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Biocoder" target="HPC-I NSTRUCT dataset">
  <data key="d5">13.0</data>
  <data key="d6">Biocoder dataset is part of the HPC-I NSTRUCT dataset used for benchmarking code generation models in bioinformatics.&lt;SEP&gt;Biocoder is part of the HPC-I NSTRUCT dataset used for benchmarking code generation models in bioinformatics.</data>
  <data key="d7">dataset, benchmarking</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Verilogeval" target="HPC-Coder-V2 models">
  <data key="d5">16.0</data>
  <data key="d6">Verilogeval evaluates the performance of HPC-Coder-V2 models specifically in Verilog code generation tasks.&lt;SEP&gt;Verilogeval is used to evaluate the performance of HPC-Coder-V2 models in Verilog code generation tasks.</data>
  <data key="d7">evaluation, hardware description language</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Reproducibility" target="HPC-I NSTRUCT dataset">
  <data key="d5">16.0</data>
  <data key="d6">Reproducibility practices include sharing datasets like HPC-I NSTRUCT for validation of code generation models.&lt;SEP&gt;Reproducibility practices include sharing datasets like HPC-I NSTRUCT to facilitate validation and replication of code generation experiments.</data>
  <data key="d7">open data, validation&lt;SEP&gt;validation, open data</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Problem Statement in code optimization" target="Parallelization using OpenMP">
  <data key="d5">20.0</data>
  <data key="d6">The core problem involves modifying the sequential aggregation code by adding OpenMP directives to enable parallel execution, reducing bottlenecks in high-performance computing applications.&lt;SEP&gt;The problem involves applying OpenMP directives to parallelize the sequential aggregation code in high-performance computing applications.</data>
  <data key="d7">optimization, parallel computing</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Luo, Y" target="Xiong, W">
  <data key="d5">6.0</data>
  <data key="d6">Both authors collaborated on research related to large language models and code intelligence, contributing to the cited publications.</data>
  <data key="d7">author collaboration, research context</data>
  <data key="d8">chunk-96146f0e8f16426a7bc0a0002ded1479</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="compute_metric" target="aggregate_metrics">
  <data key="d5">23.0</data>
  <data key="d6">The `aggregate_metrics` function calls `compute_metric` for each data point to perform calculations, then sums the results."|&gt;"function invocation, data processing&lt;SEP&gt;The `aggregate_metrics` function calls `compute_metric` for each data point to perform the calculation, and then sums the results."|&gt;"function invocation, data processing&lt;SEP&gt;The `compute_metric` function is invoked for each data element within the dataset to perform individual calculations."|&gt;"function invocation, data processing</data>
  <data key="d7">7&lt;SEP&gt;8</data>
  <data key="d8">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="aggregate_metrics" target="sum">
  <data key="d5">26.0</data>
  <data key="d6">The `sum` variable accumulates the total of all computed metrics after parallel execution."|&gt;"data storage, accumulation&lt;SEP&gt;The `sum` variable is updated with the results of `compute_metric` calls to produce a total sum."|&gt;"data accumulation, result aggregation&lt;SEP&gt;The `sum` variable stores the total of all computed metrics after parallel execution."|&gt;"data storage, accumulation</data>
  <data key="d7">8&lt;SEP&gt;9</data>
  <data key="d8">chunk-0e2888ba86c90678f67636c1c02fe25c</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Data and Fine-tuning Parameters Study" target="Performance Impact">
  <data key="d5">7.0</data>
  <data key="d6">The study investigates how variations in data and fine-tuning parameters affect the model's ability to generate parallel code effectively.</data>
  <data key="d7">study, performance factors</data>
  <data key="d8">chunk-ab4503fb357b566fec504fb7c7f77bfa</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
<edge source="Literature" target="Related work">
  <data key="d5">7.0</data>
  <data key="d6">The section discusses prior research on code LLMs for HPC, including development, fine-tuning, and application in high-performance and parallel computing contexts.</data>
  <data key="d7">background, related research</data>
  <data key="d8">chunk-aec2ac03d71774b2a191de2c6ebcd45e</data>
  <data key="d9">Chaturvedi et al. - 2024 - HPC-Coder-V2 Studying Code LLMs Across Low-Resource Parallel Languages.pdf</data>
</edge>
</graph></graphml>
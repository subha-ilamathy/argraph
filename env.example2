### This is sample file of .env

### Directory Configuration (defaults to current working directory)
WORKING_DIR=/Users/subhailamathy/Documents/MS/Sp25/CS532/Project/LightRAG/new_working_md_9_papers

### Settings for document indexing
ENABLE_LLM_CACHE_FOR_EXTRACT=true
SUMMARY_LANGUAGE=English


### LLM Configuration
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=150
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0.5
### Max concurrency requests of LLM
MAX_ASYNC=4
### Max tokens send to LLM (less than context size of the model)
MAX_TOKENS=32768


LLM_BINDING=openai
LLM_MODEL=gpt-4.1-nano
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=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


EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=mxbai-embed-large
EMBEDDING_BINDING_HOST=http://localhost:11434
EMBEDDING_DIM=786

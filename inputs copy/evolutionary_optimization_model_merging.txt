# Evolutionary Optimization of Model Merging Recipes

Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha Sakana AI Tokyo, Japan {takiba,mkshing,yujintang,qisun,hadavid}@sakana.ai

## Abstract

Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates crossdomain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.

## 1 Introduction

Model merging [\[21,](#page-18-0) [34\]](#page-19-0), a recent development in the large language model (LLM) community, presents a novel paradigm shift. By strategically combining multiple LLMs into a single architecture, this exciting development has captured the attention of researchers due to its key advantage: it requires no additional training, making it an incredibly cost-effective approach for developing new models. This accessibility has fueled a surge in interest and experimentation with model merging. The Open LLM Leaderboard [\[26\]](#page-19-1) is now dominated by merged models, showcasing its potential for democratizing foundation model development.

However, model merging is considered by many to be a form of black art or alchemy, relying on the model maker's intuition and instincts about model selection and merging recipes to create and refine a new model that performs well for a particular task. Furthermore, the model maker is often required to have some domain knowledge for the various different benchmark tasks. Given the large diversity of open models and benchmarks in the community, human intuition can only go so far, and we believe a more systematic approach for discovering new model combinations will take things much further.

We believe evolutionary algorithms will be able to discover more effective model merging solutions, and thus provide a path for automating the creation of more capable models. As a step towards this direction, in this work, we show that evolution can be employed to discover novel and unintuitive ways to merge multiple models to produce new models with a new combined ability. In this work, we present a methodology that leverages evolutionary algorithms to facilitate the merging of foundation models. Our approach is distinguished by its ability to navigate both parameter space (weights) and the data flow space (inference path), proposing a framework that integrates these two dimensions.

This work makes several key contributions to the field of foundation model development:

- 1. Automated Model Composition: We introduce Evolutionary Model Merge, a general evolutionary method to automatically discover effective combinations of selected models for creating new foundation models with user-specified capabilities. This approach harnesses the collective intelligence of existing open models, enabling the creation of powerful models without the need for extensive training data or compute.
- 2. Cross-Domain Merging: We demonstrate that our method can discover novel ways to merge models from disparate domains (e.g., non-English language and Math, non-English language and Vision), potentially exceeding the capabilities achievable through conventional human design strategies.
- 3. State-of-the-Art Performance: We showcase the effectiveness of our method by automatically generating a Japanese LLM with Math reasoning capability and a Japanese Vision-Language Model (VLM). Notably, both models achieve state-of-the-art performance on various benchmarks, even without explicit optimization for those tasks.
- 4. High Efficiency and Surprising Generalizability: We observe that our 7B parameter LLM surpasses the performance of some previous 70B parameter Japanese LLMs on benchmark datasets, highlighting the high efficiency and surprising generalization capability of our approach. We believe this model can serve as a strong general-purpose Japanese LLM.
- 5. Culturally-Aware VLM: The generated Japanese VLM achieves top results when tested on a domestically-sourced dataset of Japanese image-description pairs, demonstrating its ability to handle Japanese culture-specific content.

We are committed to open science and are excited to open-source our EvoLLM-JP and EvoVLM-JP, two state-of-the-art Japanese foundation models, to the community, enabling further research and development in the field. Our work challenges the conventional paradigm of expensive model development by demonstrating that our evolutionary-based method can produce competitive models without relying on gradient-based training. This paves the way for exploring alternative, potentially more efficient, approaches to foundation model development.

### 1.1 Background and Related Work

### 1.1.1 Overview of Model Merging.

Model merging offers a novel approach to leverage the strengths of multiple pre-trained models. It allows us to combine task-specific models, each potentially fine-tuned for a particular downstream task, into a single unified model. This technique stands in contrast to traditional transfer learning, where a pre-trained model is further fine-tuned for a new task. While transfer learning offers advantages like improved performance and faster convergence, the resulting models are typically limited to single tasks. Model merging, on the other hand, strives to create a versatile and comprehensive model by combining the knowledge from multiple pre-trained models, potentially yielding a model capable of handling various tasks simultaneously.

While fine-tuning an existing model involves training an existing model further on a specialized dataset, model merging methods work by combining existing models without any additional gradientbased training, making it very cost-effective in terms of compute requirements. Many in the machine learning community, including researchers, hackers, and hobbyists alike, are using both fine-tuning and model-merging to create the best models for their purposes. In fact, this collaborative way of developing continually improved models has parallels with the traditional open-source software development approach [\[47\]](#page-19-2) where the trend is for the community to develop foundational machine learning models like open source software [\[45,](#page-19-3) [46\]](#page-19-4).

A simple method of merging multiple models is to average the weights of multiple models finetuned from the same base initial model. This *model soup* approach [\[61\]](#page-20-0) demonstrated significant improvements on relatively large image processing and image classification models. The work pointed to similarity between weighted model averaging leading to a flat local minima. Theoretical [\[17,](#page-18-1) [31,](#page-19-5) [32,](#page-19-6) [44\]](#page-19-7) and empirical studies dating back from the 1990s [\[24,](#page-18-2) [25\]](#page-19-8) demonstrated that flatter local optima generalize better to out-of-distribution shifts. More recent work [\[14,](#page-18-3) [41\]](#page-19-9) examined the theoretical properties and practical implications of weight averaging applied to larger generative models.

Linear weight averaging is performed as follows. Let  $\theta\_1, \theta\_2 \in \mathbb{R}^d$  represent the weight vectors of two distinct models, where d is the dimension of the weight space. The merged model's weights, denoted as  $\theta\_{new}$ , are computed using the following equation:  $\theta\_{new} = \lambda \theta\_1 + (1 - \lambda)\theta\_2$ . Here,  $\lambda \in [0, 1]$  is a weighting parameter that determines the relative contribution of each model to the final merged model. By adjusting  $\lambda$ , practitioners can control the balance between the two original models' characteristics in the resulting merged model.

Linear weight averaging not only works well for image processing and classification models, but is also effective for image generation models such as latent diffusion models. The release of Stable Diffusion [\[50\]](#page-20-1) led to a variety of specialist fine-tunes of the model for a variety of different styles, from photorealism, to anime, to realistic painting styles. Popular open-source WebUIs [\[10\]](#page-18-4) enabled practitioners to experiment with merging different models using linear or spherical linear interpolation (SLERP) [\[60\]](#page-20-2) of weight parameters of distinct fine-tuned Stable Diffusion models. This has led to the discovery of merged models that combine strengths of specialized models into a single model. For some time, the most popular Stable Diffusion models were neither the original base models nor the fine-tuned versions, but rather the merged models created by enthusiasts.

#### 1.1.2 Merging Language Models

Simple weight interpolation works well for merging image generation models that operate in a real valued latent and output space, but to our surprise, they also still work for merging language models under the right conditions, but have performance issues [\[63\]](#page-20-3). Recently, new methods have been proposed to address merging language models specifically. One method, Task Arithmetic [\[27\]](#page-19-10), involves building *task vectors* by subtracting pre-trained from fine-tuned model weights, enabling manipulation through arithmetic operations to steer the merged model's behavior.

Task Arithmetic operates as follows. Let  $\theta\_{base} \in \mathbb{R}^d$  denote the weights of a pre-trained base model. For each task k, let  $\theta\_k \in \mathbb{R}^d$  represent the weights of a model fine-tuned for that specific task. The task vector  $\tau\_k$  for each task k is defined as the difference between the fine-tuned model weights and the base model weights:  $\tau\_k = \theta\_k - \theta\_{base}$ . The weights of the new, merged model  $\theta\_{new}$  are then computed by adding a weighted sum of these task vectors to the base model weights:  $\theta\_{new} = \theta\_{base} + \sum\_k \lambda\_k\tau\_k$  Here,  $\lambda\_k$  is a scaling parameter that determines the contribution of each task vector to the final merged model.

A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [\[63\]](#page-20-3) identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters. The trimming of redundant parameter changes in TIES-Merging not only resulted in higher performance of merged models, but also achieved better compression for the fine-tuned parameter updates. This compression of fine-tuned weight parameter updates is further investigated in ComPEFT [\[62\]](#page-20-4).

The three steps of TIES-Merging are implemented as follows. In the first step, minimal parameter changes are reset by retaining only the most influential components of each task vector τ<sup>k</sup> and setting the rest to zero. This effectively removes redundant or insignificant modifications. The second step resolves sign conflicts across models by determining an aggregate sign for each parameter based on the overall direction of significant changes across all tasks. This ensures that conflicting updates do not cancel each other out. In the final step, only the aligned parameters are merged by selectively averaging the values of parameters that agree with the elected sign from the previous step, while excluding those that disagree. This selective merging process ensures that the final model incorporates only consistent and significant parameter changes from each task-specific model, thereby preserving task-relevant information while minimizing interference.

Another recent work [\[65\]](#page-20-5) proposes the DARE method, which goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [\[65\]](#page-20-5) is often used together with Task Arithmetic [\[27\]](#page-19-10) or TIES-Merging [\[63\]](#page-20-3). Specifically, DARE operates as follows. It applies a random mask m<sup>k</sup> ∼ Bernoulli(p) to task vector τk, where p is the drop rate, and then rescales the remaining parameters:

$$
\hat{\tau}\_k = \frac{1}{1-p} \left( (1-m\_k) \odot \tau\_k \right). \tag{1}
$$

The resulting sparsified task vector τˆ<sup>k</sup> is then used to merge models. This task vector can be utilized in the aforementioned linear task arithmetic (referred to as DARE-Linear) or TIES (referred to as DARE-TIES) methods.

Model merging is gaining momentum in ML research, and has been applied in areas such as multimodal model development [\[7,](#page-18-5) [58\]](#page-20-6) and merging experts [\[7\]](#page-18-5). It is also becoming popular in both the broader image generation and LLM communities, where due to the low barrier to entry, many researchers and engineers are continually merging existing models to create new customized models.

In the image generation domain, model merging techniques became widely accessible to practitioners especially after their implementation in open-source toolkits [\[10\]](#page-18-4). Similarly, language model merging really gained popularity after the implementation of *mergekit* [\[21,](#page-18-0) [34\]](#page-19-0). This toolkit provides all of the popular *recipes* for merging language models. In addition to simple linear and spherical interpolation, advanced recipes such as Task Arithmetic, TIES-Merging, DARE are also implemented, offering users the ability to experiment with combining these methods to work with fine-tunes of popular base models such as Mistral [\[28\]](#page-19-11). With this toolkit, a large number of capable merged models are developed by the community, and most of the top models on the Open LLM Leaderboard [\[26\]](#page-19-1) are gradually dominated by merged models produced by the community of language model enthusiasts.

Mergekit also introduced an additional method, called *Frankenmerging* which is not based on weight merging, but for the user to experiment with trying to stack different layers from multiple models to sequentially create a new model. This method has the advantage of not tying the user to one particular family of models with a fixed architecture for merging (e.g. Mistral-based models), but potentially creating *new* architectures from merging entirely different models. It is noted that discovering new *Frankenmerging* technique remains a challenge for the community, and requires much more trial and error to discover new recipes for this technique. To date, almost everyone uses a similar *Frankenmerging* merge recipe, and there has been very little trial and error to improve upon it. The field remains highly under-explored, and we believe this is where evolution can help.

#### 1.1.3 Connection to Evolutionary Neural Architecture Search

Model merging holds significant potential and democratizes the model-building process, making it accessible to a broad range of participants. However, it heavily relies on intuition and domain knowledge. Human intuition, however, has its limits. With the growing diversity of open models and tasks, we need a more systematic approach. We believe evolutionary algorithms, inspired by natural selection, can unlock more effective merging solutions. These algorithms can explore a vast space of possibilities, discovering novel and counter-intuitive combinations that traditional methods and human intuition might miss.

In deep learning, techniques such as Neural Architecture Search (NAS) [\[66\]](#page-20-7) employed evolutionary techniques to discover new architectures [\[48,](#page-19-12) [55\]](#page-20-8) that might be non-intuitive for human designers to discover. However, NAS-methods consumed significant computational resources given how each candidate model architecture needed to be trained. Our approach aims to *save* compute resources by discovering structures that take advantage of existing capabilities of existing Transformer blocks that had already consumed considerable resources to train in the first place.

Pre-trained transformer blocks are akin to neural network components one can choose to mix and match using an evolutionary architecture search algorithm. However, unlike NAS, we do not need to train the model, and can evaluate the candidates right away. In a sense, our work resonates with the goals of earlier works of morphology search such as NEAT [\[56\]](#page-20-9) and more recent work such as Weight Agnostic Neural Networks [\[18\]](#page-18-6) which evolved neural network structures with certain task-specific inductive biases, without the need to train the weight parameters using gradient descent. Similarly, this was how NAS methods such as SMASH [\[56\]](#page-20-9) avoided costly inner-loop training through the application of a Hypernetwork [\[22\]](#page-18-7) to estimate the weights of architectural candidates.

Recent work in language models tried to duplicate the layers of a pre-trained Transformer many times [\[58\]](#page-20-6) as a starting point to pre-train a much larger network, to enable efficient training of much larger language models [\[64\]](#page-20-10). Similarly, in our approach, we are interested in applying automated search methods to discover the effective reuse of existing pre-trained layers.

In this work, we apply evolution to not only automate the optimization of model merging recipes in weight space, confined to a single architecture, but also explore applying evolution to optimize the stacking of layers from different models—a more involved approach that has the potential to create

entirely novel neural architectures from existing building blocks. Unlike weight merging optimization which confine space of models to be merged together to fine-tuned descendents of the same parent base model, layer stacking optimization has no such constraints. Our work systematically explores the application of evolution applied in both parameter and layer space.

## <span id="page-4-1"></span>2 Methods

Our goal is to create a unified framework capable of automatically generating a merged model from a any individual in the collection. Central to our approach is the application of evolutionary algorithms, which we employ to refine the intricacies involved in model merging. To systematically address this challenge, we first dissect the merging process into two distinct, orthogonal configuration spaces, analyzing their individual impacts. Building on this analysis, we then introduce a cohesive framework that seamlessly integrates these spaces. Figure [1](#page-4-0) provides a schematic representation of our approach. ✅ ❎ ✅

✅

✅

✅

Q2: Cynthia eats one serving of ice cream every night. … How much will she have spent on ice cream after 60 days?

![](_page_4_Figure_4.jpeg)

<span id="page-4-0"></span>Figure 1: Overview of Evolutionary Model Merge. Our approach encompasses (1) evolving the weights for mixing parameters at each layer in the parameter space (PS); (2) evolving layer permutations in the data flow space (DFS); and (3) an integrated strategy that combines both methods for merging in both PS and DFS. Notice that merging in the PS is not simple copying and stitching of the layers parameters, but also mixes the weights. This merging is akin to blending colors as illustrated here (e.g., red and blue becomes purple). Note that we translated the questions to English for the reader; the models operate on Japanese text.

### 2.1 Merging in the Parameter Space

Model merging in the parameter space (PS) aims to integrate the weights of multiple foundational models into a unified entity with the same neural network architecture, yet outperforming the individual models. While various strategies for combining model parameters exist [\[41,](#page-19-9) [61\]](#page-20-0), our approach leverages task vectors analysis to understand each model's strengths, based on the specific tasks they are optimized for or excel in [\[27\]](#page-19-10). Specifically, we enhance TIES-Merging with DARE [\[63,](#page-20-3) [65\]](#page-20-5), allowing for more granular, layer-wise merging (In this paper, by "layer" we mean the input/output embedding layers or a transformer block). We establish merging configuration parameters for sparsification and weight mixing at each layer, including input and output embeddings. These configurations are then optimized using an evolutionary algorithm, such as CMA-ES [\[23\]](#page-18-8), for selected tasks, guided by critical task-specific metrics (e.g., accuracy for MGSM, ROUGE score for VQA).

## 2.2 Merging in the Data Flow Space

Recent analysis and discoveries imply that knowledge is stored distributedly in language models [\[20,](#page-18-9) [42,](#page-19-13) [43\]](#page-19-14), suggesting simple yet novel model merging possibilities in the data flow space (DFS). Unlike

merging in PS, model merging in DFS preserves the original weights of each layer intact. Instead, it optimizes the inference path that tokens follow as they traverse through the neural network. For example, after the i-th layer in model A, a token may be directed to the j-th layer in model B.

In our initial effort in this domain, we limit ourselves to serial connections and non-adaptive configurations, deferring the investigation of more flexible model merging to future work. Concretely, with a collection of N models and a budget T, our method searches for a sequence of layer indices L (t) i,j that delineates the path all the tokens should follow for a specific task. Here Li,j denotes the j-th layer in the i-th model, with t ∈ [1, T] marking the step in the inference path.

One can quickly imagine how large the search space is. Assuming the total number of layers across all models is  $M$ , the size of the search space is  $(M + 1)^T$ , here the extra one indicates the inclusion of a pass-through layer. Even with a modest setting of  $M = 64$  (e.g., 2 models of 32 layers each) and  $T = 60$ , this setting translates to an astronomically large search space, a challenge even for a capable evolutionary search algorithm. Luckily, our preliminary studies indicated that certain layer arrangements, particularly repetitive or permuted sequences from earlier in the model, can adversely affect performance. Based on this, we modify our settings to include an indicator array  $\mathcal{I}$  of size  $T = M \times r$  in the evolutionary search space, here  $r$  is number of repetitions.

Conceptually, we layout all the layers in sequential order (i.e., all layers in the *i*-th model followed by those in the *i* + 1-th model) and repeat them  $r$  times, the indicator array then manages the inclusion/exclusion of layers. If  $\mathcal{I}\_i > 0$  we include the layer corresponding to index  $i$  in the slots in the merged model, otherwise we exclude it. Consequently, our search space is reduced to  $2^T$ , which is still large, but tractable for evolutionary search.

In our search, we only optimize the data inference path inside the merged model and keep parameters in the models intact. In this setting, a layer may face an input whose distribution is different from what it is used to (from its original model), leading to unexpected outputs. For example, our preliminary studies [\[57\]](#page-20-11) show that swapping a pair of neighboring layers in a language model makes its performance drop. Although more theoretical studies are needed to model the distribution shift, empirically we find that appropriately scaling an input that wishes to go from layer  $i$  to  $j$  by  $W\_{ij}$  help alleviate the problem. Here,  $W \in R^{M \times M}$  is a matrix that is also optimized by the evolutionary search together with the indicator array  $\mathcal{I}$ .

The size of  $W$  grows quadratically with  $M$ , for scenarios involving a large number of layers. An alternative approach to contain the search space size involves parameterizing  $W$  with a neural network [\[22,](#page-18-7) [51\]](#page-20-12). We can instead evolve a feed-forward network to output the scaling weights conditioned on the layer and step indices:  $W\_{ij} = \pi\_{\theta}(i, j, t)$  where  $\theta$ 's are the parameters to be evolved, whose size does not change when  $M$  grows.

#### 2.3 Merging in Both Spaces

Model merging in the PS and in the DFS are orthogonal approaches, however, it is straightforward to combine these disentangled methods and further boost the performance of a merged model. As we show in the rightmost illustration in Figure [1](#page-4-0) and in Section [3.1,](#page-6-0) it is possible to first apply PS merging to a collection of models, and then put back this merged model in the collection and apply DFS merging from this enlarged collection.

This can be extremely helpful when one considers model merging with multiple objectives, wherein PS merging can be applied first to produce several merged models each of which targets one of the muliple objectives of interest, and then DFS merging is applied with multi-objective genetic algorithms such as NSGA-II [\[16\]](#page-18-10) to further expand the final model's performance in relevant metrics.

## 3 Results

Most merged models in the community optimize for a narrow set of tasks defined in The Open LLM Leaderboard [\[26\]](#page-19-1). Our motivation is for evolutionary search to discover novel ways to merge different models from vastly different domains (e.g., non-English language and Math, or non-English language and Vision) which might be difficult for human experts to discover effective merging solutions themselves. Furthermore, effectively merging models from very different domains can lead to models of wider real-world applicability and enable us to develop models beyond the large population of models that are optimized for the narrow range of tasks defined by a leaderboard.

We demonstrate our Evolutionary Model Merge approach described in Section [2](#page-4-1) by evolving a Japanese LLM capable of Math reasoning, and a Japanese VLM proficient in handling culturallyspecific content. Specifically, in Section [3.1,](#page-6-0) we apply evolution to merge a Japanese LLM with an English Math LLM to build a Japanese Math LLM, and in Section [3.3,](#page-13-0) we apply evolution to merge a Japanese LLM with an English VLM to create a Japanese VLM.

#### <span id="page-6-0"></span>3.1 Evolving Japanese Math LLM

#### 3.1.1 Experimental Setup

Source Models To develop a model capable of solving math problems in Japanese, we apply evolutionary model merge on a set of source models containing a Japanese LLM and Math LLMs: shisa-gamma-7b-v1 [\[9\]](#page-18-11) (Japanese LLM), WizardMath-7B-V1.1 [\[40\]](#page-19-15) and Abel-7B-002 [\[12\]](#page-18-12). All these models are fine-tuned from Mistral-7B-v0.1 [\[28\]](#page-19-11).

Dataset For testing, we used the MGSM dataset [\[52\]](#page-20-13), a multilingual translation of a subset of the GSM8k dataset [\[13\]](#page-18-13). The Japanese test set of MGSM, consisting of 250 samples, was used for the final evaluation. Specifically, MGSM contains translations of the first 250 samples (ID 0-249) from the GSM8k test set. We used a different dataset for evolutionary search to avoid overfitting the test set. Specifically, we translated the remaining 1069 samples (out of 1319 examples) of the GSM8k test set that were not included in the MGSM test set into Japanese. These correspond to samples with IDs 250-1318 in the original GSM8k test set, ensuring no overlap with the MGSM Japanese test set. One may think that it is more natural to translate the GSM8k training set. However, in our preliminary efforts, this approach did not work well. Since open-source math models were trained on the GSM8k training set, we were unable to perform accurate evaluations.

Evaluation We evaluated the ability to generate Japanese answers to Japanese math problems. Therefore, we considered an answer correct if it met the following criteria: (1) the concluding numerical value must be correct, and (2) the reasoning text should be written in Japanese.

We treated the last numerical value appearing in the output as the answer. We needed to adopt this heuristic because we are merging multiple models that were trained in different formats, which made it difficult to correct the output format. This method appeared to extract the answers correctly in almost all cases. Additionally, to determine the language of the output, we utilized fasttext [\[29,](#page-19-16) [30\]](#page-19-17). We used greedy sampling for generation and calculated the zero-shot pass@1 accuracy.

Optimization For optimization in PS, we used the CMA-ES[\[23\]](#page-18-8) algorithm implemented in Optuna [\[8\]](#page-18-14) with default hyper-parameters. Specifically, we set all initial parameter values to 0.5, sigma to 1/6, and the population size to  $4 + \lfloor 3 \ln(n\_{\text{params}}) \rfloor$ , where  $n\_{\text{params}}$  is the number of parameters to optimize. The fitness value is defined as the accuracy for all 1069 training samples. Please note that this set is disjoint from MGSM's test set. The optimization was conducted for 1000 trials, and the best trial with respect to the training accuracy was chosen as the final model. We decided to employ TIES-Merging [\[63\]](#page-20-3) with DARE [\[65\]](#page-20-5) through preliminary experiments, and optimized its parameters.

In our DFS merging experiments,  $M = 64$ ,  $r = 3$ , and consequently,  $T = M \times r = 192$ . We kept the last 200 examples in the training data as our validation set and optimize on the rest of the data with a batch size of 200. We report the performance of the snapshot that achieved the highest accuracy in the validation set, and the test set is strictly isolated from the optimization process. We adopted CMA-ES in EvoJAX [\[59\]](#page-20-14), it optimized  $I$  and  $W$  for a total of 100 generations with a popluation size of 128, and we used the default hyper-parameters. We limited our DFS merging to two models A and B to ensure that the final model remains modest in size and can be run on a single GPU, but in principle, the methodology can scale to merging multiple models. During the merging, model A's tokenizer, and input/output embeddings are utilized. Furthermore, to maintain compatibility with the embedding layers, we mandate that the initial and final transformer layers of model A define the start and the end of the inference path. We initialized the indicator array  $I$  so that all layers in model A are more likely to be included as initial hops in the inference path to shorten the search time.

#### 3.1.2 Experimental Results

Table [1](#page-7-0) summarizes the performance of the LLMs on Japanese Math and overall Japanese benchmark tasks. The MGSM-JA column reports the results from the MGSM test set, employing the previously described metrics. The Japanese language model (Model 1) demonstrates limited mathematical proficiency, while the Math models (Models 2 and 3), though mathematically adept, show insufficient command of the Japanese language. Consequently, all three models score low scores on the MGSM-JA, with accuracy at or below 30.0.

Table 1: Performance Comparison of the LLMs. Models 1–3 are source models, Models 4–6 are our optimized merge models, and Models 7–11 are provided for reference. MGSM-JA measures math ability in Japanese, and JP-LMEH evaluates general Japanese language capabilities, with a detailed breakdown provided in Table [2.](#page-8-0)

<span id="page-7-0"></span>

| #  | Model                 | Type       | Size | MGSM-JA (acc ↑) | JP-LMEH (avg ↑) |
|----|-----------------------|------------|------|-----------------|-----------------|
| 0  | Mistral-7B-v0.1       | EN general | 7B   | 2.0             | 55.7            |
| 1  | Shisa Gamma 7B v1     | JA general | 7B   | 9.6             | 66.1            |
| 2  | WizardMath 7B v1.1    | EN math    | 7B   | 18.4            | 60.1            |
| 3  | Abel 7B 002           | EN math    | 7B   | 30.0            | 56.5            |
| 4  | Ours (PS)             | 1 + 2 + 3  | 7B   | 52.0            | 70.5            |
| 5  | Ours (DFS)            | 3 + 1      | 10B  | 36.4            | 53.2            |
| 6  | Ours (PS+DFS)         | 4 + 1      | 10B  | 55.2            | 66.2            |
| 7  | Llama 2 70B           | EN general | 70B  | 18.0            | 64.5            |
| 8  | Japanese StableLM 70B | JA general | 70B  | 17.2            | 68.3            |
| 9  | Swallow 70B           | JA general | 70B  | 13.6            | 71.5            |
| 10 | GPT-3.5               | commercial | -    | 50.4            | -               |
| 11 | GPT-4                 | commercial | -    | 78.8            | -               |

In contrast, our merged models (Models 4–6) manifest a substantial elevation in performance. Notably, the model merged in PS (Model 4) achieves an impressive score of 52.0, highlighting the remarkable potential in combining models with distinct expertises. The DFS-merged model (Model 5) also shows a performance enhancement, with an over 6 percent increase in accuracy compared to the source models. While the leap in performance is not as pronounced as with PS merging, it still proves to be a valid and orthogonal approach. Finally, our hybrid model (Model 6), which integrates both merging strategies, shows further enhancements on the task. The order of the source models in the indicator array  $I$  affects the performance of the DFS merging method. We conduct experiments with all possible combinations and report the best scores in Table [1,](#page-7-0) we defer detailed analysis to Sec [3.2.5.](#page-12-0)

Figure [2](#page-7-1) gives an overview of the five models' "answer sheet" on the math problems. Our merged models retain the foundational knowledge in the source models, as evidenced by the similar score patterns on problems 1–15. Moreover, they exhibit emergent capabilities, successfully tackling problems that stumped the source models (e.g., problems 20–30). Evidently, by effectively integrating a Japanese LLM and mathematical models, we have succeeded in producing models that are proficient in both Japanese language understanding and mathematical problem-solving.

![](_page_7_Figure_6.jpeg)

<span id="page-7-1"></span>Figure 2: Performance Overview. The figure depicts the success of various models on the MGSM-JA task, with each of the 250 test problems represented along the x-axis by problem ID. Correct answers are indicated by colored markers at the corresponding positions.

Furthermore, Table [2](#page-8-0) presents the results of evaluating the general Japanese language ability using the Japanese Language Model Evaluation Harness (JP-LMEH) benchmark suite. This benchmark suite consists of nine tasks, and the average score across these tasks is widely used as an indicator of overall Japanese language proficiency. Our models achieve remarkably high scores of 70.5 and 66.2, surpassing the source models and even the previous state-of-the-art 70B parameter Japanese LLM (specifically, Japanese StableLM 70B), despite having only 7B to 10B parameters.

Compared to the source Japanese model (shisa-gamma-7b-v1), it is interesting to note that there is not only a significant improvement in the MGSM scores but also improvements in other tasks such as JSQuAD and JAQKET. However, we also acknowledge that there are certain tasks, such as JNLI and MARC (for DFS), where performance has dropped. It should be noted that the MGSM scores here do not match those in Table [1,](#page-7-0) due to the differences in evaluation protocols (few-shot, prompting, etc.). Additional details are provided in Appendix [A,](#page-21-0) including extensive comparisons with other models in Table [6.](#page-22-0)

Moreover, Appendix [C](#page-21-1) showcases intriguing examples that demonstrate the utility of our models merged using evolution. The merged models correctly answered questions that require both knowledge about Japanese-culture *and* Math ability. In contrast, even if such Japanese questions were translated into English and answered in English, English math models would likely fail to provide the correct answers as they may not be aware of Japanese culture-specific context in the questions.

Table 2: Breakdown of JP-LMEH Scores for Japanese Language Proficiency. JP-LMEH (Japanese Language Model Evaluation Harness) is a benchmark suite consisting of 9 tasks, and the average score (Avg column) is used as an indicator of overall Japanese language proficiency. Full results are in Table [6](#page-22-0)

<span id="page-8-0"></span>

| Model                  | Size | JComQA | JNLI | MARC | JSQuAD | JAQKET | XLSum | XWino | MGSM | JCoLA | Avg  |
|------------------------|------|--------|------|------|--------|--------|-------|-------|------|-------|------|
| Shisa Gamma 7b v1      | 7B   | 91.2   | 72.1 | 94.6 | 73.9   | 68.0   | 25.9  | 80.5  | 29.6 | 58.7  | 66.1 |
| WizardMath 7B V1.1     | 7B   | 74.7   | 42.7 | 90.4 | 84.6   | 68.5   | 22.3  | 69.8  | 38.8 | 48.9  | 60.1 |
| Abel 7B 002            | 7B   | 70.3   | 51.8 | 62.3 | 83.8   | 69.0   | 22.5  | 68.2  | 28.0 | 52.7  | 56.5 |
| Ours (PS)              | 7B   | 89.1   | 65.7 | 95.4 | 89.5   | 77.7   | 25.5  | 81.2  | 50.0 | 60.5  | 70.5 |
| Ours (DFS)             | 10B  | 67.7   | 58.2 | 53.5 | 66.8   | 54.3   | 17.3  | 65.6  | 30.0 | 65.6  | 53.2 |
| Ours (PS+DFS)          | 10B  | 88.2   | 50.3 | 91.5 | 78.6   | 77.8   | 23.2  | 73.0  | 40.0 | 73.0  | 66.2 |
| Llama 2 70B            | 70B  | 80.2   | 53.4 | 94.4 | 91.6   | 80.1   | 21.8  | 73.6  | 30.4 | 54.6  | 64.5 |
| Japanese Stable LM 70B | 70B  | 91.2   | 50.4 | 92.9 | 87.1   | 88.4   | 24.3  | 82.0  | 37.2 | 61.7  | 68.3 |
| Swallow 70B            | 70B  | 95.3   | 57.2 | 91.7 | 94.1   | 93.9   | 23.1  | 83.3  | 45.2 | 59.5  | 71.5 |

#### 3.1.3 Analysis

In our exploration of model merging in the parameter space, we experimented with diverse configurations, such as varying the assignment of merging parameters across different layer groups. However, due to a constrained dataset, we did not witness notable improvements in performance correlating with increase in configuration complexity. Consequently, we focused our reporting on a PS merged model (Model 4 in Table [1\)](#page-7-0) that adopts the simplest setting: considering each source model as a singular layer and allocating two DARE-TIES associated parameters to each for evolutionary merging. Figure [3](#page-9-0) illustrates the evolved parameter configuration post PS merging.

The CMA-ES optimization results reveals that all three models are important, as suggested by the uniformity of the optimized weighting values. The fact that the sum of the weights exceeds 1 and is approaching 2, is noteworthy. This suggests that a combination method which amplifies the contributions of the models, rather than a simple interpolation, proved to be more effective.

The dominant density from the Japanese LLM suggests its critical contribution to solving the task. We conjecture that this may also be partially attributed to the Japanese LM's larger amount of fine-tunings from the Mistral base model. Japanese LMs based on English models, such as Shisa-Gamma-7B-v1, are typically created through a two-step process: continued pretraining and instruction fine-tuning. The continued pretraining phase involves learning from a significantly larger dataset compared to standard fine-tuning. For instance, Shisa-Gamma-7B-v1 is based on a Japanese base model that underwent continued pretraining on 100B tokens of Japanese text. The resulting differences in weights between Shisa-Gamma-7B-v1 and the original Mistral-7B-v1 likely encapsulate more information than standard fine-tuning, making them more challenging to sparsify. In line with the discussion in [\[65,](#page-20-5) Section 4.6], the sparsification of DARE tends to degrade performance when applied to such extensively fine-tuned models. Our evolutionary search has seemingly managed to address this issue by increasing the density for the Japanese LM.

The parameter settings derived from our PS merging experiments align well with outcomes from our DFS merging efforts. By incorporating the PS-merged model into our pool of source models and applying DFS merging across all potential pairings, we observed optimal performance with the combination of the PS-merged model and the Japanese language mode (Model 6 in Table [1\)](#page-7-0). This finding echoes the significant influence of the Japanese language model, as indicated by its notable presence in Figure [3,](#page-9-0) and reaffirms the substantial promise of evolutionary model merging.

![](_page_9_Figure_1.jpeg)

<span id="page-9-0"></span>Figure 3: Evolved Configurations for PS merging. Although the weights are similar across the 3 source models, the pronounced density from the Japanese LLM underscores its pivotal role.

Figure [4](#page-9-1) displays the evolution of the inference path, where our approach consistently recognized the value of initial steps, incorporating every layer from the first model (our PS-merged model), except for the last decoding layer and the embedding layer. As the process advanced, the method refined the selection to a smaller, more effective set of layers and strategically alternated between layers from both contributing models. Significantly, the scaling parameters  $W\_{ij}$  emerged as crucial elements, our ablation studies revealed that eliminating them in the evolved model (e.g., by setting  $W\_{ij} = 1$ ) led to a performance decline exceeding 20 percent, highlighting their importance in the model's efficacy.

![](_page_9_Figure_4.jpeg)

<span id="page-9-1"></span>Figure 4: Evolved Configurations for DFS Merging of models A and B. The 3 figures depict the evolution of the inference path on the MGSM-JA task. The y-axis represents the layer index  $l \in [1, M]$ , and the x-axis corresponds to the path index  $t \in [1, T]$ . Blue markers indicate path steps utilizing layers from model A, red markers denotes those from B. Marker color intensity reflects the magnitude of the scaling factor Wij . The evolutionary search result includes most layers in A at an early stage and then alternates between layers from both models. Result from our 10B model (PS+DFS).

#### 3.2 Method Behavior Analysis

In this section, we conduct a more in-depth analysis of our method's performance. We compare it with multiple types of baselines, experimentally analyze the effect of manual model selection, verify the scalability of our proposed approach to larger models, and show results from ablation studies.

#### 3.2.1 Comparison with Unoptimized Model Merging

Table [3](#page-10-0) presents the performance of models created through unoptimized model merging. We utilized three methods: TIES-Merge [\[63\]](#page-20-3), DARE-TIES [\[65\]](#page-20-5), and Frankenmerging [\[21\]](#page-18-0). We used mergekit to establish the baselines. Since merkit does not include default hyperparamters, we adopted those from a widely recognized resource for learning about model merging and mergekit [\[34\]](#page-19-0).

| Method                     | Size | MGSM-JA (acc ↑) | JP-LMEH (avg ↑) |
|----------------------------|------|-----------------|-----------------|
| Ours (PS)                  | 7B   | 52.0            | 70.5            |
| Ours (DFS)                 | 10B  | 36.4            | 53.2            |
| Ours (PS+DFS)              | 10B  | 55.2            | 66.2            |
| Unoptimized model merging: |      |                 |                 |
| TIES-Merge [63]            | 7B   | 4.4             | 63.7            |
| DARE-TIES [65]             | 7B   | 35.2            | 66.3            |
| Frankenmerging [21]        | 9B   | 0.0             | 16.1            |
| Fine-tuning:               |      |                 |                 |
| LoRA, Shisa Gamma 7B v1    | 7B   | 20.8            | 66.4            |
| LoRA, WizardMath 7B v1.1   | 7B   | 43.2            | 55.9            |
| LoRA, Abel 7B 002          | 7B   | 40.0            | 52.8            |
| Full, Shisa Gamma 7B v1    | 7B   | 19.2            | 54.5            |
| Full, WizardMath 7B v1.1   | 7B   | 33.6            | 43.4            |
| Full, Abel 7B 002          | 7B   | 36.4            | 37.5            |
| Distraction:               |      |                 |                 |
| +0 irrelevant model        | 7B   | 50.0            | 65.9            |
| +1 irrelevant model        | 7B   | 46.8            | 64.2            |
| +2 irrelevant model        | 7B   | 46.8            | 64.1            |
| +4 irrelevant model        | 7B   | 48.4            | 64.0            |
| +8 irrelevant model        | 7B   | 40.8            | 65.8            |

<span id="page-10-0"></span>Table 3: Performance Comparison of Proposed Methods and Baselines. Results show MGSM-JA accuracy and JP-LMEH average scores for our proposed methods (PS, DFS, PS+DFS), unoptimized model merging techniques, fine-tuning approaches, and experiments with irrelevant model distraction.

For the PS baseline equivalents (TIES-Merge and DARE-TIES), we used shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002 as the source models, allowing direct comparison with our PS experiments. Similarly, for the DFS baseline equivalent (Frankenmerging), we used the same combination of source models as in our DFS experiments, enabling direct comparisons.

The results indicate that TIES-Merge and Frankenmerging led to decreased performance in both MGSM-JA and JP-LMEH, suggesting unsuccessful merging. Notably, Frankenmerging resulted in a MGSM-JA score of 0, which is consistent with its general tendency to often produce models that fail to generate meaningful outputs. DARE-TIES performed relatively better among these baselines, with a slight improvement in MGSM-JA compared to the source models. However, our PS model significantly outperformed all these baselines, and the hybrid merging method (PS+DFS) gives the best test performance on the target task.

### 3.2.2 Comparison with Fine-tuning

Table [3](#page-10-0) also compares our evolutionary model merging approach with the fine-tuning method. To ensure a fair comparison, we used the identical dataset. Specifically, we utilized the 1069 samples of Japanese-translated GSM8k that were employed in our PS and DFS optimization for fine-tuning the models. We tested learning rates of 1e-5, 5e-5, and 1e-4, choosing the optimal one. We set the number of epochs to 3 and tried both LoRA and full parameter fine-tuning. We experimented with all three source models: shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002.

The MGSM-JA scores show that while fine-tuning generally improved performance over the original models, the highest score achieved was 43.2. This is notably lower than our proposed method, which exceeded a score of 50. Additionally, unlike our approach, fine-tuning often led to significant decreases in JP-LMEH scores. This suggests that fine-tuning may be more data-hungry and prone to catastrophic forgetting issues.

Other potential fine-tuning baselines. Another potential fine-tuning baseline would be combining all data used by all source models to fine-tune a base model. However, this approach is often impractical in general settings, and in our specific case, it is indeed impossible to implement for comparison. We elaborate on these points below. Our proposed approach's strength lies in its ability to freely combine openly published models from platforms like HuggingFace. However, model publishers often only release the models themselves, not the training datasets or detailed

training methods, as these are considered proprietary knowledge. For instance, the training data for WizardMath-7B-V1.1 and Abel-7B-002 is not publicly available. shisa-gamma-7b-v1 is a further fine-tuned version of japanese-stablelm-base-gamma-7b (which is Mistral-7B-v0.1 fine-tuned for Japanese), and the data used for training japanese-stablelm-base-gamma-7b is also not public. Moreover, the Japanese fine-tuning for japanese-stablelm-base-gamma-7b involved 100B tokens, indicating a substantial computational cost (such as more than 10,000 GPU hours). Therefore, a comparison with this approach is not feasible. It is also worth noting that such fine-tuning would require careful tuning of dataset mixing ratios, which is non-trivial and costly.

#### 3.2.3 Impact of Manual Model Selection

In our main experiments, we manually selected three source models. To assess the broader applicability of our approach, it is useful to investigate how sensitive the method is to the selection of source models. To this end, we conducted experiments to measure the distraction effect of including irrelevant models, which simulate the scenario where suboptimal models are inadvertently selected.

To investigate the sensitivity of our method to source model selection, we conducted experiments to measure the distraction effect of including irrelevant models. In addition to our original seed models (shisa-gamma-7b-v1, WizardMath-7B-V1.1, Abel-7B-002), we experimented with one, two, four, and eight irrelevant models, and performed PS experiments to examine the impact on performance. The irrelevant models, chosen for their lack of relation to Japanese or mathematics, were: leo-mistral-hessianai-7b (German), lince-mistral-7b-it-es (Spanish), Mistral-7B-v0.3-Chinese-Chat (Chinese), BioMistral-7B (biomedical domain), Claire-Mistral-7B-0.1 (French), komt-mistral-7b-v1 (Korean), Mistral-7B-v0.1-arabic (Arabic), and Loquace-7B-Mistral (Italy). All these models are openly available on HuggingFace.

We slightly modified the PS optimization by adding weight normalization, which became necessary due to the increased number of models (up to 11 in total). This modification explains why the "+0 irrelevant model" result does not exactly match our original PS result.

The results, shown in the "Distraction" part of Table [3,](#page-10-0) are generally favorable. As expected, using only the initial three models yielded the best results. However, the performance degradation when adding irrelevant models was limited, especially up to four additional models, and even with 8 additional models, the performance did not collapse. This demonstrates that our optimization method is relatively robust to manual source model selection. Due to the huge GPU memory requirement at inference time, the distraction experiments were not conducted for the DFS merging method. We instead present behavior analysis and ablation studies on the topic in the following sections.

### 3.2.4 Scaling to Larger Models

Finally, we demonstrate our proposed methods also work with larger models. Concretely, we used fine-tuned versions of Llama-2-13b: ELYZA-japanese-Llama-2-13b-instruct as the Japanese general model and MetaMath-13B-V1.0 as the English math model. All other settings remained identical to the 7B experiments. Due to the limited availability of suitable 13B models, we could only select two source models.

The results are shown in Table [4.](#page-12-1) Similar to the 7B case, the MGSM-JA score significantly improved over both source models. This consistent behavior at the 13B size demonstrates the scalability of our evolutionary model merging approach.

The 7B models achieved higher MGSM-JA scores (52.0, 36.4, 55.2) than the 13B models (31.2, 23.2, 34.0) for (PS, DFS, PS+DFS). This difference is attributed to the source models rather than our proposed method. Mistral-7B-v0.1 is known to outperform Llama-2-13b in performance, especially in basic mathematical abilities, despite its smaller size. This is reflected in the performance of math-specialized models: WizardMath-7B-V1.1 scores 83.2 on GSM8k (an English math benchmark), while MetaMath-13B-V1.0 scores 72.3. This difference explains the performance gap in Japanese math abilities between our merged 7B and 13B models.

On the other hand, our DFS and PS+DFS models (models #4 and #7) achieved higher JP-LMEH average scores than ELYZA-japanese-Llama-2-13b-instruct, the Japanese general model. In general, the qualitative behavior on JP-LMEH is consistent between 7B and 13B models. Both sets

<span id="page-12-1"></span>

| # | Model                               | Type       | Size | MGSM-JA (acc) | JP-LMEH (acc) |
|---|-------------------------------------|------------|------|---------------|---------------|
| 0 | Llama-2-13b                         | EN general | 13B  | 2.8           | 54.1          |
| 1 | ELYZA-japanese-Llama-2-13b-instruct | JA general | 13B  | 13.2          | 60.2          |
| 2 | MetaMath-13B-V1.0                   | EN math    | 13B  | 8.0           | 48.7          |
| 3 | Ours (PS)                           | 1 + 2      | 13B  | 31.2          | 59.7          |
| 4 | Ours (DFS)                          | 1 + 2      | 19B  | 12.8          | 61.1          |
| 5 | Ours (DFS)                          | 2 + 1      | 21B  | 23.2          | 46.6          |
| 6 | Ours (PS+DFS w/o W)                 | 3 + 1      | 19B  | 26.4          | 58.1          |
| 7 | Ours (PS+DFS)                       | 3 + 1      | 21B  | 34.0          | 60.4          |

Table 4: Scaling to 13B Models with ablation studies on DFS merging. Performance comparison and ablation study results on MGSM-JA and JP-LMEH tasks. Rows in gray are ablation studies.

of results show improvements in JSQuAD, JAQKET, and MGSM scores, with slight decreases in JNLI and MARC compared to the original Japanese model, indicating similar trends in task-specific score changes (see Table [6\)](#page-22-0).

#### <span id="page-12-0"></span>3.2.5 Analysis on DFS Merging

We included the results from two ablation studies for DFS merging in Table [4,](#page-12-1) and we wish to get insights as to how and why DFS works.

Comparing models #4 and #5, we show that the order of the source models in DFS merging plays a critical role. Specifically, if we put the layers from the English math model in front of those from the Japanese general model in the indicator array  $\mathcal{I}$  (model #4), the accuracy drops by more than 10 points than the case where the layers orders are reversed (model #5). This "sub-optimal" behavior is a trade-off between flexibility and search efficiency. In our experiments, we initialize the indicator array  $\mathcal{I}$  with zeros, but set the values of those corresponding to layers in model A in the first repetition at  $2\sigma$  where  $\sigma$  is the initial standard deviation of CMA-ES exploration (recall that in DFS, layers from model A come before those from model B in each repetition). In this way, the merged model behaves just like model A at the beginning of the DFS process, and is allowed to gradually add/remove layers if necessary. Comparing with the other scenario where we initialize I with only zeros, this setting effectively avoids the cases where poor stacking of the layers causes the LLM to output complete nonsense and waste explorations, at the cost of losing the flexibility to automatically swap the order of models in  $\mathcal{I}$ .

In our second ablation study, the difference between models #6 and #7 highlights the importance of the inclusion of the scaling matrix  $W$ . Without scaling, the performance of model #6 is even worse than the PS-merged model, one of its source models. Sufficient as it is in this setting, we wish to point out that this simple scaling treatment does not work if any of the source models has been fine-tuned for a significantly longer time. We suspect that prolonged fine-tuning might lead to increased incompatibility for data traversing through layers from different models. This could be due to permutations in the weight matrices. For example, assuming we are organizing the 4096 dimensions of the hidden states by their absolute values, data from layer 1 in model A might look like (1, 2, ... 4096), but in model B, after extended fine-tuning, it might appear as (4001, 133, 3, ... 989) vs (1.1, 1.9, ..., 4097) when it was only slightly fine-tuned.

While it is hard to understand thoroughly why DFS merging helped improve performance, we conducted analysis of the merged model (model #5 in Table [4\)](#page-12-1) to get insights. Figure [5](#page-13-1) shows the final hops and scales configuration from the merged model. It is important to notice that we forced the scaling factor  $W\_{i,j} = 1$  if  $j = i + 1$  and layers  $i, j$  are from the same model. This is to prevent the possibility of introducing extra modeling power via  $W$ , in this manner we are certain that all the performance boost comes from layer stacking and permutation. It is interesting and surprising to notice that the DFS-merged model decides to skip layer #30 from MetaMath-13B-V1.0 at the beginning of the inference path. Based on this finding, we removed layer #30 from MetaMath-13B-V1.0, and without any further modifications we found the performance increased to 10%. This implies that DFS merging works because it is able to identify the redundant/harmful operations (with respect to a specific task) embedded in certain layers, and performance increases once these layers are removed.

![](_page_13_Figure_0.jpeg)

<span id="page-13-1"></span>Figure 5: Evolved Configurations from A DFS-merged 13B model. The style and settings of this plot is identical to that of Figure [4.](#page-9-1) Surprisingly, DFS decides to skip layer #30 from the original model (model #2 in Table [4\)](#page-12-1). If we simply excluded that layer from model #2 and without any further modifications, its performance on MGSM-JA would have increased to 10%. This configuration is from model #5 in Table [4.](#page-12-1)

Furthermore, we analyzed the problems where our DFS-merged model answered correctly while the math source model didn't, and we identify mainly two scenarios where DFS helped improve the performance, see Figure [6](#page-13-2) for illustrations from these scenarios. In the first scenario, we found that the English math model was able to reason correctly. However, it outputs the reasoning steps or answers in English, violating the request that the response must be written in Japanese. In this scenario, DFS-merging managed to shift the output tokens' distribution toward Japanese vocabularies, fixing the problem. In the second scenario, the English math model was actually reasoning in Japanese. But due to the error in understanding the Japanese problem or in the reasoning, it was not able to give correct answers. DFS-merging, due to its appending of the layers from the Japanese general model, was able to largely avoid the problem. Unlike the "subtraction" it did previously, these scenarios suggest DFS also works because it is capable of "addition" – finding what is missing in one of the source models and compensate that by adding layers in the inference path from other models.

| Scenario 1                                                                                                                                                                                                                                                                                  | Scenario 2                                                                                                                                                                       | Scenario 2 (EN Translation)                                                                                                                                                                                                                                                                                                                                                   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための<br>回答を日本語で記述してください。一歩一歩考えましょう。                                                                                                                                                                                                                       | 以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための<br>回答を日本語で記述してください。一歩一歩考えましょう。                                                                                                            | Below are instructions describing a certain task. Please write your response<br>in Japanese to complete the request. Let's think step by step.                                                                                                                                                                                                                                |
| ### 指示:<br>ビリーはDVDを売ります。彼は火曜日に8人のお客様がいます。彼の最初の3人のお客<br>様はそれぞれ1枚のDVDを購入します。彼の翌日の2人の顧客は2枚のDVDをそれぞれ<br>購入しました。の最後の3人のお客様はDVDを購入しません。ビリーは火曜日に何枚<br>のDVDを売りましたか?                                                                                                                                 | ### 指示:<br>ジョーンズおばあちゃんは消防士の昼食会のためにアップルパイを5枚焼きました。<br>それぞれのパイを8個に切り分け、ゲストが自分たちで取れるよう、その5枚のパイを<br>ブッフェテーブルに置きました。ゲストがそれぞれのパイを取り、食べた後、夜更け<br>には14個のパイが残っていました。ゲストは何個のパイを食べましたか?     | ### Instruction:<br>Grandam Jones baked 5 apple pies for the firefighters' luncheon. She cut<br>each pie into 8 pieces and placed the 5 pies on the buffet table. After each<br>guest took and ate some pie, there were 14 pieces of pie left at the end of<br>the night. How many pieces of pie did the guests eat?                                                          |
| ### 応答:                                                                                                                                                                                                                                                                                     | ### 応答:<br>Question                                                                                                                                                              | ### Response:                                                                                                                                                                                                                                                                                                                                                                 |
| ビリーの最初の3人のお客様は1枚のDVDを購入しました。<br>So, 3 people x 1 DVD = 3 DVDs were sold.<br>His next day's 2 people of customers bought 2 DVDs each.<br>So, 2 people x 2 DVDs = 4 DVDs were sold.<br>In total, 3 DVDs + 4 DVDs = 7 DVDs were sold.<br>Therefore, Billy sold 7 DVDs on Tuesday.              | ジョーンズおばあちゃんは5枚のパイを8個に切り分け、ブッフェテーブルに置いた。<br>ゲストがそれぞれのパイを取り、食べた後、14個のパイが残っていました.<br>この14個のパイは、5枚のパイを8個に切り分けた8個のパイ + 14個 - 8個 = 14個のパ<br>イです.<br>ゲストは、8個 + 14個 - 8個 = 14個のパイを食べました. | Grandma Jones cut 5 pies into 8 pieces each and placed them on the buffet<br>table. After each guest took and ate some pie, 14 pieces of pie were left.<br>These 14 pieces are from the 5 pies cut into 8 pieces each, so the<br>calculations goes: 8 pieces per pie + 14 pieces - 8 pieces = 14 pieces.<br>Thus, the guests ate 8 pieces + 14 pieces - 8 pieces = 14 pieces. |
| #### 7<br>The answer is: 7                                                                                                                                                                                                                                                                  | #### 14<br>The answer is: 14<br>MetaMath-13B-V1.0                                                                                                                                | #### 14<br>The answer is: 14                                                                                                                                                                                                                                                                                                                                                  |
| ビリーの最初の3人のお客様はそれぞれ1枚のDVDを購入しました。<br>そのため, ビリーは最初の3人のお客様に対して, 1枚 x 3 = 3枚のDVDを売りました.<br>次の2人の顧客は2枚のDVDをそれぞれ購入しました.\n\nそのため, ビリーは次の2人<br>の顧客に対して, 2枚 x 2 = 4枚のDVDを売りました.<br>最後の3人のお客様はDVDを購入しません.<br>そのため, ビリーは最後の3人のお客様に対して, 0枚のDVDを売りません.<br>また, ビリーはその日のお客様に対して, 3 + 4 + 0 = 7枚のDVDを売りました. | ジョーンズおばあちゃんは5枚のパイを8個に切り分けた。<br>このため、5枚のパイは8個 x 5 = 40個になりました。<br>ゲストがそれぞれのパイを取り、食べた後、14個のパイが残っていました。<br>このため, ゲストは40個 - 14個 = 26個のパイを食べました。<br>#### 26                           | Grandma Jones cut 5 pies into 8 pieces each.<br>Therefore, there were 8 pieces x 5 = 40 pieces in total.<br>After the guests had their share, 14 pieces were left.<br>Consequently, the guests ate 40 pieces - 14 pieces = 26 pieces.<br>#### 26                                                                                                                              |
| #### 7<br>The answer is: 7                                                                                                                                                                                                                                                                  | The answer is: 26                                                                                                                                                                | The answer is: 26<br>Ours (DFS)                                                                                                                                                                                                                                                                                                                                               |

<span id="page-13-2"></span>Figure 6: Why DFS Improves Performance. DFS merged model improves performance on MGSM-JA mainly by shifting the output tokens' distribution to favor Japanese (scenario 1) and helping the LLM better understand the problem (scenario 2). We show the raw outputs from models #2 and #5 in Table [4,](#page-12-1) but have added/removed redundant lines for the purpose of visualization.

#### <span id="page-13-0"></span>3.3 Evolving Japanese VLM

#### 3.3.1 Multi-modality Extension

We now extend our method to multi-modal models, and evolve a culturally-specific content aware Japanese VLM. VLMs have recently shown remarkable progress by applying the powerful instructionfollowing capabilities of pre-trained LLMs. The architecture of a VLM generally consists of three components: (1) A vision encoder to extract image features; (2) An LLM to generate text (for the purpose of describing an image); and (3) A projection network to map image features into the LLM's

embedding space [\[11,](#page-18-15) [15,](#page-18-16) [35,](#page-19-18) [37,](#page-19-19) [39\]](#page-19-20). Crucially, the LLM component is initialized with powerful pre-trained LLMs for their text generation capabilities. During training, the projection network and optionally the LLM are trained on various vision-language datasets, while the vision encoder is fixed.

#### 3.3.2 Experimental Setup

Source Models The LLM component inside a VLM can be regarded as a standalone LLM, with the extra capability of understanding visual *soft prompts*. From this perspective, by fixing the vision encoder and the projection network and only focusing on the LLM component, it is straightforward to apply the methodologies detailed in Section [2](#page-4-1) to produce a new LLM with expanded capabilities.

In this experiment, we merge a Japanese LLM and the LLM component in a VLM in the parameter space. We select shisa-gamma-7b-v1 [\[9\]](#page-18-11) as the Japanese LLM and LLaVA-1.6-Mistral-7B [\[38\]](#page-19-21) as the VLM. Both models are fine-tunes of the Mistral-7B-v0.1 [\[28\]](#page-19-11) base model.

Dataset To the best of our knowledge, publically accessible Japanese VLM datasets are scarce. In response, we created a new open Japanese VLM benchmark and assessed our VLM on a widely recognized Japanese VQA dataset. Our new benchmark dataset consists of:

- JA-VG-VQA-500: A 500-sample test set extracted from the Japanese Visual Genome VQA dataset [\[53\]](#page-20-15).
- JA-VLM-Bench-In-the-Wild: A Japanese version of LLaVA-Bench-In-the-Wild [\[39\]](#page-19-20) . We compiled a rich collection of 42 images, accompanied by a total of 50 questions, featuring a variety of Japanese cultural elements and objects found in Japan. The QAs were crafted with the assistance of GPT-4V [\[1\]](#page-18-17) and underwent a human-in-the-loop filtering process to eliminate nonsensical outcomes. Compared to the JA-VG-VQA-500 dataset, our set poses more complex challenges, demanding more nuanced and detailed responses.

We used another subset of the Japanese Visual Genome VQA dataset during the evolutionary search. This subset is not overlapped with examples in the JA-VG-VQA-500 dataset, to avoid leakage in the optimization process.

The images in the JA-VLM-Bench-In-the-Wild dataset, which predominantly represent Japanese content, were carefully selected by native Japanese speakers to minimize the risk of insensitive or biased representations. All images were sourced from Unsplash and are published under the Unsplash license, which ensures that there are no ethical or legal issues with their use. Unsplash contributors agree to obtain necessary permissions from related individuals in their photos before uploading, which addresses potential concerns regarding the use of images containing recognizable humans. Additionally, the captions were generated using GPT-4-V and were meticulously reviewed by human annotators to ensure accuracy and cultural sensitivity. This process aims to create a dataset that is both ethically sourced and culturally appropriate.

Evaluation We consider two baselines in our experiments: LLaVA-1.6-Mistral-7B [\[38\]](#page-19-21), one of our source models, and Japanese Stable VLM [\[54\]](#page-20-16) a Japanese VLM trained from scratch on Japanese datasets.

All models adopt the same generation configurations, with deterministic decoding. We compute ROUGE-L with a Japanese language detector to replace non-Japanese responses with empty texts, resulting in a score of zero for non-Japanese responses. To be consistent with our LLM experiments in Section [3.1,](#page-6-0) we also employed fasttext [\[29,](#page-19-16) [30\]](#page-19-17) for this language detection task. However, we made an exception for cases where the ground-truth answer itself contains non-Japanese but commonly seen words in Japanese texts (e.g., a widely recognized acronym such as "UFO"). In these instances, non-Japanese responses from models are not converted to empty texts.

Optimization We use the identical settings as the earlier LLM merging experiments in Section [3.1.](#page-6-0) Concretely, we use TIES-Merging with DARE for merging the source models in the parameter space. For merging in the data flow space, we treat LlaVa 1.6 Mistral 7B as our model A and shisa-gamma-7b-v1 as model B. For PS+DFS, our PS-merged model is model A and shisa-gamma-7b-v1 is model B.

### 3.3.3 Experimental Results

Table [5](#page-15-0) compares the performance of our VLM with the baselines. Please note that the Japanese Stable VLM cannot be evaluated on JA-VG-VQA-500 because it was trained on this dataset.

Our merged VLMs' enhanced performance on the JA-VG-VQA-500 benchmark indicates their proficiency in Japanese, highlighting the successful integration of the source Japanese LLM with the LLM component of the original VLM through evolutionary merging. Consistent with the findings we show in the previous discussions, simple merging without evolutionary search do not give strong performance as ours, see the last 3 rows in Table [5.](#page-15-0) Furthermore, our models' superior results on the JA-VLM-Bench-In-the-Wild compared to both baselines exhibits its adeptness at navigating culturally-specific content.

Besides the quantitative results in Table [5,](#page-15-0) we qualitatively compare our VLM with the baseline models in Appendix [C.](#page-21-1) Our evolved model is able to handle Japanese culture-specific content remarkably well, generally producing more detailed responses with correct information.

Table 5: Performance Comparison of the VLMs. LLaVA 1.6 Mistral 7B is the source VLM and Japanese Stable VLM is an open-sourced Japanese VLM. While JA-VG-VQA-500 measures general VQA abilities in Japanese, JA-VLM-Bench-In-the-Wild evaluates the model's handling of complex VQA tasks within Japanese cultural contexts.

<span id="page-15-0"></span>

| Model                | Size | JA-VG-VQA-500<br>(ROUGE-L ↑) | JA-VLM-Bench-In-the-Wild<br>(ROUGE-L ↑) |
|----------------------|------|------------------------------|-----------------------------------------|
| LLaVA 1.6 Mistral 7B | 8B   | 14.3                         | 41.1                                    |
| Japanese Stable VLM  | 8B   | -                            | 40.5                                    |
| Ours (PS)            | 8B   | 19.7                         | 51.2                                    |
| Ours (DFS)           | 12B  | 16.8                         | 46.5                                    |
| Ours (PS+DFS)        | 11B  | 20.4                         | 47.6                                    |
| TIES [63]            | 8B   | 16.0                         | 46.5                                    |
| DARE-TIES [65]       | 8B   | 9.4                          | 36.0                                    |
| Passthrough [21]     | 9B   | 7.3                          | 26.7                                    |

## 4 Discussion

In this report, we propose a general method that uses evolutionary techniques to efficiently discover the best ways to combine different models from the vast ocean of different open-source models with diverse capabilities. By working with the vast collective intelligence of existing open models, our method is able to automatically create new foundation models with desired capabilities specified by the user. We find that our approach is able to automatically discover novel ways to merge different models from vastly different domains (e.g., non-English language and Math, or non-English language and Vision), in non-trivial ways that might be difficult for human experts to discover themselves.

To test our approach, we apply our method to automatically create a Japanese LLM capable of Math reasoning, and a culturally-specific content aware Japanese VLM. Surprisingly, we find that both models achieve state-of-the-art results on several LLM and Vision benchmarks, while not being explicitly optimized to be good at these benchmarks, attaining the top performance on a vast array of other Japanese LLM benchmarks, even exceeding the performance of some previous SOTA 70B parameter Japanese LLMs.

With these promising initial results, we believe we are just scratching the surface of unlocking the full capabilities of evolutionary model merging, and this is the inception of a long-term development of applying evolutionary principles to foundation model development.

Currently, we are already achieving promising results in applying evolutionary model merging to image diffusion models, enabling the creation of high performance cross-domain image generation models by merging existing building blocks in novel ways discovered by evolution.

The method currently requires the user to select a set of source models to use as ingredients for evolutionary search. We believe it is also possible to leverage evolution to search for candidate source models from a vast population of existing models as well. In addition to model selection, we are also exploring using evolution to produce swarms of diverse foundation models each with

its own niche and behaviors. This holds the potential of enabling the emergence of a collective intelligence consisting of a swarm of models capable of self-improvement by continuously producing new complementary internal models of the world through interaction.

Related to our work is an experiment, called Automerger [\[33\]](#page-19-22), released at around the same time as this work. This interesting experiment works by selecting two random models from the top 20 models on the Open LLM Leaderboard [\[26\]](#page-19-1) and randomly apply SLERP [\[60\]](#page-20-2) or DARE-TIES [\[63,](#page-20-3) [65\]](#page-20-5) to create new models. Over time, some of these models will do well, or even better on the benchmark tasks that define this leaderboard, becoming part of the leaderboard. We predict this approach will lead to combinations of the merged models that overfit to the benchmark tasks defined on the leaderboard. The author acknowledged that the idea behind this project was less about creating better models, but more about getting more metrics to help derive a more principled approach to model merging.

Our work takes an orthogonal approach of optimizing for tasks outside of the domain specified by the original leaderboard [\[26\]](#page-19-1), rather than being confined by it. As we have shown, surprisingly, stepping away from optimizing for a particular benchmark occasionally results in even greater generalization to numerous other benchmark tasks that we had not intended to optimize for, and such emergent generalization might be the key to unlocking the next great advancements in AI.

The ability to evolve new models with new emergent capabilities, from a large variety of existing, diverse models with various capabilities have important implications. With the rising costs and resource requirement for training foundation models, by leveraging the rich variety of foundation models in the rich open-source ecosystem, large institutions or governments may consider the cheaper evolutionary approach for developing proof-of-concept prototype models quickly, before committing substantial capital or tapping into the nation's resources to develop entirely custom models from scratch, if that is even needed at all.

Further Applications and Impact. After the release of the preprint version of this paper, researchers have explored evolutionary model merging in different domains, highlighting the method's versatility and effectiveness. A notable example is EvoSDXL [\[2\]](#page-18-18), which applied evolutionary model merging to diffusion image generation models. This proves that our method works well not just for LLMs and VLMs, but for other types of models as well. Moreover, what makes EvoSDXL particularly interesting is its success in merging SDXL-Lightning [\[36\]](#page-19-23) with other standard SDXL fine-tunes. SDXL-Lightning is a specialized variant of SDXL that employs an adversarial loss during training, enabling rapid image generation in just a few steps, compared to the 50 or 100 steps typically required by standard diffusion models. The evolutionary model merging technique effectively combined this unique model with conventional SDXL fine-tunes, despite the different protocols used in their development. This success illustrates that our method is capable of integrating models created through varying protocols, combining their strengths to create more robust and powerful models. Additionally, other unique models such as EvoVLM-JP-v2 [\[4\]](#page-18-19) and EvoUkiyoe [\[3\]](#page-18-20) have also been developed using evolutionary model merging, further demonstrating the method's potential and adaptability. Moreover, after the publication of the preprint, evolutionary model merging was implemented in two famous open-source software packages, MergeKit [\[21\]](#page-18-0) and Optuna Hub [\[8\]](#page-18-14). It has become widely available to many people, is being used practically, and further possibilities are being explored.

Limitations. We acknowledge that although our evolutionary model merging effectively integrates diverse expertise from the source models, it also inherits their limitations. For instance, we encountered instances where the merged models produced responses that lacked logical coherence. Additionally, this study does not encompass instruction fine-tuning or alignment, raising the potential for the models to yield outputs that may be factually flawed.

Author Contributions. Takuya Akiba initiated the "Evolutionary Optimization of Model Merging Recipes" project, wrote the project design document, and initiated the parameter space model merging experiments, laying the groundwork for the methodology. Makoto Shing expanded the parameter space model merging to encompass vision-language models and diffusion models. Yujin Tang directed the efforts in data flow space model merging by incorporating ideas and methods inspired from the neural architecture search and morphology evolution literature, establishing foundational methods in this domain and in hybrid merging strategies. Qi Sun contributed to the implementation of our parameter space model merging framework and assisted in model evaluation. David Ha provided overarching guidance for the research project, offering technical insight, advice, feedback and writing. Ethical and Societal Impact. Evolutionary model merging offers significant positive societal impacts by enabling the creation of small yet highly capable models at lower costs. This approach democratizes access to advanced AI capabilities, potentially reducing the environmental footprint of AI development and deployment. By efficiently combining existing models, it can lead to more accessible and versatile AI solutions, particularly benefiting regions and languages with limited resources. However, as with other model development techniques, this approach may present certain considerations. The combination of diverse models could potentially lead to unexpected behaviors or biases, and the complexity of merged models might affect their interpretability. While these challenges are common in AI development, they underscore the importance of continued research and evaluation. It's worth noting that the models and techniques presented in this work serve primarily as a proof of concept. For applications in mission-critical scenarios or models intended for wide public use, further verification and refinement of the methodology may be necessary. This ongoing improvement process is crucial for realizing the full potential of evolutionary model merging while ensuring responsible and ethical deployment.

## Data Availability Statement

All datasets used in this paper are publicly available. Detailed sources are provided below:

- LLM Experiments:
	- Data for Optimization: Available on Hugging Face at [https://huggingface.co/](https://huggingface.co/datasets/SakanaAI/gsm8k-ja-test_250-1319) [datasets/SakanaAI/gsm8k-ja-test\\_250-1319](https://huggingface.co/datasets/SakanaAI/gsm8k-ja-test_250-1319).
	- Data for Tests: Available on GitHub at [https://github.com/openai/](https://github.com/openai/grade-school-math) [grade-school-math](https://github.com/openai/grade-school-math).
- VLM Experiments:
	- VLM Benchmark Datasets:
		- \* JA-VG-VQA-500: Available on Hugging Face at [https://huggingface.co/](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) [datasets/SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500).
		- \* JA-VLM-Bench-In-the-Wild: Available on Hugging Face at [https://](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) [huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild).
	- VLM Training Data for Optimization: This subset of the Japanese Visual Genome VQA dataset is also accessible on the JA-VG-VQA-500 page under the train split.

For detailed references to these datasets within our paper, see Section [3.1](#page-6-0) for LLM experiments and Section [3.3](#page-13-0) for VLM experiments.

## References

- <span id="page-18-17"></span>[1] Open AI. 2023. GPT-4V(ision) System Card. [https://cdn.openai.com/papers/GPTV\\_System\\_](https://cdn.openai.com/papers/GPTV_System_Card.pdf) [Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf)
- <span id="page-18-18"></span>[2] Sakana AI. 2024. EvoSDXL-JP-v1. <https://sakana.ai/evosdxl-jp/>.
- <span id="page-18-20"></span>[3] Sakana AI. 2024. EvoUkiyoe. <https://sakana.ai/evo-ukiyoe/>.
- <span id="page-18-19"></span>[4] Sakana AI. 2024. EvoVLM-JP-v2. <https://sakana.ai/evovlm-jp/>.
- <span id="page-18-23"></span>[5] Stability AI. 2024. Japanese Stable LM Beta. [https://ja.stability.ai/blog/](https://ja.stability.ai/blog/japanese-stable-lm-beta) [japanese-stable-lm-beta](https://ja.stability.ai/blog/japanese-stable-lm-beta).
- <span id="page-18-21"></span>[6] Stability AI. 2024. JP Language Model Evaluation Harness. [https://github.com/Stability-AI/](https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable) [lm-evaluation-harness/tree/jp-stable](https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable).
- <span id="page-18-5"></span>[7] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. 2023. Jointly training large autoregressive multimodal models. *arXiv preprint arXiv:2309.15564* (2023).
- <span id="page-18-14"></span>[8] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining* (Anchorage, AK, USA) *(KDD '19)*. Association for Computing Machinery, New York, NY, USA, 2623–2631. [https://doi.org/10.1145/](https://doi.org/10.1145/3292500.3330701) [3292500.3330701](https://doi.org/10.1145/3292500.3330701)
- <span id="page-18-11"></span>[9] augmxnt. 2023. *shisa-gamma-7b*. HuggingFace. <https://hf.co/augmxnt/shisa-gamma-7b-v1>
- <span id="page-18-4"></span>[10] AUTOMATIC1111. 2022. Stable Diffusion WebUI. [https://github.com/AUTOMATIC1111/](https://github.com/AUTOMATIC1111/stable-diffusion-webui) [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui).
- <span id="page-18-15"></span>[11] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966 [cs.CV]
- <span id="page-18-12"></span>[12] Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu. 2023. Generative AI for Math: Abel. <https://github.com/GAIR-NLP/abel>.
- <span id="page-18-13"></span>[13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. *CoRR* abs/2110.14168 (2021). arXiv:2110.14168 <https://arxiv.org/abs/2110.14168>
- <span id="page-18-3"></span>[14] Nico Daheim, Thomas Möllenhoff, Edoardo Ponti, Iryna Gurevych, and Mohammad Emtiyaz Khan. 2024. Model Merging by Uncertainty-Based Gradient Matching. In *The Twelfth International Conference on Learning Representations*. <https://openreview.net/forum?id=D7KJmfEDQP>
- <span id="page-18-16"></span>[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv:2305.06500 [cs.CV]
- <span id="page-18-10"></span>[16] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. *IEEE transactions on evolutionary computation* 6, 2 (2002), 182–197.
- <span id="page-18-1"></span>[17] Gintare Karolina Dziugaite and Daniel M Roy. 2017. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. *arXiv preprint arXiv:1703.11008* (2017).
- <span id="page-18-6"></span>[18] Adam Gaier and David Ha. 2019. Weight agnostic neural networks. *Advances in neural information processing systems* 32 (2019).
- <span id="page-18-22"></span>[19] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. <https://doi.org/10.5281/zenodo.10256836>
- <span id="page-18-9"></span>[20] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. *arXiv preprint arXiv:2203.14680* (2022).
- <span id="page-18-0"></span>[21] Charles O. Goddard. 2024. mergekit. <https://github.com/arcee-ai/mergekit>
- <span id="page-18-7"></span>[22] David Ha, Andrew Dai, and Quoc V Le. 2016. Hypernetworks. *arXiv preprint arXiv:1609.09106* (2016).
- <span id="page-18-8"></span>[23] Nikolaus Hansen. 2006. The CMA evolution strategy: a comparing review. *Towards a new evolutionary computation: Advances in the estimation of distribution algorithms* (2006), 75–102.
- <span id="page-18-2"></span>[24] Sepp Hochreiter and Jürgen Schmidhuber. 1994. Simplifying neural nets by discovering flat minima. *Advances in neural information processing systems* 7 (1994).
- <span id="page-19-8"></span>[25] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. *Neural computation* 9, 1 (1997), 1–42.
- <span id="page-19-1"></span>[26] HuggingFace. 2023. *Open LLM Leaderboard*. HuggingFace. [https://huggingface.co/spaces/](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) [HuggingFaceH4/open\\_llm\\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- <span id="page-19-10"></span>[27] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. *arXiv preprint arXiv:2212.04089* (2022).
- <span id="page-19-11"></span>[28] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
- <span id="page-19-16"></span>[29] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. FastText.zip: Compressing text classification models. *arXiv preprint arXiv:1612.03651* (2016).
- <span id="page-19-17"></span>[30] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efficient Text Classification. *arXiv preprint arXiv:1607.01759* (2016).
- <span id="page-19-5"></span>[31] Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. 2022. When do flat minima optimizers work? *Advances in Neural Information Processing Systems* 35 (2022), 16577–16595.
- <span id="page-19-6"></span>[32] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In *International Conference on Learning Representations*. <https://openreview.net/forum?id=H1oyRlYgg>
- <span id="page-19-22"></span>[33] Maxime Labonne. 2024. Automerger Experiment. *Tweet Thread* (2024). [https://twitter.com/](https://twitter.com/maximelabonne/status/1767124527551549860) [maximelabonne/status/1767124527551549860](https://twitter.com/maximelabonne/status/1767124527551549860)
- <span id="page-19-0"></span>[34] Maxime Labonne. 2024. Merge Large Language Models with mergekit. *Hugging Face Blog* (2024). <https://huggingface.co/blog/mlabonne/merge-models>
- <span id="page-19-18"></span>[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. arXiv:2301.12597 [cs.CV]
- <span id="page-19-23"></span>[36] Shanchuan Lin, Anran Wang, and Xiao Yang. 2024. SDXL-Lightning: Progressive Adversarial Diffusion Distillation. *CoRR* abs/2402.13929 (2024). <https://doi.org/10.48550/ARXIV.2402.13929> arXiv:2402.13929
- <span id="page-19-19"></span>[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744 [cs.CV]
- <span id="page-19-21"></span>[38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. [https://llava-vl.github.io/](https://llava-vl.github.io/blog/2024-01-30-llava-next/) [blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
- <span id="page-19-20"></span>[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV]
- <span id="page-19-15"></span>[40] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. *CoRR* abs/2308.09583 (2023). [https:](https://doi.org/10.48550/ARXIV.2308.09583) [//doi.org/10.48550/ARXIV.2308.09583](https://doi.org/10.48550/ARXIV.2308.09583) arXiv:2308.09583
- <span id="page-19-9"></span>[41] Michael S Matena and Colin A Raffel. 2022. Merging models with fisher-weighted averaging. *Advances in Neural Information Processing Systems* 35 (2022), 17703–17716.
- <span id="page-19-13"></span>[42] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. *Advances in Neural Information Processing Systems* 35 (2022), 17359–17372.
- <span id="page-19-14"></span>[43] nostalgebraist. 2021. Interpreting GPT: The Logit Lens. [https://www.lesswrong.com/posts/](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) [AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). Accessed: 2024-03-08.
- <span id="page-19-7"></span>[44] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley. 2021. Relative Flatness and Generalization. In *Advances in Neural Information Processing Systems*, A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (Eds.). [https://openreview.net/forum?id=](https://openreview.net/forum?id=sygvo7ctb_) [sygvo7ctb\\_](https://openreview.net/forum?id=sygvo7ctb_)
- <span id="page-19-3"></span>[45] Colin Raffel. 2021. A call to build models like we build open-source software. [https://colinraffel.](https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html) [com/blog/a-call-to-build-models-like-we-build-open-source-software.html](https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html).
- <span id="page-19-4"></span>[46] Colin Raffel. 2023. Building machine learning models like open source software. *Commun. ACM* 66, 2 (2023), 38–40.
- <span id="page-19-2"></span>[47] Eric Raymond. 1999. The cathedral and the bazaar. *Knowledge, Technology & Policy* 12, 3 (1999), 23–49.
- <span id="page-19-12"></span>[48] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized evolution for image classifier architecture search. In *Proceedings of the aaai conference on artificial intelligence*, Vol. 33. 4780–4789.
- <span id="page-20-17"></span>[49] rinna. 2024. LM Benchmark. [https://rinnakk.github.io/research/benchmarks/lm/index.](https://rinnakk.github.io/research/benchmarks/lm/index.html) [html](https://rinnakk.github.io/research/benchmarks/lm/index.html).
- <span id="page-20-1"></span>[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 10684–10695.
- <span id="page-20-12"></span>[51] Jürgen Schmidhuber. 1992. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. *Neural Computation* 4, 1 (1992), 131–139.
- <span id="page-20-13"></span>[52] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In *The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net. [https://openreview.net/](https://openreview.net/pdf?id=fR3wGCk-IXp) [pdf?id=fR3wGCk-IXp](https://openreview.net/pdf?id=fR3wGCk-IXp)
- <span id="page-20-15"></span>[53] Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. 2018. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps. In *Proceedings of the 27th International Conference on Computational Linguistics* (Santa Fe, New Mexico, USA). Association for Computational Linguistics, 1918–1928. <http://aclweb.org/anthology/C18-1163>
- <span id="page-20-16"></span>[54] Makoto Shing and Takuya Akiba. 2023. Japanese Stable VLM. [https://huggingface.co/](https://huggingface.co/stabilityai/japanese-stable-vlm) [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
- <span id="page-20-8"></span>[55] David So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In *International conference on machine learning*. PMLR, 5877–5886.
- <span id="page-20-9"></span>[56] Kenneth O Stanley and Risto Miikkulainen. 2002. Evolving neural networks through augmenting topologies. *Evolutionary computation* 10, 2 (2002), 99–127.
- <span id="page-20-11"></span>[57] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. 2024. Transformer Layers as Painters. *arXiv preprint arXiv:2407.09298* (2024).
- <span id="page-20-6"></span>[58] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. 2023. An empirical study of multimodal model merging. *arXiv preprint arXiv:2304.14933* (2023).
- <span id="page-20-14"></span>[59] Yujin Tang, Yingtao Tian, and David Ha. 2022. EvoJAX: Hardware-Accelerated Neuroevolution. *arXiv preprint arXiv:2202.05008* (2022).
- <span id="page-20-2"></span>[60] Tom White. 2016. Sampling generative networks. *arXiv preprint arXiv:1609.04468* (2016).
- <span id="page-20-0"></span>[61] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In *International Conference on Machine Learning*. PMLR, 23965–23998.
- <span id="page-20-4"></span>[62] Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Compeft: Compression for communicating parameter efficient updates via sparsification and quantization. *arXiv preprint arXiv:2311.13171* (2023).
- <span id="page-20-3"></span>[63] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. 2023. TIES-Merging: Resolving Interference When Merging Models. In *Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). [http://papers.nips.cc/paper\\_files/paper/2023/](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html) [hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html)
- <span id="page-20-10"></span>[64] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01. ai. *arXiv preprint arXiv:2403.04652* (2024).
- <span id="page-20-5"></span>[65] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. arXiv:2311.03099 [cs.CL]
- <span id="page-20-7"></span>[66] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. *arXiv preprint arXiv:1611.01578* (2016).

## SUPPLEMENTARY INFORMATION

## <span id="page-21-0"></span>A Evaluation Details

In Table [1,](#page-7-0) GPT-3.5 is gpt-3.5-turbo-0613, GPT-4 is gpt-4-0125-preview, Llama 2 70B is Llama-2-70b-chat, Japanese Stable LM 70B is japanese-stablelm-instruct-beta-70b and Swallow 70B is Swallow-70b-instruct-hf.

For the evaluation using the Japanese LM Evaluation Harness [\[6\]](#page-18-21), we utilized Stability AI Japan's fork of lm-eval-harness [\[19\]](#page-18-22) and configured it according to their convention. This configuration is widely used and compatible with the results on their report [\[5\]](#page-18-23) and Rinna leaderboards [\[49\]](#page-20-17), thus allowing direct comparison of scores with a large number of Japanese LLMs.

Table [6](#page-22-0) is the full version of Table [2,](#page-8-0) allowing for comparisons between our models and a wider range of models. Some of the numbers are from the Stability AI Japan report and the Rinna leaderboard. Despite having only 7B to 10B parameters, our models achieve higher scores than all existing <70B parameters models and exhibit scores on par with the SOTA 70B models.

## <span id="page-21-2"></span>B Evolving for License Specific Open-Source Models

In the main section, our EvoLLM-JP results were evolved using models found on HuggingFace. However, some of the models uses, in particular, WizardMath-7B-V1.1 [\[40\]](#page-19-15) has been released under a Non-Commercial, Research-only, Microsoft License, which is not *truly* open-source. Therefore, our release of EvoLLM-JP is also released under a Non-Commercial, Research-only License to be consistent with the WizardMath-7B-V1.1 model.

As researchers who benefited from the open-source community, we would like for models that we release to also be under an open-source license. In the spirit of open-source, and to showcase the applicability of our method to tackle even challenging issues like model licenses. We have ran a similar experiment where we incorporated *only* models that have been released under a true opensource license, such as MIT or Apache 2.0, and have produced a similar performing model called EvoLLM-JP-A, which we will release under Apache 2.0. This model will also be released on our GitHub: <https://github.com/SakanaAI/>.

Specifically, our EvoLLM-JP-A is a merge of shisa-gamma-7b-v1, Arithmo2-Mistral-7B, and Abel-7B-002, all of which are under MIT or Apache 2.0 License. The MGSM-JA score measured using the protocol described in Section [3.1](#page-6-0) is 52.4, and the Japanese Language Model Evaluation Harness score is 69.0. We have included results of this Apache 2.0-licensed model for comparison in Table [6,](#page-22-0) which provides a more comprehensive comparison than Table [2](#page-8-0) in the main text.

## <span id="page-21-1"></span>C Case Study

Table [7](#page-24-0) provides an example of responses to a mathematical question by existing models and our model. By merging a Japanese language model, we not only improve our capability to understand and use Japanese in reading and writing but also expand our knowledge about Japan. This example requires both mathematical reasoning and Japanese-specific knowledge, specifically that Setsubun is the day before the beginning of spring (Risshun). Notably, only our merged model provides the correct answer. Even when the question is translated into English, the English math model WizardMath-7B-V1.1 fails to answer correctly, demonstrating that our merged model is superior to a combination of machine translation and an English math model.

It is worth noting that in Table [7,](#page-24-0) we also provide the answer from the general-purpose math model WizardMath-7B-V1.1 with the question translated into English. However, even with the translated question, WizardMath-7B-V1.1 fails to provide the correct answer. This highlights the fact that simply translating the question is not sufficient to solve the problem, as it also requires an understanding of the Japanese cultural context. Even if one were to consider a combination of machine translation and an English math model as an alternative approach, it would still fall short in capturing the nuances and context-specific knowledge required to answer the question correctly. This demonstrates the unique value of our merged model, which effectively combines mathematical reasoning capabilities with Japanese language understanding and cultural knowledge.

Table 6: Breakdown of JP-LMEH Scores for Japanese Language Proficiency (Full Version of Table [2\)](#page-8-0). JP-LMEH (Japanese Language Model Evaluation Harness) is a benchmark suite consisting of 9 tasks, and the average score (Avg column) is used as an indicator of overall Japanese language proficiency.

<span id="page-22-0"></span>

| Model                                                                          |            | Size JComQA JNLI MARC JSQuAD JAQKET XLSum XWino MGSM JCoLA Avg |              |              |              | JP Language Model Evaluation Harness |              |              |              |                        |
|--------------------------------------------------------------------------------|------------|----------------------------------------------------------------|--------------|--------------|--------------|--------------------------------------|--------------|--------------|--------------|------------------------|
| 7B source models:                                                              |            |                                                                |              |              |              |                                      |              |              |              |                        |
| Mistral-7B-v0.1                                                                | 7B         | 74.0                                                           | 44.2         | 63.3         | 85.7         | 69.8                                 | 22.8         | 69.9         | 20.4         | 51.1 55.7              |
| shisa-gamma-7b-v1                                                              | 7B         | 91.2                                                           | 72.1         | 94.6         | 73.9         | 68.0                                 | 25.9         | 80.5         | 29.6         | 58.7 66.1              |
| WizardMath-7B-V1.1                                                             | 7B         | 74.7                                                           | 42.7         | 90.4         | 84.6         | 68.5                                 | 22.3         | 69.8         | 38.8         | 48.9 60.1              |
| Abel-7B-002                                                                    | 7B         | 70.3                                                           | 51.8         | 62.3         | 83.8         | 69.0                                 | 22.5         | 68.2         | 28.0         | 52.7 56.5              |
| 7B merged models:                                                              |            |                                                                |              |              |              |                                      |              |              |              |                        |
| Ours (PS)<br>Ours (DFS)                                                        | 7B<br>10B  | 89.1<br>67.7                                                   | 65.7<br>58.2 | 95.4<br>53.5 | 89.5<br>66.8 | 77.7<br>54.3                         | 25.5<br>17.3 | 81.2<br>65.6 | 50.0<br>30.0 | 60.5 70.5<br>65.6 53.2 |
| Ours (PS+DFS)                                                                  | 10B        | 88.2                                                           | 50.3         | 91.5         | 78.6         | 77.8                                 | 23.2         | 73.0         | 40.0         | 73.0 66.2              |
| Ours (PS-A; see Appendix B)                                                    | 7B         | 87.0                                                           | 62.3         | 91.9         | 90.4         | 77.4                                 | 23.0         | 78.9         | 46.4         | 63.6 69.0              |
| 13B source models:                                                             |            |                                                                |              |              |              |                                      |              |              |              |                        |
| Llama-2-13b-hf                                                                 | 13B        | 78.3                                                           | 43.2         | 50.3         | 89.0         | 75.8                                 | 25.7         | 63.4         | 10.4         | 50.6 54.1              |
| MetaMath-13B-V1.0<br>ELYZA-japanese-Llama-2-13b-instruct                       | 13B<br>13B | 65.6<br>84.6                                                   | 35.5<br>58.3 | 51.3<br>83.5 | 78.5<br>83.8 | 56.7<br>63.3                         | 22.4<br>25.0 | 61.8<br>74.3 | 17.2<br>18.0 | 49.3 48.7<br>50.5 60.2 |
|                                                                                |            |                                                                |              |              |              |                                      |              |              |              |                        |
| 13B merged models:<br>Ours (PS)                                                | 13B        | 82.7                                                           | 49.8         | 83.1         | 88.3         | 68.0                                 | 24.4         | 62.3         | 24.8         | 54.3 59.7              |
| Ours (DFS)                                                                     | 21B        | 62.1                                                           | 36.3         | 55.6         | 76.4         | 56.7                                 | 15.9         | 60.8         | 6.4          | 49.6 46.6              |
| Ours (PS+DFS)                                                                  | 21B        | 82.2                                                           | 48.9         | 88.7         | 86.7         | 68.9                                 | 18.1         | 62.0         | 30.8         | 57.1 60.4              |
| Other models:                                                                  |            |                                                                |              |              |              |                                      |              |              |              |                        |
| Swallow-70b-instruct-hf                                                        | 70B        | 95.3                                                           | 57.2         | 91.7         | 94.1         | 93.9                                 | 23.1         | 83.3         | 45.2         | 59.5 71.5              |
| Swallow-70b-hf                                                                 | 70B        | 94.8                                                           | 55.8         | 87.8         | 93.5         | 93.8                                 | 23.2         | 83.7         | 47.2         | 60.7 71.2              |
| japanese-stablelm-base-beta-70b<br>nekomata-14b-instruction                    | 70B<br>14B | 93.7<br>93.7                                                   | 45.2<br>57.5 | 91.0<br>90.0 | 94.2<br>93.6 | 92.8<br>89.0                         | 25.4<br>25.2 | 83.6<br>77.4 | 41.2<br>35.2 | 63.8 70.1<br>67.3 69.9 |
| japanese-stablelm-instruct-beta-70b                                            | 70B        | 91.2                                                           | 50.4         | 92.9         | 87.1         | 88.4                                 | 24.3         | 82.0         | 37.2         | 61.7 68.3              |
| nekomata-14b                                                                   | 14B        | 93.1                                                           | 41.7         | 84.1         | 94.2         | 90.8                                 | 21.3         | 78.6         | 37.6         | 65.0 67.4              |
| youri-7b-chat                                                                  | 7B         | 91.8                                                           | 70.3         | 96.7         | 79.6         | 83.7                                 | 24.2         | 80.9         | 25.2         | 53.8 67.4              |
| Llama-2-70b-hf<br>nekomata-7b-instruction                                      | 70B<br>7B  | 89.3<br>88.0                                                   | 53.0<br>74.7 | 70.4<br>92.3 | 93.2<br>91.3 | 89.5<br>82.5                         | 23.8<br>21.1 | 79.8<br>76.8 | 40.0<br>19.2 | 65.4 67.2<br>53.8 66.6 |
| youri-7b-instruction                                                           | 7B         | 88.8                                                           | 63.6         | 93.8         | 92.2         | 83.9                                 | 24.7         | 78.9         | 17.2         | 54.0 66.3              |
| Qwen-14B                                                                       | 14B        | 89.5                                                           | 65.2         | 76.7         | 93.0         | 79.5                                 | 18.4         | 71.6         | 37.2         | 61.0 65.8              |
| Swallow-MX-8x7b-NVE-v0.1                                                       | 47B        | 92.0                                                           | 52.4         | 59.8         | 93.1         | 91.9                                 | 26.1         | 72.9         | 44.8         | 59.2 65.8              |
| youri-7b-chat-gptq<br>nekomata-7b-instruction-gguf                             | 7B<br>7B   | 89.5<br>87.9                                                   | 68.0<br>68.4 | 96.0<br>89.6 | 88.6<br>89.9 | 79.7<br>79.8                         | 15.6<br>20.8 | 79.1<br>77.3 | 21.6<br>19.2 | 53.8 65.8<br>55.3 65.3 |
| nekomata-14b-instruction-gguf                                                  | 14B        | 93.3                                                           | 57.8         | 51.7         | 91.3         | 86.2                                 | 23.6         | 77.4         | 35.2         | 68.7 65.0              |
| Llama-2-70b-chat-hf                                                            | 70B        | 80.2                                                           | 53.4         | 94.4         | 91.6         | 80.1                                 | 21.8         | 73.6         | 30.4         | 54.6 64.5              |
| youri-7b-instruction-gptq                                                      | 7B         | 88.1                                                           | 62.0         | 93.5         | 90.6         | 74.4                                 | 15.3         | 78.2         | 15.2         | 53.3 63.4              |
| japanese-stablelm-base-gamma-7b                                                | 7B         | 89.5                                                           | 37.6         | 79.2         | 90.6         | 89.5                                 | 22.9         | 82.9         | 20.8         | 55.6 63.2              |
| Swallow-13b-instruct-hf<br>japanese-stablelm-instruct-gamma-7b                 | 13B<br>7B  | 89.0<br>91.4                                                   | 43.1<br>40.7 | 74.1<br>72.9 | 92.8<br>89.9 | 92.5<br>88.2                         | 20.6<br>22.8 | 70.4<br>82.2 | 24.0<br>17.2 | 61.9 63.2<br>56.4 62.4 |
| Swallow-13b-hf                                                                 | 13B        | 86.0                                                           | 44.8         | 74.5         | 92.2         | 92.2                                 | 21.6         | 71.4         | 19.2         | 59.5 62.4              |
| nekomata-14b-gguf                                                              | 14B        | 93.0                                                           | 41.1         | 50.0         | 93.0         | 88.3                                 | 22.2         | 80.5         | 32.4         | 60.2 62.3              |
| Swallow-MS-7b-v0.1                                                             | 7B         | 88.3                                                           | 35.8         | 83.1         | 90.3         | 89.4                                 | 20.7         | 74.6         | 22.4         | 55.5 62.2              |
| Swallow-7b-instruct-hf<br>llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 13B | 7B         | 86.8<br>83.2                                                   | 36.3<br>74.5 | 94.2<br>74.9 | 90.0<br>94.0 | 89.4<br>85.1                         | 17.4<br>9.6  | 76.8<br>71.3 | 10.4<br>3.2  | 50.0 61.2<br>50.4 60.7 |
| Swallow-7b-hf                                                                  | 7B         | 80.2                                                           | 37.9         | 93.1         | 89.2         | 90.1                                 | 18.3         | 76.6         | 10.4         | 50.0 60.7              |
| nekomata-7b                                                                    | 7B         | 82.9                                                           | 38.6         | 67.0         | 89.8         | 82.5                                 | 16.9         | 74.5         | 16.0         | 60.0 58.7              |
| japanese-stablelm-instruct-beta-7b<br>japanese-stablelm-base-beta-7b           | 7B<br>7B   | 81.9<br>76.8                                                   | 39.9<br>38.0 | 88.3<br>86.0 | 84.1<br>87.1 | 78.2<br>79.5                         | 20.8<br>21.9 | 75.7<br>76.6 | 4.8<br>7.6   | 51.7 58.4<br>49.2 58.1 |
| Qwen-7B                                                                        | 7B         | 76.0                                                           | 46.9         | 83.3         | 89.7         | 69.2                                 | 13.8         | 60.6         | 22.0         | 57.3 57.6              |
| youri-7b-gptq                                                                  | 7B         | 78.9                                                           | 40.5         | 78.2         | 85.7         | 78.1                                 | 16.3         | 78.3         | 4.8          | 54.0 57.2              |
| youri-7b                                                                       | 7B         | 82.8                                                           | 35.9         | 90.1         | 57.2         | 82.5                                 | 20.5         | 78.0         | 6.8          | 58.5 56.9              |
| ELYZA-japanese-Llama-2-7b-instruct<br>weblab-10b-instruction-sft               | 7B<br>10B  | 79.2<br>63.6                                                   | 38.8<br>42.7 | 89.9<br>86.4 | 83.2<br>87.8 | 68.7<br>73.9                         | 16.8<br>23.1 | 71.1<br>72.4 | 11.2<br>2.8  | 50.5 56.6<br>50.5 55.9 |
| ELYZA-japanese-Llama-2-7b                                                      | 7B         | 75.2                                                           | 36.1         | 87.3         | 86.4         | 68.0                                 | 16.3         | 72.0         | 5.2          | 53.1 55.5              |
| nekomata-7b-gguf                                                               | 7B         | 82.3                                                           | 33.7         | 50.0         | 86.8         | 74.9                                 | 19.9         | 78.3         | 13.6         | 55.8 55.0              |
| japanese-stablelm-instruct-ja_vocab-beta-7b                                    | 7B         | 79.9                                                           | 39.2         | 79.5         | 82.0         | 78.9                                 | 9.9          | 68.9         | 6.0          | 49.2 54.8              |
| japanese-stablelm-base-ja_vocab-beta-7b<br>calm2-7b-chat                       | 7B<br>7B   | 69.9<br>70.3                                                   | 38.8<br>33.7 | 80.0<br>87.7 | 83.8<br>82.6 | 80.1<br>77.8                         | 7.5<br>5.4   | 72.5<br>70.9 | 3.6<br>5.6   | 50.0 54.0<br>51.9 54.0 |
| stockmark-13b                                                                  | 13B        | 70.7                                                           | 36.2         | 56.5         | 87.2         | 87.3                                 | 7.7          | 73.4         | 9.6          | 50.0 53.2              |
| Llama-2-7b-hf                                                                  | 7B         | 65.9                                                           | 35.1         | 83.2         | 82.7         | 70.6                                 | 19.0         | 65.2         | 6.8          | 47.1 52.9              |
| calm2-7b                                                                       | 7B         | 58.2                                                           | 33.3         | 81.2         | 81.8         | 81.9                                 | 2.4          | 73.0         | 5.6          | 53.3 52.3              |
| plamo-13b<br>weblab-10b                                                        | 13B<br>10B | 54.5<br>44.0                                                   | 33.8<br>30.7 | 85.0<br>64.0 | 78.6<br>83.3 | 74.5<br>74.6                         | 12.9<br>20.3 | 70.5<br>71.4 | 2.4<br>2.0   | 54.5 51.9<br>50.8 49.0 |
| llm-jp-13b-v1.0                                                                | 13B        | 39.6                                                           | 35.0         | 67.4         | 79.3         | 78.0                                 | 10.9         | 69.5         | 2.0          | 54.4 48.5              |
| plamo-13b-instruct-nc                                                          | 13B        | 45.8                                                           | 35.3         | 91.6         | 75.2         | 63.5                                 | 5.8          | 65.2         | 1.2          | 50.6 48.3              |
| plamo-13b-instruct                                                             | 13B        | 42.5                                                           | 34.9         | 90.0         | 77.3         | 67.0                                 | 5.4          | 62.9         | 2.0          | 50.0 48.0              |

On the other hand, Table [8](#page-25-0) highlights an instance where the LLMs failed to solve a problem correctly, using a question from the MGSM test set. The problem involved calculating the internal volume of boxes. When presented in Japanese, all models encountered difficulties in correctly accounting for the wall thickness. EvoLLM-JP-v1-7B attempted to adjust for it but made an arithmetic error, leading to an incorrect volume of 351 cubic inches. shisa-gamma-7b-v1 ignored the wall thickness entirely, calculating the external dimensions only, and thus arrived at 360 cubic inches. WizardMath-7B-V1.1 recognized the need for an adjustment but applied it incorrectly, resulting in 288 cubic inches. Notably, when the original English problem was presented to WizardMath-7B-V1.1, it produced the correct answer of 72 cubic inches. This suggests that there may still be some loss in transferring the model's high mathematical reasoning abilities from English to Japanese.

Additionally, Table [9](#page-26-0) provides the case studies for our VLM. We observe that our VLM successfully obtained knowledge, cultural understanding, and fluency in Japanese expression of the Japanese LLM.

The first example features a photo of Koi-nobori, a cultural tradition specific to Japan. LLaVA-1.6-Mistral-7B, lacking sufficient knowledge of Japanese culture, failed to answer Koinobori correctly. In contrast, both Japanese-Stable-VLM and EvoVLM-JP, possessing knowledge of Japanese culture, successfully identified the Koi-nobori. Notably, EvoVLM-JP's response demonstrates greater fluency in its explanation.

The third example involves a question about the color of traffic lights. In Japan, the green traffic light is called "blue" because historically, the word "blue" referred to a range of colors, including blue and green, and also because the initial traffic lights used in Japan had a bluish-green hue, leading to the continuation of this terminology today. EvoVLM-JP is the only model that correctly identifies the traffic light color as "blue" in Japanese, demonstrating its accurate understanding of this cultural and linguistic nuance. LLaVA-1.6-Mistral-7B consistently but incorrectly labels the light as "green" in Japanese, revealing its lack of specific cultural knowledge. Japanese-Stable-VLM shows inconsistent responses, alternating between "green," "green and white," and "white," indicating confusion. These results highlight EvoVLM-JP's superior grasp of Japanese language conventions and cultural context in this specific scenario.

Table 7: Case Study of EvoLLM-JP-v1-7B. This study presents the responses of the LLMs to a problem that demands both Japanese-specific knowledge and mathematical reasoning.

<span id="page-24-0"></span>

|                                      | Problem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |
|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|
| JA Instruction                       | 去年の立春は2月3日でした。今年の立春は2月4日です。去年の節分の月、去年の節分の<br>日、今年の節分の月、今年の節分の日を足し合わせると、いくつになりますか?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |
| EN Instruction                       | Last year, Risshun was on February 3rd. This year, it is on February 4th. What is the sum of the month<br>and day of last year's Setsubun, and this year's Setsubun?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |  |
| Ground Truth Answer                  | 9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |
| Note                                 | Setsubun, the eve of the first day of spring, always falls on the day before Risshun, which marks the<br>start of spring according to the lunar calendar. The date of Risshun can vary annually.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |  |
|                                      | Answers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |
| EvoLLM-JP-v1-7B<br>JA Instruction    | 去年の立春は2月3日で、今年の立春は2月4日です。 去年の節分は立春の前日、つま<br>り2月2日でした。 今年の節分は立春の前日、つまり2月3日です。 去年の節分の月は2月、<br>去年の節分の日は2日で、合計は2 + 2 = 4です。 今年の節分の月は2月、今年の節分の日<br>は3日で、合計は2 + 3 = 5です。 足し合わせると、4 + 5 = 9になります。<br>#### 9<br>The answer is: 9.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |
| shisa-gamma-7b<br>JA Instruction     | まず、去年の立春の日付を計算してみましょう。<br>去年の立春の日付 = 2021年の2月3日<br>次に、去年の節分の日付を計算してみましょう。<br>去年の節分の日付 = 2021年の2月2日<br>次に、今年の立春の日付を計算してみましょう。<br>今年の立春の日付 = 2022年の2月4日<br>次に、今年の節分の日付を計算してみましょう。<br>今年の節分の日付 = 2022年の2月1日<br>最後に、去年の節分の月と日、今年の節分の月と日を足し合わせてみましょう。<br>去年の節分の月と日 = 2021年の2月2日 今年の節分の月と日 = 2022年の2月1日<br>これで、足し合わせることができます。<br>足し合わせた月と日 = 2021年の2月2日 + 2022年の2月1日 = 2023年の2月3日<br>つまり、去年の節分の月と日、今年の節分の月と日を足し合わせると、2023年の2月3日に<br>なります。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |
| WizardMath-7B-V1.1<br>JA Instruction | Step 1: Identify the dates for the given events.<br>- Last year's Setsubun (February 3)<br>- This year's Setsubun (February 4)<br>Step 2: Determine the dates for the corresponding events in the previous and next years.<br>- Last year's Setsubun (February 3) falls on a Monday.<br>- This year's Setsubun (February 4) falls on a Tuesday.<br>Step 3: Calculate the number of days between the corresponding events.<br>- The number of days between last year's and this year's Setsubun is 1 day (Tuesday - Monday = 1 day).<br>Step 4: Determine the dates for the corresponding events in the lunar calendar.<br>- Last year's Setsubun was on the 1st day of the 2nd month.<br>- This year's Setsubun will be on the 2nd day of the 2nd month.<br>Step 5: Calculate the number of days between the corresponding events in the lunar calendar.<br>- The number of days between last year's and this year's Setsubun in the lunar calendar is 1 day (2nd<br>day - 1st day = 1 day).<br>Step 6: Add the number of days between the corresponding events in both calendars.<br>- The total number of days between last year's and this year's Setsubun is 1 (from the solar calendar) + 1<br>(from the lunar calendar) = 2 days.<br>Step 7: Write the final answer.<br>- The sum of the dates for the corresponding events is 2.<br>The answer is: 2. |  |
| WizardMath-7B-V1.1<br>EN Instruction | We are given that last year, Risshun (the beginning of spring according to the lunisolar Japanese<br>calendar) was on February 3rd, and this year it is on February 4th. We need to find the sum of the month<br>and day of last year's Setsubun, and this year's Setsubun.<br>Step 2: Identify the month and day of last year's Setsubun. Last year, Risshun was on February 3rd. So,<br>the month is February (2) and the day is 3.<br>Step 3: Identify the month and day of this year's Setsubun. This year, Risshun is on February 4th. So,<br>the month is February (2) and the day is 4.<br>Step 4: Calculate the sum of the month and day of last year's and this year's Setsubun. The sum of<br>the month and day of last year's Setsubun is 2 + 3 = 5. The sum of the month and day of this year's<br>Setsubun is 2 + 4 = 6.<br>Step 5: Add the two sums together. The total sum of the month and day of last year's and this year's<br>Setsubun is 5 + 6 = 11.<br>So, the sum of the month and day of last year's Setsubun, and this year's Setsubun is 11.<br>The answer is: 11.                                                                                                                                                                                                                                                                  |  |
| Problem                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |
| JA Instruction                       | ジョンは3箱持っています。それぞれの箱は5インチ×6インチ×4インチです。壁は1インチ<br>の暑さです。3箱全部の内容量はどれくらいですか?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |
| EN Instruction                       | John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. The walls are 1 inch thick. What is the<br>total inner volume of all 3 boxes?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |  |
| Ground Truth Answer                  | 72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |  |
| Answers                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |
| EvoLLM-JP-v1-7B<br>JA Instruction    | 1箱の内容量は、長さ×幅×高さ = 5インチ×6インチ×4インチ = 120立方インチです。 ジョ<br>ンは3箱持っているので、3箱の内容量は、3×120立方インチ = 360立方インチです。 しか<br>し、壁の厚みは1インチなので、実際の内容量は、360立方インチ - 3箱×1インチ×3面 =<br>360立方インチ - 9立方インチ = 351立方インチです。<br>#### 351<br>指示: 351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |  |
| shisa-gamma-7b<br>JA Instruction     | ジョンは3箱持っています。それぞれの箱は5インチ×6インチ×4インチです。壁は1インチ<br>の暑さです。3箱全部の内容量はどれくらいですか? [/INST] 3つの箱の体積の合計を求める<br>には、それぞれの箱の体積を計算してからそれらを合計する必要があります。<br>箱1の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>箱2の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>箱3の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>これで、3つの箱の体積を合計することができます。<br>合計体積 = 箱1の体積 + 箱2の体積 + 箱3の体積 = 120立方インチ + 120立方インチ + 120立方<br>インチ = 360立方インチ<br>つまり、3つの箱の合計体積は360立方インチです。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |
| WizardMath-7B-V1.1<br>JA Instruction | Let's think step by step.<br>Step 1: Calculate the volume of one box. The volume of a box is calculated by multiplying its length,<br>width, and height. In this case, the dimensions of the box are 5 inches by 6 inches by 4 inches. So, the<br>volume of one box is 5 inches * 6 inches * 4 inches = 120 cubic inches.<br>Step 2: Calculate the total volume of all boxes. John has 3 boxes, so the total volume of all boxes is 3<br>boxes * 120 cubic inches/box = 360 cubic inches.<br>Step 3: Adjust for the thickness of the walls. The walls of the boxes add an extra inch to each dimension,<br>so we need to subtract the volume of the walls from the total volume of the boxes. The volume of the<br>walls of one box is 1 inch * 6 inches * 4 inches = 24 cubic inches. Since there are 3 boxes, the total<br>volume of the walls is 3 boxes * 24 cubic inches/box = 72 cubic inches.<br>Step 4: Subtract the volume of the walls from the total volume of the boxes. The adjusted total volume<br>of the boxes is 360 cubic inches - 72 cubic inches = 288 cubic inches.<br>So, the total volume of the contents of the 3 boxes is 288 cubic inches.<br>The answer is: 288 cubic inches.                                                                                                                                                     |  |
| WizardMath-7B-V1.1<br>EN Instruction | John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. The walls are 1 inch thick. What is the<br>total inner volume of all 3 boxes?<br>### Response: Let's think step by step.First, we need to find the inner dimensions of one box. Since the<br>walls are 1 inch thick, we need to subtract 2 inches from each dimension (1 inch from each side of the<br>wall).<br>The original dimensions of the box are 5 inches by 6 inches by 4 inches. So, the inner dimensions will<br>be:<br>- Length: 5 inches - 2 inches = 3 inches - Width: 6 inches - 2 inches = 4 inches - Height: 4 inches - 2<br>inches = 2 inches<br>Now, we can calculate the inner volume of one box by multiplying the inner dimensions:<br>Inner Volume = Length × Width × Height Inner Volume = 3 inches × 4 inches × 2 inches Inner<br>Volume = 24 cubic inches<br>Since John has 3 boxes, we need to find the total inner volume of all 3 boxes:<br>Total Inner Volume = Inner Volume of one box × Number of boxes Total Inner Volume = 24 cubic<br>inches × 3 Total Inner Volume = 72 cubic inches<br>So, the total inner volume of all 3 boxes is 72 cubic inches.<br>The answer is: 72.                                                                                                                                                                               |  |

Table 8: Case Study of EvoLLM-JP-v1-7B. This study presents a case where LLMs were unable to solve a Japanese math problem.

<span id="page-25-0"></span>

#### Table 9: Case Study of EvoVLM-JP. This study presents the responses of the VLMs to questions that demand both Japanese-specific knowledge and VQA abilities.

<span id="page-26-0"></span>

|                      | Example 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| User                 | この画像に映っている魚の形をした飾りは、日本では何と呼ばれていますか?<br>(What is the fish-shaped decoration in this image called in Japan?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| LLaVA-1.6-Mistral-7B | この画像に映っている魚の形をした飾りは、日本では「花魚」と呼ばれています。<br>(The fish-shaped decoration in this image is called "hana-gyo" in Japan.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Japanese Stable VLM  | こいのぼり (Koi-nobori)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| EvoVLM-JP            | この画像に映っている魚の形をした飾りは、日本では「鯉のぼり」と呼ばれています。<br>(The fish-shaped decoration in this image is called "Koi-nobori" in Japan.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                      | Example 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| User                 | 写真を説明してください。<br>(Please explain this photo.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| LLaVA-1.6-Mistral-7B | この写真では、自然の中で小さな動物が歩いています。その動物は、犬のような外貌を持っていますが、実<br>際には犬ではありません。この動物は、野生動物として知られていますが、具体的な種類はわかりません。<br>背景には、秋の季節になっていることがわかります。多くの樹木には、黄色い葉が落ちています。このよう<br>な環境で、動物が歩いていることを見ることができます。 この写真では、動物が自然の中で活動しているこ<br>とを表現しています。同時に、秋の季節になっていることを伝えることもできます。<br>(In this photo, a small animal is walking in nature. The animal has a canine-like appearance but is not actually a dog. This<br>creature is known as a wild animal, but the specific species is unknown. In the background, it is evident that it is autumn.<br>Many trees have yellow leaves falling. In such an environment, the animal can be seen walking. The photo depicts the<br>animal being active in nature while also conveying that it is autumn.) |
| Japanese Stable VLM  | 紅葉と鹿の写真です。<br>(It's a photo of autumn leaves and deer.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| EvoVLM-JP            | この写真は、一匹の小さな鹿が落ち葉の上に立っている様子を捉えています。鹿は警戒しているようで、周<br>囲を注意深く見回しています。背景には紅葉した木々が見え、秋の雰囲気が感じられます。<br>(This photo captures a small deer standing on fallen leaves. The deer appears to be alert, carefully scanning its surroundings.<br>In the background, you can see trees with autumn foliage, creating an atmosphere of autumn.)                                                                                                                                                                                                                                                                                                                                                                                          |

|                      | Example 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|
| User                 | この信号機の色は何色ですか?<br>(What color is this traffic light?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |  |  |  |
| LLaVA-1.6-Mistral-7B | (A) この信号機の色は緑色です。 (The color of this traffic light is green.)<br>(B) この信号機の色は緑色です。 (The color of this traffic light is green.)<br>(C) この信号機の色は緑色です。 (The color of this traffic light is green.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |  |  |
| Japanese Stable VLM  | (A) 緑色です。 (It is green.)<br>(B) 緑と白です。 (It is green and white.)<br>(C) 白色 (White)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |  |  |
| EvoVLM-JP            | (A) この信号機の色は青です。 (The color of this traffic light is blue.)<br>(B) この信号機の色は青です。 (The color of this traffic light is blue.)<br>(C) この信号機の色は青です。 (The color of this traffic light is blue.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |
|                      | Example 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |
| User                 | この建物について教えてください。<br>(Please explain this building.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |  |  |
| LLaVA-1.6-Mistral-7B | この建物は、日本で位置しています。具体的には、東京都渋谷区にあります。この建物は、1945年8月15日<br>に、第二次世界大戦中に日本軍によって攻撃され、完全に破壊されました。その後、1964年に再構築されま<br>した。 この建物は、「平和の塔」(Peace Tower)と呼ばれています。その名前は、この建物が再構築される<br>際に決められた名前です。この建物は、平和を祈るために作られたものであり、国内外から多くの人々が訪<br>れる場所です。<br>(This building is located in Japan, specifically in Shibuya Ward, Tokyo. It was completely destroyed by the Japanese military<br>during World War II on August 15, 1945. Subsequently, it was reconstructed in 1964. This building is called the "Peace<br>Tower," a name decided upon during its reconstruction. It was created as a symbol of peace, and it is a place visited by many<br>people from both within and outside the country to pray for peace.)                       |  |  |  |
| Japanese Stable VLM  | 原爆ドーム<br>(Atomic Bomb Dome)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |  |  |  |
| EvoVLM-JP            | この建物は、日本の広島市にある「原爆ドーム」です。 1945年8月6日、広島市に原子爆弾が投下されまし<br>た。 この原子爆弾は、広島市の中心部にある「広島県産業奨励館」に直撃しました。 この奨励館は、現在は<br>「原爆ドーム」と呼ばれています。 この原爆ドームは、原子爆弾の被害を受けた建物の中で、唯一残ってい<br>る建物です。 この原爆ドームは、1996年に世界遺産に登録されました。<br>(This building is the "Atomic Bomb Dome" located in Hiroshima City, Japan. On August 6, 1945, an atomic bomb was<br>dropped on Hiroshima City. The bomb directly hit the "Hiroshima Prefectural Industrial Promotion Hall," located in the<br>central part of Hiroshima City. This promotion hall is now known as the "Atomic Bomb Dome." It is the only building that<br>remains standing among those damaged by the atomic bomb. The Atomic Bomb Dome was registered as a UNESCO World<br>Heritage Site in 1996.) |  |  |  |



